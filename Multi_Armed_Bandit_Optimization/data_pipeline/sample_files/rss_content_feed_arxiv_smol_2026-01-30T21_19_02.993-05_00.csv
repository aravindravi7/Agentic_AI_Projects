source,title,url,published_at,summary,raw
Arxiv,LVLMs and Humans Ground Differently in Referential Communication,https://arxiv.org/abs/2601.19792,2026-01-30T05:00:00.000Z,"arXiv:2601.19792v2 Announce Type: replace-cross 
Abstract: For generative AI agents to partner effectively with human users, the ability to accurately predict human intent is critical. But this ability to collaborate remains limited by a critical deficit: an inability to model common ground. Here, we present a referential communication experiment with a factorial design involving director-matcher pairs (human-human, human-AI, AI-human, and AI-AI) that interact with multiple turns in repeated rounds to match pictures of objects not associated with any obvious lexicalized labels. We release the online pipeline for data collection, the tools and analyses for accuracy, efficiency, and lexical overlap, and a corpus of 356 dialogues (89 pairs over 4 rounds each) that unmasks LVLMs' limitations in interactively resolving referring expressions, a crucial skill that underlies human language use.","{""creator"":""Peter Zeng, Weiling Li, Amie Paige, Zhengxiang Wang, Panagiotis Kaliosis, Dimitris Samaras, Gregory Zelinsky, Susan Brennan, Owen Rambow"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""LVLMs and Humans Ground Differently in Referential Communication"",""link"":""https://arxiv.org/abs/2601.19792"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Peter Zeng, Weiling Li, Amie Paige, Zhengxiang Wang, Panagiotis Kaliosis, Dimitris Samaras, Gregory Zelinsky, Susan Brennan, Owen Rambow"",""content"":""arXiv:2601.19792v2 Announce Type: replace-cross \nAbstract: For generative AI agents to partner effectively with human users, the ability to accurately predict human intent is critical. But this ability to collaborate remains limited by a critical deficit: an inability to model common ground. Here, we present a referential communication experiment with a factorial design involving director-matcher pairs (human-human, human-AI, AI-human, and AI-AI) that interact with multiple turns in repeated rounds to match pictures of objects not associated with any obvious lexicalized labels. We release the online pipeline for data collection, the tools and analyses for accuracy, efficiency, and lexical overlap, and a corpus of 356 dialogues (89 pairs over 4 rounds each) that unmasks LVLMs' limitations in interactively resolving referring expressions, a crucial skill that underlies human language use."",""contentSnippet"":""arXiv:2601.19792v2 Announce Type: replace-cross \nAbstract: For generative AI agents to partner effectively with human users, the ability to accurately predict human intent is critical. But this ability to collaborate remains limited by a critical deficit: an inability to model common ground. Here, we present a referential communication experiment with a factorial design involving director-matcher pairs (human-human, human-AI, AI-human, and AI-AI) that interact with multiple turns in repeated rounds to match pictures of objects not associated with any obvious lexicalized labels. We release the online pipeline for data collection, the tools and analyses for accuracy, efficiency, and lexical overlap, and a corpus of 356 dialogues (89 pairs over 4 rounds each) that unmasks LVLMs' limitations in interactively resolving referring expressions, a crucial skill that underlies human language use."",""guid"":""oai:arXiv.org:2601.19792v2"",""categories"":[""cs.CL"",""cs.AI"",""cs.HC""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,ProToken: Token-Level Attribution for Federated Large Language Models,https://arxiv.org/abs/2601.19672,2026-01-30T05:00:00.000Z,"arXiv:2601.19672v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) enables collaborative training of Large Language Models (LLMs) across distributed data sources while preserving privacy. However, when federated LLMs are deployed in critical applications, it remains unclear which client(s) contributed to specific generated responses, hindering debugging, malicious client identification, fair reward allocation, and trust verification. We present ProToken, a novel Provenance methodology for Token-level attribution in federated LLMs that addresses client attribution during autoregressive text generation while maintaining FL privacy constraints. ProToken leverages two key insights to enable provenance at each token: (1) transformer architectures concentrate task-specific signals in later blocks, enabling strategic layer selection for computational tractability, and (2) gradient-based relevance weighting filters out irrelevant neural activations, focusing attribution on neurons that directly influence token generation. We evaluate ProToken across 16 configurations spanning four LLM architectures (Gemma, Llama, Qwen, SmolLM) and four domains (medical, financial, mathematical, coding). ProToken achieves 98% average attribution accuracy in correctly localizing responsible client(s), and maintains high accuracy when the number of clients are scaled, validating its practical viability for real-world deployment settings.","{""creator"":""Waris Gill, Ahmad Humayun, Ali Anwar, Muhammad Ali Gulzar"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""ProToken: Token-Level Attribution for Federated Large Language Models"",""link"":""https://arxiv.org/abs/2601.19672"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Waris Gill, Ahmad Humayun, Ali Anwar, Muhammad Ali Gulzar"",""content"":""arXiv:2601.19672v2 Announce Type: replace-cross \nAbstract: Federated Learning (FL) enables collaborative training of Large Language Models (LLMs) across distributed data sources while preserving privacy. However, when federated LLMs are deployed in critical applications, it remains unclear which client(s) contributed to specific generated responses, hindering debugging, malicious client identification, fair reward allocation, and trust verification. We present ProToken, a novel Provenance methodology for Token-level attribution in federated LLMs that addresses client attribution during autoregressive text generation while maintaining FL privacy constraints. ProToken leverages two key insights to enable provenance at each token: (1) transformer architectures concentrate task-specific signals in later blocks, enabling strategic layer selection for computational tractability, and (2) gradient-based relevance weighting filters out irrelevant neural activations, focusing attribution on neurons that directly influence token generation. We evaluate ProToken across 16 configurations spanning four LLM architectures (Gemma, Llama, Qwen, SmolLM) and four domains (medical, financial, mathematical, coding). ProToken achieves 98% average attribution accuracy in correctly localizing responsible client(s), and maintains high accuracy when the number of clients are scaled, validating its practical viability for real-world deployment settings."",""contentSnippet"":""arXiv:2601.19672v2 Announce Type: replace-cross \nAbstract: Federated Learning (FL) enables collaborative training of Large Language Models (LLMs) across distributed data sources while preserving privacy. However, when federated LLMs are deployed in critical applications, it remains unclear which client(s) contributed to specific generated responses, hindering debugging, malicious client identification, fair reward allocation, and trust verification. We present ProToken, a novel Provenance methodology for Token-level attribution in federated LLMs that addresses client attribution during autoregressive text generation while maintaining FL privacy constraints. ProToken leverages two key insights to enable provenance at each token: (1) transformer architectures concentrate task-specific signals in later blocks, enabling strategic layer selection for computational tractability, and (2) gradient-based relevance weighting filters out irrelevant neural activations, focusing attribution on neurons that directly influence token generation. We evaluate ProToken across 16 configurations spanning four LLM architectures (Gemma, Llama, Qwen, SmolLM) and four domains (medical, financial, mathematical, coding). ProToken achieves 98% average attribution accuracy in correctly localizing responsible client(s), and maintains high accuracy when the number of clients are scaled, validating its practical viability for real-world deployment settings."",""guid"":""oai:arXiv.org:2601.19672v2"",""categories"":[""cs.LG"",""cs.AI"",""cs.SE""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,"R^3: Replay, Reflection, and Ranking Rewards for LLM Reinforcement Learning",https://arxiv.org/abs/2601.19620,2026-01-30T05:00:00.000Z,"arXiv:2601.19620v2 Announce Type: replace-cross 
Abstract: Large reasoning models (LRMs) aim to solve diverse and complex problems through structured reasoning. Recent advances in group-based policy optimization methods have shown promise in enabling stable advantage estimation without reliance on process-level annotations. However, these methods rely on advantage gaps induced by high-quality samples within the same batch, which makes the training process fragile and inefficient when intra-group advantages collapse under challenging tasks. To address these problems, we propose a reinforcement learning mechanism named \emph{\textbf{R^3}} that along three directions: (1) a \emph{cross-context \underline{\textbf{R}}eplay} strategy that maintains the intra-group advantage by recalling valuable examples from historical trajectories of the same query, (2) an \emph{in-context self-\underline{\textbf{R}}eflection} mechanism enabling models to refine outputs by leveraging past failures, and (3) a \emph{structural entropy \underline{\textbf{R}}anking reward}, which assigns relative rewards to truncated or failed samples by ranking responses based on token-level entropy patterns, capturing both local exploration and global stability. We implement our method on Deepseek-R1-Distill-Qwen-1.5B and train it on the DeepscaleR-40k in the math domain. Experiments demonstrate our method achieves SoTA performance on several math benchmarks, representing significant improvements and fewer reasoning tokens over the base models. Code and model will be released.","{""creator"":""Zhizheng Jiang, Kang Zhao, Weikai Xu, Xinkui Lin, Wei Liu, Jian Luan, Shuo Shang, Peng Han"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""R^3: Replay, Reflection, and Ranking Rewards for LLM Reinforcement Learning"",""link"":""https://arxiv.org/abs/2601.19620"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Zhizheng Jiang, Kang Zhao, Weikai Xu, Xinkui Lin, Wei Liu, Jian Luan, Shuo Shang, Peng Han"",""content"":""arXiv:2601.19620v2 Announce Type: replace-cross \nAbstract: Large reasoning models (LRMs) aim to solve diverse and complex problems through structured reasoning. Recent advances in group-based policy optimization methods have shown promise in enabling stable advantage estimation without reliance on process-level annotations. However, these methods rely on advantage gaps induced by high-quality samples within the same batch, which makes the training process fragile and inefficient when intra-group advantages collapse under challenging tasks. To address these problems, we propose a reinforcement learning mechanism named \\emph{\\textbf{R^3}} that along three directions: (1) a \\emph{cross-context \\underline{\\textbf{R}}eplay} strategy that maintains the intra-group advantage by recalling valuable examples from historical trajectories of the same query, (2) an \\emph{in-context self-\\underline{\\textbf{R}}eflection} mechanism enabling models to refine outputs by leveraging past failures, and (3) a \\emph{structural entropy \\underline{\\textbf{R}}anking reward}, which assigns relative rewards to truncated or failed samples by ranking responses based on token-level entropy patterns, capturing both local exploration and global stability. We implement our method on Deepseek-R1-Distill-Qwen-1.5B and train it on the DeepscaleR-40k in the math domain. Experiments demonstrate our method achieves SoTA performance on several math benchmarks, representing significant improvements and fewer reasoning tokens over the base models. Code and model will be released."",""contentSnippet"":""arXiv:2601.19620v2 Announce Type: replace-cross \nAbstract: Large reasoning models (LRMs) aim to solve diverse and complex problems through structured reasoning. Recent advances in group-based policy optimization methods have shown promise in enabling stable advantage estimation without reliance on process-level annotations. However, these methods rely on advantage gaps induced by high-quality samples within the same batch, which makes the training process fragile and inefficient when intra-group advantages collapse under challenging tasks. To address these problems, we propose a reinforcement learning mechanism named \\emph{\\textbf{R^3}} that along three directions: (1) a \\emph{cross-context \\underline{\\textbf{R}}eplay} strategy that maintains the intra-group advantage by recalling valuable examples from historical trajectories of the same query, (2) an \\emph{in-context self-\\underline{\\textbf{R}}eflection} mechanism enabling models to refine outputs by leveraging past failures, and (3) a \\emph{structural entropy \\underline{\\textbf{R}}anking reward}, which assigns relative rewards to truncated or failed samples by ranking responses based on token-level entropy patterns, capturing both local exploration and global stability. We implement our method on Deepseek-R1-Distill-Qwen-1.5B and train it on the DeepscaleR-40k in the math domain. Experiments demonstrate our method achieves SoTA performance on several math benchmarks, representing significant improvements and fewer reasoning tokens over the base models. Code and model will be released."",""guid"":""oai:arXiv.org:2601.19620v2"",""categories"":[""cs.LG"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,"Tri-Reader: An Open-Access, Multi-Stage AI Pipeline for First-Pass Lung Nodule Annotation in Screening CT",https://arxiv.org/abs/2601.19380,2026-01-30T05:00:00.000Z,"arXiv:2601.19380v2 Announce Type: replace-cross 
Abstract: Using multiple open-access models trained on public datasets, we developed Tri-Reader, a comprehensive, freely available pipeline that integrates lung segmentation, nodule detection, and malignancy classification into a unified tri-stage workflow. The pipeline is designed to prioritize sensitivity while reducing the candidate burden for annotators. To ensure accuracy and generalizability across diverse practices, we evaluated Tri-Reader on multiple internal and external datasets as compared with expert annotations and dataset-provided reference standards.","{""creator"":""Fakrul Islam Tushar, Joseph Y. Lo"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""Tri-Reader: An Open-Access, Multi-Stage AI Pipeline for First-Pass Lung Nodule Annotation in Screening CT"",""link"":""https://arxiv.org/abs/2601.19380"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Fakrul Islam Tushar, Joseph Y. Lo"",""content"":""arXiv:2601.19380v2 Announce Type: replace-cross \nAbstract: Using multiple open-access models trained on public datasets, we developed Tri-Reader, a comprehensive, freely available pipeline that integrates lung segmentation, nodule detection, and malignancy classification into a unified tri-stage workflow. The pipeline is designed to prioritize sensitivity while reducing the candidate burden for annotators. To ensure accuracy and generalizability across diverse practices, we evaluated Tri-Reader on multiple internal and external datasets as compared with expert annotations and dataset-provided reference standards."",""contentSnippet"":""arXiv:2601.19380v2 Announce Type: replace-cross \nAbstract: Using multiple open-access models trained on public datasets, we developed Tri-Reader, a comprehensive, freely available pipeline that integrates lung segmentation, nodule detection, and malignancy classification into a unified tri-stage workflow. The pipeline is designed to prioritize sensitivity while reducing the candidate burden for annotators. To ensure accuracy and generalizability across diverse practices, we evaluated Tri-Reader on multiple internal and external datasets as compared with expert annotations and dataset-provided reference standards."",""guid"":""oai:arXiv.org:2601.19380v2"",""categories"":[""cs.CV"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,RPO-RAG: Aligning Small LLMs with Relation-aware Preference Optimization for Knowledge Graph Question Answering,https://arxiv.org/abs/2601.19225,2026-01-30T05:00:00.000Z,"arXiv:2601.19225v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have recently demonstrated remarkable reasoning abilities, yet hallucinate on knowledge-intensive tasks. Retrieval-augmented generation (RAG) mitigates this issue by grounding answers in external sources, e.g., knowledge graphs (KGs). However, existing KG-based RAG approaches rely on semantics-unaware path sampling and are weakly aligned with KG reasoning objectives, which limits further accuracy gains. They also feed retrieved paths directly into the reasoner without organizing them into answer-centered reasoning paths, hindering small LLMs' ability to leverage the retrieved knowledge. Furthermore, prior works predominantly rely on large LLMs (e.g., ChatGPT/GPT-4) or assume backbones above 7B parameters, leaving sub-7B models underexplored. We address this gap with RPO-RAG, the first KG-based RAG framework specifically designed for small LLMs, to the best of our knowledge. RPO-RAG introduces three key innovations: (1) a query-path semantic sampling strategy that provides informative supervisory signals; (2) a relation-aware preference optimization that aligns training with intermediate KG reasoning signals (e.g., relation); and (3) an answer-centered prompt design that organizes entities and reasoning paths in an interpretable format. Extensive experiments on two benchmark Knowledge Graph Question Answering (KGQA) datasets, WebQSP and CWQ, demonstrate that RPO-RAG effectively bridges the performance gap between small and large language models. On WebQSP, it improves F1 by up to 8.8%, reflecting enhanced answer precision, while on CWQ it achieves new state-of-the-art results among models under 8B parameters in both Hit and F1. Overall, RPO-RAG substantially improves the reasoning capability of small LLMs, even under 3B parameters-highlighting their potential for resource-efficient and practical on-device KGQA applications.","{""creator"":""Kaehyun Um, KyuHwan Yeom, Haerim Yang, Minyoung Choi, Hyeongjun Yang, Kyong-Ho Lee"",""rights"":""http://creativecommons.org/licenses/by-nc-nd/4.0/"",""title"":""RPO-RAG: Aligning Small LLMs with Relation-aware Preference Optimization for Knowledge Graph Question Answering"",""link"":""https://arxiv.org/abs/2601.19225"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Kaehyun Um, KyuHwan Yeom, Haerim Yang, Minyoung Choi, Hyeongjun Yang, Kyong-Ho Lee"",""content"":""arXiv:2601.19225v2 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) have recently demonstrated remarkable reasoning abilities, yet hallucinate on knowledge-intensive tasks. Retrieval-augmented generation (RAG) mitigates this issue by grounding answers in external sources, e.g., knowledge graphs (KGs). However, existing KG-based RAG approaches rely on semantics-unaware path sampling and are weakly aligned with KG reasoning objectives, which limits further accuracy gains. They also feed retrieved paths directly into the reasoner without organizing them into answer-centered reasoning paths, hindering small LLMs' ability to leverage the retrieved knowledge. Furthermore, prior works predominantly rely on large LLMs (e.g., ChatGPT/GPT-4) or assume backbones above 7B parameters, leaving sub-7B models underexplored. We address this gap with RPO-RAG, the first KG-based RAG framework specifically designed for small LLMs, to the best of our knowledge. RPO-RAG introduces three key innovations: (1) a query-path semantic sampling strategy that provides informative supervisory signals; (2) a relation-aware preference optimization that aligns training with intermediate KG reasoning signals (e.g., relation); and (3) an answer-centered prompt design that organizes entities and reasoning paths in an interpretable format. Extensive experiments on two benchmark Knowledge Graph Question Answering (KGQA) datasets, WebQSP and CWQ, demonstrate that RPO-RAG effectively bridges the performance gap between small and large language models. On WebQSP, it improves F1 by up to 8.8%, reflecting enhanced answer precision, while on CWQ it achieves new state-of-the-art results among models under 8B parameters in both Hit and F1. Overall, RPO-RAG substantially improves the reasoning capability of small LLMs, even under 3B parameters-highlighting their potential for resource-efficient and practical on-device KGQA applications."",""contentSnippet"":""arXiv:2601.19225v2 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) have recently demonstrated remarkable reasoning abilities, yet hallucinate on knowledge-intensive tasks. Retrieval-augmented generation (RAG) mitigates this issue by grounding answers in external sources, e.g., knowledge graphs (KGs). However, existing KG-based RAG approaches rely on semantics-unaware path sampling and are weakly aligned with KG reasoning objectives, which limits further accuracy gains. They also feed retrieved paths directly into the reasoner without organizing them into answer-centered reasoning paths, hindering small LLMs' ability to leverage the retrieved knowledge. Furthermore, prior works predominantly rely on large LLMs (e.g., ChatGPT/GPT-4) or assume backbones above 7B parameters, leaving sub-7B models underexplored. We address this gap with RPO-RAG, the first KG-based RAG framework specifically designed for small LLMs, to the best of our knowledge. RPO-RAG introduces three key innovations: (1) a query-path semantic sampling strategy that provides informative supervisory signals; (2) a relation-aware preference optimization that aligns training with intermediate KG reasoning signals (e.g., relation); and (3) an answer-centered prompt design that organizes entities and reasoning paths in an interpretable format. Extensive experiments on two benchmark Knowledge Graph Question Answering (KGQA) datasets, WebQSP and CWQ, demonstrate that RPO-RAG effectively bridges the performance gap between small and large language models. On WebQSP, it improves F1 by up to 8.8%, reflecting enhanced answer precision, while on CWQ it achieves new state-of-the-art results among models under 8B parameters in both Hit and F1. Overall, RPO-RAG substantially improves the reasoning capability of small LLMs, even under 3B parameters-highlighting their potential for resource-efficient and practical on-device KGQA applications."",""guid"":""oai:arXiv.org:2601.19225v2"",""categories"":[""cs.CL"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,A Scalable Inter-edge Correlation Modeling in CopulaGNN for Link Sign Prediction,https://arxiv.org/abs/2601.19175,2026-01-30T05:00:00.000Z,"arXiv:2601.19175v2 Announce Type: replace-cross 
Abstract: Link sign prediction on a signed graph is a task to determine whether the relationship represented by an edge is positive or negative. Since the presence of negative edges violates the graph homophily assumption that adjacent nodes are similar, regular graph methods have not been applicable without auxiliary structures to handle them. We aim to directly model the latent statistical dependency among edges with the Gaussian copula and its corresponding correlation matrix, extending CopulaGNN (Ma et al., 2021). However, a naive modeling of edge-edge relations is computationally intractable even for a graph with moderate scale. To address this, we propose to 1) represent the correlation matrix as a Gramian of edge embeddings, significantly reducing the number of parameters, and 2) reformulate the conditional probability distribution to dramatically reduce the inference cost. We theoretically verify scalability of our method by proving its linear convergence. Also, our extensive experiments demonstrate that it achieves significantly faster convergence than baselines, maintaining competitive prediction performance to the state-of-the-art models.","{""creator"":""Jinkyu Sung, Myunggeum Jee, Joonseok Lee"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""A Scalable Inter-edge Correlation Modeling in CopulaGNN for Link Sign Prediction"",""link"":""https://arxiv.org/abs/2601.19175"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Jinkyu Sung, Myunggeum Jee, Joonseok Lee"",""content"":""arXiv:2601.19175v2 Announce Type: replace-cross \nAbstract: Link sign prediction on a signed graph is a task to determine whether the relationship represented by an edge is positive or negative. Since the presence of negative edges violates the graph homophily assumption that adjacent nodes are similar, regular graph methods have not been applicable without auxiliary structures to handle them. We aim to directly model the latent statistical dependency among edges with the Gaussian copula and its corresponding correlation matrix, extending CopulaGNN (Ma et al., 2021). However, a naive modeling of edge-edge relations is computationally intractable even for a graph with moderate scale. To address this, we propose to 1) represent the correlation matrix as a Gramian of edge embeddings, significantly reducing the number of parameters, and 2) reformulate the conditional probability distribution to dramatically reduce the inference cost. We theoretically verify scalability of our method by proving its linear convergence. Also, our extensive experiments demonstrate that it achieves significantly faster convergence than baselines, maintaining competitive prediction performance to the state-of-the-art models."",""contentSnippet"":""arXiv:2601.19175v2 Announce Type: replace-cross \nAbstract: Link sign prediction on a signed graph is a task to determine whether the relationship represented by an edge is positive or negative. Since the presence of negative edges violates the graph homophily assumption that adjacent nodes are similar, regular graph methods have not been applicable without auxiliary structures to handle them. We aim to directly model the latent statistical dependency among edges with the Gaussian copula and its corresponding correlation matrix, extending CopulaGNN (Ma et al., 2021). However, a naive modeling of edge-edge relations is computationally intractable even for a graph with moderate scale. To address this, we propose to 1) represent the correlation matrix as a Gramian of edge embeddings, significantly reducing the number of parameters, and 2) reformulate the conditional probability distribution to dramatically reduce the inference cost. We theoretically verify scalability of our method by proving its linear convergence. Also, our extensive experiments demonstrate that it achieves significantly faster convergence than baselines, maintaining competitive prediction performance to the state-of-the-art models."",""guid"":""oai:arXiv.org:2601.19175v2"",""categories"":[""cs.LG"",""cs.AI"",""cs.IR"",""cs.SI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,CLIP-Guided Unsupervised Semantic-Aware Exposure Correction,https://arxiv.org/abs/2601.19129,2026-01-30T05:00:00.000Z,"arXiv:2601.19129v2 Announce Type: replace-cross 
Abstract: Improper exposure often leads to severe loss of details, color distortion, and reduced contrast. Exposure correction still faces two critical challenges: (1) the ignorance of object-wise regional semantic information causes the color shift artifacts; (2) real-world exposure images generally have no ground-truth labels, and its labeling entails massive manual editing. To tackle the challenges, we propose a new unsupervised semantic-aware exposure correction network. It contains an adaptive semantic-aware fusion module, which effectively fuses the semantic information extracted from a pre-trained Fast Segment Anything Model into a shared image feature space. Then the fused features are used by our multi-scale residual spatial mamba group to restore the details and adjust the exposure. To avoid manual editing, we propose a pseudo-ground truth generator guided by CLIP, which is fine-tuned to automatically identify exposure situations and instruct the tailored corrections. Also, we leverage the rich priors from the FastSAM and CLIP to develop a semantic-prompt consistency loss to enforce semantic consistency and image-prompt alignment for unsupervised training. Comprehensive experimental results illustrate the effectiveness of our method in correcting real-world exposure images and outperforms state-of-the-art unsupervised methods both numerically and visually.","{""creator"":""Puzhen Wu, Han Weng, Quan Zheng, Yi Zhan, Hewei Wang, Yiming Li, Jiahui Han, Rui Xu"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""CLIP-Guided Unsupervised Semantic-Aware Exposure Correction"",""link"":""https://arxiv.org/abs/2601.19129"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Puzhen Wu, Han Weng, Quan Zheng, Yi Zhan, Hewei Wang, Yiming Li, Jiahui Han, Rui Xu"",""content"":""arXiv:2601.19129v2 Announce Type: replace-cross \nAbstract: Improper exposure often leads to severe loss of details, color distortion, and reduced contrast. Exposure correction still faces two critical challenges: (1) the ignorance of object-wise regional semantic information causes the color shift artifacts; (2) real-world exposure images generally have no ground-truth labels, and its labeling entails massive manual editing. To tackle the challenges, we propose a new unsupervised semantic-aware exposure correction network. It contains an adaptive semantic-aware fusion module, which effectively fuses the semantic information extracted from a pre-trained Fast Segment Anything Model into a shared image feature space. Then the fused features are used by our multi-scale residual spatial mamba group to restore the details and adjust the exposure. To avoid manual editing, we propose a pseudo-ground truth generator guided by CLIP, which is fine-tuned to automatically identify exposure situations and instruct the tailored corrections. Also, we leverage the rich priors from the FastSAM and CLIP to develop a semantic-prompt consistency loss to enforce semantic consistency and image-prompt alignment for unsupervised training. Comprehensive experimental results illustrate the effectiveness of our method in correcting real-world exposure images and outperforms state-of-the-art unsupervised methods both numerically and visually."",""contentSnippet"":""arXiv:2601.19129v2 Announce Type: replace-cross \nAbstract: Improper exposure often leads to severe loss of details, color distortion, and reduced contrast. Exposure correction still faces two critical challenges: (1) the ignorance of object-wise regional semantic information causes the color shift artifacts; (2) real-world exposure images generally have no ground-truth labels, and its labeling entails massive manual editing. To tackle the challenges, we propose a new unsupervised semantic-aware exposure correction network. It contains an adaptive semantic-aware fusion module, which effectively fuses the semantic information extracted from a pre-trained Fast Segment Anything Model into a shared image feature space. Then the fused features are used by our multi-scale residual spatial mamba group to restore the details and adjust the exposure. To avoid manual editing, we propose a pseudo-ground truth generator guided by CLIP, which is fine-tuned to automatically identify exposure situations and instruct the tailored corrections. Also, we leverage the rich priors from the FastSAM and CLIP to develop a semantic-prompt consistency loss to enforce semantic consistency and image-prompt alignment for unsupervised training. Comprehensive experimental results illustrate the effectiveness of our method in correcting real-world exposure images and outperforms state-of-the-art unsupervised methods both numerically and visually."",""guid"":""oai:arXiv.org:2601.19129v2"",""categories"":[""cs.CV"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,"EVEREST: An Evidential, Tail-Aware Transformer for Rare-Event Time-Series Forecasting",https://arxiv.org/abs/2601.19022,2026-01-30T05:00:00.000Z,"arXiv:2601.19022v2 Announce Type: replace-cross 
Abstract: Forecasting rare events in multivariate time-series data is challenging due to severe class imbalance, long-range dependencies, and distributional uncertainty. We introduce EVEREST, a transformer-based architecture for probabilistic rare-event forecasting that delivers calibrated predictions and tail-aware risk estimation, with auxiliary interpretability via attention-based signal attribution. EVEREST integrates four components: (i) a learnable attention bottleneck for soft aggregation of temporal dynamics; (ii) an evidential head for estimating aleatoric and epistemic uncertainty via a Normal--Inverse--Gamma distribution; (iii) an extreme-value head that models tail risk using a Generalized Pareto Distribution; and (iv) a lightweight precursor head for early-event detection. These modules are jointly optimized with a composite loss (focal loss, evidential NLL, and a tail-sensitive EVT penalty) and act only at training time; deployment uses a single classification head with no inference overhead (approximately 0.81M parameters). On a decade of space-weather data, EVEREST achieves state-of-the-art True Skill Statistic (TSS) of 0.973/0.970/0.966 at 24/48/72-hour horizons for C-class flares. The model is compact, efficient to train on commodity hardware, and applicable to high-stakes domains such as industrial monitoring, weather, and satellite diagnostics. Limitations include reliance on fixed-length inputs and exclusion of image-based modalities, motivating future extensions to streaming and multimodal forecasting.","{""creator"":""Antanas Zilinskas, Robert N. Shorten, Jakub Marecek"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""EVEREST: An Evidential, Tail-Aware Transformer for Rare-Event Time-Series Forecasting"",""link"":""https://arxiv.org/abs/2601.19022"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Antanas Zilinskas, Robert N. Shorten, Jakub Marecek"",""content"":""arXiv:2601.19022v2 Announce Type: replace-cross \nAbstract: Forecasting rare events in multivariate time-series data is challenging due to severe class imbalance, long-range dependencies, and distributional uncertainty. We introduce EVEREST, a transformer-based architecture for probabilistic rare-event forecasting that delivers calibrated predictions and tail-aware risk estimation, with auxiliary interpretability via attention-based signal attribution. EVEREST integrates four components: (i) a learnable attention bottleneck for soft aggregation of temporal dynamics; (ii) an evidential head for estimating aleatoric and epistemic uncertainty via a Normal--Inverse--Gamma distribution; (iii) an extreme-value head that models tail risk using a Generalized Pareto Distribution; and (iv) a lightweight precursor head for early-event detection. These modules are jointly optimized with a composite loss (focal loss, evidential NLL, and a tail-sensitive EVT penalty) and act only at training time; deployment uses a single classification head with no inference overhead (approximately 0.81M parameters). On a decade of space-weather data, EVEREST achieves state-of-the-art True Skill Statistic (TSS) of 0.973/0.970/0.966 at 24/48/72-hour horizons for C-class flares. The model is compact, efficient to train on commodity hardware, and applicable to high-stakes domains such as industrial monitoring, weather, and satellite diagnostics. Limitations include reliance on fixed-length inputs and exclusion of image-based modalities, motivating future extensions to streaming and multimodal forecasting."",""contentSnippet"":""arXiv:2601.19022v2 Announce Type: replace-cross \nAbstract: Forecasting rare events in multivariate time-series data is challenging due to severe class imbalance, long-range dependencies, and distributional uncertainty. We introduce EVEREST, a transformer-based architecture for probabilistic rare-event forecasting that delivers calibrated predictions and tail-aware risk estimation, with auxiliary interpretability via attention-based signal attribution. EVEREST integrates four components: (i) a learnable attention bottleneck for soft aggregation of temporal dynamics; (ii) an evidential head for estimating aleatoric and epistemic uncertainty via a Normal--Inverse--Gamma distribution; (iii) an extreme-value head that models tail risk using a Generalized Pareto Distribution; and (iv) a lightweight precursor head for early-event detection. These modules are jointly optimized with a composite loss (focal loss, evidential NLL, and a tail-sensitive EVT penalty) and act only at training time; deployment uses a single classification head with no inference overhead (approximately 0.81M parameters). On a decade of space-weather data, EVEREST achieves state-of-the-art True Skill Statistic (TSS) of 0.973/0.970/0.966 at 24/48/72-hour horizons for C-class flares. The model is compact, efficient to train on commodity hardware, and applicable to high-stakes domains such as industrial monitoring, weather, and satellite diagnostics. Limitations include reliance on fixed-length inputs and exclusion of image-based modalities, motivating future extensions to streaming and multimodal forecasting."",""guid"":""oai:arXiv.org:2601.19022v2"",""categories"":[""cs.LG"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,LLMs versus the Halting Problem: Revisiting Program Termination Prediction,https://arxiv.org/abs/2601.18987,2026-01-30T05:00:00.000Z,"arXiv:2601.18987v3 Announce Type: replace-cross 
Abstract: Determining whether a program terminates is a central problem in computer science. Turing's foundational result established the Halting Problem as undecidable, showing that no algorithm can universally determine termination for all programs and inputs. Consequently, automatic verification tools approximate termination, sometimes failing to prove or disprove; these tools rely on problem-specific architectures and abstractions, and are usually tied to particular programming languages. Recent success and progress in large language models (LLMs) raises the following question: can LLMs reliably predict program termination? In this work, we evaluate LLMs on a diverse set of C programs from the Termination category of the International Competition on Software Verification (SV-Comp) 2025. Our results suggest that LLMs perform remarkably well at predicting program termination, where GPT-5 and Claude Sonnet-4.5 would rank just behind the top-ranked tool (using test-time-scaling), and Code World Model (CWM) would place just behind the second-ranked tool. While LLMs are effective at predicting program termination, they often fail to provide a valid witness as a proof. Moreover, LLMs performance drops as program length increases. We hope these insights motivate further research into program termination and the broader potential of LLMs for reasoning about undecidable problems.","{""creator"":""Oren Sultan, Jordi Armengol-Estape, Pascal Kesseli, Julien Vanegue, Dafna Shahaf, Yossi Adi, Peter O'Hearn"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""LLMs versus the Halting Problem: Revisiting Program Termination Prediction"",""link"":""https://arxiv.org/abs/2601.18987"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Oren Sultan, Jordi Armengol-Estape, Pascal Kesseli, Julien Vanegue, Dafna Shahaf, Yossi Adi, Peter O'Hearn"",""content"":""arXiv:2601.18987v3 Announce Type: replace-cross \nAbstract: Determining whether a program terminates is a central problem in computer science. Turing's foundational result established the Halting Problem as undecidable, showing that no algorithm can universally determine termination for all programs and inputs. Consequently, automatic verification tools approximate termination, sometimes failing to prove or disprove; these tools rely on problem-specific architectures and abstractions, and are usually tied to particular programming languages. Recent success and progress in large language models (LLMs) raises the following question: can LLMs reliably predict program termination? In this work, we evaluate LLMs on a diverse set of C programs from the Termination category of the International Competition on Software Verification (SV-Comp) 2025. Our results suggest that LLMs perform remarkably well at predicting program termination, where GPT-5 and Claude Sonnet-4.5 would rank just behind the top-ranked tool (using test-time-scaling), and Code World Model (CWM) would place just behind the second-ranked tool. While LLMs are effective at predicting program termination, they often fail to provide a valid witness as a proof. Moreover, LLMs performance drops as program length increases. We hope these insights motivate further research into program termination and the broader potential of LLMs for reasoning about undecidable problems."",""contentSnippet"":""arXiv:2601.18987v3 Announce Type: replace-cross \nAbstract: Determining whether a program terminates is a central problem in computer science. Turing's foundational result established the Halting Problem as undecidable, showing that no algorithm can universally determine termination for all programs and inputs. Consequently, automatic verification tools approximate termination, sometimes failing to prove or disprove; these tools rely on problem-specific architectures and abstractions, and are usually tied to particular programming languages. Recent success and progress in large language models (LLMs) raises the following question: can LLMs reliably predict program termination? In this work, we evaluate LLMs on a diverse set of C programs from the Termination category of the International Competition on Software Verification (SV-Comp) 2025. Our results suggest that LLMs perform remarkably well at predicting program termination, where GPT-5 and Claude Sonnet-4.5 would rank just behind the top-ranked tool (using test-time-scaling), and Code World Model (CWM) would place just behind the second-ranked tool. While LLMs are effective at predicting program termination, they often fail to provide a valid witness as a proof. Moreover, LLMs performance drops as program length increases. We hope these insights motivate further research into program termination and the broader potential of LLMs for reasoning about undecidable problems."",""guid"":""oai:arXiv.org:2601.18987v3"",""categories"":[""cs.CL"",""cs.AI"",""cs.PL""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,The Geometric Reasoner: Manifold-Informed Latent Foresight Search for Long-Context Reasoning,https://arxiv.org/abs/2601.18832,2026-01-30T05:00:00.000Z,"arXiv:2601.18832v2 Announce Type: replace-cross 
Abstract: Scaling test-time compute enhances long chain-of-thought (CoT) reasoning, yet existing approaches face a fundamental trade-off between computational cost and coverage quality: either incurring high training expense or yielding redundant trajectories. We introduce The Geometric Reasoner (TGR), a training-free framework that performs manifold-informed latent foresight search under strict memory bounds. At each chunk boundary, TGR scores candidate latent anchors via a lightweight look-ahead estimate combined with soft geometric regularizers that encourage smooth trajectories and diverse exploration. Chunk-wise KV cache resets keep memory linear in chunk length. On challenging math and code benchmarks, TGR improves robust trajectory coverage, measured by the area under the Pass@$k$ curve (AUC), by up to 13 points on Qwen3-8B, with negligible overhead of about 1.1--1.3 times.","{""creator"":""Ren Zhuang, Ben Wang, Shuifa Sun"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""The Geometric Reasoner: Manifold-Informed Latent Foresight Search for Long-Context Reasoning"",""link"":""https://arxiv.org/abs/2601.18832"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Ren Zhuang, Ben Wang, Shuifa Sun"",""content"":""arXiv:2601.18832v2 Announce Type: replace-cross \nAbstract: Scaling test-time compute enhances long chain-of-thought (CoT) reasoning, yet existing approaches face a fundamental trade-off between computational cost and coverage quality: either incurring high training expense or yielding redundant trajectories. We introduce The Geometric Reasoner (TGR), a training-free framework that performs manifold-informed latent foresight search under strict memory bounds. At each chunk boundary, TGR scores candidate latent anchors via a lightweight look-ahead estimate combined with soft geometric regularizers that encourage smooth trajectories and diverse exploration. Chunk-wise KV cache resets keep memory linear in chunk length. On challenging math and code benchmarks, TGR improves robust trajectory coverage, measured by the area under the Pass@$k$ curve (AUC), by up to 13 points on Qwen3-8B, with negligible overhead of about 1.1--1.3 times."",""contentSnippet"":""arXiv:2601.18832v2 Announce Type: replace-cross \nAbstract: Scaling test-time compute enhances long chain-of-thought (CoT) reasoning, yet existing approaches face a fundamental trade-off between computational cost and coverage quality: either incurring high training expense or yielding redundant trajectories. We introduce The Geometric Reasoner (TGR), a training-free framework that performs manifold-informed latent foresight search under strict memory bounds. At each chunk boundary, TGR scores candidate latent anchors via a lightweight look-ahead estimate combined with soft geometric regularizers that encourage smooth trajectories and diverse exploration. Chunk-wise KV cache resets keep memory linear in chunk length. On challenging math and code benchmarks, TGR improves robust trajectory coverage, measured by the area under the Pass@$k$ curve (AUC), by up to 13 points on Qwen3-8B, with negligible overhead of about 1.1--1.3 times."",""guid"":""oai:arXiv.org:2601.18832v2"",""categories"":[""cs.LG"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,From Specialist to Generalist: Unlocking SAM's Learning Potential on Unlabeled Medical Images,https://arxiv.org/abs/2601.17934,2026-01-30T05:00:00.000Z,"arXiv:2601.17934v2 Announce Type: replace-cross 
Abstract: Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM's adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at https://github.com/vnlvi2k3/SC-SAM.","{""creator"":""Vi Vu, Thanh-Huy Nguyen, Tien-Thinh Nguyen, Ba-Thinh Lam, Hoang-Thien Nguyen, Tianyang Wang, Xingjian Li, Min Xu"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""From Specialist to Generalist: Unlocking SAM's Learning Potential on Unlabeled Medical Images"",""link"":""https://arxiv.org/abs/2601.17934"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Vi Vu, Thanh-Huy Nguyen, Tien-Thinh Nguyen, Ba-Thinh Lam, Hoang-Thien Nguyen, Tianyang Wang, Xingjian Li, Min Xu"",""content"":""arXiv:2601.17934v2 Announce Type: replace-cross \nAbstract: Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM's adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at https://github.com/vnlvi2k3/SC-SAM."",""contentSnippet"":""arXiv:2601.17934v2 Announce Type: replace-cross \nAbstract: Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM's adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at https://github.com/vnlvi2k3/SC-SAM."",""guid"":""oai:arXiv.org:2601.17934v2"",""categories"":[""cs.CV"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers,https://arxiv.org/abs/2601.17367,2026-01-30T05:00:00.000Z,"arXiv:2601.17367v2 Announce Type: replace-cross 
Abstract: The quadratic complexity of standard attention mechanisms poses a significant scalability bottleneck for large language models (LLMs) in long-context scenarios. While hybrid attention strategies that combine sparse and full attention within a single model offer a viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating a lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8xA800 GPUs, our method enables models to achieve both strong performance and efficient inference. Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method.","{""creator"":""Zecheng Tang, Quantong Qiu, Yi Yang, Zhiyi Hong, Haiya Xiang, Kebin Liu, Qingqing Dang, Juntao Li, Min Zhang"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers"",""link"":""https://arxiv.org/abs/2601.17367"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Zecheng Tang, Quantong Qiu, Yi Yang, Zhiyi Hong, Haiya Xiang, Kebin Liu, Qingqing Dang, Juntao Li, Min Zhang"",""content"":""arXiv:2601.17367v2 Announce Type: replace-cross \nAbstract: The quadratic complexity of standard attention mechanisms poses a significant scalability bottleneck for large language models (LLMs) in long-context scenarios. While hybrid attention strategies that combine sparse and full attention within a single model offer a viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating a lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8xA800 GPUs, our method enables models to achieve both strong performance and efficient inference. Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method."",""contentSnippet"":""arXiv:2601.17367v2 Announce Type: replace-cross \nAbstract: The quadratic complexity of standard attention mechanisms poses a significant scalability bottleneck for large language models (LLMs) in long-context scenarios. While hybrid attention strategies that combine sparse and full attention within a single model offer a viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating a lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8xA800 GPUs, our method enables models to achieve both strong performance and efficient inference. Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method."",""guid"":""oai:arXiv.org:2601.17367v2"",""categories"":[""cs.CL"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Spatiotemporal Semantic V2X Framework for Cooperative Collision Prediction,https://arxiv.org/abs/2601.17216,2026-01-30T05:00:00.000Z,"arXiv:2601.17216v2 Announce Type: replace-cross 
Abstract: Intelligent Transportation Systems (ITS) demand real-time collision prediction to ensure road safety and reduce accident severity. Conventional approaches rely on transmitting raw video or high-dimensional sensory data from roadside units (RSUs) to vehicles, which is impractical under vehicular communication bandwidth and latency constraints. In this work, we propose a semantic V2X framework in which RSU-mounted cameras generate spatiotemporal semantic embeddings of future frames using the Video Joint Embedding Predictive Architecture (V-JEPA). To evaluate the system, we construct a digital twin of an urban traffic environment enabling the generation of d verse traffic scenarios with both safe and collision events. These embeddings of the future frame, extracted from V-JEPA, capture task-relevant traffic dynamics and are transmitted via V2X links to vehicles, where a lightweight attentive probe and classifier decode them to predict imminent collisions. By transmitting only semantic embeddings instead of raw frames, the proposed system significantly reduces communication overhead while maintaining predictive accuracy. Experimental results demonstrate that the framework with an appropriate processing method achieves a 10% F1-score improvement for collision prediction while reducing transmission requirements by four orders of magnitude compared to raw video. This validates the potential of semantic V2X communication to enable cooperative, real-time collision prediction in ITS.","{""creator"":""Murat Arda Onsu, Poonam Lohan, Burak Kantarci, Aisha Syed, Matthew Andrews, Sean Kennedy"",""rights"":""http://creativecommons.org/licenses/by-nc-nd/4.0/"",""title"":""Spatiotemporal Semantic V2X Framework for Cooperative Collision Prediction"",""link"":""https://arxiv.org/abs/2601.17216"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Murat Arda Onsu, Poonam Lohan, Burak Kantarci, Aisha Syed, Matthew Andrews, Sean Kennedy"",""content"":""arXiv:2601.17216v2 Announce Type: replace-cross \nAbstract: Intelligent Transportation Systems (ITS) demand real-time collision prediction to ensure road safety and reduce accident severity. Conventional approaches rely on transmitting raw video or high-dimensional sensory data from roadside units (RSUs) to vehicles, which is impractical under vehicular communication bandwidth and latency constraints. In this work, we propose a semantic V2X framework in which RSU-mounted cameras generate spatiotemporal semantic embeddings of future frames using the Video Joint Embedding Predictive Architecture (V-JEPA). To evaluate the system, we construct a digital twin of an urban traffic environment enabling the generation of d verse traffic scenarios with both safe and collision events. These embeddings of the future frame, extracted from V-JEPA, capture task-relevant traffic dynamics and are transmitted via V2X links to vehicles, where a lightweight attentive probe and classifier decode them to predict imminent collisions. By transmitting only semantic embeddings instead of raw frames, the proposed system significantly reduces communication overhead while maintaining predictive accuracy. Experimental results demonstrate that the framework with an appropriate processing method achieves a 10% F1-score improvement for collision prediction while reducing transmission requirements by four orders of magnitude compared to raw video. This validates the potential of semantic V2X communication to enable cooperative, real-time collision prediction in ITS."",""contentSnippet"":""arXiv:2601.17216v2 Announce Type: replace-cross \nAbstract: Intelligent Transportation Systems (ITS) demand real-time collision prediction to ensure road safety and reduce accident severity. Conventional approaches rely on transmitting raw video or high-dimensional sensory data from roadside units (RSUs) to vehicles, which is impractical under vehicular communication bandwidth and latency constraints. In this work, we propose a semantic V2X framework in which RSU-mounted cameras generate spatiotemporal semantic embeddings of future frames using the Video Joint Embedding Predictive Architecture (V-JEPA). To evaluate the system, we construct a digital twin of an urban traffic environment enabling the generation of d verse traffic scenarios with both safe and collision events. These embeddings of the future frame, extracted from V-JEPA, capture task-relevant traffic dynamics and are transmitted via V2X links to vehicles, where a lightweight attentive probe and classifier decode them to predict imminent collisions. By transmitting only semantic embeddings instead of raw frames, the proposed system significantly reduces communication overhead while maintaining predictive accuracy. Experimental results demonstrate that the framework with an appropriate processing method achieves a 10% F1-score improvement for collision prediction while reducing transmission requirements by four orders of magnitude compared to raw video. This validates the potential of semantic V2X communication to enable cooperative, real-time collision prediction in ITS."",""guid"":""oai:arXiv.org:2601.17216v2"",""categories"":[""cs.CV"",""cs.AI"",""cs.LG"",""eess.IV""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Lost in Simulation: LLM-Simulated Users are Unreliable Proxies for Human Users in Agentic Evaluations,https://arxiv.org/abs/2601.17087,2026-01-30T05:00:00.000Z,"arXiv:2601.17087v2 Announce Type: replace-cross 
Abstract: Agentic benchmarks increasingly rely on LLM-simulated users to scalably evaluate agent performance, yet the robustness, validity, and fairness of this approach remain unexamined. Through a user study with participants across the United States, India, Kenya, and Nigeria, we investigate whether LLM-simulated users serve as reliable proxies for real human users in evaluating agents on {\tau}-Bench retail tasks. We find that user simulation lacks robustness, with agent success rates varying up to 9 percentage points across different user LLMs. Furthermore, evaluations using simulated users exhibit systematic miscalibration, underestimating agent performance on challenging tasks and overestimating it on moderately difficult ones. African American Vernacular English (AAVE) speakers experience consistently worse success rates and calibration errors than Standard American English (SAE) speakers, with disparities compounding significantly with age. We also find simulated users to be a differentially effective proxy for different populations, performing worst for AAVE and Indian English speakers. Additionally, simulated users introduce conversational artifacts and surface different failure patterns than human users. These findings demonstrate that current evaluation practices risk misrepresenting agent capabilities across diverse user populations and may obscure real-world deployment challenges.","{""creator"":""Preethi Seshadri, Samuel Cahyawijaya, Ayomide Odumakinde, Sameer Singh, Seraphina Goldfarb-Tarrant"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""Lost in Simulation: LLM-Simulated Users are Unreliable Proxies for Human Users in Agentic Evaluations"",""link"":""https://arxiv.org/abs/2601.17087"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Preethi Seshadri, Samuel Cahyawijaya, Ayomide Odumakinde, Sameer Singh, Seraphina Goldfarb-Tarrant"",""content"":""arXiv:2601.17087v2 Announce Type: replace-cross \nAbstract: Agentic benchmarks increasingly rely on LLM-simulated users to scalably evaluate agent performance, yet the robustness, validity, and fairness of this approach remain unexamined. Through a user study with participants across the United States, India, Kenya, and Nigeria, we investigate whether LLM-simulated users serve as reliable proxies for real human users in evaluating agents on {\\tau}-Bench retail tasks. We find that user simulation lacks robustness, with agent success rates varying up to 9 percentage points across different user LLMs. Furthermore, evaluations using simulated users exhibit systematic miscalibration, underestimating agent performance on challenging tasks and overestimating it on moderately difficult ones. African American Vernacular English (AAVE) speakers experience consistently worse success rates and calibration errors than Standard American English (SAE) speakers, with disparities compounding significantly with age. We also find simulated users to be a differentially effective proxy for different populations, performing worst for AAVE and Indian English speakers. Additionally, simulated users introduce conversational artifacts and surface different failure patterns than human users. These findings demonstrate that current evaluation practices risk misrepresenting agent capabilities across diverse user populations and may obscure real-world deployment challenges."",""contentSnippet"":""arXiv:2601.17087v2 Announce Type: replace-cross \nAbstract: Agentic benchmarks increasingly rely on LLM-simulated users to scalably evaluate agent performance, yet the robustness, validity, and fairness of this approach remain unexamined. Through a user study with participants across the United States, India, Kenya, and Nigeria, we investigate whether LLM-simulated users serve as reliable proxies for real human users in evaluating agents on {\\tau}-Bench retail tasks. We find that user simulation lacks robustness, with agent success rates varying up to 9 percentage points across different user LLMs. Furthermore, evaluations using simulated users exhibit systematic miscalibration, underestimating agent performance on challenging tasks and overestimating it on moderately difficult ones. African American Vernacular English (AAVE) speakers experience consistently worse success rates and calibration errors than Standard American English (SAE) speakers, with disparities compounding significantly with age. We also find simulated users to be a differentially effective proxy for different populations, performing worst for AAVE and Indian English speakers. Additionally, simulated users introduce conversational artifacts and surface different failure patterns than human users. These findings demonstrate that current evaluation practices risk misrepresenting agent capabilities across diverse user populations and may obscure real-world deployment challenges."",""guid"":""oai:arXiv.org:2601.17087v2"",""categories"":[""cs.HC"",""cs.AI"",""cs.CY"",""cs.LG""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Sparsity-Aware Low-Rank Representation for Efficient Fine-Tuning of Large Language Models,https://arxiv.org/abs/2601.16991,2026-01-30T05:00:00.000Z,"arXiv:2601.16991v2 Announce Type: replace-cross 
Abstract: Adapting large pre-trained language models to downstream tasks often entails fine-tuning millions of parameters or deploying costly dense weight updates, which hinders their use in resource-constrained environments. Low-rank Adaptation (LoRA) reduces trainable parameters by factorizing weight updates, yet the underlying dense weights still impose high storage and computation costs. Magnitude-based pruning can yield sparse models but typically degrades LoRA's performance when applied naively. In this paper, we introduce SALR (Sparsity-Aware Low-Rank Representation), a novel fine-tuning paradigm that unifies low-rank adaptation with sparse pruning under a rigorous mean-squared-error framework. We prove that statically pruning only the frozen base weights minimizes the pruning error bound, and we recover the discarded residual information via a truncated-SVD low-rank adapter, which provably reduces per-entry MSE by a factor of $(1 - r/\min(d,k))$. To maximize hardware efficiency, we fuse multiple low-rank adapters into a single concatenated GEMM, and we adopt a bitmap-based encoding with a two-stage pipelined decoding + GEMM design to achieve true model compression and speedup. Empirically, SALR attains 50\% sparsity on various LLMs while matching the performance of LoRA on GSM8K and MMLU, reduces model size by $2\times$, and delivers up to a $1.7\times$ inference speedup.","{""creator"":""Longteng Zhang, Sen Wu, Shuai Hou, Zhengyu Qing, Zhuo Zheng, Danning Ke, Qihong Lin, Qiang Wang, Shaohuai Shi, Xiaowen Chu"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""Sparsity-Aware Low-Rank Representation for Efficient Fine-Tuning of Large Language Models"",""link"":""https://arxiv.org/abs/2601.16991"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Longteng Zhang, Sen Wu, Shuai Hou, Zhengyu Qing, Zhuo Zheng, Danning Ke, Qihong Lin, Qiang Wang, Shaohuai Shi, Xiaowen Chu"",""content"":""arXiv:2601.16991v2 Announce Type: replace-cross \nAbstract: Adapting large pre-trained language models to downstream tasks often entails fine-tuning millions of parameters or deploying costly dense weight updates, which hinders their use in resource-constrained environments. Low-rank Adaptation (LoRA) reduces trainable parameters by factorizing weight updates, yet the underlying dense weights still impose high storage and computation costs. Magnitude-based pruning can yield sparse models but typically degrades LoRA's performance when applied naively. In this paper, we introduce SALR (Sparsity-Aware Low-Rank Representation), a novel fine-tuning paradigm that unifies low-rank adaptation with sparse pruning under a rigorous mean-squared-error framework. We prove that statically pruning only the frozen base weights minimizes the pruning error bound, and we recover the discarded residual information via a truncated-SVD low-rank adapter, which provably reduces per-entry MSE by a factor of $(1 - r/\\min(d,k))$. To maximize hardware efficiency, we fuse multiple low-rank adapters into a single concatenated GEMM, and we adopt a bitmap-based encoding with a two-stage pipelined decoding + GEMM design to achieve true model compression and speedup. Empirically, SALR attains 50\\% sparsity on various LLMs while matching the performance of LoRA on GSM8K and MMLU, reduces model size by $2\\times$, and delivers up to a $1.7\\times$ inference speedup."",""contentSnippet"":""arXiv:2601.16991v2 Announce Type: replace-cross \nAbstract: Adapting large pre-trained language models to downstream tasks often entails fine-tuning millions of parameters or deploying costly dense weight updates, which hinders their use in resource-constrained environments. Low-rank Adaptation (LoRA) reduces trainable parameters by factorizing weight updates, yet the underlying dense weights still impose high storage and computation costs. Magnitude-based pruning can yield sparse models but typically degrades LoRA's performance when applied naively. In this paper, we introduce SALR (Sparsity-Aware Low-Rank Representation), a novel fine-tuning paradigm that unifies low-rank adaptation with sparse pruning under a rigorous mean-squared-error framework. We prove that statically pruning only the frozen base weights minimizes the pruning error bound, and we recover the discarded residual information via a truncated-SVD low-rank adapter, which provably reduces per-entry MSE by a factor of $(1 - r/\\min(d,k))$. To maximize hardware efficiency, we fuse multiple low-rank adapters into a single concatenated GEMM, and we adopt a bitmap-based encoding with a two-stage pipelined decoding + GEMM design to achieve true model compression and speedup. Empirically, SALR attains 50\\% sparsity on various LLMs while matching the performance of LoRA on GSM8K and MMLU, reduces model size by $2\\times$, and delivers up to a $1.7\\times$ inference speedup."",""guid"":""oai:arXiv.org:2601.16991v2"",""categories"":[""cs.LG"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,PLawBench: A Rubric-Based Benchmark for Evaluating LLMs in Real-World Legal Practice,https://arxiv.org/abs/2601.16669,2026-01-30T05:00:00.000Z,"arXiv:2601.16669v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) are increasingly applied to legal domain-specific tasks, evaluating their ability to perform legal work in real-world settings has become essential. However, existing legal benchmarks rely on simplified and highly standardized tasks, failing to capture the ambiguity, complexity, and reasoning demands of real legal practice. Moreover, prior evaluations often adopt coarse, single-dimensional metrics and do not explicitly assess fine-grained legal reasoning. To address these limitations, we introduce PLawBench, a Practical Law Benchmark designed to evaluate LLMs in realistic legal practice scenarios. Grounded in real-world legal workflows, PLawBench models the core processes of legal practitioners through three task categories: public legal consultation, practical case analysis, and legal document generation. These tasks assess a model's ability to identify legal issues and key facts, perform structured legal reasoning, and generate legally coherent documents. PLawBench comprises 850 questions across 13 practical legal scenarios, with each question accompanied by expert-designed evaluation rubrics, resulting in approximately 12,500 rubric items for fine-grained assessment. Using an LLM-based evaluator aligned with human expert judgments, we evaluate 10 state-of-the-art LLMs. Experimental results show that none achieves strong performance on PLawBench, revealing substantial limitations in the fine-grained legal reasoning capabilities of current LLMs and highlighting important directions for future evaluation and development of legal LLMs. Data is available at: https://github.com/skylenage/PLawbench.","{""creator"":""Yuzhen Shi, Huanghai Liu, Yiran Hu, Gaojie Song, Xinran Xu, Yubo Ma, Tianyi Tang, Li Zhang, Qingjing Chen, Di Feng, Wenbo Lv, Weiheng Wu, Kexin Yang, Sen Yang, Wei Wang, Rongyao Shi, Yuanyang Qiu, Yuemeng Qi, Jingwen Zhang, Xiaoyu Sui, Yifan Chen, Yi Zhang, An Yang, Bowen Yu, Dayiheng Liu, Junyang Lin, Weixing Shen, Bing Zhao, Charles L. A. Clarke, Hu Wei"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""PLawBench: A Rubric-Based Benchmark for Evaluating LLMs in Real-World Legal Practice"",""link"":""https://arxiv.org/abs/2601.16669"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Yuzhen Shi, Huanghai Liu, Yiran Hu, Gaojie Song, Xinran Xu, Yubo Ma, Tianyi Tang, Li Zhang, Qingjing Chen, Di Feng, Wenbo Lv, Weiheng Wu, Kexin Yang, Sen Yang, Wei Wang, Rongyao Shi, Yuanyang Qiu, Yuemeng Qi, Jingwen Zhang, Xiaoyu Sui, Yifan Chen, Yi Zhang, An Yang, Bowen Yu, Dayiheng Liu, Junyang Lin, Weixing Shen, Bing Zhao, Charles L. A. Clarke, Hu Wei"",""content"":""arXiv:2601.16669v2 Announce Type: replace-cross \nAbstract: As large language models (LLMs) are increasingly applied to legal domain-specific tasks, evaluating their ability to perform legal work in real-world settings has become essential. However, existing legal benchmarks rely on simplified and highly standardized tasks, failing to capture the ambiguity, complexity, and reasoning demands of real legal practice. Moreover, prior evaluations often adopt coarse, single-dimensional metrics and do not explicitly assess fine-grained legal reasoning. To address these limitations, we introduce PLawBench, a Practical Law Benchmark designed to evaluate LLMs in realistic legal practice scenarios. Grounded in real-world legal workflows, PLawBench models the core processes of legal practitioners through three task categories: public legal consultation, practical case analysis, and legal document generation. These tasks assess a model's ability to identify legal issues and key facts, perform structured legal reasoning, and generate legally coherent documents. PLawBench comprises 850 questions across 13 practical legal scenarios, with each question accompanied by expert-designed evaluation rubrics, resulting in approximately 12,500 rubric items for fine-grained assessment. Using an LLM-based evaluator aligned with human expert judgments, we evaluate 10 state-of-the-art LLMs. Experimental results show that none achieves strong performance on PLawBench, revealing substantial limitations in the fine-grained legal reasoning capabilities of current LLMs and highlighting important directions for future evaluation and development of legal LLMs. Data is available at: https://github.com/skylenage/PLawbench."",""contentSnippet"":""arXiv:2601.16669v2 Announce Type: replace-cross \nAbstract: As large language models (LLMs) are increasingly applied to legal domain-specific tasks, evaluating their ability to perform legal work in real-world settings has become essential. However, existing legal benchmarks rely on simplified and highly standardized tasks, failing to capture the ambiguity, complexity, and reasoning demands of real legal practice. Moreover, prior evaluations often adopt coarse, single-dimensional metrics and do not explicitly assess fine-grained legal reasoning. To address these limitations, we introduce PLawBench, a Practical Law Benchmark designed to evaluate LLMs in realistic legal practice scenarios. Grounded in real-world legal workflows, PLawBench models the core processes of legal practitioners through three task categories: public legal consultation, practical case analysis, and legal document generation. These tasks assess a model's ability to identify legal issues and key facts, perform structured legal reasoning, and generate legally coherent documents. PLawBench comprises 850 questions across 13 practical legal scenarios, with each question accompanied by expert-designed evaluation rubrics, resulting in approximately 12,500 rubric items for fine-grained assessment. Using an LLM-based evaluator aligned with human expert judgments, we evaluate 10 state-of-the-art LLMs. Experimental results show that none achieves strong performance on PLawBench, revealing substantial limitations in the fine-grained legal reasoning capabilities of current LLMs and highlighting important directions for future evaluation and development of legal LLMs. Data is available at: https://github.com/skylenage/PLawbench."",""guid"":""oai:arXiv.org:2601.16669v2"",""categories"":[""cs.CL"",""cs.AI"",""cs.CY""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,"Human Values in a Single Sentence: Moral Presence, Hierarchies, and Transformer Ensembles on the Schwartz Continuum",https://arxiv.org/abs/2601.14172,2026-01-30T05:00:00.000Z,"arXiv:2601.14172v2 Announce Type: replace-cross 
Abstract: We study sentence-level detection of the 19 human values in the refined Schwartz continuum in about 74k English sentences from news and political manifestos (ValueEval'24 corpus). Each sentence is annotated with value presence, yielding a binary moral-presence label and a 19-way multi-label task under severe class imbalance. First, we show that moral presence is learnable from single sentences: a DeBERTa-base classifier attains positive-class F1 = 0.74 with calibrated thresholds. Second, we compare direct multi-label value detectors with presence-gated hierarchies under a single 8 GB GPU budget. Under matched compute, presence gating does not improve over direct prediction, indicating that gate recall becomes a bottleneck. Third, we investigate lightweight auxiliary signals - short-range context, LIWC-22 and moral lexica, and topic features - and small ensembles. Our best supervised configuration, a soft-voting ensemble of DeBERTa-based models enriched with such signals, reaches macro-F1 = 0.332 on the 19 values, improving over the best previous English-only baseline on this corpus (macro-F1 $\approx$ 0.28). We additionally benchmark 7-9B instruction-tuned LLMs (Gemma 2 9B, Llama 3.1 8B, Mistral 8B, Qwen 2.5 7B) in zero-/few-shot and QLoRA setups, and find that they lag behind the supervised ensemble under the same hardware constraint. Overall, our results provide empirical guidance for building compute-efficient, value-aware NLP models under realistic GPU budgets.","{""creator"":""V\\'ictor Yeste, Paolo Rosso"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""Human Values in a Single Sentence: Moral Presence, Hierarchies, and Transformer Ensembles on the Schwartz Continuum"",""link"":""https://arxiv.org/abs/2601.14172"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""V\\'ictor Yeste, Paolo Rosso"",""content"":""arXiv:2601.14172v2 Announce Type: replace-cross \nAbstract: We study sentence-level detection of the 19 human values in the refined Schwartz continuum in about 74k English sentences from news and political manifestos (ValueEval'24 corpus). Each sentence is annotated with value presence, yielding a binary moral-presence label and a 19-way multi-label task under severe class imbalance. First, we show that moral presence is learnable from single sentences: a DeBERTa-base classifier attains positive-class F1 = 0.74 with calibrated thresholds. Second, we compare direct multi-label value detectors with presence-gated hierarchies under a single 8 GB GPU budget. Under matched compute, presence gating does not improve over direct prediction, indicating that gate recall becomes a bottleneck. Third, we investigate lightweight auxiliary signals - short-range context, LIWC-22 and moral lexica, and topic features - and small ensembles. Our best supervised configuration, a soft-voting ensemble of DeBERTa-based models enriched with such signals, reaches macro-F1 = 0.332 on the 19 values, improving over the best previous English-only baseline on this corpus (macro-F1 $\\approx$ 0.28). We additionally benchmark 7-9B instruction-tuned LLMs (Gemma 2 9B, Llama 3.1 8B, Mistral 8B, Qwen 2.5 7B) in zero-/few-shot and QLoRA setups, and find that they lag behind the supervised ensemble under the same hardware constraint. Overall, our results provide empirical guidance for building compute-efficient, value-aware NLP models under realistic GPU budgets."",""contentSnippet"":""arXiv:2601.14172v2 Announce Type: replace-cross \nAbstract: We study sentence-level detection of the 19 human values in the refined Schwartz continuum in about 74k English sentences from news and political manifestos (ValueEval'24 corpus). Each sentence is annotated with value presence, yielding a binary moral-presence label and a 19-way multi-label task under severe class imbalance. First, we show that moral presence is learnable from single sentences: a DeBERTa-base classifier attains positive-class F1 = 0.74 with calibrated thresholds. Second, we compare direct multi-label value detectors with presence-gated hierarchies under a single 8 GB GPU budget. Under matched compute, presence gating does not improve over direct prediction, indicating that gate recall becomes a bottleneck. Third, we investigate lightweight auxiliary signals - short-range context, LIWC-22 and moral lexica, and topic features - and small ensembles. Our best supervised configuration, a soft-voting ensemble of DeBERTa-based models enriched with such signals, reaches macro-F1 = 0.332 on the 19 values, improving over the best previous English-only baseline on this corpus (macro-F1 $\\approx$ 0.28). We additionally benchmark 7-9B instruction-tuned LLMs (Gemma 2 9B, Llama 3.1 8B, Mistral 8B, Qwen 2.5 7B) in zero-/few-shot and QLoRA setups, and find that they lag behind the supervised ensemble under the same hardware constraint. Overall, our results provide empirical guidance for building compute-efficient, value-aware NLP models under realistic GPU budgets."",""guid"":""oai:arXiv.org:2601.14172v2"",""categories"":[""cs.CL"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,LAPS: A Length-Aware-Prefill LLM Serving System,https://arxiv.org/abs/2601.11589,2026-01-30T05:00:00.000Z,"arXiv:2601.11589v2 Announce Type: replace-cross 
Abstract: LAPS identifies and disaggregates requests with different prompt lengths in LLM serving to reduce TTFT latency. While recent systems have decoupled the prefill and decode stages to improve throughput, they still rely on unified scheduling policies that fail to adapt to heterogeneous workload characteristics. We observe that prompt-length variations lead to distinct performance bottlenecks, motivating an adaptive scheduling strategy. LAPS disaggregates multi-turn long-prefill requests from short-prefill ones and introduces a length-aware smart batching mechanism for short-prefill workloads. It adopts a dual-queue design that supports temporal disaggregation on a single prefill instance or spatial disaggregation across multiple instances. For short-prefill batches, a batch waiting window and CUDA Graph-based clustering mitigate interference from heterogeneous computation, reducing batching delay and lowering average latency. In real multi-turn workloads, LAPS reduces prefill latency by over 30\% compared to vanilla SGLang under prefill-decode disaggregation, and further decreases SLO violations by 28\% in multi-instance deployments with vanilla data-parallel configuration. Compared to the SGLang router with load balancing, it further lowers SLO violations by 12\% in multi-GPU settings. Under high concurrency and mixed-request scenarios, LAPS improves request throughput by 35\% serving Qwen2.5-32B model for prefill instance, demonstrating its effectiveness in optimizing heterogeneous LLM serving workloads.","{""creator"":""Jianshu She, Zonghang Li, Hongchao Du, Shangyu Wu, Wenhao Zheng, Eric Xing, Zhengzhong Liu, Huaxiu Yao, Jason Xue, Qirong Ho"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""LAPS: A Length-Aware-Prefill LLM Serving System"",""link"":""https://arxiv.org/abs/2601.11589"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Jianshu She, Zonghang Li, Hongchao Du, Shangyu Wu, Wenhao Zheng, Eric Xing, Zhengzhong Liu, Huaxiu Yao, Jason Xue, Qirong Ho"",""content"":""arXiv:2601.11589v2 Announce Type: replace-cross \nAbstract: LAPS identifies and disaggregates requests with different prompt lengths in LLM serving to reduce TTFT latency. While recent systems have decoupled the prefill and decode stages to improve throughput, they still rely on unified scheduling policies that fail to adapt to heterogeneous workload characteristics. We observe that prompt-length variations lead to distinct performance bottlenecks, motivating an adaptive scheduling strategy. LAPS disaggregates multi-turn long-prefill requests from short-prefill ones and introduces a length-aware smart batching mechanism for short-prefill workloads. It adopts a dual-queue design that supports temporal disaggregation on a single prefill instance or spatial disaggregation across multiple instances. For short-prefill batches, a batch waiting window and CUDA Graph-based clustering mitigate interference from heterogeneous computation, reducing batching delay and lowering average latency. In real multi-turn workloads, LAPS reduces prefill latency by over 30\\% compared to vanilla SGLang under prefill-decode disaggregation, and further decreases SLO violations by 28\\% in multi-instance deployments with vanilla data-parallel configuration. Compared to the SGLang router with load balancing, it further lowers SLO violations by 12\\% in multi-GPU settings. Under high concurrency and mixed-request scenarios, LAPS improves request throughput by 35\\% serving Qwen2.5-32B model for prefill instance, demonstrating its effectiveness in optimizing heterogeneous LLM serving workloads."",""contentSnippet"":""arXiv:2601.11589v2 Announce Type: replace-cross \nAbstract: LAPS identifies and disaggregates requests with different prompt lengths in LLM serving to reduce TTFT latency. While recent systems have decoupled the prefill and decode stages to improve throughput, they still rely on unified scheduling policies that fail to adapt to heterogeneous workload characteristics. We observe that prompt-length variations lead to distinct performance bottlenecks, motivating an adaptive scheduling strategy. LAPS disaggregates multi-turn long-prefill requests from short-prefill ones and introduces a length-aware smart batching mechanism for short-prefill workloads. It adopts a dual-queue design that supports temporal disaggregation on a single prefill instance or spatial disaggregation across multiple instances. For short-prefill batches, a batch waiting window and CUDA Graph-based clustering mitigate interference from heterogeneous computation, reducing batching delay and lowering average latency. In real multi-turn workloads, LAPS reduces prefill latency by over 30\\% compared to vanilla SGLang under prefill-decode disaggregation, and further decreases SLO violations by 28\\% in multi-instance deployments with vanilla data-parallel configuration. Compared to the SGLang router with load balancing, it further lowers SLO violations by 12\\% in multi-GPU settings. Under high concurrency and mixed-request scenarios, LAPS improves request throughput by 35\\% serving Qwen2.5-32B model for prefill instance, demonstrating its effectiveness in optimizing heterogeneous LLM serving workloads."",""guid"":""oai:arXiv.org:2601.11589v2"",""categories"":[""cs.DC"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,HOMURA: Taming the Sand-Glass for Time-Constrained LLM Translation via Reinforcement Learning,https://arxiv.org/abs/2601.10187,2026-01-30T05:00:00.000Z,"arXiv:2601.10187v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have achieved remarkable strides in multilingual translation but are hindered by a systemic cross-lingual verbosity bias, rendering them unsuitable for strict time-constrained tasks like subtitling and dubbing. Current prompt-engineering approaches struggle to resolve this conflict between semantic fidelity and rigid temporal feasibility. To bridge this gap, we first introduce Sand-Glass, a benchmark specifically designed to evaluate translation under syllable-level duration constraints. Furthermore, we propose HOMURA, a reinforcement learning framework that explicitly optimizes the trade-off between semantic preservation and temporal compliance. By employing a KL-regularized objective with a novel dynamic syllable-ratio reward, HOMURA effectively ""tames"" the output length. Experimental results demonstrate that our method significantly outperforms strong LLM baselines, achieving precise length control that respects linguistic density hierarchies without compromising semantic adequacy.","{""creator"":""Ziang Cui, Mengran Yu, Tianjiao Li, Chenyu Shi, Yingxuan Shi, Lusheng Zhang, Hongwei Lin"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""HOMURA: Taming the Sand-Glass for Time-Constrained LLM Translation via Reinforcement Learning"",""link"":""https://arxiv.org/abs/2601.10187"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Ziang Cui, Mengran Yu, Tianjiao Li, Chenyu Shi, Yingxuan Shi, Lusheng Zhang, Hongwei Lin"",""content"":""arXiv:2601.10187v2 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) have achieved remarkable strides in multilingual translation but are hindered by a systemic cross-lingual verbosity bias, rendering them unsuitable for strict time-constrained tasks like subtitling and dubbing. Current prompt-engineering approaches struggle to resolve this conflict between semantic fidelity and rigid temporal feasibility. To bridge this gap, we first introduce Sand-Glass, a benchmark specifically designed to evaluate translation under syllable-level duration constraints. Furthermore, we propose HOMURA, a reinforcement learning framework that explicitly optimizes the trade-off between semantic preservation and temporal compliance. By employing a KL-regularized objective with a novel dynamic syllable-ratio reward, HOMURA effectively \""tames\"" the output length. Experimental results demonstrate that our method significantly outperforms strong LLM baselines, achieving precise length control that respects linguistic density hierarchies without compromising semantic adequacy."",""contentSnippet"":""arXiv:2601.10187v2 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) have achieved remarkable strides in multilingual translation but are hindered by a systemic cross-lingual verbosity bias, rendering them unsuitable for strict time-constrained tasks like subtitling and dubbing. Current prompt-engineering approaches struggle to resolve this conflict between semantic fidelity and rigid temporal feasibility. To bridge this gap, we first introduce Sand-Glass, a benchmark specifically designed to evaluate translation under syllable-level duration constraints. Furthermore, we propose HOMURA, a reinforcement learning framework that explicitly optimizes the trade-off between semantic preservation and temporal compliance. By employing a KL-regularized objective with a novel dynamic syllable-ratio reward, HOMURA effectively \""tames\"" the output length. Experimental results demonstrate that our method significantly outperforms strong LLM baselines, achieving precise length control that respects linguistic density hierarchies without compromising semantic adequacy."",""guid"":""oai:arXiv.org:2601.10187v2"",""categories"":[""cs.CL"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Demystifying the Slash Pattern in Attention: The Role of RoPE,https://arxiv.org/abs/2601.08297,2026-01-30T05:00:00.000Z,"arXiv:2601.08297v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) often exhibit slash attention patterns, where attention scores concentrate along the $\Delta$-th sub-diagonal for some offset $\Delta$. These patterns play a key role in passing information across tokens. But why do they emerge? In this paper, we demystify the emergence of these Slash-Dominant Heads (SDHs) from both empirical and theoretical perspectives. First, by analyzing open-source LLMs, we find that SDHs are intrinsic to models and generalize to out-of-distribution prompts. To explain the intrinsic emergence, we analyze the queries, keys, and Rotary Position Embedding (RoPE), which jointly determine attention scores. Our empirical analysis reveals two characteristic conditions of SDHs: (1) Queries and keys are almost rank-one, and (2) RoPE is dominated by medium- and high-frequency components. Under these conditions, queries and keys are nearly identical across tokens, and interactions between medium- and high-frequency components of RoPE give rise to SDHs. Beyond empirical evidence, we theoretically show that these conditions are sufficient to ensure the emergence of SDHs by formalizing them as our modeling assumptions. Particularly, we analyze the training dynamics of a shallow Transformer equipped with RoPE under these conditions, and prove that models trained via gradient descent exhibit SDHs. The SDHs generalize to out-of-distribution prompts.","{""creator"":""Yuan Cheng, Fengzhuo Zhang, Yunlong Hou, Cunxiao Du, Chao Du, Tianyu Pang, Aixin Sun, Zhuoran Yang"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""Demystifying the Slash Pattern in Attention: The Role of RoPE"",""link"":""https://arxiv.org/abs/2601.08297"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Yuan Cheng, Fengzhuo Zhang, Yunlong Hou, Cunxiao Du, Chao Du, Tianyu Pang, Aixin Sun, Zhuoran Yang"",""content"":""arXiv:2601.08297v2 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) often exhibit slash attention patterns, where attention scores concentrate along the $\\Delta$-th sub-diagonal for some offset $\\Delta$. These patterns play a key role in passing information across tokens. But why do they emerge? In this paper, we demystify the emergence of these Slash-Dominant Heads (SDHs) from both empirical and theoretical perspectives. First, by analyzing open-source LLMs, we find that SDHs are intrinsic to models and generalize to out-of-distribution prompts. To explain the intrinsic emergence, we analyze the queries, keys, and Rotary Position Embedding (RoPE), which jointly determine attention scores. Our empirical analysis reveals two characteristic conditions of SDHs: (1) Queries and keys are almost rank-one, and (2) RoPE is dominated by medium- and high-frequency components. Under these conditions, queries and keys are nearly identical across tokens, and interactions between medium- and high-frequency components of RoPE give rise to SDHs. Beyond empirical evidence, we theoretically show that these conditions are sufficient to ensure the emergence of SDHs by formalizing them as our modeling assumptions. Particularly, we analyze the training dynamics of a shallow Transformer equipped with RoPE under these conditions, and prove that models trained via gradient descent exhibit SDHs. The SDHs generalize to out-of-distribution prompts."",""contentSnippet"":""arXiv:2601.08297v2 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) often exhibit slash attention patterns, where attention scores concentrate along the $\\Delta$-th sub-diagonal for some offset $\\Delta$. These patterns play a key role in passing information across tokens. But why do they emerge? In this paper, we demystify the emergence of these Slash-Dominant Heads (SDHs) from both empirical and theoretical perspectives. First, by analyzing open-source LLMs, we find that SDHs are intrinsic to models and generalize to out-of-distribution prompts. To explain the intrinsic emergence, we analyze the queries, keys, and Rotary Position Embedding (RoPE), which jointly determine attention scores. Our empirical analysis reveals two characteristic conditions of SDHs: (1) Queries and keys are almost rank-one, and (2) RoPE is dominated by medium- and high-frequency components. Under these conditions, queries and keys are nearly identical across tokens, and interactions between medium- and high-frequency components of RoPE give rise to SDHs. Beyond empirical evidence, we theoretically show that these conditions are sufficient to ensure the emergence of SDHs by formalizing them as our modeling assumptions. Particularly, we analyze the training dynamics of a shallow Transformer equipped with RoPE under these conditions, and prove that models trained via gradient descent exhibit SDHs. The SDHs generalize to out-of-distribution prompts."",""guid"":""oai:arXiv.org:2601.08297v2"",""categories"":[""cs.LG"",""cs.AI"",""cs.CL""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,VLM-CAD: VLM-Optimized Collaborative Agent Design Workflow for Analog Circuit Sizing,https://arxiv.org/abs/2601.07315,2026-01-30T05:00:00.000Z,"arXiv:2601.07315v3 Announce Type: replace-cross 
Abstract: Analog mixed-signal circuit sizing involves complex trade-offs within high-dimensional design spaces. Existing automatic analog circuit sizing approaches rely solely on netlists, ignoring the circuit schematic, which hinders the cognitive link between the schematic and its performance. Furthermore, the black-box nature of machine learning methods and hallucination risks in large language models fail to provide the necessary ground-truth explainability required for industrial sign-off. To address these challenges, we propose a Vision Language Model-optimized collaborative agent design workflow (VLM-CAD), which analyzes circuits, optimizes DC operating points, performs inference-based sizing, and executes external sizing optimization. We integrate Image2Net to annotate circuit schematics and generate a structured JSON description for precise interpretation by Vision Language Models. Furthermore, we propose an Explainable Trust Region Bayesian Optimization method (ExTuRBO) that employs collaborative warm-start from agent-generated seeds and offers dual-granularity sensitivity analysis for external sizing optimization, supporting a comprehensive final design report. Experiment results on amplifier sizing tasks using 180nm, 90nm, and 45nm Predictive Technology Models demonstrate that VLM-CAD effectively balances power and performance while maintaining physics-based explainability. VLM-CAD meets all specification requirements while maintaining low power consumption in optimizing an amplifier with a complementary input and a class-AB output stage, with a total runtime under 66 minutes across all experiments on two amplifiers.","{""creator"":""Guanyuan Pan, Shuai Wang, Yugui Lin, Tiansheng Zhou, Pietro Li\\`o, Zhenxin Zhao, Yaqi Wang"",""rights"":""http://creativecommons.org/licenses/by-nc-nd/4.0/"",""title"":""VLM-CAD: VLM-Optimized Collaborative Agent Design Workflow for Analog Circuit Sizing"",""link"":""https://arxiv.org/abs/2601.07315"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Guanyuan Pan, Shuai Wang, Yugui Lin, Tiansheng Zhou, Pietro Li\\`o, Zhenxin Zhao, Yaqi Wang"",""content"":""arXiv:2601.07315v3 Announce Type: replace-cross \nAbstract: Analog mixed-signal circuit sizing involves complex trade-offs within high-dimensional design spaces. Existing automatic analog circuit sizing approaches rely solely on netlists, ignoring the circuit schematic, which hinders the cognitive link between the schematic and its performance. Furthermore, the black-box nature of machine learning methods and hallucination risks in large language models fail to provide the necessary ground-truth explainability required for industrial sign-off. To address these challenges, we propose a Vision Language Model-optimized collaborative agent design workflow (VLM-CAD), which analyzes circuits, optimizes DC operating points, performs inference-based sizing, and executes external sizing optimization. We integrate Image2Net to annotate circuit schematics and generate a structured JSON description for precise interpretation by Vision Language Models. Furthermore, we propose an Explainable Trust Region Bayesian Optimization method (ExTuRBO) that employs collaborative warm-start from agent-generated seeds and offers dual-granularity sensitivity analysis for external sizing optimization, supporting a comprehensive final design report. Experiment results on amplifier sizing tasks using 180nm, 90nm, and 45nm Predictive Technology Models demonstrate that VLM-CAD effectively balances power and performance while maintaining physics-based explainability. VLM-CAD meets all specification requirements while maintaining low power consumption in optimizing an amplifier with a complementary input and a class-AB output stage, with a total runtime under 66 minutes across all experiments on two amplifiers."",""contentSnippet"":""arXiv:2601.07315v3 Announce Type: replace-cross \nAbstract: Analog mixed-signal circuit sizing involves complex trade-offs within high-dimensional design spaces. Existing automatic analog circuit sizing approaches rely solely on netlists, ignoring the circuit schematic, which hinders the cognitive link between the schematic and its performance. Furthermore, the black-box nature of machine learning methods and hallucination risks in large language models fail to provide the necessary ground-truth explainability required for industrial sign-off. To address these challenges, we propose a Vision Language Model-optimized collaborative agent design workflow (VLM-CAD), which analyzes circuits, optimizes DC operating points, performs inference-based sizing, and executes external sizing optimization. We integrate Image2Net to annotate circuit schematics and generate a structured JSON description for precise interpretation by Vision Language Models. Furthermore, we propose an Explainable Trust Region Bayesian Optimization method (ExTuRBO) that employs collaborative warm-start from agent-generated seeds and offers dual-granularity sensitivity analysis for external sizing optimization, supporting a comprehensive final design report. Experiment results on amplifier sizing tasks using 180nm, 90nm, and 45nm Predictive Technology Models demonstrate that VLM-CAD effectively balances power and performance while maintaining physics-based explainability. VLM-CAD meets all specification requirements while maintaining low power consumption in optimizing an amplifier with a complementary input and a class-AB output stage, with a total runtime under 66 minutes across all experiments on two amplifiers."",""guid"":""oai:arXiv.org:2601.07315v3"",""categories"":[""cs.MA"",""cs.AI"",""cs.AR""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Diffusion Timbre Transfer Via Mutual Information Guided Inpainting,https://arxiv.org/abs/2601.01294,2026-01-30T05:00:00.000Z,"arXiv:2601.01294v2 Announce Type: replace-cross 
Abstract: We study timbre transfer as an inference-time editing problem for music audio. Starting from a strong pre-trained latent diffusion model, we introduce a lightweight procedure that requires no additional training: (i) a dimension-wise noise injection that targets latent channels most informative of instrument identity, and (ii) an early-step clamping mechanism that re-imposes the input's melodic and rhythmic structure during reverse diffusion. The method operates directly on audio latents and is compatible with text/audio conditioning (e.g., CLAP). We discuss design choices,analyze trade-offs between timbral change and structural preservation, and show that simple inference-time controls can meaningfully steer pre-trained models for style-transfer use cases.","{""creator"":""Ching Ho Lee, Javier Nistal, Stefan Lattner, Marco Pasini, George Fazekas"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""Diffusion Timbre Transfer Via Mutual Information Guided Inpainting"",""link"":""https://arxiv.org/abs/2601.01294"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Ching Ho Lee, Javier Nistal, Stefan Lattner, Marco Pasini, George Fazekas"",""content"":""arXiv:2601.01294v2 Announce Type: replace-cross \nAbstract: We study timbre transfer as an inference-time editing problem for music audio. Starting from a strong pre-trained latent diffusion model, we introduce a lightweight procedure that requires no additional training: (i) a dimension-wise noise injection that targets latent channels most informative of instrument identity, and (ii) an early-step clamping mechanism that re-imposes the input's melodic and rhythmic structure during reverse diffusion. The method operates directly on audio latents and is compatible with text/audio conditioning (e.g., CLAP). We discuss design choices,analyze trade-offs between timbral change and structural preservation, and show that simple inference-time controls can meaningfully steer pre-trained models for style-transfer use cases."",""contentSnippet"":""arXiv:2601.01294v2 Announce Type: replace-cross \nAbstract: We study timbre transfer as an inference-time editing problem for music audio. Starting from a strong pre-trained latent diffusion model, we introduce a lightweight procedure that requires no additional training: (i) a dimension-wise noise injection that targets latent channels most informative of instrument identity, and (ii) an early-step clamping mechanism that re-imposes the input's melodic and rhythmic structure during reverse diffusion. The method operates directly on audio latents and is compatible with text/audio conditioning (e.g., CLAP). We discuss design choices,analyze trade-offs between timbral change and structural preservation, and show that simple inference-time controls can meaningfully steer pre-trained models for style-transfer use cases."",""guid"":""oai:arXiv.org:2601.01294v2"",""categories"":[""cs.SD"",""cs.AI"",""eess.AS""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering,https://arxiv.org/abs/2601.00269,2026-01-30T05:00:00.000Z,"arXiv:2601.00269v2 Announce Type: replace-cross 
Abstract: Faithfulness hallucinations in VQA occur when vision-language models produce fluent yet visually ungrounded answers, severely undermining their reliability in safety-critical applications. Existing detection methods mainly fall into two categories: external verification approaches relying on auxiliary models or knowledge bases, and uncertainty-driven approaches using repeated sampling or uncertainty estimates. The former suffer from high computational overhead and are limited by external resource quality, while the latter capture only limited facets of model uncertainty and fail to sufficiently explore the rich internal signals associated with the diverse failure modes. Both paradigms thus have inherent limitations in efficiency, robustness, and detection performance. To address these challenges, we propose FaithSCAN: a lightweight network that detects hallucinations by exploiting rich internal signals of VLMs, including token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features. These signals are fused via branch-wise evidence encoding and uncertainty-aware attention. We also extend the LLM-as-a-Judge paradigm to VQA hallucination and propose a low-cost strategy to automatically generate model-dependent supervision signals, enabling supervised training without costly human labels while maintaining high detection accuracy. Experiments on multiple VQA benchmarks show that FaithSCAN significantly outperforms existing methods in both effectiveness and efficiency. In-depth analysis shows hallucinations arise from systematic internal state variations in visual perception, cross-modal reasoning, and language decoding. Different internal signals provide complementary diagnostic cues, and hallucination patterns vary across VLM architectures, offering new insights into the underlying causes of multimodal hallucinations.","{""creator"":""Chaodong Tong, Qi Zhang, Chen Li, Lei Jiang, Yanbing Liu"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering"",""link"":""https://arxiv.org/abs/2601.00269"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Chaodong Tong, Qi Zhang, Chen Li, Lei Jiang, Yanbing Liu"",""content"":""arXiv:2601.00269v2 Announce Type: replace-cross \nAbstract: Faithfulness hallucinations in VQA occur when vision-language models produce fluent yet visually ungrounded answers, severely undermining their reliability in safety-critical applications. Existing detection methods mainly fall into two categories: external verification approaches relying on auxiliary models or knowledge bases, and uncertainty-driven approaches using repeated sampling or uncertainty estimates. The former suffer from high computational overhead and are limited by external resource quality, while the latter capture only limited facets of model uncertainty and fail to sufficiently explore the rich internal signals associated with the diverse failure modes. Both paradigms thus have inherent limitations in efficiency, robustness, and detection performance. To address these challenges, we propose FaithSCAN: a lightweight network that detects hallucinations by exploiting rich internal signals of VLMs, including token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features. These signals are fused via branch-wise evidence encoding and uncertainty-aware attention. We also extend the LLM-as-a-Judge paradigm to VQA hallucination and propose a low-cost strategy to automatically generate model-dependent supervision signals, enabling supervised training without costly human labels while maintaining high detection accuracy. Experiments on multiple VQA benchmarks show that FaithSCAN significantly outperforms existing methods in both effectiveness and efficiency. In-depth analysis shows hallucinations arise from systematic internal state variations in visual perception, cross-modal reasoning, and language decoding. Different internal signals provide complementary diagnostic cues, and hallucination patterns vary across VLM architectures, offering new insights into the underlying causes of multimodal hallucinations."",""contentSnippet"":""arXiv:2601.00269v2 Announce Type: replace-cross \nAbstract: Faithfulness hallucinations in VQA occur when vision-language models produce fluent yet visually ungrounded answers, severely undermining their reliability in safety-critical applications. Existing detection methods mainly fall into two categories: external verification approaches relying on auxiliary models or knowledge bases, and uncertainty-driven approaches using repeated sampling or uncertainty estimates. The former suffer from high computational overhead and are limited by external resource quality, while the latter capture only limited facets of model uncertainty and fail to sufficiently explore the rich internal signals associated with the diverse failure modes. Both paradigms thus have inherent limitations in efficiency, robustness, and detection performance. To address these challenges, we propose FaithSCAN: a lightweight network that detects hallucinations by exploiting rich internal signals of VLMs, including token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features. These signals are fused via branch-wise evidence encoding and uncertainty-aware attention. We also extend the LLM-as-a-Judge paradigm to VQA hallucination and propose a low-cost strategy to automatically generate model-dependent supervision signals, enabling supervised training without costly human labels while maintaining high detection accuracy. Experiments on multiple VQA benchmarks show that FaithSCAN significantly outperforms existing methods in both effectiveness and efficiency. In-depth analysis shows hallucinations arise from systematic internal state variations in visual perception, cross-modal reasoning, and language decoding. Different internal signals provide complementary diagnostic cues, and hallucination patterns vary across VLM architectures, offering new insights into the underlying causes of multimodal hallucinations."",""guid"":""oai:arXiv.org:2601.00269v2"",""categories"":[""cs.CV"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Geometric Scaling of Bayesian Inference in LLMs,https://arxiv.org/abs/2512.23752,2026-01-30T05:00:00.000Z,"arXiv:2512.23752v3 Announce Type: replace-cross 
Abstract: Recent work has shown that small transformers trained in controlled ""wind-tunnel'' settings can implement exact Bayesian inference, and that their training dynamics produce a geometric substrate -- low-dimensional value manifolds and progressively orthogonal keys -- that encodes posterior structure. We investigate whether this geometric signature persists in production-grade language models. Across Pythia, Phi-2, Llama-3, and Mistral families, we find that last-layer value representations organize along a single dominant axis whose position strongly correlates with predictive entropy, and that domain-restricted prompts collapse this structure into the same low-dimensional manifolds observed in synthetic settings.
  To probe the role of this geometry, we perform targeted interventions on the entropy-aligned axis of Pythia-410M during in-context learning. Removing or perturbing this axis selectively disrupts the local uncertainty geometry, whereas matched random-axis interventions leave it intact. However, these single-layer manipulations do not produce proportionally specific degradation in Bayesian-like behavior, indicating that the geometry is a privileged readout of uncertainty rather than a singular computational bottleneck. Taken together, our results show that modern language models preserve the geometric substrate that enables Bayesian inference in wind tunnels, and organize their approximate Bayesian updates along this substrate.","{""creator"":""Naman Agarwal, Siddhartha R. Dalal, Vishal Misra"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""Geometric Scaling of Bayesian Inference in LLMs"",""link"":""https://arxiv.org/abs/2512.23752"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Naman Agarwal, Siddhartha R. Dalal, Vishal Misra"",""content"":""arXiv:2512.23752v3 Announce Type: replace-cross \nAbstract: Recent work has shown that small transformers trained in controlled \""wind-tunnel'' settings can implement exact Bayesian inference, and that their training dynamics produce a geometric substrate -- low-dimensional value manifolds and progressively orthogonal keys -- that encodes posterior structure. We investigate whether this geometric signature persists in production-grade language models. Across Pythia, Phi-2, Llama-3, and Mistral families, we find that last-layer value representations organize along a single dominant axis whose position strongly correlates with predictive entropy, and that domain-restricted prompts collapse this structure into the same low-dimensional manifolds observed in synthetic settings.\n  To probe the role of this geometry, we perform targeted interventions on the entropy-aligned axis of Pythia-410M during in-context learning. Removing or perturbing this axis selectively disrupts the local uncertainty geometry, whereas matched random-axis interventions leave it intact. However, these single-layer manipulations do not produce proportionally specific degradation in Bayesian-like behavior, indicating that the geometry is a privileged readout of uncertainty rather than a singular computational bottleneck. Taken together, our results show that modern language models preserve the geometric substrate that enables Bayesian inference in wind tunnels, and organize their approximate Bayesian updates along this substrate."",""contentSnippet"":""arXiv:2512.23752v3 Announce Type: replace-cross \nAbstract: Recent work has shown that small transformers trained in controlled \""wind-tunnel'' settings can implement exact Bayesian inference, and that their training dynamics produce a geometric substrate -- low-dimensional value manifolds and progressively orthogonal keys -- that encodes posterior structure. We investigate whether this geometric signature persists in production-grade language models. Across Pythia, Phi-2, Llama-3, and Mistral families, we find that last-layer value representations organize along a single dominant axis whose position strongly correlates with predictive entropy, and that domain-restricted prompts collapse this structure into the same low-dimensional manifolds observed in synthetic settings.\n  To probe the role of this geometry, we perform targeted interventions on the entropy-aligned axis of Pythia-410M during in-context learning. Removing or perturbing this axis selectively disrupts the local uncertainty geometry, whereas matched random-axis interventions leave it intact. However, these single-layer manipulations do not produce proportionally specific degradation in Bayesian-like behavior, indicating that the geometry is a privileged readout of uncertainty rather than a singular computational bottleneck. Taken together, our results show that modern language models preserve the geometric substrate that enables Bayesian inference in wind tunnels, and organize their approximate Bayesian updates along this substrate."",""guid"":""oai:arXiv.org:2512.23752v3"",""categories"":[""cs.LG"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature,https://arxiv.org/abs/2512.23565,2026-01-30T05:00:00.000Z,"arXiv:2512.23565v5 Announce Type: replace-cross 
Abstract: The integration of Multimodal Large Language Models (MLLMs) into chemistry promises to revolutionize scientific discovery, yet their ability to comprehend the dense, graphical language of reactions within authentic literature remains underexplored. Here, we introduce RxnBench, a multi-tiered benchmark designed to rigorously evaluate MLLMs on chemical reaction understanding from scientific PDFs. RxnBench comprises two tasks: Single-Figure QA (SF-QA), which tests fine-grained visual perception and mechanistic reasoning using 1,525 questions derived from 305 curated reaction schemes, and Full-Document QA (FD-QA), which challenges models to synthesize information from 108 articles, requiring cross-modal integration of text, schemes, and tables. Our evaluation of MLLMs reveals a critical capability gap: while models excel at extracting explicit text, they struggle with deep chemical logic and precise structural recognition. Notably, models with inference-time reasoning significantly outperform standard architectures, yet none achieve 50\% accuracy on FD-QA. These findings underscore the urgent need for domain-specific visual encoders and stronger reasoning engines to advance autonomous AI chemists.","{""creator"":""Hanzheng Li, Xi Fang, Yixuan Li, Chaozheng Huang, Junjie Wang, Xi Wang, Hongzhe Bai, Bojun Hao, Shenyu Lin, Huiqi Liang, Linfeng Zhang, Guolin Ke"",""rights"":""http://creativecommons.org/licenses/by-nc-sa/4.0/"",""title"":""RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature"",""link"":""https://arxiv.org/abs/2512.23565"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Hanzheng Li, Xi Fang, Yixuan Li, Chaozheng Huang, Junjie Wang, Xi Wang, Hongzhe Bai, Bojun Hao, Shenyu Lin, Huiqi Liang, Linfeng Zhang, Guolin Ke"",""content"":""arXiv:2512.23565v5 Announce Type: replace-cross \nAbstract: The integration of Multimodal Large Language Models (MLLMs) into chemistry promises to revolutionize scientific discovery, yet their ability to comprehend the dense, graphical language of reactions within authentic literature remains underexplored. Here, we introduce RxnBench, a multi-tiered benchmark designed to rigorously evaluate MLLMs on chemical reaction understanding from scientific PDFs. RxnBench comprises two tasks: Single-Figure QA (SF-QA), which tests fine-grained visual perception and mechanistic reasoning using 1,525 questions derived from 305 curated reaction schemes, and Full-Document QA (FD-QA), which challenges models to synthesize information from 108 articles, requiring cross-modal integration of text, schemes, and tables. Our evaluation of MLLMs reveals a critical capability gap: while models excel at extracting explicit text, they struggle with deep chemical logic and precise structural recognition. Notably, models with inference-time reasoning significantly outperform standard architectures, yet none achieve 50\\% accuracy on FD-QA. These findings underscore the urgent need for domain-specific visual encoders and stronger reasoning engines to advance autonomous AI chemists."",""contentSnippet"":""arXiv:2512.23565v5 Announce Type: replace-cross \nAbstract: The integration of Multimodal Large Language Models (MLLMs) into chemistry promises to revolutionize scientific discovery, yet their ability to comprehend the dense, graphical language of reactions within authentic literature remains underexplored. Here, we introduce RxnBench, a multi-tiered benchmark designed to rigorously evaluate MLLMs on chemical reaction understanding from scientific PDFs. RxnBench comprises two tasks: Single-Figure QA (SF-QA), which tests fine-grained visual perception and mechanistic reasoning using 1,525 questions derived from 305 curated reaction schemes, and Full-Document QA (FD-QA), which challenges models to synthesize information from 108 articles, requiring cross-modal integration of text, schemes, and tables. Our evaluation of MLLMs reveals a critical capability gap: while models excel at extracting explicit text, they struggle with deep chemical logic and precise structural recognition. Notably, models with inference-time reasoning significantly outperform standard architectures, yet none achieve 50\\% accuracy on FD-QA. These findings underscore the urgent need for domain-specific visual encoders and stronger reasoning engines to advance autonomous AI chemists."",""guid"":""oai:arXiv.org:2512.23565v5"",""categories"":[""cs.CV"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,The Law of Multi-Model Collaboration: Scaling Limits of Model Ensembling for Large Language Models,https://arxiv.org/abs/2512.23340,2026-01-30T05:00:00.000Z,"arXiv:2512.23340v2 Announce Type: replace-cross 
Abstract: Recent advances in large language models (LLMs) have been largely driven by scaling laws for individual models, which predict performance improvements as model parameters and data volume increase. However, the capabilities of any single LLM are inherently bounded. One solution originates from intricate interactions among multiple LLMs, rendering their collective performance surpasses that of any constituent model. Despite the rapid proliferation of multi-model integration techniques such as model routing and post-hoc ensembling, a unifying theoretical framework of performance scaling for multi-model collaboration remains absent. In this work, we propose the Law of Multi-model Collaboration, a scaling law that predicts the performance limits of LLM ensembles based on their aggregated parameter budget. To quantify the intrinsic upper bound of multi-model collaboration, we adopt a method-agnostic formulation and assume an idealized integration oracle where the total cross-entropy loss of each sample is determined by the minimum loss of any model in the model pool. Experimental results reveal that multi-model systems follow a power-law scaling with respect to the total parameter count, exhibiting a more significant improvement trend and a lower theoretical loss floor compared to single model scaling. Moreover, ensembles of heterogeneous model families achieve better performance scaling than those formed within a single model family, indicating that model diversity is a primary driver of collaboration gains. These findings suggest that model collaboration represents a critical axis for extending the intelligence frontier of LLMs.","{""creator"":""Dakuan Lu, Jiaqi Zhang, Cheng Yuan, Jiawei Shao, Xuelong Li"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""The Law of Multi-Model Collaboration: Scaling Limits of Model Ensembling for Large Language Models"",""link"":""https://arxiv.org/abs/2512.23340"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Dakuan Lu, Jiaqi Zhang, Cheng Yuan, Jiawei Shao, Xuelong Li"",""content"":""arXiv:2512.23340v2 Announce Type: replace-cross \nAbstract: Recent advances in large language models (LLMs) have been largely driven by scaling laws for individual models, which predict performance improvements as model parameters and data volume increase. However, the capabilities of any single LLM are inherently bounded. One solution originates from intricate interactions among multiple LLMs, rendering their collective performance surpasses that of any constituent model. Despite the rapid proliferation of multi-model integration techniques such as model routing and post-hoc ensembling, a unifying theoretical framework of performance scaling for multi-model collaboration remains absent. In this work, we propose the Law of Multi-model Collaboration, a scaling law that predicts the performance limits of LLM ensembles based on their aggregated parameter budget. To quantify the intrinsic upper bound of multi-model collaboration, we adopt a method-agnostic formulation and assume an idealized integration oracle where the total cross-entropy loss of each sample is determined by the minimum loss of any model in the model pool. Experimental results reveal that multi-model systems follow a power-law scaling with respect to the total parameter count, exhibiting a more significant improvement trend and a lower theoretical loss floor compared to single model scaling. Moreover, ensembles of heterogeneous model families achieve better performance scaling than those formed within a single model family, indicating that model diversity is a primary driver of collaboration gains. These findings suggest that model collaboration represents a critical axis for extending the intelligence frontier of LLMs."",""contentSnippet"":""arXiv:2512.23340v2 Announce Type: replace-cross \nAbstract: Recent advances in large language models (LLMs) have been largely driven by scaling laws for individual models, which predict performance improvements as model parameters and data volume increase. However, the capabilities of any single LLM are inherently bounded. One solution originates from intricate interactions among multiple LLMs, rendering their collective performance surpasses that of any constituent model. Despite the rapid proliferation of multi-model integration techniques such as model routing and post-hoc ensembling, a unifying theoretical framework of performance scaling for multi-model collaboration remains absent. In this work, we propose the Law of Multi-model Collaboration, a scaling law that predicts the performance limits of LLM ensembles based on their aggregated parameter budget. To quantify the intrinsic upper bound of multi-model collaboration, we adopt a method-agnostic formulation and assume an idealized integration oracle where the total cross-entropy loss of each sample is determined by the minimum loss of any model in the model pool. Experimental results reveal that multi-model systems follow a power-law scaling with respect to the total parameter count, exhibiting a more significant improvement trend and a lower theoretical loss floor compared to single model scaling. Moreover, ensembles of heterogeneous model families achieve better performance scaling than those formed within a single model family, indicating that model diversity is a primary driver of collaboration gains. These findings suggest that model collaboration represents a critical axis for extending the intelligence frontier of LLMs."",""guid"":""oai:arXiv.org:2512.23340v2"",""categories"":[""cs.LG"",""cs.AI"",""cs.MA""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Gradient Dynamics of Attention: How Cross-Entropy Sculpts Bayesian Manifolds,https://arxiv.org/abs/2512.22473,2026-01-30T05:00:00.000Z,"arXiv:2512.22473v3 Announce Type: replace-cross 
Abstract: Transformers empirically perform precise probabilistic reasoning in carefully constructed ``Bayesian wind tunnels'' and in large-scale language models, yet the mechanisms by which gradient-based learning creates the required internal geometry remain opaque. We provide a complete first-order analysis of how cross-entropy training reshapes attention scores and value vectors in a transformer attention head. Our core result is an \emph{advantage-based routing law} for attention scores, \[ \frac{\partial L}{\partial s_{ij}} = \alpha_{ij}\bigl(b_{ij}-\mathbb{E}_{\alpha_i}[b]\bigr), \qquad b_{ij} := u_i^\top v_j, \] coupled with a \emph{responsibility-weighted update} for values, \[ \Delta v_j = -\eta\sum_i \alpha_{ij} u_i, \] where $u_i$ is the upstream gradient at position $i$ and $\alpha_{ij}$ are attention weights. These equations induce a positive feedback loop in which routing and content specialize together: queries route more strongly to values that are above-average for their error signal, and those values are pulled toward the queries that use them. We show that this coupled specialization behaves like a two-timescale EM procedure: attention weights implement an E-step (soft responsibilities), while values implement an M-step (responsibility-weighted prototype updates), with queries and keys adjusting the hypothesis frame. Through controlled simulations, including a sticky Markov-chain task where we compare a closed-form EM-style update to standard SGD, we demonstrate that the same gradient dynamics that minimize cross-entropy also sculpt the low-dimensional manifolds identified in our companion work as implementing Bayesian inference. This yields a unified picture in which optimization (gradient flow) gives rise to geometry (Bayesian manifolds), which in turn supports function (in-context probabilistic reasoning).","{""creator"":""Naman Agarwal, Siddhartha R. Dalal, Vishal Misra"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""Gradient Dynamics of Attention: How Cross-Entropy Sculpts Bayesian Manifolds"",""link"":""https://arxiv.org/abs/2512.22473"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Naman Agarwal, Siddhartha R. Dalal, Vishal Misra"",""content"":""arXiv:2512.22473v3 Announce Type: replace-cross \nAbstract: Transformers empirically perform precise probabilistic reasoning in carefully constructed ``Bayesian wind tunnels'' and in large-scale language models, yet the mechanisms by which gradient-based learning creates the required internal geometry remain opaque. We provide a complete first-order analysis of how cross-entropy training reshapes attention scores and value vectors in a transformer attention head. Our core result is an \\emph{advantage-based routing law} for attention scores, \\[ \\frac{\\partial L}{\\partial s_{ij}} = \\alpha_{ij}\\bigl(b_{ij}-\\mathbb{E}_{\\alpha_i}[b]\\bigr), \\qquad b_{ij} := u_i^\\top v_j, \\] coupled with a \\emph{responsibility-weighted update} for values, \\[ \\Delta v_j = -\\eta\\sum_i \\alpha_{ij} u_i, \\] where $u_i$ is the upstream gradient at position $i$ and $\\alpha_{ij}$ are attention weights. These equations induce a positive feedback loop in which routing and content specialize together: queries route more strongly to values that are above-average for their error signal, and those values are pulled toward the queries that use them. We show that this coupled specialization behaves like a two-timescale EM procedure: attention weights implement an E-step (soft responsibilities), while values implement an M-step (responsibility-weighted prototype updates), with queries and keys adjusting the hypothesis frame. Through controlled simulations, including a sticky Markov-chain task where we compare a closed-form EM-style update to standard SGD, we demonstrate that the same gradient dynamics that minimize cross-entropy also sculpt the low-dimensional manifolds identified in our companion work as implementing Bayesian inference. This yields a unified picture in which optimization (gradient flow) gives rise to geometry (Bayesian manifolds), which in turn supports function (in-context probabilistic reasoning)."",""contentSnippet"":""arXiv:2512.22473v3 Announce Type: replace-cross \nAbstract: Transformers empirically perform precise probabilistic reasoning in carefully constructed ``Bayesian wind tunnels'' and in large-scale language models, yet the mechanisms by which gradient-based learning creates the required internal geometry remain opaque. We provide a complete first-order analysis of how cross-entropy training reshapes attention scores and value vectors in a transformer attention head. Our core result is an \\emph{advantage-based routing law} for attention scores, \\[ \\frac{\\partial L}{\\partial s_{ij}} = \\alpha_{ij}\\bigl(b_{ij}-\\mathbb{E}_{\\alpha_i}[b]\\bigr), \\qquad b_{ij} := u_i^\\top v_j, \\] coupled with a \\emph{responsibility-weighted update} for values, \\[ \\Delta v_j = -\\eta\\sum_i \\alpha_{ij} u_i, \\] where $u_i$ is the upstream gradient at position $i$ and $\\alpha_{ij}$ are attention weights. These equations induce a positive feedback loop in which routing and content specialize together: queries route more strongly to values that are above-average for their error signal, and those values are pulled toward the queries that use them. We show that this coupled specialization behaves like a two-timescale EM procedure: attention weights implement an E-step (soft responsibilities), while values implement an M-step (responsibility-weighted prototype updates), with queries and keys adjusting the hypothesis frame. Through controlled simulations, including a sticky Markov-chain task where we compare a closed-form EM-style update to standard SGD, we demonstrate that the same gradient dynamics that minimize cross-entropy also sculpt the low-dimensional manifolds identified in our companion work as implementing Bayesian inference. This yields a unified picture in which optimization (gradient flow) gives rise to geometry (Bayesian manifolds), which in turn supports function (in-context probabilistic reasoning)."",""guid"":""oai:arXiv.org:2512.22473v3"",""categories"":[""stat.ML"",""cs.AI"",""cs.LG""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,The Bayesian Geometry of Transformer Attention,https://arxiv.org/abs/2512.22471,2026-01-30T05:00:00.000Z,"arXiv:2512.22471v3 Announce Type: replace-cross 
Abstract: Transformers often appear to perform Bayesian reasoning in context, but verifying this rigorously has been impossible: natural data lack analytic posteriors, and large models conflate reasoning with memorization. We address this by constructing \emph{Bayesian wind tunnels} -- controlled environments where the true posterior is known in closed form and memorization is provably impossible. In these settings, small transformers reproduce Bayesian posteriors with $10^{-3}$-$10^{-4}$ bit accuracy, while capacity-matched MLPs fail by orders of magnitude, establishing a clear architectural separation.
  Across two tasks -- bijection elimination and Hidden Markov Model (HMM) state tracking -- we find that transformers implement Bayesian inference through a consistent geometric mechanism: residual streams serve as the belief substrate, feed-forward networks perform the posterior update, and attention provides content-addressable routing. Geometric diagnostics reveal orthogonal key bases, progressive query-key alignment, and a low-dimensional value manifold parameterized by posterior entropy. During training this manifold unfurls while attention patterns remain stable, a \emph{frame-precision dissociation} predicted by recent gradient analyses.
  Taken together, these results demonstrate that hierarchical attention realizes Bayesian inference by geometric design, explaining both the necessity of attention and the failure of flat architectures. Bayesian wind tunnels provide a foundation for mechanistically connecting small, verifiable systems to reasoning phenomena observed in large language models.","{""creator"":""Naman Agarwal, Siddhartha R. Dalal, Vishal Misra"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""The Bayesian Geometry of Transformer Attention"",""link"":""https://arxiv.org/abs/2512.22471"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Naman Agarwal, Siddhartha R. Dalal, Vishal Misra"",""content"":""arXiv:2512.22471v3 Announce Type: replace-cross \nAbstract: Transformers often appear to perform Bayesian reasoning in context, but verifying this rigorously has been impossible: natural data lack analytic posteriors, and large models conflate reasoning with memorization. We address this by constructing \\emph{Bayesian wind tunnels} -- controlled environments where the true posterior is known in closed form and memorization is provably impossible. In these settings, small transformers reproduce Bayesian posteriors with $10^{-3}$-$10^{-4}$ bit accuracy, while capacity-matched MLPs fail by orders of magnitude, establishing a clear architectural separation.\n  Across two tasks -- bijection elimination and Hidden Markov Model (HMM) state tracking -- we find that transformers implement Bayesian inference through a consistent geometric mechanism: residual streams serve as the belief substrate, feed-forward networks perform the posterior update, and attention provides content-addressable routing. Geometric diagnostics reveal orthogonal key bases, progressive query-key alignment, and a low-dimensional value manifold parameterized by posterior entropy. During training this manifold unfurls while attention patterns remain stable, a \\emph{frame-precision dissociation} predicted by recent gradient analyses.\n  Taken together, these results demonstrate that hierarchical attention realizes Bayesian inference by geometric design, explaining both the necessity of attention and the failure of flat architectures. Bayesian wind tunnels provide a foundation for mechanistically connecting small, verifiable systems to reasoning phenomena observed in large language models."",""contentSnippet"":""arXiv:2512.22471v3 Announce Type: replace-cross \nAbstract: Transformers often appear to perform Bayesian reasoning in context, but verifying this rigorously has been impossible: natural data lack analytic posteriors, and large models conflate reasoning with memorization. We address this by constructing \\emph{Bayesian wind tunnels} -- controlled environments where the true posterior is known in closed form and memorization is provably impossible. In these settings, small transformers reproduce Bayesian posteriors with $10^{-3}$-$10^{-4}$ bit accuracy, while capacity-matched MLPs fail by orders of magnitude, establishing a clear architectural separation.\n  Across two tasks -- bijection elimination and Hidden Markov Model (HMM) state tracking -- we find that transformers implement Bayesian inference through a consistent geometric mechanism: residual streams serve as the belief substrate, feed-forward networks perform the posterior update, and attention provides content-addressable routing. Geometric diagnostics reveal orthogonal key bases, progressive query-key alignment, and a low-dimensional value manifold parameterized by posterior entropy. During training this manifold unfurls while attention patterns remain stable, a \\emph{frame-precision dissociation} predicted by recent gradient analyses.\n  Taken together, these results demonstrate that hierarchical attention realizes Bayesian inference by geometric design, explaining both the necessity of attention and the failure of flat architectures. Bayesian wind tunnels provide a foundation for mechanistically connecting small, verifiable systems to reasoning phenomena observed in large language models."",""guid"":""oai:arXiv.org:2512.22471v3"",""categories"":[""cs.LG"",""cs.AI"",""stat.ML""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,"Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs",https://arxiv.org/abs/2512.20573,2026-01-30T05:00:00.000Z,"arXiv:2512.20573v3 Announce Type: replace-cross 
Abstract: Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff. We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers. Our core insight is that dLLM's speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding. We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length. It ""fails fast"" by spending minimal compute in hard-to-speculate regions to shrink speculation latency and ""wins big"" by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!). Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9$\times$ speedup over vanilla decoding, 1.7$\times$ over the best naive dLLM drafter, and 1.7$\times$ over EAGLE-3 across diverse models and workloads. We open-source FailFast at https://github.com/ruipeterpan/failfast.","{""creator"":""Rui Pan, Zhuofu Chen, Hongyi Liu, Arvind Krishnamurthy, Ravi Netravali"",""rights"":""http://creativecommons.org/licenses/by-sa/4.0/"",""title"":""Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs"",""link"":""https://arxiv.org/abs/2512.20573"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Rui Pan, Zhuofu Chen, Hongyi Liu, Arvind Krishnamurthy, Ravi Netravali"",""content"":""arXiv:2512.20573v3 Announce Type: replace-cross \nAbstract: Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff. We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers. Our core insight is that dLLM's speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding. We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length. It \""fails fast\"" by spending minimal compute in hard-to-speculate regions to shrink speculation latency and \""wins big\"" by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!). Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9$\\times$ speedup over vanilla decoding, 1.7$\\times$ over the best naive dLLM drafter, and 1.7$\\times$ over EAGLE-3 across diverse models and workloads. We open-source FailFast at https://github.com/ruipeterpan/failfast."",""contentSnippet"":""arXiv:2512.20573v3 Announce Type: replace-cross \nAbstract: Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff. We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers. Our core insight is that dLLM's speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding. We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length. It \""fails fast\"" by spending minimal compute in hard-to-speculate regions to shrink speculation latency and \""wins big\"" by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!). Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9$\\times$ speedup over vanilla decoding, 1.7$\\times$ over the best naive dLLM drafter, and 1.7$\\times$ over EAGLE-3 across diverse models and workloads. We open-source FailFast at https://github.com/ruipeterpan/failfast."",""guid"":""oai:arXiv.org:2512.20573v3"",""categories"":[""cs.LG"",""cs.AI"",""cs.DC""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning,https://arxiv.org/abs/2512.19920,2026-01-30T05:00:00.000Z,"arXiv:2512.19920v3 Announce Type: replace-cross 
Abstract: LLM deployment in critical domains is currently impeded by persistent hallucinations--generating plausible but factually incorrect assertions. While scaling laws drove significant improvements in general capabilities, theoretical frameworks suggest hallucination is not merely stochastic error but a predictable statistical consequence of training objectives prioritizing mimicking data distribution over epistemic honesty. Standard RLVR paradigms, utilizing binary reward signals, inadvertently incentivize models as good test-takers rather than honest communicators, encouraging guessing whenever correctness probability exceeds zero. This paper presents an exhaustive investigation into behavioral calibration, which incentivizes models to stochastically admit uncertainty by abstaining when not confident, aligning model behavior with accuracy. Synthesizing recent advances, we propose and evaluate training interventions optimizing strictly proper scoring rules for models to output a calibrated probability of correctness. Our methods enable models to either abstain from producing a complete response or flag individual claims where uncertainty remains. Utilizing Qwen3-4B-Instruct, empirical analysis reveals behavior-calibrated reinforcement learning allows smaller models to surpass frontier models in uncertainty quantification--a transferable meta-skill decouplable from raw predictive accuracy. Trained on math reasoning tasks, our model's log-scale Accuracy-to-Hallucination Ratio gain (0.806) exceeds GPT-5's (0.207) in a challenging in-domain evaluation (BeyondAIME). Moreover, in cross-domain factual QA (SimpleQA), our 4B LLM achieves zero-shot calibration error on par with frontier models including Grok-4 and Gemini-2.5-Pro, even though its factual accuracy is much lower.","{""creator"":""Jiayun Wu, Jiashuo Liu, Zhiyuan Zeng, Tianyang Zhan, Tianle Cai, Wenhao Huang"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning"",""link"":""https://arxiv.org/abs/2512.19920"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Jiayun Wu, Jiashuo Liu, Zhiyuan Zeng, Tianyang Zhan, Tianle Cai, Wenhao Huang"",""content"":""arXiv:2512.19920v3 Announce Type: replace-cross \nAbstract: LLM deployment in critical domains is currently impeded by persistent hallucinations--generating plausible but factually incorrect assertions. While scaling laws drove significant improvements in general capabilities, theoretical frameworks suggest hallucination is not merely stochastic error but a predictable statistical consequence of training objectives prioritizing mimicking data distribution over epistemic honesty. Standard RLVR paradigms, utilizing binary reward signals, inadvertently incentivize models as good test-takers rather than honest communicators, encouraging guessing whenever correctness probability exceeds zero. This paper presents an exhaustive investigation into behavioral calibration, which incentivizes models to stochastically admit uncertainty by abstaining when not confident, aligning model behavior with accuracy. Synthesizing recent advances, we propose and evaluate training interventions optimizing strictly proper scoring rules for models to output a calibrated probability of correctness. Our methods enable models to either abstain from producing a complete response or flag individual claims where uncertainty remains. Utilizing Qwen3-4B-Instruct, empirical analysis reveals behavior-calibrated reinforcement learning allows smaller models to surpass frontier models in uncertainty quantification--a transferable meta-skill decouplable from raw predictive accuracy. Trained on math reasoning tasks, our model's log-scale Accuracy-to-Hallucination Ratio gain (0.806) exceeds GPT-5's (0.207) in a challenging in-domain evaluation (BeyondAIME). Moreover, in cross-domain factual QA (SimpleQA), our 4B LLM achieves zero-shot calibration error on par with frontier models including Grok-4 and Gemini-2.5-Pro, even though its factual accuracy is much lower."",""contentSnippet"":""arXiv:2512.19920v3 Announce Type: replace-cross \nAbstract: LLM deployment in critical domains is currently impeded by persistent hallucinations--generating plausible but factually incorrect assertions. While scaling laws drove significant improvements in general capabilities, theoretical frameworks suggest hallucination is not merely stochastic error but a predictable statistical consequence of training objectives prioritizing mimicking data distribution over epistemic honesty. Standard RLVR paradigms, utilizing binary reward signals, inadvertently incentivize models as good test-takers rather than honest communicators, encouraging guessing whenever correctness probability exceeds zero. This paper presents an exhaustive investigation into behavioral calibration, which incentivizes models to stochastically admit uncertainty by abstaining when not confident, aligning model behavior with accuracy. Synthesizing recent advances, we propose and evaluate training interventions optimizing strictly proper scoring rules for models to output a calibrated probability of correctness. Our methods enable models to either abstain from producing a complete response or flag individual claims where uncertainty remains. Utilizing Qwen3-4B-Instruct, empirical analysis reveals behavior-calibrated reinforcement learning allows smaller models to surpass frontier models in uncertainty quantification--a transferable meta-skill decouplable from raw predictive accuracy. Trained on math reasoning tasks, our model's log-scale Accuracy-to-Hallucination Ratio gain (0.806) exceeds GPT-5's (0.207) in a challenging in-domain evaluation (BeyondAIME). Moreover, in cross-domain factual QA (SimpleQA), our 4B LLM achieves zero-shot calibration error on par with frontier models including Grok-4 and Gemini-2.5-Pro, even though its factual accuracy is much lower."",""guid"":""oai:arXiv.org:2512.19920v3"",""categories"":[""cs.LG"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,ShareChat: A Dataset of Chatbot Conversations in the Wild,https://arxiv.org/abs/2512.17843,2026-01-30T05:00:00.000Z,"arXiv:2512.17843v3 Announce Type: replace-cross 
Abstract: While academic research typically treats Large Language Models (LLM) as generic text generators, they are distinct commercial products with unique interfaces and capabilities that fundamentally shape user behavior. Current datasets obscure this reality by collecting text-only data through uniform interfaces that fail to capture authentic chatbot usage. To address this limitation, we present ShareChat, a large-scale corpus of 142,808 conversations (660,293 turns) sourced directly from publicly shared URLs on ChatGPT, Perplexity, Grok, Gemini, and Claude. ShareChat distinguishes itself by preserving native platform affordances, such as citations and thinking traces, across a diverse collection covering 101 languages and the period from April 2023 to October 2025. Furthermore, ShareChat offers substantially longer context windows and greater interaction depth than prior datasets. To illustrate the dataset's breadth, we present three case studies: a completeness analysis of intent satisfaction, a citation study of model grounding, and a temporal analysis of engagement rhythms. This work provides the community with a vital and timely resource for understanding authentic user-LLM chatbot interactions in the wild. The dataset is publicly available via Hugging Face.","{""creator"":""Yueru Yan, Tuc Nguyen, Bo Su, Melissa Lieffers, Thai Le"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""ShareChat: A Dataset of Chatbot Conversations in the Wild"",""link"":""https://arxiv.org/abs/2512.17843"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Yueru Yan, Tuc Nguyen, Bo Su, Melissa Lieffers, Thai Le"",""content"":""arXiv:2512.17843v3 Announce Type: replace-cross \nAbstract: While academic research typically treats Large Language Models (LLM) as generic text generators, they are distinct commercial products with unique interfaces and capabilities that fundamentally shape user behavior. Current datasets obscure this reality by collecting text-only data through uniform interfaces that fail to capture authentic chatbot usage. To address this limitation, we present ShareChat, a large-scale corpus of 142,808 conversations (660,293 turns) sourced directly from publicly shared URLs on ChatGPT, Perplexity, Grok, Gemini, and Claude. ShareChat distinguishes itself by preserving native platform affordances, such as citations and thinking traces, across a diverse collection covering 101 languages and the period from April 2023 to October 2025. Furthermore, ShareChat offers substantially longer context windows and greater interaction depth than prior datasets. To illustrate the dataset's breadth, we present three case studies: a completeness analysis of intent satisfaction, a citation study of model grounding, and a temporal analysis of engagement rhythms. This work provides the community with a vital and timely resource for understanding authentic user-LLM chatbot interactions in the wild. The dataset is publicly available via Hugging Face."",""contentSnippet"":""arXiv:2512.17843v3 Announce Type: replace-cross \nAbstract: While academic research typically treats Large Language Models (LLM) as generic text generators, they are distinct commercial products with unique interfaces and capabilities that fundamentally shape user behavior. Current datasets obscure this reality by collecting text-only data through uniform interfaces that fail to capture authentic chatbot usage. To address this limitation, we present ShareChat, a large-scale corpus of 142,808 conversations (660,293 turns) sourced directly from publicly shared URLs on ChatGPT, Perplexity, Grok, Gemini, and Claude. ShareChat distinguishes itself by preserving native platform affordances, such as citations and thinking traces, across a diverse collection covering 101 languages and the period from April 2023 to October 2025. Furthermore, ShareChat offers substantially longer context windows and greater interaction depth than prior datasets. To illustrate the dataset's breadth, we present three case studies: a completeness analysis of intent satisfaction, a citation study of model grounding, and a temporal analysis of engagement rhythms. This work provides the community with a vital and timely resource for understanding authentic user-LLM chatbot interactions in the wild. The dataset is publicly available via Hugging Face."",""guid"":""oai:arXiv.org:2512.17843v3"",""categories"":[""cs.CL"",""cs.AI"",""cs.HC""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,KV Admission: Learning What to Write for Efficient Long-Context Inference,https://arxiv.org/abs/2512.17452,2026-01-30T05:00:00.000Z,"arXiv:2512.17452v3 Announce Type: replace-cross 
Abstract: Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV (WG-KV), a lightweight mechanism that learns to predict token utility before cache entry. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, WG-KV reduces memory usage by 46-68% and delivers 3.03-3.70x prefill and 1.85-2.56x decode speedups on Llama and Qwen models, while maintaining compatibility with FlashAttention and Paged-KV systems. These results demonstrate that learning what to write is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV.","{""creator"":""Yen-Chieh Huang, Pi-Cheng Hsiu, Rui Fang, Ming-Syan Chen"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""KV Admission: Learning What to Write for Efficient Long-Context Inference"",""link"":""https://arxiv.org/abs/2512.17452"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Yen-Chieh Huang, Pi-Cheng Hsiu, Rui Fang, Ming-Syan Chen"",""content"":""arXiv:2512.17452v3 Announce Type: replace-cross \nAbstract: Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV (WG-KV), a lightweight mechanism that learns to predict token utility before cache entry. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, WG-KV reduces memory usage by 46-68% and delivers 3.03-3.70x prefill and 1.85-2.56x decode speedups on Llama and Qwen models, while maintaining compatibility with FlashAttention and Paged-KV systems. These results demonstrate that learning what to write is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV."",""contentSnippet"":""arXiv:2512.17452v3 Announce Type: replace-cross \nAbstract: Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV (WG-KV), a lightweight mechanism that learns to predict token utility before cache entry. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, WG-KV reduces memory usage by 46-68% and delivers 3.03-3.70x prefill and 1.85-2.56x decode speedups on Llama and Qwen models, while maintaining compatibility with FlashAttention and Paged-KV systems. These results demonstrate that learning what to write is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV."",""guid"":""oai:arXiv.org:2512.17452v3"",""categories"":[""cs.LG"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Spectral Representation-based Reinforcement Learning,https://arxiv.org/abs/2512.15036,2026-01-30T05:00:00.000Z,"arXiv:2512.15036v2 Announce Type: replace-cross 
Abstract: In real-world applications with large state and action spaces, reinforcement learning (RL) typically employs function approximations to represent core components like the policies, value functions, and dynamics models. Although powerful approximations such as neural networks offer great expressiveness, they often present theoretical ambiguities, suffer from optimization instability and exploration difficulty, and incur substantial computational costs in practice. In this paper, we introduce the perspective of spectral representations as a solution to address these difficulties in RL. Stemming from the spectral decomposition of the transition operator, this framework yields an effective abstraction of the system dynamics for subsequent policy optimization while also providing a clear theoretical characterization. We reveal how to construct spectral representations for transition operators that possess latent variable structures or energy-based structures, which implies different learning methods to extract spectral representations from data. Notably, each of these learning methods realizes an effective RL algorithm under this framework. We also provably extend this spectral view to partially observable MDPs. Finally, we validate these algorithms on over 20 challenging tasks from the DeepMind Control Suite, where they achieve performances comparable or superior to current state-of-the-art model-free and model-based baselines.","{""creator"":""Chenxiao Gao, Haotian Sun, Na Li, Dale Schuurmans, Bo Dai"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""Spectral Representation-based Reinforcement Learning"",""link"":""https://arxiv.org/abs/2512.15036"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Chenxiao Gao, Haotian Sun, Na Li, Dale Schuurmans, Bo Dai"",""content"":""arXiv:2512.15036v2 Announce Type: replace-cross \nAbstract: In real-world applications with large state and action spaces, reinforcement learning (RL) typically employs function approximations to represent core components like the policies, value functions, and dynamics models. Although powerful approximations such as neural networks offer great expressiveness, they often present theoretical ambiguities, suffer from optimization instability and exploration difficulty, and incur substantial computational costs in practice. In this paper, we introduce the perspective of spectral representations as a solution to address these difficulties in RL. Stemming from the spectral decomposition of the transition operator, this framework yields an effective abstraction of the system dynamics for subsequent policy optimization while also providing a clear theoretical characterization. We reveal how to construct spectral representations for transition operators that possess latent variable structures or energy-based structures, which implies different learning methods to extract spectral representations from data. Notably, each of these learning methods realizes an effective RL algorithm under this framework. We also provably extend this spectral view to partially observable MDPs. Finally, we validate these algorithms on over 20 challenging tasks from the DeepMind Control Suite, where they achieve performances comparable or superior to current state-of-the-art model-free and model-based baselines."",""contentSnippet"":""arXiv:2512.15036v2 Announce Type: replace-cross \nAbstract: In real-world applications with large state and action spaces, reinforcement learning (RL) typically employs function approximations to represent core components like the policies, value functions, and dynamics models. Although powerful approximations such as neural networks offer great expressiveness, they often present theoretical ambiguities, suffer from optimization instability and exploration difficulty, and incur substantial computational costs in practice. In this paper, we introduce the perspective of spectral representations as a solution to address these difficulties in RL. Stemming from the spectral decomposition of the transition operator, this framework yields an effective abstraction of the system dynamics for subsequent policy optimization while also providing a clear theoretical characterization. We reveal how to construct spectral representations for transition operators that possess latent variable structures or energy-based structures, which implies different learning methods to extract spectral representations from data. Notably, each of these learning methods realizes an effective RL algorithm under this framework. We also provably extend this spectral view to partially observable MDPs. Finally, we validate these algorithms on over 20 challenging tasks from the DeepMind Control Suite, where they achieve performances comparable or superior to current state-of-the-art model-free and model-based baselines."",""guid"":""oai:arXiv.org:2512.15036v2"",""categories"":[""cs.LG"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Dual-objective Language Models: Training Efficiency Without Overfitting,https://arxiv.org/abs/2512.14549,2026-01-30T05:00:00.000Z,"arXiv:2512.14549v2 Announce Type: replace-cross 
Abstract: This paper combines autoregressive and masked-diffusion training objectives without any architectural modifications, resulting in flexible language models that outperform single-objective models. Autoregressive modeling has been a popular approach, partly because of its training efficiency; however, that comes at the cost of sensitivity to overfitting. On the other hand, masked-diffusion models are less efficient to train while being more resilient to overfitting. In this work, we demonstrate that dual-objective training achieves the best of both worlds. To derive the optimal balance between both objectives, we train and evaluate 50 language models under varying levels of data repetition. We show that it is optimal to combine both objectives under all evaluated settings and that the optimal balance is similar whether targeting autoregressive or masked-diffusion downstream performance.","{""creator"":""David Samuel, Lucas Georges Gabriel Charpentier"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""Dual-objective Language Models: Training Efficiency Without Overfitting"",""link"":""https://arxiv.org/abs/2512.14549"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""David Samuel, Lucas Georges Gabriel Charpentier"",""content"":""arXiv:2512.14549v2 Announce Type: replace-cross \nAbstract: This paper combines autoregressive and masked-diffusion training objectives without any architectural modifications, resulting in flexible language models that outperform single-objective models. Autoregressive modeling has been a popular approach, partly because of its training efficiency; however, that comes at the cost of sensitivity to overfitting. On the other hand, masked-diffusion models are less efficient to train while being more resilient to overfitting. In this work, we demonstrate that dual-objective training achieves the best of both worlds. To derive the optimal balance between both objectives, we train and evaluate 50 language models under varying levels of data repetition. We show that it is optimal to combine both objectives under all evaluated settings and that the optimal balance is similar whether targeting autoregressive or masked-diffusion downstream performance."",""contentSnippet"":""arXiv:2512.14549v2 Announce Type: replace-cross \nAbstract: This paper combines autoregressive and masked-diffusion training objectives without any architectural modifications, resulting in flexible language models that outperform single-objective models. Autoregressive modeling has been a popular approach, partly because of its training efficiency; however, that comes at the cost of sensitivity to overfitting. On the other hand, masked-diffusion models are less efficient to train while being more resilient to overfitting. In this work, we demonstrate that dual-objective training achieves the best of both worlds. To derive the optimal balance between both objectives, we train and evaluate 50 language models under varying levels of data repetition. We show that it is optimal to combine both objectives under all evaluated settings and that the optimal balance is similar whether targeting autoregressive or masked-diffusion downstream performance."",""guid"":""oai:arXiv.org:2512.14549v2"",""categories"":[""cs.CL"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation,https://arxiv.org/abs/2512.07051,2026-01-30T05:00:00.000Z,"arXiv:2512.07051v2 Announce Type: replace-cross 
Abstract: Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet's bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet's robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments.","{""creator"":""Adnan Munir, Muhammad Shahid Jabbar, Shujaat Khan"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation"",""link"":""https://arxiv.org/abs/2512.07051"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Adnan Munir, Muhammad Shahid Jabbar, Shujaat Khan"",""content"":""arXiv:2512.07051v2 Announce Type: replace-cross \nAbstract: Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet's bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet's robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments."",""contentSnippet"":""arXiv:2512.07051v2 Announce Type: replace-cross \nAbstract: Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet's bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet's robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments."",""guid"":""oai:arXiv.org:2512.07051v2"",""categories"":[""cs.CV"",""cs.AI"",""cs.LG""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Dynamic Content Moderation in Livestreams: Combining Supervised Classification with MLLM-Boosted Similarity Matching,https://arxiv.org/abs/2512.03553,2026-01-30T05:00:00.000Z,"arXiv:2512.03553v2 Announce Type: replace-cross 
Abstract: Content moderation remains a critical yet challenging task for large-scale user-generated video platforms, especially in livestreaming environments where moderation must be timely, multimodal, and robust to evolving forms of unwanted content. We present a hybrid moderation framework deployed at production scale that combines supervised classification for known violations with reference-based similarity matching for novel or subtle cases. This hybrid design enables robust detection of both explicit violations and novel edge cases that evade traditional classifiers. Multimodal inputs (text, audio, visual) are processed through both pipelines, with a multimodal large language model (MLLM) distilling knowledge into each to boost accuracy while keeping inference lightweight. In production, the classification pipeline achieves 67% recall at 80% precision, and the similarity pipeline achieves 76% recall at 80% precision. Large-scale A/B tests show a 6-8% reduction in user views of unwanted livestreams}. These results demonstrate a scalable and adaptable approach to multimodal content governance, capable of addressing both explicit violations and emerging adversarial behaviors.","{""creator"":""Wei Chee Yew, Hailun Xu, Sanjay Saha, Xiaotian Fan, Hiok Hian Ong, David Yuchen Wang, Kanchan Sarkar, Zhenheng Yang, Danhui Guan"",""rights"":""http://creativecommons.org/licenses/by-nc-nd/4.0/"",""title"":""Dynamic Content Moderation in Livestreams: Combining Supervised Classification with MLLM-Boosted Similarity Matching"",""link"":""https://arxiv.org/abs/2512.03553"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Wei Chee Yew, Hailun Xu, Sanjay Saha, Xiaotian Fan, Hiok Hian Ong, David Yuchen Wang, Kanchan Sarkar, Zhenheng Yang, Danhui Guan"",""content"":""arXiv:2512.03553v2 Announce Type: replace-cross \nAbstract: Content moderation remains a critical yet challenging task for large-scale user-generated video platforms, especially in livestreaming environments where moderation must be timely, multimodal, and robust to evolving forms of unwanted content. We present a hybrid moderation framework deployed at production scale that combines supervised classification for known violations with reference-based similarity matching for novel or subtle cases. This hybrid design enables robust detection of both explicit violations and novel edge cases that evade traditional classifiers. Multimodal inputs (text, audio, visual) are processed through both pipelines, with a multimodal large language model (MLLM) distilling knowledge into each to boost accuracy while keeping inference lightweight. In production, the classification pipeline achieves 67% recall at 80% precision, and the similarity pipeline achieves 76% recall at 80% precision. Large-scale A/B tests show a 6-8% reduction in user views of unwanted livestreams}. These results demonstrate a scalable and adaptable approach to multimodal content governance, capable of addressing both explicit violations and emerging adversarial behaviors."",""contentSnippet"":""arXiv:2512.03553v2 Announce Type: replace-cross \nAbstract: Content moderation remains a critical yet challenging task for large-scale user-generated video platforms, especially in livestreaming environments where moderation must be timely, multimodal, and robust to evolving forms of unwanted content. We present a hybrid moderation framework deployed at production scale that combines supervised classification for known violations with reference-based similarity matching for novel or subtle cases. This hybrid design enables robust detection of both explicit violations and novel edge cases that evade traditional classifiers. Multimodal inputs (text, audio, visual) are processed through both pipelines, with a multimodal large language model (MLLM) distilling knowledge into each to boost accuracy while keeping inference lightweight. In production, the classification pipeline achieves 67% recall at 80% precision, and the similarity pipeline achieves 76% recall at 80% precision. Large-scale A/B tests show a 6-8% reduction in user views of unwanted livestreams}. These results demonstrate a scalable and adaptable approach to multimodal content governance, capable of addressing both explicit violations and emerging adversarial behaviors."",""guid"":""oai:arXiv.org:2512.03553v2"",""categories"":[""cs.CV"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Tracing Mathematical Proficiency Through Problem-Solving Processes,https://arxiv.org/abs/2512.00311,2026-01-30T05:00:00.000Z,"arXiv:2512.00311v2 Announce Type: replace-cross 
Abstract: Knowledge Tracing (KT) aims to model student's knowledge state and predict future performance to enable personalized learning in Intelligent Tutoring Systems. However, traditional KT methods face fundamental limitations in explainability, as they rely solely on the response correctness, neglecting the rich information embedded in students' problem-solving processes. To address this gap, we propose Knowledge Tracing Leveraging Problem-Solving Process (KT-PSP), which incorporates students' problem-solving processes to capture the multidimensional aspects of mathematical proficiency. We also introduce KT-PSP-25, a new dataset specifically designed for the KT-PSP. Building on this, we present StatusKT, a KT framework that employs a teacher-student-teacher three-stage LLM pipeline to extract students' MP as intermediate signals. In this pipeline, the teacher LLM first extracts problem-specific proficiency indicators, then a student LLM generates responses based on the student's solution process, and a teacher LLM evaluates these responses to determine mastery of each indicator. The experimental results on KT-PSP-25 demonstrate that StatusKT improves the prediction performance of existing KT methods. Moreover, StatusKT provides interpretable explanations for its predictions by explicitly modeling students' mathematical proficiency.","{""creator"":""Jungyang Park, Suho Kang, Jaewoo Park, Jaehong Kim, Jaewoo Shin, Seonjoon Park, Youngjae Yu"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""Tracing Mathematical Proficiency Through Problem-Solving Processes"",""link"":""https://arxiv.org/abs/2512.00311"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Jungyang Park, Suho Kang, Jaewoo Park, Jaehong Kim, Jaewoo Shin, Seonjoon Park, Youngjae Yu"",""content"":""arXiv:2512.00311v2 Announce Type: replace-cross \nAbstract: Knowledge Tracing (KT) aims to model student's knowledge state and predict future performance to enable personalized learning in Intelligent Tutoring Systems. However, traditional KT methods face fundamental limitations in explainability, as they rely solely on the response correctness, neglecting the rich information embedded in students' problem-solving processes. To address this gap, we propose Knowledge Tracing Leveraging Problem-Solving Process (KT-PSP), which incorporates students' problem-solving processes to capture the multidimensional aspects of mathematical proficiency. We also introduce KT-PSP-25, a new dataset specifically designed for the KT-PSP. Building on this, we present StatusKT, a KT framework that employs a teacher-student-teacher three-stage LLM pipeline to extract students' MP as intermediate signals. In this pipeline, the teacher LLM first extracts problem-specific proficiency indicators, then a student LLM generates responses based on the student's solution process, and a teacher LLM evaluates these responses to determine mastery of each indicator. The experimental results on KT-PSP-25 demonstrate that StatusKT improves the prediction performance of existing KT methods. Moreover, StatusKT provides interpretable explanations for its predictions by explicitly modeling students' mathematical proficiency."",""contentSnippet"":""arXiv:2512.00311v2 Announce Type: replace-cross \nAbstract: Knowledge Tracing (KT) aims to model student's knowledge state and predict future performance to enable personalized learning in Intelligent Tutoring Systems. However, traditional KT methods face fundamental limitations in explainability, as they rely solely on the response correctness, neglecting the rich information embedded in students' problem-solving processes. To address this gap, we propose Knowledge Tracing Leveraging Problem-Solving Process (KT-PSP), which incorporates students' problem-solving processes to capture the multidimensional aspects of mathematical proficiency. We also introduce KT-PSP-25, a new dataset specifically designed for the KT-PSP. Building on this, we present StatusKT, a KT framework that employs a teacher-student-teacher three-stage LLM pipeline to extract students' MP as intermediate signals. In this pipeline, the teacher LLM first extracts problem-specific proficiency indicators, then a student LLM generates responses based on the student's solution process, and a teacher LLM evaluates these responses to determine mastery of each indicator. The experimental results on KT-PSP-25 demonstrate that StatusKT improves the prediction performance of existing KT methods. Moreover, StatusKT provides interpretable explanations for its predictions by explicitly modeling students' mathematical proficiency."",""guid"":""oai:arXiv.org:2512.00311v2"",""categories"":[""cs.LG"",""cs.AI"",""cs.CY""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,RacketVision: A Multiple Racket Sports Benchmark for Unified Ball and Racket Analysis,https://arxiv.org/abs/2511.17045,2026-01-30T05:00:00.000Z,"arXiv:2511.17045v3 Announce Type: replace-cross 
Abstract: We introduce RacketVision, a novel dataset and benchmark for advancing computer vision in sports analytics, covering table tennis, tennis, and badminton. The dataset is the first to provide large-scale, fine-grained annotations for racket pose alongside traditional ball positions, enabling research into complex human-object interactions. It is designed to tackle three interconnected tasks: fine-grained ball tracking, articulated racket pose estimation, and predictive ball trajectory forecasting. Our evaluation of established baselines reveals a critical insight for multi-modal fusion: while naively concatenating racket pose features degrades performance, a CrossAttention mechanism is essential to unlock their value, leading to trajectory prediction results that surpass strong unimodal baselines. RacketVision provides a versatile resource and a strong starting point for future research in dynamic object tracking, conditional motion forecasting, and multimodal analysis in sports. Project page at https://github.com/OrcustD/RacketVision","{""creator"":""Linfeng Dong, Yuchen Yang, Hao Wu, Wei Wang, Yuenan Hou, Zhihang Zhong, Xiao Sun"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""RacketVision: A Multiple Racket Sports Benchmark for Unified Ball and Racket Analysis"",""link"":""https://arxiv.org/abs/2511.17045"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Linfeng Dong, Yuchen Yang, Hao Wu, Wei Wang, Yuenan Hou, Zhihang Zhong, Xiao Sun"",""content"":""arXiv:2511.17045v3 Announce Type: replace-cross \nAbstract: We introduce RacketVision, a novel dataset and benchmark for advancing computer vision in sports analytics, covering table tennis, tennis, and badminton. The dataset is the first to provide large-scale, fine-grained annotations for racket pose alongside traditional ball positions, enabling research into complex human-object interactions. It is designed to tackle three interconnected tasks: fine-grained ball tracking, articulated racket pose estimation, and predictive ball trajectory forecasting. Our evaluation of established baselines reveals a critical insight for multi-modal fusion: while naively concatenating racket pose features degrades performance, a CrossAttention mechanism is essential to unlock their value, leading to trajectory prediction results that surpass strong unimodal baselines. RacketVision provides a versatile resource and a strong starting point for future research in dynamic object tracking, conditional motion forecasting, and multimodal analysis in sports. Project page at https://github.com/OrcustD/RacketVision"",""contentSnippet"":""arXiv:2511.17045v3 Announce Type: replace-cross \nAbstract: We introduce RacketVision, a novel dataset and benchmark for advancing computer vision in sports analytics, covering table tennis, tennis, and badminton. The dataset is the first to provide large-scale, fine-grained annotations for racket pose alongside traditional ball positions, enabling research into complex human-object interactions. It is designed to tackle three interconnected tasks: fine-grained ball tracking, articulated racket pose estimation, and predictive ball trajectory forecasting. Our evaluation of established baselines reveals a critical insight for multi-modal fusion: while naively concatenating racket pose features degrades performance, a CrossAttention mechanism is essential to unlock their value, leading to trajectory prediction results that surpass strong unimodal baselines. RacketVision provides a versatile resource and a strong starting point for future research in dynamic object tracking, conditional motion forecasting, and multimodal analysis in sports. Project page at https://github.com/OrcustD/RacketVision"",""guid"":""oai:arXiv.org:2511.17045v3"",""categories"":[""cs.CV"",""cs.AI"",""cs.MM""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,DeepBooTS: Dual-Stream Residual Boosting for Drift-Resilient Time-Series Forecasting,https://arxiv.org/abs/2511.06893,2026-01-30T05:00:00.000Z,"arXiv:2511.06893v2 Announce Type: replace-cross 
Abstract: Time-Series (TS) exhibits pronounced non-stationarity. Consequently, most forecasting methods display compromised robustness to concept drift, despite the prevalent application of instance normalization. We tackle this challenge by first analysing concept drift through a bias-variance lens and proving that weighted ensemble reduces variance without increasing bias. These insights motivate DeepBooTS, a novel end-to-end dual-stream residual-decreasing boosting method that progressively reconstructs the intrinsic signal. In our design, each block of a deep model becomes an ensemble of learners with an auxiliary output branch forming a highway to the final prediction. The block-wise outputs correct the residuals of previous blocks, leading to a learning-driven decomposition of both inputs and targets. This method enhances versatility and interpretability while substantially improving robustness to concept drift. Extensive experiments, including those on large-scale datasets, show that the proposed method outperforms existing methods by a large margin, yielding an average performance improvement of 15.8% across various datasets, establishing a new benchmark for TS forecasting.","{""creator"":""Daojun Liang, Jing Chen, Xiao Wang, Yinglong Wang, Shuo Li"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""DeepBooTS: Dual-Stream Residual Boosting for Drift-Resilient Time-Series Forecasting"",""link"":""https://arxiv.org/abs/2511.06893"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Daojun Liang, Jing Chen, Xiao Wang, Yinglong Wang, Shuo Li"",""content"":""arXiv:2511.06893v2 Announce Type: replace-cross \nAbstract: Time-Series (TS) exhibits pronounced non-stationarity. Consequently, most forecasting methods display compromised robustness to concept drift, despite the prevalent application of instance normalization. We tackle this challenge by first analysing concept drift through a bias-variance lens and proving that weighted ensemble reduces variance without increasing bias. These insights motivate DeepBooTS, a novel end-to-end dual-stream residual-decreasing boosting method that progressively reconstructs the intrinsic signal. In our design, each block of a deep model becomes an ensemble of learners with an auxiliary output branch forming a highway to the final prediction. The block-wise outputs correct the residuals of previous blocks, leading to a learning-driven decomposition of both inputs and targets. This method enhances versatility and interpretability while substantially improving robustness to concept drift. Extensive experiments, including those on large-scale datasets, show that the proposed method outperforms existing methods by a large margin, yielding an average performance improvement of 15.8% across various datasets, establishing a new benchmark for TS forecasting."",""contentSnippet"":""arXiv:2511.06893v2 Announce Type: replace-cross \nAbstract: Time-Series (TS) exhibits pronounced non-stationarity. Consequently, most forecasting methods display compromised robustness to concept drift, despite the prevalent application of instance normalization. We tackle this challenge by first analysing concept drift through a bias-variance lens and proving that weighted ensemble reduces variance without increasing bias. These insights motivate DeepBooTS, a novel end-to-end dual-stream residual-decreasing boosting method that progressively reconstructs the intrinsic signal. In our design, each block of a deep model becomes an ensemble of learners with an auxiliary output branch forming a highway to the final prediction. The block-wise outputs correct the residuals of previous blocks, leading to a learning-driven decomposition of both inputs and targets. This method enhances versatility and interpretability while substantially improving robustness to concept drift. Extensive experiments, including those on large-scale datasets, show that the proposed method outperforms existing methods by a large margin, yielding an average performance improvement of 15.8% across various datasets, establishing a new benchmark for TS forecasting."",""guid"":""oai:arXiv.org:2511.06893v2"",""categories"":[""cs.LG"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,First is Not Really Better Than Last: Evaluating Layer Choice and Aggregation Strategies in Language Model Data Influence Estimation,https://arxiv.org/abs/2511.04715,2026-01-30T05:00:00.000Z,"arXiv:2511.04715v2 Announce Type: replace-cross 
Abstract: Identifying how training samples influence/impact Large Language Model (LLM) decision-making is essential for effectively interpreting model decisions and auditing large-scale datasets. Current training sample influence estimation methods (also known as influence functions) undertake this goal by utilizing information flow through the model via its first-order and higher-order gradient terms. However, owing to the large model sizes of today consisting of billions of parameters, these influence computations are often restricted to some subset of model layers to ensure computational feasibility. Prior seminal work by Yeh et al. (2022) in assessing which layers are best suited for computing language data influence concluded that the first (embedding) layers are the most informative for this purpose, using a hypothesis based on influence scores canceling out (i.e., the cancellation effect). In this work, we propose theoretical and empirical evidence demonstrating how the cancellation effect is unreliable, and that middle attention layers are better estimators for influence. Furthermore, we address the broader challenge of aggregating influence scores across layers, and showcase how alternatives to standard averaging (such as ranking and vote-based methods) can lead to significantly improved performance. Finally, we propose better methods for evaluating influence score efficacy in LLMs without undertaking model retraining, and propose a new metric known as the Noise Detection Rate (NDR) that exhibits strong predictive capability compared to the cancellation effect. Through extensive experiments across LLMs of varying types and scales, we concretely determine that the first (layers) are not necessarily better than the last (layers) for LLM influence estimation, contrasting with prior knowledge in the field.","{""creator"":""Dmytro Vitel, Anshuman Chhabra"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""First is Not Really Better Than Last: Evaluating Layer Choice and Aggregation Strategies in Language Model Data Influence Estimation"",""link"":""https://arxiv.org/abs/2511.04715"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Dmytro Vitel, Anshuman Chhabra"",""content"":""arXiv:2511.04715v2 Announce Type: replace-cross \nAbstract: Identifying how training samples influence/impact Large Language Model (LLM) decision-making is essential for effectively interpreting model decisions and auditing large-scale datasets. Current training sample influence estimation methods (also known as influence functions) undertake this goal by utilizing information flow through the model via its first-order and higher-order gradient terms. However, owing to the large model sizes of today consisting of billions of parameters, these influence computations are often restricted to some subset of model layers to ensure computational feasibility. Prior seminal work by Yeh et al. (2022) in assessing which layers are best suited for computing language data influence concluded that the first (embedding) layers are the most informative for this purpose, using a hypothesis based on influence scores canceling out (i.e., the cancellation effect). In this work, we propose theoretical and empirical evidence demonstrating how the cancellation effect is unreliable, and that middle attention layers are better estimators for influence. Furthermore, we address the broader challenge of aggregating influence scores across layers, and showcase how alternatives to standard averaging (such as ranking and vote-based methods) can lead to significantly improved performance. Finally, we propose better methods for evaluating influence score efficacy in LLMs without undertaking model retraining, and propose a new metric known as the Noise Detection Rate (NDR) that exhibits strong predictive capability compared to the cancellation effect. Through extensive experiments across LLMs of varying types and scales, we concretely determine that the first (layers) are not necessarily better than the last (layers) for LLM influence estimation, contrasting with prior knowledge in the field."",""contentSnippet"":""arXiv:2511.04715v2 Announce Type: replace-cross \nAbstract: Identifying how training samples influence/impact Large Language Model (LLM) decision-making is essential for effectively interpreting model decisions and auditing large-scale datasets. Current training sample influence estimation methods (also known as influence functions) undertake this goal by utilizing information flow through the model via its first-order and higher-order gradient terms. However, owing to the large model sizes of today consisting of billions of parameters, these influence computations are often restricted to some subset of model layers to ensure computational feasibility. Prior seminal work by Yeh et al. (2022) in assessing which layers are best suited for computing language data influence concluded that the first (embedding) layers are the most informative for this purpose, using a hypothesis based on influence scores canceling out (i.e., the cancellation effect). In this work, we propose theoretical and empirical evidence demonstrating how the cancellation effect is unreliable, and that middle attention layers are better estimators for influence. Furthermore, we address the broader challenge of aggregating influence scores across layers, and showcase how alternatives to standard averaging (such as ranking and vote-based methods) can lead to significantly improved performance. Finally, we propose better methods for evaluating influence score efficacy in LLMs without undertaking model retraining, and propose a new metric known as the Noise Detection Rate (NDR) that exhibits strong predictive capability compared to the cancellation effect. Through extensive experiments across LLMs of varying types and scales, we concretely determine that the first (layers) are not necessarily better than the last (layers) for LLM influence estimation, contrasting with prior knowledge in the field."",""guid"":""oai:arXiv.org:2511.04715v2"",""categories"":[""cs.CL"",""cs.AI"",""cs.LG""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,COMMUNITYNOTES: A Dataset for Exploring the Helpfulness of Fact-Checking Explanations,https://arxiv.org/abs/2510.24810,2026-01-30T05:00:00.000Z,"arXiv:2510.24810v2 Announce Type: replace-cross 
Abstract: Fact-checking on major platforms, such as X, Meta, and TikTok, is shifting from expert-driven verification to a community-based setup, where users contribute explanatory notes to clarify why a post might be misleading. An important challenge here is determining whether an explanation is helpful for understanding real-world claims and the reasons why, which remains largely underexplored in prior research. In practice, most community notes remain unpublished due to slow community annotation, and the reasons for helpfulness lack clear definitions. To bridge these gaps, we introduce the task of predicting both the helpfulness of explanatory notes and the reason for this. We present COMMUNITYNOTES, a large-scale multilingual dataset of 104k posts with user-provided notes and helpfulness labels. We further propose a framework that automatically generates and improves reason definitions via automatic prompt optimization, and integrate them into prediction. Our experiments show that the optimized definitions can improve both helpfulness and reason prediction. Finally, we show that the helpfulness information is beneficial for existing fact-checking systems.","{""creator"":""Rui Xing, Preslav Nakov, Timothy Baldwin, Jey Han Lau"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""COMMUNITYNOTES: A Dataset for Exploring the Helpfulness of Fact-Checking Explanations"",""link"":""https://arxiv.org/abs/2510.24810"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Rui Xing, Preslav Nakov, Timothy Baldwin, Jey Han Lau"",""content"":""arXiv:2510.24810v2 Announce Type: replace-cross \nAbstract: Fact-checking on major platforms, such as X, Meta, and TikTok, is shifting from expert-driven verification to a community-based setup, where users contribute explanatory notes to clarify why a post might be misleading. An important challenge here is determining whether an explanation is helpful for understanding real-world claims and the reasons why, which remains largely underexplored in prior research. In practice, most community notes remain unpublished due to slow community annotation, and the reasons for helpfulness lack clear definitions. To bridge these gaps, we introduce the task of predicting both the helpfulness of explanatory notes and the reason for this. We present COMMUNITYNOTES, a large-scale multilingual dataset of 104k posts with user-provided notes and helpfulness labels. We further propose a framework that automatically generates and improves reason definitions via automatic prompt optimization, and integrate them into prediction. Our experiments show that the optimized definitions can improve both helpfulness and reason prediction. Finally, we show that the helpfulness information is beneficial for existing fact-checking systems."",""contentSnippet"":""arXiv:2510.24810v2 Announce Type: replace-cross \nAbstract: Fact-checking on major platforms, such as X, Meta, and TikTok, is shifting from expert-driven verification to a community-based setup, where users contribute explanatory notes to clarify why a post might be misleading. An important challenge here is determining whether an explanation is helpful for understanding real-world claims and the reasons why, which remains largely underexplored in prior research. In practice, most community notes remain unpublished due to slow community annotation, and the reasons for helpfulness lack clear definitions. To bridge these gaps, we introduce the task of predicting both the helpfulness of explanatory notes and the reason for this. We present COMMUNITYNOTES, a large-scale multilingual dataset of 104k posts with user-provided notes and helpfulness labels. We further propose a framework that automatically generates and improves reason definitions via automatic prompt optimization, and integrate them into prediction. Our experiments show that the optimized definitions can improve both helpfulness and reason prediction. Finally, we show that the helpfulness information is beneficial for existing fact-checking systems."",""guid"":""oai:arXiv.org:2510.24810v2"",""categories"":[""cs.CL"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Learning Linearity in Audio Consistency Autoencoders via Implicit Regularization,https://arxiv.org/abs/2510.23530,2026-01-30T05:00:00.000Z,"arXiv:2510.23530v2 Announce Type: replace-cross 
Abstract: Audio autoencoders learn useful, compressed audio representations, but their non-linear latent spaces prevent intuitive algebraic manipulation such as mixing or scaling. We introduce a simple training methodology to induce linearity in a high-compression Consistency Autoencoder (CAE) by using data augmentation, thereby inducing homogeneity (equivariance to scalar gain) and additivity (the decoder preserves addition) without altering the model's architecture or loss function. When trained with our method, the CAE exhibits linear behavior in both the encoder and decoder while preserving reconstruction fidelity. We test the practical utility of our learned space on music source composition and separation via simple latent arithmetic. This work presents a straightforward technique for constructing structured latent spaces, enabling more intuitive and efficient audio processing.","{""creator"":""Bernardo Torres, Manuel Moussallam, Gabriel Meseguer-Brocal"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""Learning Linearity in Audio Consistency Autoencoders via Implicit Regularization"",""link"":""https://arxiv.org/abs/2510.23530"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Bernardo Torres, Manuel Moussallam, Gabriel Meseguer-Brocal"",""content"":""arXiv:2510.23530v2 Announce Type: replace-cross \nAbstract: Audio autoencoders learn useful, compressed audio representations, but their non-linear latent spaces prevent intuitive algebraic manipulation such as mixing or scaling. We introduce a simple training methodology to induce linearity in a high-compression Consistency Autoencoder (CAE) by using data augmentation, thereby inducing homogeneity (equivariance to scalar gain) and additivity (the decoder preserves addition) without altering the model's architecture or loss function. When trained with our method, the CAE exhibits linear behavior in both the encoder and decoder while preserving reconstruction fidelity. We test the practical utility of our learned space on music source composition and separation via simple latent arithmetic. This work presents a straightforward technique for constructing structured latent spaces, enabling more intuitive and efficient audio processing."",""contentSnippet"":""arXiv:2510.23530v2 Announce Type: replace-cross \nAbstract: Audio autoencoders learn useful, compressed audio representations, but their non-linear latent spaces prevent intuitive algebraic manipulation such as mixing or scaling. We introduce a simple training methodology to induce linearity in a high-compression Consistency Autoencoder (CAE) by using data augmentation, thereby inducing homogeneity (equivariance to scalar gain) and additivity (the decoder preserves addition) without altering the model's architecture or loss function. When trained with our method, the CAE exhibits linear behavior in both the encoder and decoder while preserving reconstruction fidelity. We test the practical utility of our learned space on music source composition and separation via simple latent arithmetic. This work presents a straightforward technique for constructing structured latent spaces, enabling more intuitive and efficient audio processing."",""guid"":""oai:arXiv.org:2510.23530v2"",""categories"":[""cs.SD"",""cs.AI"",""cs.LG"",""eess.AS""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Beyond Correctness: Evaluating Subjective Writing Preferences Across Cultures,https://arxiv.org/abs/2510.14616,2026-01-30T05:00:00.000Z,"arXiv:2510.14616v2 Announce Type: replace-cross 
Abstract: Current preference learning methods achieve high accuracy on standard benchmarks but exhibit significant performance degradation when objective quality signals are removed. We introduce WritingPreferenceBench, a dataset of 1,800 human-annotated preference pairs (1,200 English, 600 Chinese) across 8 creative writing genres, where responses are matched for objective correctness, factual accuracy, and length. On this benchmark, sequence-based reward models--the standard architecture for RLHF--achieve only 52.7% mean accuracy, while zero-shot language model judges perform at 53.9%. In contrast, generative reward models that produce explicit reasoning chains achieve 81.8% accuracy. We observe high within-model variance across genres: individual models range from 18.2% to 81.8% accuracy across different writing categories, with standard deviations averaging 10.1%. This variance persists regardless of model scale, with 27B parameter models showing no consistent improvement over 8B variants. Our results suggest that current RLHF methods primarily learn to detect objective errors rather than capture subjective quality preferences (e.g., creativity, stylistic flair, and emotional resonance), and that successful preference modeling may require intermediate reasoning representations rather than direct classification.","{""creator"":""Shuangshuang Ying, Yunwen Li, Xingwei Qu, Xin Li, Sheng Jin, Minghao Liu, Zhoufutu Wen, Xeron Du, Tianyu Zheng, Yichi Zhang, Letian Ni, Yuyang Cheng, Zhenzhu Yang, Qiguang Chen, Jingzhe Ding, Shengda Long, Wangchunshu Zhou, Jiazhan Feng, Wanjun Zhong, Libo Qin, Ge Zhang, Wenhao Huang, Wanxiang Che, Chenghua Lin"",""rights"":""http://creativecommons.org/licenses/by-sa/4.0/"",""title"":""Beyond Correctness: Evaluating Subjective Writing Preferences Across Cultures"",""link"":""https://arxiv.org/abs/2510.14616"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Shuangshuang Ying, Yunwen Li, Xingwei Qu, Xin Li, Sheng Jin, Minghao Liu, Zhoufutu Wen, Xeron Du, Tianyu Zheng, Yichi Zhang, Letian Ni, Yuyang Cheng, Zhenzhu Yang, Qiguang Chen, Jingzhe Ding, Shengda Long, Wangchunshu Zhou, Jiazhan Feng, Wanjun Zhong, Libo Qin, Ge Zhang, Wenhao Huang, Wanxiang Che, Chenghua Lin"",""content"":""arXiv:2510.14616v2 Announce Type: replace-cross \nAbstract: Current preference learning methods achieve high accuracy on standard benchmarks but exhibit significant performance degradation when objective quality signals are removed. We introduce WritingPreferenceBench, a dataset of 1,800 human-annotated preference pairs (1,200 English, 600 Chinese) across 8 creative writing genres, where responses are matched for objective correctness, factual accuracy, and length. On this benchmark, sequence-based reward models--the standard architecture for RLHF--achieve only 52.7% mean accuracy, while zero-shot language model judges perform at 53.9%. In contrast, generative reward models that produce explicit reasoning chains achieve 81.8% accuracy. We observe high within-model variance across genres: individual models range from 18.2% to 81.8% accuracy across different writing categories, with standard deviations averaging 10.1%. This variance persists regardless of model scale, with 27B parameter models showing no consistent improvement over 8B variants. Our results suggest that current RLHF methods primarily learn to detect objective errors rather than capture subjective quality preferences (e.g., creativity, stylistic flair, and emotional resonance), and that successful preference modeling may require intermediate reasoning representations rather than direct classification."",""contentSnippet"":""arXiv:2510.14616v2 Announce Type: replace-cross \nAbstract: Current preference learning methods achieve high accuracy on standard benchmarks but exhibit significant performance degradation when objective quality signals are removed. We introduce WritingPreferenceBench, a dataset of 1,800 human-annotated preference pairs (1,200 English, 600 Chinese) across 8 creative writing genres, where responses are matched for objective correctness, factual accuracy, and length. On this benchmark, sequence-based reward models--the standard architecture for RLHF--achieve only 52.7% mean accuracy, while zero-shot language model judges perform at 53.9%. In contrast, generative reward models that produce explicit reasoning chains achieve 81.8% accuracy. We observe high within-model variance across genres: individual models range from 18.2% to 81.8% accuracy across different writing categories, with standard deviations averaging 10.1%. This variance persists regardless of model scale, with 27B parameter models showing no consistent improvement over 8B variants. Our results suggest that current RLHF methods primarily learn to detect objective errors rather than capture subjective quality preferences (e.g., creativity, stylistic flair, and emotional resonance), and that successful preference modeling may require intermediate reasoning representations rather than direct classification."",""guid"":""oai:arXiv.org:2510.14616v2"",""categories"":[""cs.CL"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space,https://arxiv.org/abs/2510.12603,2026-01-30T05:00:00.000Z,"arXiv:2510.12603v2 Announce Type: replace-cross 
Abstract: Multimodal reasoning aims to enhance the capabilities of MLLMs by incorporating intermediate reasoning steps before reaching the final answer. It has evolved from text-only reasoning to the integration of visual information, enabling the thought process to be conveyed through both images and text. Despite its effectiveness, current multimodal reasoning methods depend on explicit reasoning steps that require labor-intensive vision-text annotations and inherently introduce significant inference latency. To address these issues, we introduce multimodal latent reasoning with the advantages of multimodal representation, reduced annotation, and inference efficiency. To facilitate it, we propose Interleaved Vision-Text Latent Reasoning (IVT-LR), which injects both visual and textual information in the reasoning process within the latent space. Specifically, IVT-LR represents each reasoning step by combining two implicit parts: latent text (the hidden states from the previous step) and latent vision (a set of selected image embeddings). We further introduce a progressive multi-stage training strategy to enable MLLMs to perform the above multimodal latent reasoning steps. Experiments on M$^3$CoT and ScienceQA demonstrate that our IVT-LR method achieves an average performance increase of 5.45\% in accuracy, while simultaneously achieving a speed increase of over 5 times compared to existing approaches.","{""creator"":""Chao Chen, Zhixin Ma, Yongqi Li, Yupeng Hu, Yinwei Wei, Wenjie Li, Liqiang Nie"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space"",""link"":""https://arxiv.org/abs/2510.12603"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Chao Chen, Zhixin Ma, Yongqi Li, Yupeng Hu, Yinwei Wei, Wenjie Li, Liqiang Nie"",""content"":""arXiv:2510.12603v2 Announce Type: replace-cross \nAbstract: Multimodal reasoning aims to enhance the capabilities of MLLMs by incorporating intermediate reasoning steps before reaching the final answer. It has evolved from text-only reasoning to the integration of visual information, enabling the thought process to be conveyed through both images and text. Despite its effectiveness, current multimodal reasoning methods depend on explicit reasoning steps that require labor-intensive vision-text annotations and inherently introduce significant inference latency. To address these issues, we introduce multimodal latent reasoning with the advantages of multimodal representation, reduced annotation, and inference efficiency. To facilitate it, we propose Interleaved Vision-Text Latent Reasoning (IVT-LR), which injects both visual and textual information in the reasoning process within the latent space. Specifically, IVT-LR represents each reasoning step by combining two implicit parts: latent text (the hidden states from the previous step) and latent vision (a set of selected image embeddings). We further introduce a progressive multi-stage training strategy to enable MLLMs to perform the above multimodal latent reasoning steps. Experiments on M$^3$CoT and ScienceQA demonstrate that our IVT-LR method achieves an average performance increase of 5.45\\% in accuracy, while simultaneously achieving a speed increase of over 5 times compared to existing approaches."",""contentSnippet"":""arXiv:2510.12603v2 Announce Type: replace-cross \nAbstract: Multimodal reasoning aims to enhance the capabilities of MLLMs by incorporating intermediate reasoning steps before reaching the final answer. It has evolved from text-only reasoning to the integration of visual information, enabling the thought process to be conveyed through both images and text. Despite its effectiveness, current multimodal reasoning methods depend on explicit reasoning steps that require labor-intensive vision-text annotations and inherently introduce significant inference latency. To address these issues, we introduce multimodal latent reasoning with the advantages of multimodal representation, reduced annotation, and inference efficiency. To facilitate it, we propose Interleaved Vision-Text Latent Reasoning (IVT-LR), which injects both visual and textual information in the reasoning process within the latent space. Specifically, IVT-LR represents each reasoning step by combining two implicit parts: latent text (the hidden states from the previous step) and latent vision (a set of selected image embeddings). We further introduce a progressive multi-stage training strategy to enable MLLMs to perform the above multimodal latent reasoning steps. Experiments on M$^3$CoT and ScienceQA demonstrate that our IVT-LR method achieves an average performance increase of 5.45\\% in accuracy, while simultaneously achieving a speed increase of over 5 times compared to existing approaches."",""guid"":""oai:arXiv.org:2510.12603v2"",""categories"":[""cs.CV"",""cs.AI"",""cs.CL""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Deep SPI: Safe Policy Improvement via World Models,https://arxiv.org/abs/2510.12312,2026-01-30T05:00:00.000Z,"arXiv:2510.12312v2 Announce Type: replace-cross 
Abstract: Safe policy improvement (SPI) offers theoretical control over policy updates, yet existing guarantees largely concern offline, tabular reinforcement learning (RL). We study SPI in general online settings, when combined with world model and representation learning. We develop a theoretical framework showing that restricting policy updates to a well-defined neighborhood of the current policy ensures monotonic improvement and convergence. This analysis links transition and reward prediction losses to representation quality, yielding online, ""deep"" analogues of classical SPI theorems from the offline RL literature. Building on these results, we introduce DeepSPI, a principled on-policy algorithm that couples local transition and reward losses with regularised policy updates. On the ALE-57 benchmark, DeepSPI matches or exceeds strong baselines, including PPO and DeepMDPs, while retaining theoretical guarantees.","{""creator"":""Florent Delgrange, Raphael Avalos, Willem R\\\""opke"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""Deep SPI: Safe Policy Improvement via World Models"",""link"":""https://arxiv.org/abs/2510.12312"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Florent Delgrange, Raphael Avalos, Willem R\\\""opke"",""content"":""arXiv:2510.12312v2 Announce Type: replace-cross \nAbstract: Safe policy improvement (SPI) offers theoretical control over policy updates, yet existing guarantees largely concern offline, tabular reinforcement learning (RL). We study SPI in general online settings, when combined with world model and representation learning. We develop a theoretical framework showing that restricting policy updates to a well-defined neighborhood of the current policy ensures monotonic improvement and convergence. This analysis links transition and reward prediction losses to representation quality, yielding online, \""deep\"" analogues of classical SPI theorems from the offline RL literature. Building on these results, we introduce DeepSPI, a principled on-policy algorithm that couples local transition and reward losses with regularised policy updates. On the ALE-57 benchmark, DeepSPI matches or exceeds strong baselines, including PPO and DeepMDPs, while retaining theoretical guarantees."",""contentSnippet"":""arXiv:2510.12312v2 Announce Type: replace-cross \nAbstract: Safe policy improvement (SPI) offers theoretical control over policy updates, yet existing guarantees largely concern offline, tabular reinforcement learning (RL). We study SPI in general online settings, when combined with world model and representation learning. We develop a theoretical framework showing that restricting policy updates to a well-defined neighborhood of the current policy ensures monotonic improvement and convergence. This analysis links transition and reward prediction losses to representation quality, yielding online, \""deep\"" analogues of classical SPI theorems from the offline RL literature. Building on these results, we introduce DeepSPI, a principled on-policy algorithm that couples local transition and reward losses with regularised policy updates. On the ALE-57 benchmark, DeepSPI matches or exceeds strong baselines, including PPO and DeepMDPs, while retaining theoretical guarantees."",""guid"":""oai:arXiv.org:2510.12312v2"",""categories"":[""cs.LG"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Closing the Data-Efficiency Gap Between Autoregressive and Masked Diffusion LLMs,https://arxiv.org/abs/2510.09885,2026-01-30T05:00:00.000Z,"arXiv:2510.09885v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are often used in environments where facts evolve, yet factual knowledge updates via fine-tuning on unstructured text often suffers from 1) reliance on compute-heavy paraphrase augmentation and 2) the reversal curse. Recent studies show diffusion large language models (dLLMs) require fewer training samples to achieve lower loss in pre-training and are more resistant to the reversal curse, suggesting dLLMs may learn new knowledge more easily than autoregressive LLMs (arLLMs). We test this hypothesis in controlled knowledge fine-tuning experiments and find that while arLLMs rely on paraphrase augmentation to generalize knowledge text into question-answering (QA) capability, dLLMs do not require paraphrases to achieve high QA accuracy. To further investigate whether the demasking objective alone can induce such a knowledge injection advantage in dLLMs regardless of their diffusion denoising paradigm, we propose masked fine-tuning for arLLMs, which prompts an arLLM to reconstruct the original text given a masked version in context. The masked fine-tuning for arLLMs substantially improves the efficacy of knowledge injection, i.e. no paraphrase needed and resistant to the reversal curse, closing the gap between arLLMs and dLLMs. We also demonstrate that the same demasking objective improves supervised fine-tuning (SFT) on math tasks over standard SFT, suggesting broader applicability of the demasking objective.","{""creator"":""Xu Pan, Ely Hahami, Jingxuan Fan, Ziqian Xie, Haim Sompolinsky"",""rights"":""http://creativecommons.org/licenses/by-nc-sa/4.0/"",""title"":""Closing the Data-Efficiency Gap Between Autoregressive and Masked Diffusion LLMs"",""link"":""https://arxiv.org/abs/2510.09885"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Xu Pan, Ely Hahami, Jingxuan Fan, Ziqian Xie, Haim Sompolinsky"",""content"":""arXiv:2510.09885v3 Announce Type: replace-cross \nAbstract: Large language models (LLMs) are often used in environments where facts evolve, yet factual knowledge updates via fine-tuning on unstructured text often suffers from 1) reliance on compute-heavy paraphrase augmentation and 2) the reversal curse. Recent studies show diffusion large language models (dLLMs) require fewer training samples to achieve lower loss in pre-training and are more resistant to the reversal curse, suggesting dLLMs may learn new knowledge more easily than autoregressive LLMs (arLLMs). We test this hypothesis in controlled knowledge fine-tuning experiments and find that while arLLMs rely on paraphrase augmentation to generalize knowledge text into question-answering (QA) capability, dLLMs do not require paraphrases to achieve high QA accuracy. To further investigate whether the demasking objective alone can induce such a knowledge injection advantage in dLLMs regardless of their diffusion denoising paradigm, we propose masked fine-tuning for arLLMs, which prompts an arLLM to reconstruct the original text given a masked version in context. The masked fine-tuning for arLLMs substantially improves the efficacy of knowledge injection, i.e. no paraphrase needed and resistant to the reversal curse, closing the gap between arLLMs and dLLMs. We also demonstrate that the same demasking objective improves supervised fine-tuning (SFT) on math tasks over standard SFT, suggesting broader applicability of the demasking objective."",""contentSnippet"":""arXiv:2510.09885v3 Announce Type: replace-cross \nAbstract: Large language models (LLMs) are often used in environments where facts evolve, yet factual knowledge updates via fine-tuning on unstructured text often suffers from 1) reliance on compute-heavy paraphrase augmentation and 2) the reversal curse. Recent studies show diffusion large language models (dLLMs) require fewer training samples to achieve lower loss in pre-training and are more resistant to the reversal curse, suggesting dLLMs may learn new knowledge more easily than autoregressive LLMs (arLLMs). We test this hypothesis in controlled knowledge fine-tuning experiments and find that while arLLMs rely on paraphrase augmentation to generalize knowledge text into question-answering (QA) capability, dLLMs do not require paraphrases to achieve high QA accuracy. To further investigate whether the demasking objective alone can induce such a knowledge injection advantage in dLLMs regardless of their diffusion denoising paradigm, we propose masked fine-tuning for arLLMs, which prompts an arLLM to reconstruct the original text given a masked version in context. The masked fine-tuning for arLLMs substantially improves the efficacy of knowledge injection, i.e. no paraphrase needed and resistant to the reversal curse, closing the gap between arLLMs and dLLMs. We also demonstrate that the same demasking objective improves supervised fine-tuning (SFT) on math tasks over standard SFT, suggesting broader applicability of the demasking objective."",""guid"":""oai:arXiv.org:2510.09885v3"",""categories"":[""cs.CL"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,GRACE: A Language Model Framework for Explainable Inverse Reinforcement Learning,https://arxiv.org/abs/2510.02180,2026-01-30T05:00:00.000Z,"arXiv:2510.02180v2 Announce Type: replace-cross 
Abstract: Inverse Reinforcement Learning aims to recover reward models from expert demonstrations, but traditional methods yield black-box models that are difficult to interpret and debug. In this work, we introduce GRACE (Generating Rewards As CodE), a method for using Large Language Models within an evolutionary search to reverse-engineer an interpretable, code-based reward function directly from expert trajectories. The resulting reward function is executable code that can be inspected and verified. We empirically validate GRACE on the MuJoCo, BabyAI and AndroidWorld benchmarks, where it efficiently learns highly accurate rewards, even in complex, multi-task settings. Further, we demonstrate that the resulting reward leads to strong policies, compared to both competitive Imitation Learning and online RL approaches with ground-truth rewards. Finally, we show that GRACE is able to build complex reward APIs in multi-task setups.","{""creator"":""Silvia Sapora, Devon Hjelm, Alexander Toshev, Omar Attia, Bogdan Mazoure"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""GRACE: A Language Model Framework for Explainable Inverse Reinforcement Learning"",""link"":""https://arxiv.org/abs/2510.02180"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Silvia Sapora, Devon Hjelm, Alexander Toshev, Omar Attia, Bogdan Mazoure"",""content"":""arXiv:2510.02180v2 Announce Type: replace-cross \nAbstract: Inverse Reinforcement Learning aims to recover reward models from expert demonstrations, but traditional methods yield black-box models that are difficult to interpret and debug. In this work, we introduce GRACE (Generating Rewards As CodE), a method for using Large Language Models within an evolutionary search to reverse-engineer an interpretable, code-based reward function directly from expert trajectories. The resulting reward function is executable code that can be inspected and verified. We empirically validate GRACE on the MuJoCo, BabyAI and AndroidWorld benchmarks, where it efficiently learns highly accurate rewards, even in complex, multi-task settings. Further, we demonstrate that the resulting reward leads to strong policies, compared to both competitive Imitation Learning and online RL approaches with ground-truth rewards. Finally, we show that GRACE is able to build complex reward APIs in multi-task setups."",""contentSnippet"":""arXiv:2510.02180v2 Announce Type: replace-cross \nAbstract: Inverse Reinforcement Learning aims to recover reward models from expert demonstrations, but traditional methods yield black-box models that are difficult to interpret and debug. In this work, we introduce GRACE (Generating Rewards As CodE), a method for using Large Language Models within an evolutionary search to reverse-engineer an interpretable, code-based reward function directly from expert trajectories. The resulting reward function is executable code that can be inspected and verified. We empirically validate GRACE on the MuJoCo, BabyAI and AndroidWorld benchmarks, where it efficiently learns highly accurate rewards, even in complex, multi-task settings. Further, we demonstrate that the resulting reward leads to strong policies, compared to both competitive Imitation Learning and online RL approaches with ground-truth rewards. Finally, we show that GRACE is able to build complex reward APIs in multi-task setups."",""guid"":""oai:arXiv.org:2510.02180v2"",""categories"":[""cs.LG"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,mR3: Multilingual Rubric-Agnostic Reward Reasoning Models,https://arxiv.org/abs/2510.01146,2026-01-30T05:00:00.000Z,"arXiv:2510.01146v2 Announce Type: replace-cross 
Abstract: Evaluation using Large Language Model (LLM) judges has been widely adopted in English and shown to be effective for automatic evaluation. However, their performance does not generalize well to non-English settings, and it remains unclear what constitutes effective multilingual training for such judges. In this paper, we introduce mR3, a massively multilingual, rubric-agnostic reward reasoning model trained on 72 languages, achieving the broadest language coverage in reward modeling to date. We present a comprehensive study of data and curriculum selection for training to identify effective strategies and data sources for building high-quality reward models, including support for reasoning in the target language. Our approach attains state-of-the-art performance on multilingual reward model benchmarks, surpassing much larger models (i.e., GPT-OSS-120B) while being up to 9x smaller, and its effectiveness is further confirmed through extensive ablation studies. Finally, we demonstrate the effectiveness of mR3 in off-policy preference optimization and validate the quality of its reasoning traces and rubric-based evaluations through human studies with 20 annotators across 12 languages, where mR3 models' reasoning is preferred, including for extremely low-resource languages that are entirely unseen during training. Our models, data, and code are available as open source at https://github.com/rubricreward/mr3.","{""creator"":""David Anugraha, Shou-Yi Hung, Zilu Tang, Annie En-Shiun Lee, Derry Tanti Wijaya, Genta Indra Winata"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""mR3: Multilingual Rubric-Agnostic Reward Reasoning Models"",""link"":""https://arxiv.org/abs/2510.01146"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""David Anugraha, Shou-Yi Hung, Zilu Tang, Annie En-Shiun Lee, Derry Tanti Wijaya, Genta Indra Winata"",""content"":""arXiv:2510.01146v2 Announce Type: replace-cross \nAbstract: Evaluation using Large Language Model (LLM) judges has been widely adopted in English and shown to be effective for automatic evaluation. However, their performance does not generalize well to non-English settings, and it remains unclear what constitutes effective multilingual training for such judges. In this paper, we introduce mR3, a massively multilingual, rubric-agnostic reward reasoning model trained on 72 languages, achieving the broadest language coverage in reward modeling to date. We present a comprehensive study of data and curriculum selection for training to identify effective strategies and data sources for building high-quality reward models, including support for reasoning in the target language. Our approach attains state-of-the-art performance on multilingual reward model benchmarks, surpassing much larger models (i.e., GPT-OSS-120B) while being up to 9x smaller, and its effectiveness is further confirmed through extensive ablation studies. Finally, we demonstrate the effectiveness of mR3 in off-policy preference optimization and validate the quality of its reasoning traces and rubric-based evaluations through human studies with 20 annotators across 12 languages, where mR3 models' reasoning is preferred, including for extremely low-resource languages that are entirely unseen during training. Our models, data, and code are available as open source at https://github.com/rubricreward/mr3."",""contentSnippet"":""arXiv:2510.01146v2 Announce Type: replace-cross \nAbstract: Evaluation using Large Language Model (LLM) judges has been widely adopted in English and shown to be effective for automatic evaluation. However, their performance does not generalize well to non-English settings, and it remains unclear what constitutes effective multilingual training for such judges. In this paper, we introduce mR3, a massively multilingual, rubric-agnostic reward reasoning model trained on 72 languages, achieving the broadest language coverage in reward modeling to date. We present a comprehensive study of data and curriculum selection for training to identify effective strategies and data sources for building high-quality reward models, including support for reasoning in the target language. Our approach attains state-of-the-art performance on multilingual reward model benchmarks, surpassing much larger models (i.e., GPT-OSS-120B) while being up to 9x smaller, and its effectiveness is further confirmed through extensive ablation studies. Finally, we demonstrate the effectiveness of mR3 in off-policy preference optimization and validate the quality of its reasoning traces and rubric-based evaluations through human studies with 20 annotators across 12 languages, where mR3 models' reasoning is preferred, including for extremely low-resource languages that are entirely unseen during training. Our models, data, and code are available as open source at https://github.com/rubricreward/mr3."",""guid"":""oai:arXiv.org:2510.01146v2"",""categories"":[""cs.CL"",""cs.AI"",""cs.LG""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Discrete Variational Autoencoding via Policy Search,https://arxiv.org/abs/2509.24716,2026-01-30T05:00:00.000Z,"arXiv:2509.24716v2 Announce Type: replace-cross 
Abstract: Discrete latent bottlenecks in variational autoencoders (VAEs) offer high bit efficiency and can be modeled with autoregressive discrete distributions, enabling parameter-efficient multimodal search with transformers. However, discrete random variables do not allow for exact differentiable parameterization; therefore, discrete VAEs typically rely on approximations, such as Gumbel-Softmax reparameterization or straight-through gradient estimates, or employ high-variance gradient-free methods such as REINFORCE that have had limited success on high-dimensional tasks such as image reconstruction. Inspired by popular techniques in policy search, we propose a training framework for discrete VAEs that leverages the natural gradient of a non-parametric encoder to update the parametric encoder without requiring reparameterization. Our method, combined with automatic step size adaptation and a transformer-based encoder, scales to challenging datasets such as ImageNet and outperforms both approximate reparameterization methods and quantization-based discrete autoencoders in reconstructing high-dimensional data from compact latent spaces.","{""creator"":""Michael Drolet, Firas Al-Hafez, Aditya Bhatt, Jan Peters, Oleg Arenz"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""Discrete Variational Autoencoding via Policy Search"",""link"":""https://arxiv.org/abs/2509.24716"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Michael Drolet, Firas Al-Hafez, Aditya Bhatt, Jan Peters, Oleg Arenz"",""content"":""arXiv:2509.24716v2 Announce Type: replace-cross \nAbstract: Discrete latent bottlenecks in variational autoencoders (VAEs) offer high bit efficiency and can be modeled with autoregressive discrete distributions, enabling parameter-efficient multimodal search with transformers. However, discrete random variables do not allow for exact differentiable parameterization; therefore, discrete VAEs typically rely on approximations, such as Gumbel-Softmax reparameterization or straight-through gradient estimates, or employ high-variance gradient-free methods such as REINFORCE that have had limited success on high-dimensional tasks such as image reconstruction. Inspired by popular techniques in policy search, we propose a training framework for discrete VAEs that leverages the natural gradient of a non-parametric encoder to update the parametric encoder without requiring reparameterization. Our method, combined with automatic step size adaptation and a transformer-based encoder, scales to challenging datasets such as ImageNet and outperforms both approximate reparameterization methods and quantization-based discrete autoencoders in reconstructing high-dimensional data from compact latent spaces."",""contentSnippet"":""arXiv:2509.24716v2 Announce Type: replace-cross \nAbstract: Discrete latent bottlenecks in variational autoencoders (VAEs) offer high bit efficiency and can be modeled with autoregressive discrete distributions, enabling parameter-efficient multimodal search with transformers. However, discrete random variables do not allow for exact differentiable parameterization; therefore, discrete VAEs typically rely on approximations, such as Gumbel-Softmax reparameterization or straight-through gradient estimates, or employ high-variance gradient-free methods such as REINFORCE that have had limited success on high-dimensional tasks such as image reconstruction. Inspired by popular techniques in policy search, we propose a training framework for discrete VAEs that leverages the natural gradient of a non-parametric encoder to update the parametric encoder without requiring reparameterization. Our method, combined with automatic step size adaptation and a transformer-based encoder, scales to challenging datasets such as ImageNet and outperforms both approximate reparameterization methods and quantization-based discrete autoencoders in reconstructing high-dimensional data from compact latent spaces."",""guid"":""oai:arXiv.org:2509.24716v2"",""categories"":[""cs.LG"",""cs.AI"",""cs.RO""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Look Back to Reason Forward: Revisitable Memory for Long-Context LLM Agents,https://arxiv.org/abs/2509.23040,2026-01-30T05:00:00.000Z,"arXiv:2509.23040v2 Announce Type: replace-cross 
Abstract: Large language models face challenges in long-context question answering, where key evidence of a query may be dispersed across millions of tokens. Existing works equip large language models with a memory buffer that is dynamically updated via a linear document scan, also known as the ""memorize while reading"" methods. While this approach scales efficiently, it suffers from pruning of latent evidence, information loss through overwriting, and sparse reinforcement learning signals. To tackle these challenges, we present ReMemR1, which integrates the mechanism of memory retrieval into the memory update process, enabling the agent to selectively callback historical memories for non-linear reasoning. To further strengthen training, we propose a multi-level reward design, which combines final-answer rewards with dense, step-level signals that guide effective memory use. Together, these contributions mitigate information degradation, improve supervision, and support complex multi-hop reasoning. Extensive experiments demonstrate that ReMemR1 significantly outperforms state-of-the-art baselines on long-context question answering while incurring negligible computational overhead, validating its ability to trade marginal cost for robust long-context reasoning.","{""creator"":""Yaorui Shi, Yuxin Chen, Siyuan Wang, Sihang Li, Hengxing Cai, Qi Gu, Xiang Wang, An Zhang"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""Look Back to Reason Forward: Revisitable Memory for Long-Context LLM Agents"",""link"":""https://arxiv.org/abs/2509.23040"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Yaorui Shi, Yuxin Chen, Siyuan Wang, Sihang Li, Hengxing Cai, Qi Gu, Xiang Wang, An Zhang"",""content"":""arXiv:2509.23040v2 Announce Type: replace-cross \nAbstract: Large language models face challenges in long-context question answering, where key evidence of a query may be dispersed across millions of tokens. Existing works equip large language models with a memory buffer that is dynamically updated via a linear document scan, also known as the \""memorize while reading\"" methods. While this approach scales efficiently, it suffers from pruning of latent evidence, information loss through overwriting, and sparse reinforcement learning signals. To tackle these challenges, we present ReMemR1, which integrates the mechanism of memory retrieval into the memory update process, enabling the agent to selectively callback historical memories for non-linear reasoning. To further strengthen training, we propose a multi-level reward design, which combines final-answer rewards with dense, step-level signals that guide effective memory use. Together, these contributions mitigate information degradation, improve supervision, and support complex multi-hop reasoning. Extensive experiments demonstrate that ReMemR1 significantly outperforms state-of-the-art baselines on long-context question answering while incurring negligible computational overhead, validating its ability to trade marginal cost for robust long-context reasoning."",""contentSnippet"":""arXiv:2509.23040v2 Announce Type: replace-cross \nAbstract: Large language models face challenges in long-context question answering, where key evidence of a query may be dispersed across millions of tokens. Existing works equip large language models with a memory buffer that is dynamically updated via a linear document scan, also known as the \""memorize while reading\"" methods. While this approach scales efficiently, it suffers from pruning of latent evidence, information loss through overwriting, and sparse reinforcement learning signals. To tackle these challenges, we present ReMemR1, which integrates the mechanism of memory retrieval into the memory update process, enabling the agent to selectively callback historical memories for non-linear reasoning. To further strengthen training, we propose a multi-level reward design, which combines final-answer rewards with dense, step-level signals that guide effective memory use. Together, these contributions mitigate information degradation, improve supervision, and support complex multi-hop reasoning. Extensive experiments demonstrate that ReMemR1 significantly outperforms state-of-the-art baselines on long-context question answering while incurring negligible computational overhead, validating its ability to trade marginal cost for robust long-context reasoning."",""guid"":""oai:arXiv.org:2509.23040v2"",""categories"":[""cs.CL"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper Reasoning Benchmarks,https://arxiv.org/abs/2509.22258,2026-01-30T05:00:00.000Z,"arXiv:2509.22258v4 Announce Type: replace-cross 
Abstract: Recent advances in vision-language models (VLMs) have achieved remarkable performance on standard medical benchmarks, yet their true clinical reasoning ability remains unclear. Existing datasets predominantly emphasize classification accuracy, creating an evaluation illusion in which models appear proficient while still failing at high-stakes diagnostic reasoning. We introduce Neural-MedBench, a compact yet reasoning-intensive benchmark specifically designed to probe the limits of multimodal clinical reasoning in neurology. Neural-MedBench integrates multi-sequence MRI scans, structured electronic health records, and clinical notes, and encompasses three core task families: differential diagnosis, lesion recognition, and rationale generation. To ensure reliable evaluation, we develop a hybrid scoring pipeline that combines LLM-based graders, clinician validation, and semantic similarity metrics. Through systematic evaluation of state-of-the-art VLMs, including GPT-4o, Claude-4, and MedGemma, we observe a sharp performance drop compared to conventional datasets. Error analysis shows that reasoning failures, rather than perceptual errors, dominate model shortcomings. Our findings highlight the necessity of a Two-Axis Evaluation Framework: breadth-oriented large datasets for statistical generalization, and depth-oriented, compact benchmarks such as Neural-MedBench for reasoning fidelity. We release Neural-MedBench at https://neuromedbench.github.io/ as an open and extensible diagnostic testbed, which guides the expansion of future benchmarks and enables rigorous yet cost-effective assessment of clinically trustworthy AI.","{""creator"":""Miao Jing, Mengting Jia, Junling Lin, Zhongxia Shen, Huan Gao, Mingkun Xu, Shangyang Li"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper Reasoning Benchmarks"",""link"":""https://arxiv.org/abs/2509.22258"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Miao Jing, Mengting Jia, Junling Lin, Zhongxia Shen, Huan Gao, Mingkun Xu, Shangyang Li"",""content"":""arXiv:2509.22258v4 Announce Type: replace-cross \nAbstract: Recent advances in vision-language models (VLMs) have achieved remarkable performance on standard medical benchmarks, yet their true clinical reasoning ability remains unclear. Existing datasets predominantly emphasize classification accuracy, creating an evaluation illusion in which models appear proficient while still failing at high-stakes diagnostic reasoning. We introduce Neural-MedBench, a compact yet reasoning-intensive benchmark specifically designed to probe the limits of multimodal clinical reasoning in neurology. Neural-MedBench integrates multi-sequence MRI scans, structured electronic health records, and clinical notes, and encompasses three core task families: differential diagnosis, lesion recognition, and rationale generation. To ensure reliable evaluation, we develop a hybrid scoring pipeline that combines LLM-based graders, clinician validation, and semantic similarity metrics. Through systematic evaluation of state-of-the-art VLMs, including GPT-4o, Claude-4, and MedGemma, we observe a sharp performance drop compared to conventional datasets. Error analysis shows that reasoning failures, rather than perceptual errors, dominate model shortcomings. Our findings highlight the necessity of a Two-Axis Evaluation Framework: breadth-oriented large datasets for statistical generalization, and depth-oriented, compact benchmarks such as Neural-MedBench for reasoning fidelity. We release Neural-MedBench at https://neuromedbench.github.io/ as an open and extensible diagnostic testbed, which guides the expansion of future benchmarks and enables rigorous yet cost-effective assessment of clinically trustworthy AI."",""contentSnippet"":""arXiv:2509.22258v4 Announce Type: replace-cross \nAbstract: Recent advances in vision-language models (VLMs) have achieved remarkable performance on standard medical benchmarks, yet their true clinical reasoning ability remains unclear. Existing datasets predominantly emphasize classification accuracy, creating an evaluation illusion in which models appear proficient while still failing at high-stakes diagnostic reasoning. We introduce Neural-MedBench, a compact yet reasoning-intensive benchmark specifically designed to probe the limits of multimodal clinical reasoning in neurology. Neural-MedBench integrates multi-sequence MRI scans, structured electronic health records, and clinical notes, and encompasses three core task families: differential diagnosis, lesion recognition, and rationale generation. To ensure reliable evaluation, we develop a hybrid scoring pipeline that combines LLM-based graders, clinician validation, and semantic similarity metrics. Through systematic evaluation of state-of-the-art VLMs, including GPT-4o, Claude-4, and MedGemma, we observe a sharp performance drop compared to conventional datasets. Error analysis shows that reasoning failures, rather than perceptual errors, dominate model shortcomings. Our findings highlight the necessity of a Two-Axis Evaluation Framework: breadth-oriented large datasets for statistical generalization, and depth-oriented, compact benchmarks such as Neural-MedBench for reasoning fidelity. We release Neural-MedBench at https://neuromedbench.github.io/ as an open and extensible diagnostic testbed, which guides the expansion of future benchmarks and enables rigorous yet cost-effective assessment of clinically trustworthy AI."",""guid"":""oai:arXiv.org:2509.22258v4"",""categories"":[""cs.CV"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Mechanism of Task-oriented Information Removal in In-context Learning,https://arxiv.org/abs/2509.21012,2026-01-30T05:00:00.000Z,"arXiv:2509.21012v3 Announce Type: replace-cross 
Abstract: In-context Learning (ICL) is an emerging few-shot learning paradigm based on modern Language Models (LMs), yet its inner mechanism remains unclear. In this paper, we investigate the mechanism through a novel perspective of information removal. Specifically, we demonstrate that in the zero-shot scenario, LMs encode queries into non-selective representations in hidden states containing information for all possible tasks, leading to arbitrary outputs without focusing on the intended task, resulting in near-zero accuracy. Meanwhile, we find that selectively removing specific information from hidden states by a low-rank filter effectively steers LMs toward the intended task. Building on these findings, by measuring the hidden states on carefully designed metrics, we observe that few-shot ICL effectively simulates such task-oriented information removal processes, selectively removing the redundant information from entangled non-selective representations, and improving the output based on the demonstrations, which constitutes a key mechanism underlying ICL. Moreover, we identify essential attention heads inducing the removal operation, termed Denoising Heads, which enables the ablation experiments blocking the information removal operation from the inference, where the ICL accuracy significantly degrades, especially when the correct label is absent from the few-shot demonstrations, confirming both the critical role of the information removal mechanism and denoising heads.","{""creator"":""Hakaze Cho, Haolin Yang, Gouki Minegishi, Naoya Inoue"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""Mechanism of Task-oriented Information Removal in In-context Learning"",""link"":""https://arxiv.org/abs/2509.21012"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Hakaze Cho, Haolin Yang, Gouki Minegishi, Naoya Inoue"",""content"":""arXiv:2509.21012v3 Announce Type: replace-cross \nAbstract: In-context Learning (ICL) is an emerging few-shot learning paradigm based on modern Language Models (LMs), yet its inner mechanism remains unclear. In this paper, we investigate the mechanism through a novel perspective of information removal. Specifically, we demonstrate that in the zero-shot scenario, LMs encode queries into non-selective representations in hidden states containing information for all possible tasks, leading to arbitrary outputs without focusing on the intended task, resulting in near-zero accuracy. Meanwhile, we find that selectively removing specific information from hidden states by a low-rank filter effectively steers LMs toward the intended task. Building on these findings, by measuring the hidden states on carefully designed metrics, we observe that few-shot ICL effectively simulates such task-oriented information removal processes, selectively removing the redundant information from entangled non-selective representations, and improving the output based on the demonstrations, which constitutes a key mechanism underlying ICL. Moreover, we identify essential attention heads inducing the removal operation, termed Denoising Heads, which enables the ablation experiments blocking the information removal operation from the inference, where the ICL accuracy significantly degrades, especially when the correct label is absent from the few-shot demonstrations, confirming both the critical role of the information removal mechanism and denoising heads."",""contentSnippet"":""arXiv:2509.21012v3 Announce Type: replace-cross \nAbstract: In-context Learning (ICL) is an emerging few-shot learning paradigm based on modern Language Models (LMs), yet its inner mechanism remains unclear. In this paper, we investigate the mechanism through a novel perspective of information removal. Specifically, we demonstrate that in the zero-shot scenario, LMs encode queries into non-selective representations in hidden states containing information for all possible tasks, leading to arbitrary outputs without focusing on the intended task, resulting in near-zero accuracy. Meanwhile, we find that selectively removing specific information from hidden states by a low-rank filter effectively steers LMs toward the intended task. Building on these findings, by measuring the hidden states on carefully designed metrics, we observe that few-shot ICL effectively simulates such task-oriented information removal processes, selectively removing the redundant information from entangled non-selective representations, and improving the output based on the demonstrations, which constitutes a key mechanism underlying ICL. Moreover, we identify essential attention heads inducing the removal operation, termed Denoising Heads, which enables the ablation experiments blocking the information removal operation from the inference, where the ICL accuracy significantly degrades, especially when the correct label is absent from the few-shot demonstrations, confirming both the critical role of the information removal mechanism and denoising heads."",""guid"":""oai:arXiv.org:2509.21012v3"",""categories"":[""cs.LG"",""cs.AI"",""cs.CL""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,SiNGER: A Clearer Voice Distills Vision Transformers Further,https://arxiv.org/abs/2509.20986,2026-01-30T05:00:00.000Z,"arXiv:2509.20986v3 Announce Type: replace-cross 
Abstract: Vision Transformers are widely adopted as the backbone of vision foundation models, but they are known to produce high-norm artifacts that degrade representation quality. When knowledge distillation transfers these features to students, high-norm artifacts dominate the objective, so students overfit to artifacts and underweight informative signals, diminishing the gains from larger models. Prior work attempted to remove artifacts but encountered an inherent trade-off between artifact suppression and preserving informative signals from teachers. To address this, we introduce Singular Nullspace-Guided Energy Reallocation (SiNGER), a novel distillation framework that suppresses artifacts while preserving informative signals. The key idea is principled teacher feature refinement: during refinement, we leverage the nullspace-guided perturbation to preserve information while suppressing artifacts. Then, the refined teacher's features are distilled to a student. We implement this perturbation efficiently with a LoRA-based adapter that requires minimal structural modification. Extensive experiments show that \oursname consistently improves student models, achieving state-of-the-art performance in multiple downstream tasks and producing clearer and more interpretable representations.","{""creator"":""Geunhyeok Yu, Sunjae Jeong, Yoonyoung Choi, Jaeseung Kim, Hyoseok Hwang"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""SiNGER: A Clearer Voice Distills Vision Transformers Further"",""link"":""https://arxiv.org/abs/2509.20986"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Geunhyeok Yu, Sunjae Jeong, Yoonyoung Choi, Jaeseung Kim, Hyoseok Hwang"",""content"":""arXiv:2509.20986v3 Announce Type: replace-cross \nAbstract: Vision Transformers are widely adopted as the backbone of vision foundation models, but they are known to produce high-norm artifacts that degrade representation quality. When knowledge distillation transfers these features to students, high-norm artifacts dominate the objective, so students overfit to artifacts and underweight informative signals, diminishing the gains from larger models. Prior work attempted to remove artifacts but encountered an inherent trade-off between artifact suppression and preserving informative signals from teachers. To address this, we introduce Singular Nullspace-Guided Energy Reallocation (SiNGER), a novel distillation framework that suppresses artifacts while preserving informative signals. The key idea is principled teacher feature refinement: during refinement, we leverage the nullspace-guided perturbation to preserve information while suppressing artifacts. Then, the refined teacher's features are distilled to a student. We implement this perturbation efficiently with a LoRA-based adapter that requires minimal structural modification. Extensive experiments show that \\oursname consistently improves student models, achieving state-of-the-art performance in multiple downstream tasks and producing clearer and more interpretable representations."",""contentSnippet"":""arXiv:2509.20986v3 Announce Type: replace-cross \nAbstract: Vision Transformers are widely adopted as the backbone of vision foundation models, but they are known to produce high-norm artifacts that degrade representation quality. When knowledge distillation transfers these features to students, high-norm artifacts dominate the objective, so students overfit to artifacts and underweight informative signals, diminishing the gains from larger models. Prior work attempted to remove artifacts but encountered an inherent trade-off between artifact suppression and preserving informative signals from teachers. To address this, we introduce Singular Nullspace-Guided Energy Reallocation (SiNGER), a novel distillation framework that suppresses artifacts while preserving informative signals. The key idea is principled teacher feature refinement: during refinement, we leverage the nullspace-guided perturbation to preserve information while suppressing artifacts. Then, the refined teacher's features are distilled to a student. We implement this perturbation efficiently with a LoRA-based adapter that requires minimal structural modification. Extensive experiments show that \\oursname consistently improves student models, achieving state-of-the-art performance in multiple downstream tasks and producing clearer and more interpretable representations."",""guid"":""oai:arXiv.org:2509.20986v3"",""categories"":[""cs.CV"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Addressing Gradient Misalignment in Data-Augmented Training for Robust Speech Deepfake Detection,https://arxiv.org/abs/2509.20682,2026-01-30T05:00:00.000Z,"arXiv:2509.20682v2 Announce Type: replace-cross 
Abstract: In speech deepfake detection (SDD), data augmentation (DA) is commonly used to improve model generalization across varied speech conditions and spoofing attacks. However, during training, the backpropagated gradients from original and augmented inputs may misalign, which can result in conflicting parameter updates. These conflicts could hinder convergence and push the model toward suboptimal solutions, thereby reducing the benefits of DA. To investigate and address this issue, we design a dual-path data-augmented (DPDA) training framework with gradient alignment for SDD. In our framework, each training utterance is processed through two input paths: one using the original speech and the other with its augmented version. This design allows us to compare and align their backpropagated gradient directions to reduce optimization conflicts. Our analysis shows that approximately 25% of training iterations exhibit gradient conflicts between the original inputs and their augmented counterparts when using RawBoost augmentation. By resolving these conflicts with gradient alignment, our method accelerates convergence by reducing the number of training epochs and achieves up to an 18.69% relative reduction in Equal Error Rate on the In-the-Wild dataset compared to the baseline.","{""creator"":""Duc-Tuan Truong, Tianchi Liu, Junjie Li, Ruijie Tao, Kong Aik Lee, Eng Siong Chng"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""Addressing Gradient Misalignment in Data-Augmented Training for Robust Speech Deepfake Detection"",""link"":""https://arxiv.org/abs/2509.20682"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Duc-Tuan Truong, Tianchi Liu, Junjie Li, Ruijie Tao, Kong Aik Lee, Eng Siong Chng"",""content"":""arXiv:2509.20682v2 Announce Type: replace-cross \nAbstract: In speech deepfake detection (SDD), data augmentation (DA) is commonly used to improve model generalization across varied speech conditions and spoofing attacks. However, during training, the backpropagated gradients from original and augmented inputs may misalign, which can result in conflicting parameter updates. These conflicts could hinder convergence and push the model toward suboptimal solutions, thereby reducing the benefits of DA. To investigate and address this issue, we design a dual-path data-augmented (DPDA) training framework with gradient alignment for SDD. In our framework, each training utterance is processed through two input paths: one using the original speech and the other with its augmented version. This design allows us to compare and align their backpropagated gradient directions to reduce optimization conflicts. Our analysis shows that approximately 25% of training iterations exhibit gradient conflicts between the original inputs and their augmented counterparts when using RawBoost augmentation. By resolving these conflicts with gradient alignment, our method accelerates convergence by reducing the number of training epochs and achieves up to an 18.69% relative reduction in Equal Error Rate on the In-the-Wild dataset compared to the baseline."",""contentSnippet"":""arXiv:2509.20682v2 Announce Type: replace-cross \nAbstract: In speech deepfake detection (SDD), data augmentation (DA) is commonly used to improve model generalization across varied speech conditions and spoofing attacks. However, during training, the backpropagated gradients from original and augmented inputs may misalign, which can result in conflicting parameter updates. These conflicts could hinder convergence and push the model toward suboptimal solutions, thereby reducing the benefits of DA. To investigate and address this issue, we design a dual-path data-augmented (DPDA) training framework with gradient alignment for SDD. In our framework, each training utterance is processed through two input paths: one using the original speech and the other with its augmented version. This design allows us to compare and align their backpropagated gradient directions to reduce optimization conflicts. Our analysis shows that approximately 25% of training iterations exhibit gradient conflicts between the original inputs and their augmented counterparts when using RawBoost augmentation. By resolving these conflicts with gradient alignment, our method accelerates convergence by reducing the number of training epochs and achieves up to an 18.69% relative reduction in Equal Error Rate on the In-the-Wild dataset compared to the baseline."",""guid"":""oai:arXiv.org:2509.20682v2"",""categories"":[""cs.SD"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Understanding Post-Training Structural Changes in Large Language Models,https://arxiv.org/abs/2509.17866,2026-01-30T05:00:00.000Z,"arXiv:2509.17866v4 Announce Type: replace-cross 
Abstract: Post-training fundamentally alters the behavior of large language models (LLMs), yet its impact on the internal parameter space remains poorly understood. In this work, we conduct a systematic singular value decomposition (SVD) analysis of principal linear layers in pretrained LLMs, focusing on two widely adopted post-training methods: instruction tuning and long-chain-of-thought (Long-CoT) distillation. Our analysis reveals two unexpected and robust structural changes: (1) a near-uniform geometric scaling of singular values across layers; and (2) highly consistent orthogonal transformations are applied to the left and right singular vectors of each matrix. Based on these findings, We propose a simple yet effective framework to describe the coordinated dynamics of parameters in LLMs, which elucidates why post-training inherently relies on the foundational capabilities developed during pre-training. Further experiments demonstrate that singular value scaling underpins the temperature-controlled regulatory mechanisms of post-training, while the coordinated rotation of singular vectors encodes the essential semantic alignment. These results challenge the prevailing view of the parameter space in large models as a black box, uncovering the first clear regularities in how parameters evolve during training, and providing a new perspective for deeper investigation into model parameter changes.","{""creator"":""Xinyu He, Xianghui Cao"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""Understanding Post-Training Structural Changes in Large Language Models"",""link"":""https://arxiv.org/abs/2509.17866"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Xinyu He, Xianghui Cao"",""content"":""arXiv:2509.17866v4 Announce Type: replace-cross \nAbstract: Post-training fundamentally alters the behavior of large language models (LLMs), yet its impact on the internal parameter space remains poorly understood. In this work, we conduct a systematic singular value decomposition (SVD) analysis of principal linear layers in pretrained LLMs, focusing on two widely adopted post-training methods: instruction tuning and long-chain-of-thought (Long-CoT) distillation. Our analysis reveals two unexpected and robust structural changes: (1) a near-uniform geometric scaling of singular values across layers; and (2) highly consistent orthogonal transformations are applied to the left and right singular vectors of each matrix. Based on these findings, We propose a simple yet effective framework to describe the coordinated dynamics of parameters in LLMs, which elucidates why post-training inherently relies on the foundational capabilities developed during pre-training. Further experiments demonstrate that singular value scaling underpins the temperature-controlled regulatory mechanisms of post-training, while the coordinated rotation of singular vectors encodes the essential semantic alignment. These results challenge the prevailing view of the parameter space in large models as a black box, uncovering the first clear regularities in how parameters evolve during training, and providing a new perspective for deeper investigation into model parameter changes."",""contentSnippet"":""arXiv:2509.17866v4 Announce Type: replace-cross \nAbstract: Post-training fundamentally alters the behavior of large language models (LLMs), yet its impact on the internal parameter space remains poorly understood. In this work, we conduct a systematic singular value decomposition (SVD) analysis of principal linear layers in pretrained LLMs, focusing on two widely adopted post-training methods: instruction tuning and long-chain-of-thought (Long-CoT) distillation. Our analysis reveals two unexpected and robust structural changes: (1) a near-uniform geometric scaling of singular values across layers; and (2) highly consistent orthogonal transformations are applied to the left and right singular vectors of each matrix. Based on these findings, We propose a simple yet effective framework to describe the coordinated dynamics of parameters in LLMs, which elucidates why post-training inherently relies on the foundational capabilities developed during pre-training. Further experiments demonstrate that singular value scaling underpins the temperature-controlled regulatory mechanisms of post-training, while the coordinated rotation of singular vectors encodes the essential semantic alignment. These results challenge the prevailing view of the parameter space in large models as a black box, uncovering the first clear regularities in how parameters evolve during training, and providing a new perspective for deeper investigation into model parameter changes."",""guid"":""oai:arXiv.org:2509.17866v4"",""categories"":[""cs.LG"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?,https://arxiv.org/abs/2509.17641,2026-01-30T05:00:00.000Z,"arXiv:2509.17641v2 Announce Type: replace-cross 
Abstract: Even without directly hearing sounds, humans can effortlessly reason about auditory properties, such as pitch, loudness, or sound-source associations, drawing on auditory commonsense. In contrast, language models often lack this capability, limiting their effectiveness in multimodal interactions. As an initial step to address this gap, we present AuditoryBench++, a comprehensive benchmark for evaluating auditory knowledge and reasoning in text-only settings. The benchmark encompasses tasks that range from basic auditory comparisons to contextually grounded reasoning, enabling fine-grained analysis of how models process and integrate auditory concepts. In addition, we introduce AIR-CoT, a novel auditory imagination reasoning method that generates and integrates auditory information during inference through span detection with special tokens and knowledge injection. Extensive experiments with recent LLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both the off-the-shelf models and those augmented with auditory knowledge. The project page is available at https://auditorybenchpp.github.io.","{""creator"":""Hyunjong Ok, Suho Yoo, Hyeonjun Kim, Jaeho Lee"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?"",""link"":""https://arxiv.org/abs/2509.17641"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Hyunjong Ok, Suho Yoo, Hyeonjun Kim, Jaeho Lee"",""content"":""arXiv:2509.17641v2 Announce Type: replace-cross \nAbstract: Even without directly hearing sounds, humans can effortlessly reason about auditory properties, such as pitch, loudness, or sound-source associations, drawing on auditory commonsense. In contrast, language models often lack this capability, limiting their effectiveness in multimodal interactions. As an initial step to address this gap, we present AuditoryBench++, a comprehensive benchmark for evaluating auditory knowledge and reasoning in text-only settings. The benchmark encompasses tasks that range from basic auditory comparisons to contextually grounded reasoning, enabling fine-grained analysis of how models process and integrate auditory concepts. In addition, we introduce AIR-CoT, a novel auditory imagination reasoning method that generates and integrates auditory information during inference through span detection with special tokens and knowledge injection. Extensive experiments with recent LLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both the off-the-shelf models and those augmented with auditory knowledge. The project page is available at https://auditorybenchpp.github.io."",""contentSnippet"":""arXiv:2509.17641v2 Announce Type: replace-cross \nAbstract: Even without directly hearing sounds, humans can effortlessly reason about auditory properties, such as pitch, loudness, or sound-source associations, drawing on auditory commonsense. In contrast, language models often lack this capability, limiting their effectiveness in multimodal interactions. As an initial step to address this gap, we present AuditoryBench++, a comprehensive benchmark for evaluating auditory knowledge and reasoning in text-only settings. The benchmark encompasses tasks that range from basic auditory comparisons to contextually grounded reasoning, enabling fine-grained analysis of how models process and integrate auditory concepts. In addition, we introduce AIR-CoT, a novel auditory imagination reasoning method that generates and integrates auditory information during inference through span detection with special tokens and knowledge injection. Extensive experiments with recent LLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both the off-the-shelf models and those augmented with auditory knowledge. The project page is available at https://auditorybenchpp.github.io."",""guid"":""oai:arXiv.org:2509.17641v2"",""categories"":[""cs.CL"",""cs.AI"",""cs.LG"",""cs.SD""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,ArchesClimate: Probabilistic Decadal Ensemble Generation With Flow Matching,https://arxiv.org/abs/2509.15942,2026-01-30T05:00:00.000Z,"arXiv:2509.15942v2 Announce Type: replace-cross 
Abstract: Climate projections have uncertainties related to components of the climate system and their interactions. A typical approach to quantifying these uncertainties is to use climate models to create ensembles of repeated simulations under different initial conditions. Due to the complexity of these simulations, generating such ensembles of projections is computationally expensive. In this work, we present ArchesClimate, a deep learning-based climate model emulator that aims to reduce this cost. ArchesClimate is trained on decadal hindcasts of the IPSL-CM6A-LR climate model at a spatial resolution of approximately 2.5x1.25 degrees. We train a flow matching model following ArchesWeatherGen, which we adapt to predict near-term climate. Once trained, the model generates states at a one-month lead time and can be used to auto-regressively emulate climate model simulations of any length. We show that for up to 10 years, these generations are stable and physically consistent. We also show that for several important climate variables, ArchesClimate generates simulations that are interchangeable with the IPSL model. This work suggests that climate model emulators could significantly reduce the cost of climate model simulations.","{""creator"":""Graham Clyne, Guillaume Couairon, Guillaume Gastineau, Claire Monteleoni, Anastase Charantonis"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""ArchesClimate: Probabilistic Decadal Ensemble Generation With Flow Matching"",""link"":""https://arxiv.org/abs/2509.15942"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Graham Clyne, Guillaume Couairon, Guillaume Gastineau, Claire Monteleoni, Anastase Charantonis"",""content"":""arXiv:2509.15942v2 Announce Type: replace-cross \nAbstract: Climate projections have uncertainties related to components of the climate system and their interactions. A typical approach to quantifying these uncertainties is to use climate models to create ensembles of repeated simulations under different initial conditions. Due to the complexity of these simulations, generating such ensembles of projections is computationally expensive. In this work, we present ArchesClimate, a deep learning-based climate model emulator that aims to reduce this cost. ArchesClimate is trained on decadal hindcasts of the IPSL-CM6A-LR climate model at a spatial resolution of approximately 2.5x1.25 degrees. We train a flow matching model following ArchesWeatherGen, which we adapt to predict near-term climate. Once trained, the model generates states at a one-month lead time and can be used to auto-regressively emulate climate model simulations of any length. We show that for up to 10 years, these generations are stable and physically consistent. We also show that for several important climate variables, ArchesClimate generates simulations that are interchangeable with the IPSL model. This work suggests that climate model emulators could significantly reduce the cost of climate model simulations."",""contentSnippet"":""arXiv:2509.15942v2 Announce Type: replace-cross \nAbstract: Climate projections have uncertainties related to components of the climate system and their interactions. A typical approach to quantifying these uncertainties is to use climate models to create ensembles of repeated simulations under different initial conditions. Due to the complexity of these simulations, generating such ensembles of projections is computationally expensive. In this work, we present ArchesClimate, a deep learning-based climate model emulator that aims to reduce this cost. ArchesClimate is trained on decadal hindcasts of the IPSL-CM6A-LR climate model at a spatial resolution of approximately 2.5x1.25 degrees. We train a flow matching model following ArchesWeatherGen, which we adapt to predict near-term climate. Once trained, the model generates states at a one-month lead time and can be used to auto-regressively emulate climate model simulations of any length. We show that for up to 10 years, these generations are stable and physically consistent. We also show that for several important climate variables, ArchesClimate generates simulations that are interchangeable with the IPSL model. This work suggests that climate model emulators could significantly reduce the cost of climate model simulations."",""guid"":""oai:arXiv.org:2509.15942v2"",""categories"":[""physics.ao-ph"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,DoubleAgents: Interactive Simulations for Alignment in Agentic AI,https://arxiv.org/abs/2509.12626,2026-01-30T05:00:00.000Z,"arXiv:2509.12626v2 Announce Type: replace-cross 
Abstract: Agentic workflows promise efficiency, but adoption hinges on whether people can align systems that act on their behalf with their goals, values, and situational expectations. We present DoubleAgents, an agentic planning tool that embeds transparency and control through user intervention, value-reflecting policies, rich state visualizations, and uncertainty flagging for human coordination tasks. A built-in respondent simulation generates realistic scenarios, allowing users to rehearse and refine policies and calibrate their use of agentic behavior before live deployment. We evaluate DoubleAgents in a two-day lab study (n = 10), three deployment studies, and a technical evaluation. Results show that participants initially hesitated to delegate but used simulation to probe system behavior and adjust policies, gradually increasing delegation as agent actions became better aligned with their intentions and context. Deployment results demonstrate DoubleAgents' real-world relevance and usefulness, showing that simulation helps users effectively manage real-world tasks with higher complexity and uncertainty. We contribute interactive simulation as a practical pathway for users to iteratively align and calibrate agentic systems.","{""creator"":""Tao Long, Xuanming Zhang, Sitong Wang, Zhou Yu, Lydia B Chilton"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""DoubleAgents: Interactive Simulations for Alignment in Agentic AI"",""link"":""https://arxiv.org/abs/2509.12626"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Tao Long, Xuanming Zhang, Sitong Wang, Zhou Yu, Lydia B Chilton"",""content"":""arXiv:2509.12626v2 Announce Type: replace-cross \nAbstract: Agentic workflows promise efficiency, but adoption hinges on whether people can align systems that act on their behalf with their goals, values, and situational expectations. We present DoubleAgents, an agentic planning tool that embeds transparency and control through user intervention, value-reflecting policies, rich state visualizations, and uncertainty flagging for human coordination tasks. A built-in respondent simulation generates realistic scenarios, allowing users to rehearse and refine policies and calibrate their use of agentic behavior before live deployment. We evaluate DoubleAgents in a two-day lab study (n = 10), three deployment studies, and a technical evaluation. Results show that participants initially hesitated to delegate but used simulation to probe system behavior and adjust policies, gradually increasing delegation as agent actions became better aligned with their intentions and context. Deployment results demonstrate DoubleAgents' real-world relevance and usefulness, showing that simulation helps users effectively manage real-world tasks with higher complexity and uncertainty. We contribute interactive simulation as a practical pathway for users to iteratively align and calibrate agentic systems."",""contentSnippet"":""arXiv:2509.12626v2 Announce Type: replace-cross \nAbstract: Agentic workflows promise efficiency, but adoption hinges on whether people can align systems that act on their behalf with their goals, values, and situational expectations. We present DoubleAgents, an agentic planning tool that embeds transparency and control through user intervention, value-reflecting policies, rich state visualizations, and uncertainty flagging for human coordination tasks. A built-in respondent simulation generates realistic scenarios, allowing users to rehearse and refine policies and calibrate their use of agentic behavior before live deployment. We evaluate DoubleAgents in a two-day lab study (n = 10), three deployment studies, and a technical evaluation. Results show that participants initially hesitated to delegate but used simulation to probe system behavior and adjust policies, gradually increasing delegation as agent actions became better aligned with their intentions and context. Deployment results demonstrate DoubleAgents' real-world relevance and usefulness, showing that simulation helps users effectively manage real-world tasks with higher complexity and uncertainty. We contribute interactive simulation as a practical pathway for users to iteratively align and calibrate agentic systems."",""guid"":""oai:arXiv.org:2509.12626v2"",""categories"":[""cs.HC"",""cs.AI"",""cs.CY"",""cs.ET""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning,https://arxiv.org/abs/2509.09332,2026-01-30T05:00:00.000Z,"arXiv:2509.09332v3 Announce Type: replace-cross 
Abstract: Recent advances in multimodal large language models (MLLMs) have opened new opportunities for embodied intelligence, enabling multimodal understanding, reasoning, and interaction, as well as continuous spatial decision-making. Nevertheless, current MLLM-based embodied systems face two critical limitations. First, Geometric Adaptability Gap: models trained solely on 2D inputs or with hard-coded 3D geometry injection suffer from either insufficient spatial information or restricted 2D generalization, leading to poor adaptability across tasks with diverse spatial demands. Second, Embodiment Constraint Gap: prior work often neglects the physical constraints and capacities of real robots, resulting in task plans that are theoretically valid but practically infeasible. To address these gaps, we introduce OmniEVA -- an embodied versatile planner that enables advanced embodied reasoning and task planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding mechanism, which introduces a gated router to perform explicit selective regulation of 3D fusion based on contextual requirements, enabling context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware Reasoning framework that jointly incorporates task goals and embodiment constraints into the reasoning loop, resulting in planning decisions that are both goal-directed and executable. Extensive experimental results demonstrate that OmniEVA not only achieves state-of-the-art general embodied reasoning performance, but also exhibits a strong ability across a wide range of downstream scenarios. Evaluations of a suite of proposed embodied benchmarks, including both primitive and composite tasks, confirm its robust and versatile planning capabilities. Project page: https://omnieva.github.io","{""creator"":""Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yuzheng Zhuang, Bowen Yang, He Zhu, Lingfeng Zhang, Pengwei Xie, David Gamaliel Arcos Bravo, Yingxue Zhang, Jianye Hao, Xingyue Quan"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning"",""link"":""https://arxiv.org/abs/2509.09332"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yuzheng Zhuang, Bowen Yang, He Zhu, Lingfeng Zhang, Pengwei Xie, David Gamaliel Arcos Bravo, Yingxue Zhang, Jianye Hao, Xingyue Quan"",""content"":""arXiv:2509.09332v3 Announce Type: replace-cross \nAbstract: Recent advances in multimodal large language models (MLLMs) have opened new opportunities for embodied intelligence, enabling multimodal understanding, reasoning, and interaction, as well as continuous spatial decision-making. Nevertheless, current MLLM-based embodied systems face two critical limitations. First, Geometric Adaptability Gap: models trained solely on 2D inputs or with hard-coded 3D geometry injection suffer from either insufficient spatial information or restricted 2D generalization, leading to poor adaptability across tasks with diverse spatial demands. Second, Embodiment Constraint Gap: prior work often neglects the physical constraints and capacities of real robots, resulting in task plans that are theoretically valid but practically infeasible. To address these gaps, we introduce OmniEVA -- an embodied versatile planner that enables advanced embodied reasoning and task planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding mechanism, which introduces a gated router to perform explicit selective regulation of 3D fusion based on contextual requirements, enabling context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware Reasoning framework that jointly incorporates task goals and embodiment constraints into the reasoning loop, resulting in planning decisions that are both goal-directed and executable. Extensive experimental results demonstrate that OmniEVA not only achieves state-of-the-art general embodied reasoning performance, but also exhibits a strong ability across a wide range of downstream scenarios. Evaluations of a suite of proposed embodied benchmarks, including both primitive and composite tasks, confirm its robust and versatile planning capabilities. Project page: https://omnieva.github.io"",""contentSnippet"":""arXiv:2509.09332v3 Announce Type: replace-cross \nAbstract: Recent advances in multimodal large language models (MLLMs) have opened new opportunities for embodied intelligence, enabling multimodal understanding, reasoning, and interaction, as well as continuous spatial decision-making. Nevertheless, current MLLM-based embodied systems face two critical limitations. First, Geometric Adaptability Gap: models trained solely on 2D inputs or with hard-coded 3D geometry injection suffer from either insufficient spatial information or restricted 2D generalization, leading to poor adaptability across tasks with diverse spatial demands. Second, Embodiment Constraint Gap: prior work often neglects the physical constraints and capacities of real robots, resulting in task plans that are theoretically valid but practically infeasible. To address these gaps, we introduce OmniEVA -- an embodied versatile planner that enables advanced embodied reasoning and task planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding mechanism, which introduces a gated router to perform explicit selective regulation of 3D fusion based on contextual requirements, enabling context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware Reasoning framework that jointly incorporates task goals and embodiment constraints into the reasoning loop, resulting in planning decisions that are both goal-directed and executable. Extensive experimental results demonstrate that OmniEVA not only achieves state-of-the-art general embodied reasoning performance, but also exhibits a strong ability across a wide range of downstream scenarios. Evaluations of a suite of proposed embodied benchmarks, including both primitive and composite tasks, confirm its robust and versatile planning capabilities. Project page: https://omnieva.github.io"",""guid"":""oai:arXiv.org:2509.09332v3"",""categories"":[""cs.RO"",""cs.AI"",""cs.CL"",""cs.CV""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?,https://arxiv.org/abs/2509.06350,2026-01-30T05:00:00.000Z,"arXiv:2509.06350v2 Announce Type: replace-cross 
Abstract: Jailbreak attacks on Large Language Models (LLMs) have demonstrated various successful methods whereby attackers manipulate models into generating harmful responses that they are designed to avoid. Among these, Greedy Coordinate Gradient (GCG) has emerged as a general and effective approach that optimizes the tokens in a suffix to generate jailbreakable prompts. While several improved variants of GCG have been proposed, they all rely on fixed-length suffixes. However, the potential redundancy within these suffixes remains unexplored. In this work, we propose Mask-GCG, a plug-and-play method that employs learnable token masking to identify impactful tokens within the suffix. Our approach increases the update probability for tokens at high-impact positions while pruning those at low-impact positions. This pruning not only reduces redundancy but also decreases the size of the gradient space, thereby lowering computational overhead and shortening the time required to achieve successful attacks compared to GCG. We evaluate Mask-GCG by applying it to the original GCG and several improved variants. Experimental results show that most tokens in the suffix contribute significantly to attack success, and pruning a minority of low-impact tokens does not affect the loss values or compromise the attack success rate (ASR), thereby revealing token redundancy in LLM prompts. Our findings provide insights for developing efficient and interpretable LLMs from the perspective of jailbreak attacks.","{""creator"":""Junjie Mu, Zonghao Ying, Zhekui Fan, Zonglei Jing, Yaoyuan Zhang, Zhengmin Yu, Wenxin Zhang, Quanchen Zou, Xiangzheng Zhang"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?"",""link"":""https://arxiv.org/abs/2509.06350"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Junjie Mu, Zonghao Ying, Zhekui Fan, Zonglei Jing, Yaoyuan Zhang, Zhengmin Yu, Wenxin Zhang, Quanchen Zou, Xiangzheng Zhang"",""content"":""arXiv:2509.06350v2 Announce Type: replace-cross \nAbstract: Jailbreak attacks on Large Language Models (LLMs) have demonstrated various successful methods whereby attackers manipulate models into generating harmful responses that they are designed to avoid. Among these, Greedy Coordinate Gradient (GCG) has emerged as a general and effective approach that optimizes the tokens in a suffix to generate jailbreakable prompts. While several improved variants of GCG have been proposed, they all rely on fixed-length suffixes. However, the potential redundancy within these suffixes remains unexplored. In this work, we propose Mask-GCG, a plug-and-play method that employs learnable token masking to identify impactful tokens within the suffix. Our approach increases the update probability for tokens at high-impact positions while pruning those at low-impact positions. This pruning not only reduces redundancy but also decreases the size of the gradient space, thereby lowering computational overhead and shortening the time required to achieve successful attacks compared to GCG. We evaluate Mask-GCG by applying it to the original GCG and several improved variants. Experimental results show that most tokens in the suffix contribute significantly to attack success, and pruning a minority of low-impact tokens does not affect the loss values or compromise the attack success rate (ASR), thereby revealing token redundancy in LLM prompts. Our findings provide insights for developing efficient and interpretable LLMs from the perspective of jailbreak attacks."",""contentSnippet"":""arXiv:2509.06350v2 Announce Type: replace-cross \nAbstract: Jailbreak attacks on Large Language Models (LLMs) have demonstrated various successful methods whereby attackers manipulate models into generating harmful responses that they are designed to avoid. Among these, Greedy Coordinate Gradient (GCG) has emerged as a general and effective approach that optimizes the tokens in a suffix to generate jailbreakable prompts. While several improved variants of GCG have been proposed, they all rely on fixed-length suffixes. However, the potential redundancy within these suffixes remains unexplored. In this work, we propose Mask-GCG, a plug-and-play method that employs learnable token masking to identify impactful tokens within the suffix. Our approach increases the update probability for tokens at high-impact positions while pruning those at low-impact positions. This pruning not only reduces redundancy but also decreases the size of the gradient space, thereby lowering computational overhead and shortening the time required to achieve successful attacks compared to GCG. We evaluate Mask-GCG by applying it to the original GCG and several improved variants. Experimental results show that most tokens in the suffix contribute significantly to attack success, and pruning a minority of low-impact tokens does not affect the loss values or compromise the attack success rate (ASR), thereby revealing token redundancy in LLM prompts. Our findings provide insights for developing efficient and interpretable LLMs from the perspective of jailbreak attacks."",""guid"":""oai:arXiv.org:2509.06350v2"",""categories"":[""cs.CL"",""cs.AI"",""cs.CR""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,OPERA: A Reinforcement Learning--Enhanced Orchestrated Planner-Executor Architecture for Reasoning-Oriented Multi-Hop Retrieval,https://arxiv.org/abs/2508.16438,2026-01-30T05:00:00.000Z,"arXiv:2508.16438v3 Announce Type: replace-cross 
Abstract: Recent advances in large language models (LLMs) and dense retrievers have driven significant progress in retrieval-augmented generation (RAG). However, existing approaches face significant challenges in complex reasoning-oriented multi-hop retrieval tasks: 1) Ineffective reasoning-oriented planning: Prior methods struggle to generate robust multi-step plans for complex queries, as rule-based decomposers perform poorly on out-of-template questions. 2) Suboptimal reasoning-driven retrieval: Related methods employ limited query reformulation, leading to iterative retrieval loops that often fail to locate golden documents. 3) Insufficient reasoning-guided filtering: Prevailing methods lack the fine-grained reasoning to effectively filter salient information from noisy results, hindering utilization of retrieved knowledge. Fundamentally, these limitations all stem from the weak coupling between retrieval and reasoning in current RAG architectures. We introduce the Orchestrated Planner-Executor Reasoning Architecture (OPERA), a novel reasoning-driven retrieval framework. OPERA's Goal Planning Module (GPM) decomposes questions into sub-goals, which are executed by a Reason-Execute Module (REM) with specialized components for precise reasoning and effective retrieval. To train OPERA, we propose Multi-Agents Progressive Group Relative Policy Optimization (MAPGRPO), a novel variant of GRPO. Experiments on complex multi-hop benchmarks show OPERA's superior performance, validating both the MAPGRPO method and OPERA's design.","{""creator"":""Yu Liu, Yanbing Liu, Fangfang Yuan, Cong Cao, Youbang Sun, Kun Peng, WeiZhuo Chen, Jianjun Li, Zhiyuan Ma"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""OPERA: A Reinforcement Learning--Enhanced Orchestrated Planner-Executor Architecture for Reasoning-Oriented Multi-Hop Retrieval"",""link"":""https://arxiv.org/abs/2508.16438"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Yu Liu, Yanbing Liu, Fangfang Yuan, Cong Cao, Youbang Sun, Kun Peng, WeiZhuo Chen, Jianjun Li, Zhiyuan Ma"",""content"":""arXiv:2508.16438v3 Announce Type: replace-cross \nAbstract: Recent advances in large language models (LLMs) and dense retrievers have driven significant progress in retrieval-augmented generation (RAG). However, existing approaches face significant challenges in complex reasoning-oriented multi-hop retrieval tasks: 1) Ineffective reasoning-oriented planning: Prior methods struggle to generate robust multi-step plans for complex queries, as rule-based decomposers perform poorly on out-of-template questions. 2) Suboptimal reasoning-driven retrieval: Related methods employ limited query reformulation, leading to iterative retrieval loops that often fail to locate golden documents. 3) Insufficient reasoning-guided filtering: Prevailing methods lack the fine-grained reasoning to effectively filter salient information from noisy results, hindering utilization of retrieved knowledge. Fundamentally, these limitations all stem from the weak coupling between retrieval and reasoning in current RAG architectures. We introduce the Orchestrated Planner-Executor Reasoning Architecture (OPERA), a novel reasoning-driven retrieval framework. OPERA's Goal Planning Module (GPM) decomposes questions into sub-goals, which are executed by a Reason-Execute Module (REM) with specialized components for precise reasoning and effective retrieval. To train OPERA, we propose Multi-Agents Progressive Group Relative Policy Optimization (MAPGRPO), a novel variant of GRPO. Experiments on complex multi-hop benchmarks show OPERA's superior performance, validating both the MAPGRPO method and OPERA's design."",""contentSnippet"":""arXiv:2508.16438v3 Announce Type: replace-cross \nAbstract: Recent advances in large language models (LLMs) and dense retrievers have driven significant progress in retrieval-augmented generation (RAG). However, existing approaches face significant challenges in complex reasoning-oriented multi-hop retrieval tasks: 1) Ineffective reasoning-oriented planning: Prior methods struggle to generate robust multi-step plans for complex queries, as rule-based decomposers perform poorly on out-of-template questions. 2) Suboptimal reasoning-driven retrieval: Related methods employ limited query reformulation, leading to iterative retrieval loops that often fail to locate golden documents. 3) Insufficient reasoning-guided filtering: Prevailing methods lack the fine-grained reasoning to effectively filter salient information from noisy results, hindering utilization of retrieved knowledge. Fundamentally, these limitations all stem from the weak coupling between retrieval and reasoning in current RAG architectures. We introduce the Orchestrated Planner-Executor Reasoning Architecture (OPERA), a novel reasoning-driven retrieval framework. OPERA's Goal Planning Module (GPM) decomposes questions into sub-goals, which are executed by a Reason-Execute Module (REM) with specialized components for precise reasoning and effective retrieval. To train OPERA, we propose Multi-Agents Progressive Group Relative Policy Optimization (MAPGRPO), a novel variant of GRPO. Experiments on complex multi-hop benchmarks show OPERA's superior performance, validating both the MAPGRPO method and OPERA's design."",""guid"":""oai:arXiv.org:2508.16438v3"",""categories"":[""cs.IR"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,X-SAM: From Segment Anything to Any Segmentation,https://arxiv.org/abs/2508.04655,2026-01-30T05:00:00.000Z,"arXiv:2508.04655v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate strong capabilities in broad knowledge representation, yet they are inherently deficient in pixel-level perceptual understanding. Although the Segment Anything Model (SAM) represents a significant advancement in visual-prompt-driven image segmentation, it exhibits notable limitations in multi-mask prediction and category-specific segmentation tasks, and it cannot integrate all segmentation tasks within a unified model architecture. To address these limitations, we present X-SAM, a streamlined Multimodal Large Language Model (MLLM) framework that extends the segmentation paradigm from \textit{segment anything} to \textit{any segmentation}. Specifically, we introduce a novel unified framework that enables more advanced pixel-level perceptual comprehension for MLLMs. Furthermore, we propose a new segmentation task, termed Visual GrounDed (VGD) segmentation, which segments all instance objects with interactive visual prompts and empowers MLLMs with visual grounded, pixel-wise interpretative capabilities. To enable effective training on diverse data sources, we present a unified training strategy that supports co-training across multiple datasets. Experimental results demonstrate that X-SAM achieves state-of-the-art performance on a wide range of image segmentation benchmarks, highlighting its efficiency for multimodal, pixel-level visual understanding. Code is available at https://github.com/wanghao9610/X-SAM.","{""creator"":""Hao Wang, Limeng Qiao, Zequn Jie, Zhijian Huang, Chengjian Feng, Qingfang Zheng, Lin Ma, Xiangyuan Lan, Xiaodan Liang"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""X-SAM: From Segment Anything to Any Segmentation"",""link"":""https://arxiv.org/abs/2508.04655"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Hao Wang, Limeng Qiao, Zequn Jie, Zhijian Huang, Chengjian Feng, Qingfang Zheng, Lin Ma, Xiangyuan Lan, Xiaodan Liang"",""content"":""arXiv:2508.04655v2 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) demonstrate strong capabilities in broad knowledge representation, yet they are inherently deficient in pixel-level perceptual understanding. Although the Segment Anything Model (SAM) represents a significant advancement in visual-prompt-driven image segmentation, it exhibits notable limitations in multi-mask prediction and category-specific segmentation tasks, and it cannot integrate all segmentation tasks within a unified model architecture. To address these limitations, we present X-SAM, a streamlined Multimodal Large Language Model (MLLM) framework that extends the segmentation paradigm from \\textit{segment anything} to \\textit{any segmentation}. Specifically, we introduce a novel unified framework that enables more advanced pixel-level perceptual comprehension for MLLMs. Furthermore, we propose a new segmentation task, termed Visual GrounDed (VGD) segmentation, which segments all instance objects with interactive visual prompts and empowers MLLMs with visual grounded, pixel-wise interpretative capabilities. To enable effective training on diverse data sources, we present a unified training strategy that supports co-training across multiple datasets. Experimental results demonstrate that X-SAM achieves state-of-the-art performance on a wide range of image segmentation benchmarks, highlighting its efficiency for multimodal, pixel-level visual understanding. Code is available at https://github.com/wanghao9610/X-SAM."",""contentSnippet"":""arXiv:2508.04655v2 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) demonstrate strong capabilities in broad knowledge representation, yet they are inherently deficient in pixel-level perceptual understanding. Although the Segment Anything Model (SAM) represents a significant advancement in visual-prompt-driven image segmentation, it exhibits notable limitations in multi-mask prediction and category-specific segmentation tasks, and it cannot integrate all segmentation tasks within a unified model architecture. To address these limitations, we present X-SAM, a streamlined Multimodal Large Language Model (MLLM) framework that extends the segmentation paradigm from \\textit{segment anything} to \\textit{any segmentation}. Specifically, we introduce a novel unified framework that enables more advanced pixel-level perceptual comprehension for MLLMs. Furthermore, we propose a new segmentation task, termed Visual GrounDed (VGD) segmentation, which segments all instance objects with interactive visual prompts and empowers MLLMs with visual grounded, pixel-wise interpretative capabilities. To enable effective training on diverse data sources, we present a unified training strategy that supports co-training across multiple datasets. Experimental results demonstrate that X-SAM achieves state-of-the-art performance on a wide range of image segmentation benchmarks, highlighting its efficiency for multimodal, pixel-level visual understanding. Code is available at https://github.com/wanghao9610/X-SAM."",""guid"":""oai:arXiv.org:2508.04655v2"",""categories"":[""cs.CV"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,FAIR-MATCH: A Multi-Objective Framework for Bias Mitigation in Reciprocal Dating Recommendations,https://arxiv.org/abs/2507.01063,2026-01-30T05:00:00.000Z,"arXiv:2507.01063v2 Announce Type: replace-cross 
Abstract: Online dating platforms have fundamentally transformed the formation of romantic relationships, with millions of users worldwide relying on algorithmic matching systems to find compatible partners. However, current recommendation systems in dating applications suffer from significant algorithmic deficiencies, including but not limited to popularity bias, filter bubble effects, and inadequate reciprocity modeling that limit effectiveness and introduce harmful biases. This research integrates foundational work with recent empirical findings to deliver a detailed analysis of dating app recommendation systems, highlighting key issues and suggesting research-backed solutions. Through analysis of reciprocal recommendation frameworks, fairness evaluation metrics, and industry implementations, we demonstrate that current systems achieve modest performance with collaborative filtering reaching 25.1\% while reciprocal methods achieve 28.7\%. Our proposed mathematical framework addresses these limitations through enhanced similarity measures, multi-objective optimization, and fairness-aware algorithms that maintain competitive accuracy while improving demographic representation to reduce algorithmic bias.","{""creator"":""Madhav Kotecha"",""rights"":""http://creativecommons.org/licenses/by-nc-sa/4.0/"",""title"":""FAIR-MATCH: A Multi-Objective Framework for Bias Mitigation in Reciprocal Dating Recommendations"",""link"":""https://arxiv.org/abs/2507.01063"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Madhav Kotecha"",""content"":""arXiv:2507.01063v2 Announce Type: replace-cross \nAbstract: Online dating platforms have fundamentally transformed the formation of romantic relationships, with millions of users worldwide relying on algorithmic matching systems to find compatible partners. However, current recommendation systems in dating applications suffer from significant algorithmic deficiencies, including but not limited to popularity bias, filter bubble effects, and inadequate reciprocity modeling that limit effectiveness and introduce harmful biases. This research integrates foundational work with recent empirical findings to deliver a detailed analysis of dating app recommendation systems, highlighting key issues and suggesting research-backed solutions. Through analysis of reciprocal recommendation frameworks, fairness evaluation metrics, and industry implementations, we demonstrate that current systems achieve modest performance with collaborative filtering reaching 25.1\\% while reciprocal methods achieve 28.7\\%. Our proposed mathematical framework addresses these limitations through enhanced similarity measures, multi-objective optimization, and fairness-aware algorithms that maintain competitive accuracy while improving demographic representation to reduce algorithmic bias."",""contentSnippet"":""arXiv:2507.01063v2 Announce Type: replace-cross \nAbstract: Online dating platforms have fundamentally transformed the formation of romantic relationships, with millions of users worldwide relying on algorithmic matching systems to find compatible partners. However, current recommendation systems in dating applications suffer from significant algorithmic deficiencies, including but not limited to popularity bias, filter bubble effects, and inadequate reciprocity modeling that limit effectiveness and introduce harmful biases. This research integrates foundational work with recent empirical findings to deliver a detailed analysis of dating app recommendation systems, highlighting key issues and suggesting research-backed solutions. Through analysis of reciprocal recommendation frameworks, fairness evaluation metrics, and industry implementations, we demonstrate that current systems achieve modest performance with collaborative filtering reaching 25.1\\% while reciprocal methods achieve 28.7\\%. Our proposed mathematical framework addresses these limitations through enhanced similarity measures, multi-objective optimization, and fairness-aware algorithms that maintain competitive accuracy while improving demographic representation to reduce algorithmic bias."",""guid"":""oai:arXiv.org:2507.01063v2"",""categories"":[""cs.IR"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Governing Strategic Dynamics: Equilibrium Stabilization via Divergence-Driven Control,https://arxiv.org/abs/2506.23734,2026-01-30T05:00:00.000Z,"arXiv:2506.23734v2 Announce Type: replace-cross 
Abstract: Black-box coevolution in mixed-motive games is often undermined by opponent-drift non-stationarity and noisy rollouts, which distort progress signals and can induce cycling, Red-Queen dynamics, and detachment. We propose the \emph{Marker Gene Method} (MGM), a curriculum-inspired governance mechanism that stabilizes selection by anchoring evaluation to cross-generational marker individuals, together with DWAM and conservative marker-update rules to reduce spurious updates. We also introduce NGD-Div, which adapts the key update threshold using a divergence proxy and natural-gradient optimization. We provide theoretical analysis in strictly competitive settings and evaluate MGM integrated with evolution strategies (MGM-E-NES) on coordination games and a resource-depletion Markov game. MGM-E-NES reliably recovers target coordination in Stag Hunt and Battle of the Sexes, achieving final cooperation probabilities close to $(1,1)$ (e.g., $0.991\pm0.01/1.00\pm0.00$ and $0.97\pm0.00/0.97\pm0.00$ for the two players). In the Markov resource game, it maintains high and stable state-conditioned cooperation across 30 seeds, with final cooperation of $\approx 0.954/0.980/0.916$ in \textsc{Rich}/\textsc{Poor}/\textsc{Collapsed} (both players; small standard deviations), indicating welfare-aligned and state-dependent behavior. Overall, MGM-E-NES transfers across tasks with minimal hyperparameter changes and yields consistently stable training dynamics, showing that top-level governance can substantially improve the robustness of black-box coevolution in dynamic environments.","{""creator"":""Hao Shi, Xi Li, Fangfang Xie"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""Governing Strategic Dynamics: Equilibrium Stabilization via Divergence-Driven Control"",""link"":""https://arxiv.org/abs/2506.23734"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Hao Shi, Xi Li, Fangfang Xie"",""content"":""arXiv:2506.23734v2 Announce Type: replace-cross \nAbstract: Black-box coevolution in mixed-motive games is often undermined by opponent-drift non-stationarity and noisy rollouts, which distort progress signals and can induce cycling, Red-Queen dynamics, and detachment. We propose the \\emph{Marker Gene Method} (MGM), a curriculum-inspired governance mechanism that stabilizes selection by anchoring evaluation to cross-generational marker individuals, together with DWAM and conservative marker-update rules to reduce spurious updates. We also introduce NGD-Div, which adapts the key update threshold using a divergence proxy and natural-gradient optimization. We provide theoretical analysis in strictly competitive settings and evaluate MGM integrated with evolution strategies (MGM-E-NES) on coordination games and a resource-depletion Markov game. MGM-E-NES reliably recovers target coordination in Stag Hunt and Battle of the Sexes, achieving final cooperation probabilities close to $(1,1)$ (e.g., $0.991\\pm0.01/1.00\\pm0.00$ and $0.97\\pm0.00/0.97\\pm0.00$ for the two players). In the Markov resource game, it maintains high and stable state-conditioned cooperation across 30 seeds, with final cooperation of $\\approx 0.954/0.980/0.916$ in \\textsc{Rich}/\\textsc{Poor}/\\textsc{Collapsed} (both players; small standard deviations), indicating welfare-aligned and state-dependent behavior. Overall, MGM-E-NES transfers across tasks with minimal hyperparameter changes and yields consistently stable training dynamics, showing that top-level governance can substantially improve the robustness of black-box coevolution in dynamic environments."",""contentSnippet"":""arXiv:2506.23734v2 Announce Type: replace-cross \nAbstract: Black-box coevolution in mixed-motive games is often undermined by opponent-drift non-stationarity and noisy rollouts, which distort progress signals and can induce cycling, Red-Queen dynamics, and detachment. We propose the \\emph{Marker Gene Method} (MGM), a curriculum-inspired governance mechanism that stabilizes selection by anchoring evaluation to cross-generational marker individuals, together with DWAM and conservative marker-update rules to reduce spurious updates. We also introduce NGD-Div, which adapts the key update threshold using a divergence proxy and natural-gradient optimization. We provide theoretical analysis in strictly competitive settings and evaluate MGM integrated with evolution strategies (MGM-E-NES) on coordination games and a resource-depletion Markov game. MGM-E-NES reliably recovers target coordination in Stag Hunt and Battle of the Sexes, achieving final cooperation probabilities close to $(1,1)$ (e.g., $0.991\\pm0.01/1.00\\pm0.00$ and $0.97\\pm0.00/0.97\\pm0.00$ for the two players). In the Markov resource game, it maintains high and stable state-conditioned cooperation across 30 seeds, with final cooperation of $\\approx 0.954/0.980/0.916$ in \\textsc{Rich}/\\textsc{Poor}/\\textsc{Collapsed} (both players; small standard deviations), indicating welfare-aligned and state-dependent behavior. Overall, MGM-E-NES transfers across tasks with minimal hyperparameter changes and yields consistently stable training dynamics, showing that top-level governance can substantially improve the robustness of black-box coevolution in dynamic environments."",""guid"":""oai:arXiv.org:2506.23734v2"",""categories"":[""cs.NE"",""cs.AI"",""cs.GT""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning with Video LLMs,https://arxiv.org/abs/2506.11558,2026-01-30T05:00:00.000Z,"arXiv:2506.11558v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have recently been extended to the video domain, enabling sophisticated video-language understanding. However, existing Video LLMs often exhibit limitations in fine-grained temporal reasoning, restricting their ability to precisely attribute responses to specific video moments, especially under constrained supervision. We introduce DaMO, a data-efficient Video LLM explicitly designed for accurate temporal reasoning and multimodal understanding. At its core, the proposed Temporal-aware Fuseformer employs a hierarchical dual-stream architecture that progressively captures temporal dynamics within each modality and effectively fuses complementary visual and audio information. To further enhance computational efficiency, DaMO integrates a global residual that reduces spatial redundancy while preserving essential semantic details. We train DaMO via a structured four-stage progressive training paradigm, incrementally equipping the model with multimodal alignment, semantic grounding, and temporal reasoning capabilities. This work also contributes multiple datasets augmented from existing ones with LLM-generated temporally grounded QA pairs for tasks requiring temporal supervision. Comprehensive experiments on temporal grounding and video QA benchmarks demonstrate that DaMO consistently surpasses prior methods, particularly in tasks demanding precise temporal alignment and reasoning. Our work establishes a promising direction for data-efficient video-language modeling.","{""creator"":""Bo-Cheng Chiu, Jen-Jee Chen, Yu-Chee Tseng, Feng-Chi Chen, An-Zi Yen"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning with Video LLMs"",""link"":""https://arxiv.org/abs/2506.11558"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Bo-Cheng Chiu, Jen-Jee Chen, Yu-Chee Tseng, Feng-Chi Chen, An-Zi Yen"",""content"":""arXiv:2506.11558v4 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) have recently been extended to the video domain, enabling sophisticated video-language understanding. However, existing Video LLMs often exhibit limitations in fine-grained temporal reasoning, restricting their ability to precisely attribute responses to specific video moments, especially under constrained supervision. We introduce DaMO, a data-efficient Video LLM explicitly designed for accurate temporal reasoning and multimodal understanding. At its core, the proposed Temporal-aware Fuseformer employs a hierarchical dual-stream architecture that progressively captures temporal dynamics within each modality and effectively fuses complementary visual and audio information. To further enhance computational efficiency, DaMO integrates a global residual that reduces spatial redundancy while preserving essential semantic details. We train DaMO via a structured four-stage progressive training paradigm, incrementally equipping the model with multimodal alignment, semantic grounding, and temporal reasoning capabilities. This work also contributes multiple datasets augmented from existing ones with LLM-generated temporally grounded QA pairs for tasks requiring temporal supervision. Comprehensive experiments on temporal grounding and video QA benchmarks demonstrate that DaMO consistently surpasses prior methods, particularly in tasks demanding precise temporal alignment and reasoning. Our work establishes a promising direction for data-efficient video-language modeling."",""contentSnippet"":""arXiv:2506.11558v4 Announce Type: replace-cross \nAbstract: Large Language Models (LLMs) have recently been extended to the video domain, enabling sophisticated video-language understanding. However, existing Video LLMs often exhibit limitations in fine-grained temporal reasoning, restricting their ability to precisely attribute responses to specific video moments, especially under constrained supervision. We introduce DaMO, a data-efficient Video LLM explicitly designed for accurate temporal reasoning and multimodal understanding. At its core, the proposed Temporal-aware Fuseformer employs a hierarchical dual-stream architecture that progressively captures temporal dynamics within each modality and effectively fuses complementary visual and audio information. To further enhance computational efficiency, DaMO integrates a global residual that reduces spatial redundancy while preserving essential semantic details. We train DaMO via a structured four-stage progressive training paradigm, incrementally equipping the model with multimodal alignment, semantic grounding, and temporal reasoning capabilities. This work also contributes multiple datasets augmented from existing ones with LLM-generated temporally grounded QA pairs for tasks requiring temporal supervision. Comprehensive experiments on temporal grounding and video QA benchmarks demonstrate that DaMO consistently surpasses prior methods, particularly in tasks demanding precise temporal alignment and reasoning. Our work establishes a promising direction for data-efficient video-language modeling."",""guid"":""oai:arXiv.org:2506.11558v4"",""categories"":[""cs.CV"",""cs.AI"",""cs.CL""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Beyond Random Sampling: Efficient Language Model Pretraining via Curriculum Learning,https://arxiv.org/abs/2506.11300,2026-01-30T05:00:00.000Z,"arXiv:2506.11300v2 Announce Type: replace-cross 
Abstract: Curriculum learning-organizing training data from easy to hard-has improved efficiency across machine learning domains, yet remains underexplored for language model pretraining. We present the first systematic investigation of curriculum learning in LLM pretraining, with over 200 models trained on up to 100B tokens across three strategies: vanilla curriculum learning, pacing-based sampling, and interleaved curricula, guided by six difficulty metrics spanning linguistic and information-theoretic properties. We evaluate performance on eight benchmarks under three realistic scenarios: limited data, unlimited data, and continual training. Our experiments show that curriculum learning consistently accelerates convergence in early and mid-training phases,reducing training steps by $18-45\%$ to reach baseline performance. When applied as a warmup strategy before standard random sampling, curriculum learning yields sustained improvements up to $3.5\%$. We identify compression ratio, lexical diversity (MTLD), and readability (Flesch Reading Ease) as the most effective difficulty signals. Our findings demonstrate that data ordering-orthogonal to existing data selection methods-provides a practical mechanism for more efficient LLM pretraining.","{""creator"":""Yang Zhang, Amr Mohamed, Hadi Abdine, Guokan Shang, Michalis Vazirgiannis"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""Beyond Random Sampling: Efficient Language Model Pretraining via Curriculum Learning"",""link"":""https://arxiv.org/abs/2506.11300"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Yang Zhang, Amr Mohamed, Hadi Abdine, Guokan Shang, Michalis Vazirgiannis"",""content"":""arXiv:2506.11300v2 Announce Type: replace-cross \nAbstract: Curriculum learning-organizing training data from easy to hard-has improved efficiency across machine learning domains, yet remains underexplored for language model pretraining. We present the first systematic investigation of curriculum learning in LLM pretraining, with over 200 models trained on up to 100B tokens across three strategies: vanilla curriculum learning, pacing-based sampling, and interleaved curricula, guided by six difficulty metrics spanning linguistic and information-theoretic properties. We evaluate performance on eight benchmarks under three realistic scenarios: limited data, unlimited data, and continual training. Our experiments show that curriculum learning consistently accelerates convergence in early and mid-training phases,reducing training steps by $18-45\\%$ to reach baseline performance. When applied as a warmup strategy before standard random sampling, curriculum learning yields sustained improvements up to $3.5\\%$. We identify compression ratio, lexical diversity (MTLD), and readability (Flesch Reading Ease) as the most effective difficulty signals. Our findings demonstrate that data ordering-orthogonal to existing data selection methods-provides a practical mechanism for more efficient LLM pretraining."",""contentSnippet"":""arXiv:2506.11300v2 Announce Type: replace-cross \nAbstract: Curriculum learning-organizing training data from easy to hard-has improved efficiency across machine learning domains, yet remains underexplored for language model pretraining. We present the first systematic investigation of curriculum learning in LLM pretraining, with over 200 models trained on up to 100B tokens across three strategies: vanilla curriculum learning, pacing-based sampling, and interleaved curricula, guided by six difficulty metrics spanning linguistic and information-theoretic properties. We evaluate performance on eight benchmarks under three realistic scenarios: limited data, unlimited data, and continual training. Our experiments show that curriculum learning consistently accelerates convergence in early and mid-training phases,reducing training steps by $18-45\\%$ to reach baseline performance. When applied as a warmup strategy before standard random sampling, curriculum learning yields sustained improvements up to $3.5\\%$. We identify compression ratio, lexical diversity (MTLD), and readability (Flesch Reading Ease) as the most effective difficulty signals. Our findings demonstrate that data ordering-orthogonal to existing data selection methods-provides a practical mechanism for more efficient LLM pretraining."",""guid"":""oai:arXiv.org:2506.11300v2"",""categories"":[""cs.CL"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization,https://arxiv.org/abs/2506.07972,2026-01-30T05:00:00.000Z,"arXiv:2506.07972v2 Announce Type: replace-cross 
Abstract: While Large Language Models (LLMs) have demonstrated significant advancements in reasoning and agent-based problem-solving, current evaluation methodologies fail to adequately assess their capabilities: existing benchmarks either rely on closed-ended questions prone to saturation and memorization, or subjective comparisons that lack consistency and rigor. In this work, we introduce HeuriGym, an agentic framework designed for evaluating heuristic algorithms generated by LLMs for combinatorial optimization problems, characterized by clearly defined objectives and expansive solution spaces. HeuriGym empowers LLMs to propose heuristics, receive evaluative feedback via code execution, and iteratively refine their solutions. We evaluate nine state-of-the-art models on nine problems across domains such as computer systems, logistics, and biology, exposing persistent limitations in tool use, planning, and adaptive reasoning. To quantify performance, we propose the Quality-Yield Index (QYI), a metric that captures both solution pass rate and quality. Even top models like GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below the expert baseline of 1. Our open-source benchmark aims to guide the development of LLMs toward more effective and realistic problem-solving in scientific and engineering domains.","{""creator"":""Hongzheng Chen, Yingheng Wang, Yaohui Cai, Hins Hu, Jiajie Li, Shirley Huang, Chenhui Deng, Rongjian Liang, Shufeng Kong, Haoxing Ren, Samitha Samaranayake, Carla P. Gomes, Zhiru Zhang"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization"",""link"":""https://arxiv.org/abs/2506.07972"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Hongzheng Chen, Yingheng Wang, Yaohui Cai, Hins Hu, Jiajie Li, Shirley Huang, Chenhui Deng, Rongjian Liang, Shufeng Kong, Haoxing Ren, Samitha Samaranayake, Carla P. Gomes, Zhiru Zhang"",""content"":""arXiv:2506.07972v2 Announce Type: replace-cross \nAbstract: While Large Language Models (LLMs) have demonstrated significant advancements in reasoning and agent-based problem-solving, current evaluation methodologies fail to adequately assess their capabilities: existing benchmarks either rely on closed-ended questions prone to saturation and memorization, or subjective comparisons that lack consistency and rigor. In this work, we introduce HeuriGym, an agentic framework designed for evaluating heuristic algorithms generated by LLMs for combinatorial optimization problems, characterized by clearly defined objectives and expansive solution spaces. HeuriGym empowers LLMs to propose heuristics, receive evaluative feedback via code execution, and iteratively refine their solutions. We evaluate nine state-of-the-art models on nine problems across domains such as computer systems, logistics, and biology, exposing persistent limitations in tool use, planning, and adaptive reasoning. To quantify performance, we propose the Quality-Yield Index (QYI), a metric that captures both solution pass rate and quality. Even top models like GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below the expert baseline of 1. Our open-source benchmark aims to guide the development of LLMs toward more effective and realistic problem-solving in scientific and engineering domains."",""contentSnippet"":""arXiv:2506.07972v2 Announce Type: replace-cross \nAbstract: While Large Language Models (LLMs) have demonstrated significant advancements in reasoning and agent-based problem-solving, current evaluation methodologies fail to adequately assess their capabilities: existing benchmarks either rely on closed-ended questions prone to saturation and memorization, or subjective comparisons that lack consistency and rigor. In this work, we introduce HeuriGym, an agentic framework designed for evaluating heuristic algorithms generated by LLMs for combinatorial optimization problems, characterized by clearly defined objectives and expansive solution spaces. HeuriGym empowers LLMs to propose heuristics, receive evaluative feedback via code execution, and iteratively refine their solutions. We evaluate nine state-of-the-art models on nine problems across domains such as computer systems, logistics, and biology, exposing persistent limitations in tool use, planning, and adaptive reasoning. To quantify performance, we propose the Quality-Yield Index (QYI), a metric that captures both solution pass rate and quality. Even top models like GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below the expert baseline of 1. Our open-source benchmark aims to guide the development of LLMs toward more effective and realistic problem-solving in scientific and engineering domains."",""guid"":""oai:arXiv.org:2506.07972v2"",""categories"":[""cs.LG"",""cs.AI"",""cs.CL""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning,https://arxiv.org/abs/2506.04207,2026-01-30T05:00:00.000Z,"arXiv:2506.04207v2 Announce Type: replace-cross 
Abstract: Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex textual tasks, many works attempt to incentivize similar capabilities in Multimodal Large Language Models (MLLMs) by directly applying reinforcement learning (RL). However, they still struggle to activate complex reasoning. In this paper, rather than examining multimodal RL in isolation, we delve into current training pipelines and identify three crucial phenomena: 1) Effective cold start initialization is critical for enhancing MLLM reasoning. Intriguingly, we find that initializing with carefully selected text data alone can lead to performance surpassing many recent multimodal reasoning models, even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers from gradient stagnation, which degrades training stability and performance. 3) Subsequent text-only RL training, following the multimodal RL phase, further enhances multimodal reasoning. This staged training approach effectively balances perceptual grounding and cognitive reasoning development. By incorporating the above insights and addressing multimodal RL issues, we introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath, LogicVista, DynaMath, and challenging AIME2024 and AIME2025.","{""creator"":""Shuang Chen, Yue Guo, Zhaochen Su, Yafu Li, Yulun Wu, Jiacheng Chen, Jiayu Chen, Weijie Wang, Xiaoye Qu, Yu Cheng"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning"",""link"":""https://arxiv.org/abs/2506.04207"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Shuang Chen, Yue Guo, Zhaochen Su, Yafu Li, Yulun Wu, Jiacheng Chen, Jiayu Chen, Weijie Wang, Xiaoye Qu, Yu Cheng"",""content"":""arXiv:2506.04207v2 Announce Type: replace-cross \nAbstract: Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex textual tasks, many works attempt to incentivize similar capabilities in Multimodal Large Language Models (MLLMs) by directly applying reinforcement learning (RL). However, they still struggle to activate complex reasoning. In this paper, rather than examining multimodal RL in isolation, we delve into current training pipelines and identify three crucial phenomena: 1) Effective cold start initialization is critical for enhancing MLLM reasoning. Intriguingly, we find that initializing with carefully selected text data alone can lead to performance surpassing many recent multimodal reasoning models, even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers from gradient stagnation, which degrades training stability and performance. 3) Subsequent text-only RL training, following the multimodal RL phase, further enhances multimodal reasoning. This staged training approach effectively balances perceptual grounding and cognitive reasoning development. By incorporating the above insights and addressing multimodal RL issues, we introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath, LogicVista, DynaMath, and challenging AIME2024 and AIME2025."",""contentSnippet"":""arXiv:2506.04207v2 Announce Type: replace-cross \nAbstract: Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex textual tasks, many works attempt to incentivize similar capabilities in Multimodal Large Language Models (MLLMs) by directly applying reinforcement learning (RL). However, they still struggle to activate complex reasoning. In this paper, rather than examining multimodal RL in isolation, we delve into current training pipelines and identify three crucial phenomena: 1) Effective cold start initialization is critical for enhancing MLLM reasoning. Intriguingly, we find that initializing with carefully selected text data alone can lead to performance surpassing many recent multimodal reasoning models, even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers from gradient stagnation, which degrades training stability and performance. 3) Subsequent text-only RL training, following the multimodal RL phase, further enhances multimodal reasoning. This staged training approach effectively balances perceptual grounding and cognitive reasoning development. By incorporating the above insights and addressing multimodal RL issues, we introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath, LogicVista, DynaMath, and challenging AIME2024 and AIME2025."",""guid"":""oai:arXiv.org:2506.04207v2"",""categories"":[""cs.LG"",""cs.AI"",""cs.CL"",""cs.CV""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Can AI Master Econometrics? Evidence from Econometrics AI Agent on Expert-Level Tasks,https://arxiv.org/abs/2506.00856,2026-01-30T05:00:00.000Z,"arXiv:2506.00856v3 Announce Type: replace-cross 
Abstract: Can AI effectively perform complex econometric analysis traditionally requiring human expertise? This paper evaluates AI agents' capability to master econometrics, focusing on empirical analysis performance. We develop ``MetricsAI'', an Econometrics AI Agent built on the open-source MetaGPT framework. This agent exhibits outstanding performance in: (1) planning econometric tasks strategically, (2) generating and executing code, (3) employing error-based reflection for improved robustness, and (4) allowing iterative refinement through multi-round conversations. We construct two datasets from academic coursework materials and published research papers to evaluate performance against real-world challenges. Comparative testing shows our domain-specialized AI agent significantly outperforms both benchmark large language models (LLMs) and general-purpose AI agents. This work establishes a testbed for exploring AI's impact on social science research and enables cost-effective integration of domain expertise, making advanced econometric methods accessible to users with minimal coding skills. Furthermore, our AI agent enhances research reproducibility and offers promising pedagogical applications for econometrics teaching.","{""creator"":""Qiang Chen, Tianyang Han, Jin Li, Ye Luo, Zigan Wang, Yuxiao Wu, Xiaowei Zhang, Tuo Zhou"",""rights"":""http://creativecommons.org/licenses/by-nc-nd/4.0/"",""title"":""Can AI Master Econometrics? Evidence from Econometrics AI Agent on Expert-Level Tasks"",""link"":""https://arxiv.org/abs/2506.00856"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Qiang Chen, Tianyang Han, Jin Li, Ye Luo, Zigan Wang, Yuxiao Wu, Xiaowei Zhang, Tuo Zhou"",""content"":""arXiv:2506.00856v3 Announce Type: replace-cross \nAbstract: Can AI effectively perform complex econometric analysis traditionally requiring human expertise? This paper evaluates AI agents' capability to master econometrics, focusing on empirical analysis performance. We develop ``MetricsAI'', an Econometrics AI Agent built on the open-source MetaGPT framework. This agent exhibits outstanding performance in: (1) planning econometric tasks strategically, (2) generating and executing code, (3) employing error-based reflection for improved robustness, and (4) allowing iterative refinement through multi-round conversations. We construct two datasets from academic coursework materials and published research papers to evaluate performance against real-world challenges. Comparative testing shows our domain-specialized AI agent significantly outperforms both benchmark large language models (LLMs) and general-purpose AI agents. This work establishes a testbed for exploring AI's impact on social science research and enables cost-effective integration of domain expertise, making advanced econometric methods accessible to users with minimal coding skills. Furthermore, our AI agent enhances research reproducibility and offers promising pedagogical applications for econometrics teaching."",""contentSnippet"":""arXiv:2506.00856v3 Announce Type: replace-cross \nAbstract: Can AI effectively perform complex econometric analysis traditionally requiring human expertise? This paper evaluates AI agents' capability to master econometrics, focusing on empirical analysis performance. We develop ``MetricsAI'', an Econometrics AI Agent built on the open-source MetaGPT framework. This agent exhibits outstanding performance in: (1) planning econometric tasks strategically, (2) generating and executing code, (3) employing error-based reflection for improved robustness, and (4) allowing iterative refinement through multi-round conversations. We construct two datasets from academic coursework materials and published research papers to evaluate performance against real-world challenges. Comparative testing shows our domain-specialized AI agent significantly outperforms both benchmark large language models (LLMs) and general-purpose AI agents. This work establishes a testbed for exploring AI's impact on social science research and enables cost-effective integration of domain expertise, making advanced econometric methods accessible to users with minimal coding skills. Furthermore, our AI agent enhances research reproducibility and offers promising pedagogical applications for econometrics teaching."",""guid"":""oai:arXiv.org:2506.00856v3"",""categories"":[""econ.EM"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Orca: Browsing at Scale Through User-Driven and AI-Facilitated Orchestration Across Malleable Webpages,https://arxiv.org/abs/2505.22831,2026-01-30T05:00:00.000Z,"arXiv:2505.22831v2 Announce Type: replace-cross 
Abstract: Web-based activities span multiple webpages. However, conventional browsers with stacks of tabs cannot support operating and synthesizing large volumes of information across pages. While recent AI systems enable fully automated web browsing and information synthesis, they often diminish user agency and hinder contextual understanding. We explore how AI could instead augment user interactions with content across webpages and mitigate cognitive and manual efforts. Through literature on information tasks and web browsing challenges, and an iterative design process, we present novel interactions with our prototype web browser, Orca. Leveraging AI, Orca supports user-driven exploration, operation, organization, and synthesis of web content at scale. To enable browsing at scale, webpages are treated as malleable materials that humans and AI can collaboratively manipulate and compose into a malleable, dynamic, and browser-level workspace. Our evaluation revealed an increased ""appetite"" for information foraging, enhanced control, and more flexible sensemaking across a broader web information landscape.","{""creator"":""Peiling Jiang, Haijun Xia"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""Orca: Browsing at Scale Through User-Driven and AI-Facilitated Orchestration Across Malleable Webpages"",""link"":""https://arxiv.org/abs/2505.22831"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Peiling Jiang, Haijun Xia"",""content"":""arXiv:2505.22831v2 Announce Type: replace-cross \nAbstract: Web-based activities span multiple webpages. However, conventional browsers with stacks of tabs cannot support operating and synthesizing large volumes of information across pages. While recent AI systems enable fully automated web browsing and information synthesis, they often diminish user agency and hinder contextual understanding. We explore how AI could instead augment user interactions with content across webpages and mitigate cognitive and manual efforts. Through literature on information tasks and web browsing challenges, and an iterative design process, we present novel interactions with our prototype web browser, Orca. Leveraging AI, Orca supports user-driven exploration, operation, organization, and synthesis of web content at scale. To enable browsing at scale, webpages are treated as malleable materials that humans and AI can collaboratively manipulate and compose into a malleable, dynamic, and browser-level workspace. Our evaluation revealed an increased \""appetite\"" for information foraging, enhanced control, and more flexible sensemaking across a broader web information landscape."",""contentSnippet"":""arXiv:2505.22831v2 Announce Type: replace-cross \nAbstract: Web-based activities span multiple webpages. However, conventional browsers with stacks of tabs cannot support operating and synthesizing large volumes of information across pages. While recent AI systems enable fully automated web browsing and information synthesis, they often diminish user agency and hinder contextual understanding. We explore how AI could instead augment user interactions with content across webpages and mitigate cognitive and manual efforts. Through literature on information tasks and web browsing challenges, and an iterative design process, we present novel interactions with our prototype web browser, Orca. Leveraging AI, Orca supports user-driven exploration, operation, organization, and synthesis of web content at scale. To enable browsing at scale, webpages are treated as malleable materials that humans and AI can collaboratively manipulate and compose into a malleable, dynamic, and browser-level workspace. Our evaluation revealed an increased \""appetite\"" for information foraging, enhanced control, and more flexible sensemaking across a broader web information landscape."",""guid"":""oai:arXiv.org:2505.22831v2"",""categories"":[""cs.HC"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review,https://arxiv.org/abs/2505.20503,2026-01-30T05:00:00.000Z,"arXiv:2505.20503v2 Announce Type: replace-cross 
Abstract: Rapid advancements in foundation models, including Large Language Models, Vision-Language Models, Multimodal Large Language Models, and Vision-Language-Action Models, have opened new avenues for embodied AI in mobile service robotics. By combining foundation models with the principles of embodied AI, where intelligent systems perceive, reason, and act through physical interaction, mobile service robots can achieve more flexible understanding, adaptive behavior, and robust task execution in dynamic real-world environments. Despite this progress, embodied AI for mobile service robots continues to face fundamental challenges related to the translation of natural language instructions into executable robot actions, multimodal perception in human-centered environments, uncertainty estimation for safe decision-making, and computational constraints for real-time onboard deployment. In this paper, we present the first systematic review focused specifically on the integration of foundation models in mobile service robotics. We analyze how recent advances in foundation models address these core challenges through language-conditioned control, multimodal sensor fusion, uncertainty-aware reasoning, and efficient model scaling. We further examine real-world applications in domestic assistance, healthcare, and service automation, highlighting how foundation models enable context-aware, socially responsive, and generalizable robot behaviors. Beyond technical considerations, we discuss ethical, societal, and human-interaction implications associated with deploying foundation model-enabled service robots in human environments. Finally, we outline future research directions emphasizing reliability and lifelong adaptation, privacy-aware and resource-constrained deployment, and governance and human-in-the-loop frameworks required for safe, scalable, and trustworthy mobile service robotics.","{""creator"":""Matthew Lisondra, Beno Benhabib, Goldie Nejat"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review"",""link"":""https://arxiv.org/abs/2505.20503"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Matthew Lisondra, Beno Benhabib, Goldie Nejat"",""content"":""arXiv:2505.20503v2 Announce Type: replace-cross \nAbstract: Rapid advancements in foundation models, including Large Language Models, Vision-Language Models, Multimodal Large Language Models, and Vision-Language-Action Models, have opened new avenues for embodied AI in mobile service robotics. By combining foundation models with the principles of embodied AI, where intelligent systems perceive, reason, and act through physical interaction, mobile service robots can achieve more flexible understanding, adaptive behavior, and robust task execution in dynamic real-world environments. Despite this progress, embodied AI for mobile service robots continues to face fundamental challenges related to the translation of natural language instructions into executable robot actions, multimodal perception in human-centered environments, uncertainty estimation for safe decision-making, and computational constraints for real-time onboard deployment. In this paper, we present the first systematic review focused specifically on the integration of foundation models in mobile service robotics. We analyze how recent advances in foundation models address these core challenges through language-conditioned control, multimodal sensor fusion, uncertainty-aware reasoning, and efficient model scaling. We further examine real-world applications in domestic assistance, healthcare, and service automation, highlighting how foundation models enable context-aware, socially responsive, and generalizable robot behaviors. Beyond technical considerations, we discuss ethical, societal, and human-interaction implications associated with deploying foundation model-enabled service robots in human environments. Finally, we outline future research directions emphasizing reliability and lifelong adaptation, privacy-aware and resource-constrained deployment, and governance and human-in-the-loop frameworks required for safe, scalable, and trustworthy mobile service robotics."",""contentSnippet"":""arXiv:2505.20503v2 Announce Type: replace-cross \nAbstract: Rapid advancements in foundation models, including Large Language Models, Vision-Language Models, Multimodal Large Language Models, and Vision-Language-Action Models, have opened new avenues for embodied AI in mobile service robotics. By combining foundation models with the principles of embodied AI, where intelligent systems perceive, reason, and act through physical interaction, mobile service robots can achieve more flexible understanding, adaptive behavior, and robust task execution in dynamic real-world environments. Despite this progress, embodied AI for mobile service robots continues to face fundamental challenges related to the translation of natural language instructions into executable robot actions, multimodal perception in human-centered environments, uncertainty estimation for safe decision-making, and computational constraints for real-time onboard deployment. In this paper, we present the first systematic review focused specifically on the integration of foundation models in mobile service robotics. We analyze how recent advances in foundation models address these core challenges through language-conditioned control, multimodal sensor fusion, uncertainty-aware reasoning, and efficient model scaling. We further examine real-world applications in domestic assistance, healthcare, and service automation, highlighting how foundation models enable context-aware, socially responsive, and generalizable robot behaviors. Beyond technical considerations, we discuss ethical, societal, and human-interaction implications associated with deploying foundation model-enabled service robots in human environments. Finally, we outline future research directions emphasizing reliability and lifelong adaptation, privacy-aware and resource-constrained deployment, and governance and human-in-the-loop frameworks required for safe, scalable, and trustworthy mobile service robotics."",""guid"":""oai:arXiv.org:2505.20503v2"",""categories"":[""cs.RO"",""cs.AI"",""cs.CL"",""cs.CV"",""cs.LG""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,In-context Language Learning for Endangered Languages in Speech Recognition,https://arxiv.org/abs/2505.20445,2026-01-30T05:00:00.000Z,"arXiv:2505.20445v5 Announce Type: replace-cross 
Abstract: With approximately 7,000 languages spoken worldwide, current large language models (LLMs) support only a small subset. Prior research indicates LLMs can learn new languages for certain tasks without supervised data. We extend this investigation to speech recognition, investigating whether LLMs can learn unseen, low-resource languages through in-context learning (ICL). With experiments on four diverse endangered languages that LLMs have not been trained on, we find that providing more relevant text samples enhances performance in both language modelling and Automatic Speech Recognition (ASR) tasks. Furthermore, we show that the probability-based approach outperforms the traditional instruction-based approach in language learning. Lastly, we show ICL enables LLMs to achieve ASR performance that is comparable to or even surpasses dedicated language models trained specifically for these languages, while preserving the original capabilities of the LLMs. Our code is publicly available.","{""creator"":""Zhaolin Li, Jan Niehues"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""In-context Language Learning for Endangered Languages in Speech Recognition"",""link"":""https://arxiv.org/abs/2505.20445"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Zhaolin Li, Jan Niehues"",""content"":""arXiv:2505.20445v5 Announce Type: replace-cross \nAbstract: With approximately 7,000 languages spoken worldwide, current large language models (LLMs) support only a small subset. Prior research indicates LLMs can learn new languages for certain tasks without supervised data. We extend this investigation to speech recognition, investigating whether LLMs can learn unseen, low-resource languages through in-context learning (ICL). With experiments on four diverse endangered languages that LLMs have not been trained on, we find that providing more relevant text samples enhances performance in both language modelling and Automatic Speech Recognition (ASR) tasks. Furthermore, we show that the probability-based approach outperforms the traditional instruction-based approach in language learning. Lastly, we show ICL enables LLMs to achieve ASR performance that is comparable to or even surpasses dedicated language models trained specifically for these languages, while preserving the original capabilities of the LLMs. Our code is publicly available."",""contentSnippet"":""arXiv:2505.20445v5 Announce Type: replace-cross \nAbstract: With approximately 7,000 languages spoken worldwide, current large language models (LLMs) support only a small subset. Prior research indicates LLMs can learn new languages for certain tasks without supervised data. We extend this investigation to speech recognition, investigating whether LLMs can learn unseen, low-resource languages through in-context learning (ICL). With experiments on four diverse endangered languages that LLMs have not been trained on, we find that providing more relevant text samples enhances performance in both language modelling and Automatic Speech Recognition (ASR) tasks. Furthermore, we show that the probability-based approach outperforms the traditional instruction-based approach in language learning. Lastly, we show ICL enables LLMs to achieve ASR performance that is comparable to or even surpasses dedicated language models trained specifically for these languages, while preserving the original capabilities of the LLMs. Our code is publicly available."",""guid"":""oai:arXiv.org:2505.20445v5"",""categories"":[""cs.CL"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Beyond Face Swapping: A Diffusion-Based Digital Human Benchmark for Multimodal Deepfake Detection,https://arxiv.org/abs/2505.16512,2026-01-30T05:00:00.000Z,"arXiv:2505.16512v5 Announce Type: replace-cross 
Abstract: In recent years, the explosive advancement of deepfake technology has posed a critical and escalating threat to public security: diffusion-based digital human generation. Unlike traditional face manipulation methods, such models can generate highly realistic videos with consistency via multimodal control signals. Their flexibility and covertness pose severe challenges to existing detection strategies. To bridge this gap, we introduce DigiFakeAV, the new large-scale multimodal digital human forgery dataset based on diffusion models. Leveraging five of the latest digital human generation methods and a voice cloning method, we systematically construct a dataset comprising 60,000 videos (8.4 million frames), covering multiple nationalities, skin tones, genders, and real-world scenarios, significantly enhancing data diversity and realism. User studies demonstrate that the misrecognition rate by participants for DigiFakeAV reaches as high as 68%. Moreover, the substantial performance degradation of existing detection models on our dataset further highlights its challenges. To address this problem, we propose DigiShield, an effective detection baseline based on spatiotemporal and cross-modal fusion. By jointly modeling the 3D spatiotemporal features of videos and the semantic-acoustic features of audio, DigiShield achieves state-of-the-art (SOTA) performance on the DigiFakeAV and shows strong generalization on other datasets.","{""creator"":""Jiaxin Liu, Jia Wang, Saihui Hou, Min Ren, Huijia Wu, Long Ma, Renwang Pei, Zhaofeng He"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""Beyond Face Swapping: A Diffusion-Based Digital Human Benchmark for Multimodal Deepfake Detection"",""link"":""https://arxiv.org/abs/2505.16512"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Jiaxin Liu, Jia Wang, Saihui Hou, Min Ren, Huijia Wu, Long Ma, Renwang Pei, Zhaofeng He"",""content"":""arXiv:2505.16512v5 Announce Type: replace-cross \nAbstract: In recent years, the explosive advancement of deepfake technology has posed a critical and escalating threat to public security: diffusion-based digital human generation. Unlike traditional face manipulation methods, such models can generate highly realistic videos with consistency via multimodal control signals. Their flexibility and covertness pose severe challenges to existing detection strategies. To bridge this gap, we introduce DigiFakeAV, the new large-scale multimodal digital human forgery dataset based on diffusion models. Leveraging five of the latest digital human generation methods and a voice cloning method, we systematically construct a dataset comprising 60,000 videos (8.4 million frames), covering multiple nationalities, skin tones, genders, and real-world scenarios, significantly enhancing data diversity and realism. User studies demonstrate that the misrecognition rate by participants for DigiFakeAV reaches as high as 68%. Moreover, the substantial performance degradation of existing detection models on our dataset further highlights its challenges. To address this problem, we propose DigiShield, an effective detection baseline based on spatiotemporal and cross-modal fusion. By jointly modeling the 3D spatiotemporal features of videos and the semantic-acoustic features of audio, DigiShield achieves state-of-the-art (SOTA) performance on the DigiFakeAV and shows strong generalization on other datasets."",""contentSnippet"":""arXiv:2505.16512v5 Announce Type: replace-cross \nAbstract: In recent years, the explosive advancement of deepfake technology has posed a critical and escalating threat to public security: diffusion-based digital human generation. Unlike traditional face manipulation methods, such models can generate highly realistic videos with consistency via multimodal control signals. Their flexibility and covertness pose severe challenges to existing detection strategies. To bridge this gap, we introduce DigiFakeAV, the new large-scale multimodal digital human forgery dataset based on diffusion models. Leveraging five of the latest digital human generation methods and a voice cloning method, we systematically construct a dataset comprising 60,000 videos (8.4 million frames), covering multiple nationalities, skin tones, genders, and real-world scenarios, significantly enhancing data diversity and realism. User studies demonstrate that the misrecognition rate by participants for DigiFakeAV reaches as high as 68%. Moreover, the substantial performance degradation of existing detection models on our dataset further highlights its challenges. To address this problem, we propose DigiShield, an effective detection baseline based on spatiotemporal and cross-modal fusion. By jointly modeling the 3D spatiotemporal features of videos and the semantic-acoustic features of audio, DigiShield achieves state-of-the-art (SOTA) performance on the DigiFakeAV and shows strong generalization on other datasets."",""guid"":""oai:arXiv.org:2505.16512v5"",""categories"":[""cs.CV"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Field Matters: A Lightweight LLM-enhanced Method for CTR Prediction,https://arxiv.org/abs/2505.14057,2026-01-30T05:00:00.000Z,"arXiv:2505.14057v2 Announce Type: replace-cross 
Abstract: Click-through rate (CTR) prediction is a fundamental task in modern recommender systems. In recent years, the integration of large language models (LLMs) has been shown to effectively enhance the performance of traditional CTR methods. However, existing LLM-enhanced methods often require extensive processing of detailed textual descriptions for large-scale instances or user/item entities, leading to substantial computational overhead. To address this challenge, this work introduces LLaCTR, a novel and lightweight LLM-enhanced CTR method that employs a field-level enhancement paradigm. Specifically, LLaCTR first utilizes LLMs to distill crucial and lightweight semantic knowledge from small-scale feature fields through self-supervised field-feature fine-tuning. Subsequently, it leverages this field-level semantic knowledge to enhance both feature representation and feature interactions. In our experiments, we integrate LLaCTR with six representative CTR models across four datasets, demonstrating its superior performance in terms of both effectiveness and efficiency compared to existing LLM-enhanced methods. Our code is available at https://github.com/istarryn/LLaCTR.","{""creator"":""Yu Cui, Feng Liu, Jiawei Chen, Xingyu Lou, Changwang Zhang, Jun Wang, Yuegang Sun, Xiaohu Yang, Can Wang"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""Field Matters: A Lightweight LLM-enhanced Method for CTR Prediction"",""link"":""https://arxiv.org/abs/2505.14057"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Yu Cui, Feng Liu, Jiawei Chen, Xingyu Lou, Changwang Zhang, Jun Wang, Yuegang Sun, Xiaohu Yang, Can Wang"",""content"":""arXiv:2505.14057v2 Announce Type: replace-cross \nAbstract: Click-through rate (CTR) prediction is a fundamental task in modern recommender systems. In recent years, the integration of large language models (LLMs) has been shown to effectively enhance the performance of traditional CTR methods. However, existing LLM-enhanced methods often require extensive processing of detailed textual descriptions for large-scale instances or user/item entities, leading to substantial computational overhead. To address this challenge, this work introduces LLaCTR, a novel and lightweight LLM-enhanced CTR method that employs a field-level enhancement paradigm. Specifically, LLaCTR first utilizes LLMs to distill crucial and lightweight semantic knowledge from small-scale feature fields through self-supervised field-feature fine-tuning. Subsequently, it leverages this field-level semantic knowledge to enhance both feature representation and feature interactions. In our experiments, we integrate LLaCTR with six representative CTR models across four datasets, demonstrating its superior performance in terms of both effectiveness and efficiency compared to existing LLM-enhanced methods. Our code is available at https://github.com/istarryn/LLaCTR."",""contentSnippet"":""arXiv:2505.14057v2 Announce Type: replace-cross \nAbstract: Click-through rate (CTR) prediction is a fundamental task in modern recommender systems. In recent years, the integration of large language models (LLMs) has been shown to effectively enhance the performance of traditional CTR methods. However, existing LLM-enhanced methods often require extensive processing of detailed textual descriptions for large-scale instances or user/item entities, leading to substantial computational overhead. To address this challenge, this work introduces LLaCTR, a novel and lightweight LLM-enhanced CTR method that employs a field-level enhancement paradigm. Specifically, LLaCTR first utilizes LLMs to distill crucial and lightweight semantic knowledge from small-scale feature fields through self-supervised field-feature fine-tuning. Subsequently, it leverages this field-level semantic knowledge to enhance both feature representation and feature interactions. In our experiments, we integrate LLaCTR with six representative CTR models across four datasets, demonstrating its superior performance in terms of both effectiveness and efficiency compared to existing LLM-enhanced methods. Our code is available at https://github.com/istarryn/LLaCTR."",""guid"":""oai:arXiv.org:2505.14057v2"",""categories"":[""cs.IR"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Doxing via the Lens: Revealing Location-related Privacy Leakage on Multi-modal Large Reasoning Models,https://arxiv.org/abs/2504.19373,2026-01-30T05:00:00.000Z,"arXiv:2504.19373v4 Announce Type: replace-cross 
Abstract: Recent advances in multi-modal large reasoning models (MLRMs) have shown significant ability to interpret complex visual content. While these models enable impressive reasoning capabilities, they also introduce novel and underexplored privacy risks. In this paper, we identify a novel category of privacy leakage in MLRMs: Adversaries can infer sensitive geolocation information, such as a user's home address or neighborhood, from user-generated images, including selfies captured in private settings. To formalize and evaluate these risks, we propose a three-level visual privacy risk framework that categorizes image content based on contextual sensitivity and potential for location inference. We further introduce DoxBench, a curated dataset of 500 real-world images reflecting diverse privacy scenarios. Our evaluation across 11 advanced MLRMs and MLLMs demonstrates that these models consistently outperform non-expert humans in geolocation inference and can effectively leak location-related private information. This significantly lowers the barrier for adversaries to obtain users' sensitive geolocation information. We further analyze and identify two primary factors contributing to this vulnerability: (1) MLRMs exhibit strong reasoning capabilities by leveraging visual clues in combination with their internal world knowledge; and (2) MLRMs frequently rely on privacy-related visual clues for inference without any built-in mechanisms to suppress or avoid such usage. To better understand and demonstrate real-world attack feasibility, we propose GeoMiner, a collaborative attack framework that decomposes the prediction process into two stages: clue extraction and reasoning to improve geolocation performance while introducing a novel attack perspective. Our findings highlight the urgent need to reassess inference-time privacy risks in MLRMs to better protect users' sensitive information.","{""creator"":""Weidi Luo, Tianyu Lu, Qiming Zhang, Xiaogeng Liu, Bin Hu, Yue Zhao, Jieyu Zhao, Song Gao, Patrick McDaniel, Zhen Xiang, Chaowei Xiao"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""Doxing via the Lens: Revealing Location-related Privacy Leakage on Multi-modal Large Reasoning Models"",""link"":""https://arxiv.org/abs/2504.19373"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Weidi Luo, Tianyu Lu, Qiming Zhang, Xiaogeng Liu, Bin Hu, Yue Zhao, Jieyu Zhao, Song Gao, Patrick McDaniel, Zhen Xiang, Chaowei Xiao"",""content"":""arXiv:2504.19373v4 Announce Type: replace-cross \nAbstract: Recent advances in multi-modal large reasoning models (MLRMs) have shown significant ability to interpret complex visual content. While these models enable impressive reasoning capabilities, they also introduce novel and underexplored privacy risks. In this paper, we identify a novel category of privacy leakage in MLRMs: Adversaries can infer sensitive geolocation information, such as a user's home address or neighborhood, from user-generated images, including selfies captured in private settings. To formalize and evaluate these risks, we propose a three-level visual privacy risk framework that categorizes image content based on contextual sensitivity and potential for location inference. We further introduce DoxBench, a curated dataset of 500 real-world images reflecting diverse privacy scenarios. Our evaluation across 11 advanced MLRMs and MLLMs demonstrates that these models consistently outperform non-expert humans in geolocation inference and can effectively leak location-related private information. This significantly lowers the barrier for adversaries to obtain users' sensitive geolocation information. We further analyze and identify two primary factors contributing to this vulnerability: (1) MLRMs exhibit strong reasoning capabilities by leveraging visual clues in combination with their internal world knowledge; and (2) MLRMs frequently rely on privacy-related visual clues for inference without any built-in mechanisms to suppress or avoid such usage. To better understand and demonstrate real-world attack feasibility, we propose GeoMiner, a collaborative attack framework that decomposes the prediction process into two stages: clue extraction and reasoning to improve geolocation performance while introducing a novel attack perspective. Our findings highlight the urgent need to reassess inference-time privacy risks in MLRMs to better protect users' sensitive information."",""contentSnippet"":""arXiv:2504.19373v4 Announce Type: replace-cross \nAbstract: Recent advances in multi-modal large reasoning models (MLRMs) have shown significant ability to interpret complex visual content. While these models enable impressive reasoning capabilities, they also introduce novel and underexplored privacy risks. In this paper, we identify a novel category of privacy leakage in MLRMs: Adversaries can infer sensitive geolocation information, such as a user's home address or neighborhood, from user-generated images, including selfies captured in private settings. To formalize and evaluate these risks, we propose a three-level visual privacy risk framework that categorizes image content based on contextual sensitivity and potential for location inference. We further introduce DoxBench, a curated dataset of 500 real-world images reflecting diverse privacy scenarios. Our evaluation across 11 advanced MLRMs and MLLMs demonstrates that these models consistently outperform non-expert humans in geolocation inference and can effectively leak location-related private information. This significantly lowers the barrier for adversaries to obtain users' sensitive geolocation information. We further analyze and identify two primary factors contributing to this vulnerability: (1) MLRMs exhibit strong reasoning capabilities by leveraging visual clues in combination with their internal world knowledge; and (2) MLRMs frequently rely on privacy-related visual clues for inference without any built-in mechanisms to suppress or avoid such usage. To better understand and demonstrate real-world attack feasibility, we propose GeoMiner, a collaborative attack framework that decomposes the prediction process into two stages: clue extraction and reasoning to improve geolocation performance while introducing a novel attack perspective. Our findings highlight the urgent need to reassess inference-time privacy risks in MLRMs to better protect users' sensitive information."",""guid"":""oai:arXiv.org:2504.19373v4"",""categories"":[""cs.CR"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,NoWag: A Unified Framework for Shape Preserving Compression of Large Language Models,https://arxiv.org/abs/2504.14569,2026-01-30T05:00:00.000Z,"arXiv:2504.14569v5 Announce Type: replace-cross 
Abstract: Large language models (LLMs) exhibit remarkable performance across various natural language processing tasks but suffer from immense computational and memory demands, limiting their deployment in resource-constrained environments. To address this challenge, we propose NoWag (Normalized Weight and Activation Guided Compression), a unified framework for one-shot shape preserving compression algorithms. We apply NoWag to compress Llama-2 (7B, 13B, 70B) and Llama-3 (8B, 70B) models using two popular shape-preserving techniques: vector quantization (NoWag-VQ) and unstructured/semi-structured pruning (NoWag-P). Our results show that NoWag-VQ significantly outperforms state-of-the-art one-shot vector quantization methods, while NoWag-P performs competitively against leading pruning techniques. These findings highlight underlying commonalities between these compression paradigms and suggest promising directions for future research. Our code is available at https://github.com/LawrenceRLiu/NoWag","{""creator"":""Lawrence Liu, Inesh Chakrabarti, Yixiao Li, Mengdi Wang, Tuo Zhao, Lin F. Yang"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""NoWag: A Unified Framework for Shape Preserving Compression of Large Language Models"",""link"":""https://arxiv.org/abs/2504.14569"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Lawrence Liu, Inesh Chakrabarti, Yixiao Li, Mengdi Wang, Tuo Zhao, Lin F. Yang"",""content"":""arXiv:2504.14569v5 Announce Type: replace-cross \nAbstract: Large language models (LLMs) exhibit remarkable performance across various natural language processing tasks but suffer from immense computational and memory demands, limiting their deployment in resource-constrained environments. To address this challenge, we propose NoWag (Normalized Weight and Activation Guided Compression), a unified framework for one-shot shape preserving compression algorithms. We apply NoWag to compress Llama-2 (7B, 13B, 70B) and Llama-3 (8B, 70B) models using two popular shape-preserving techniques: vector quantization (NoWag-VQ) and unstructured/semi-structured pruning (NoWag-P). Our results show that NoWag-VQ significantly outperforms state-of-the-art one-shot vector quantization methods, while NoWag-P performs competitively against leading pruning techniques. These findings highlight underlying commonalities between these compression paradigms and suggest promising directions for future research. Our code is available at https://github.com/LawrenceRLiu/NoWag"",""contentSnippet"":""arXiv:2504.14569v5 Announce Type: replace-cross \nAbstract: Large language models (LLMs) exhibit remarkable performance across various natural language processing tasks but suffer from immense computational and memory demands, limiting their deployment in resource-constrained environments. To address this challenge, we propose NoWag (Normalized Weight and Activation Guided Compression), a unified framework for one-shot shape preserving compression algorithms. We apply NoWag to compress Llama-2 (7B, 13B, 70B) and Llama-3 (8B, 70B) models using two popular shape-preserving techniques: vector quantization (NoWag-VQ) and unstructured/semi-structured pruning (NoWag-P). Our results show that NoWag-VQ significantly outperforms state-of-the-art one-shot vector quantization methods, while NoWag-P performs competitively against leading pruning techniques. These findings highlight underlying commonalities between these compression paradigms and suggest promising directions for future research. Our code is available at https://github.com/LawrenceRLiu/NoWag"",""guid"":""oai:arXiv.org:2504.14569v5"",""categories"":[""cs.LG"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Physics-Guided Multimodal Transformers are the Necessary Foundation for the Next Generation of Meteorological Science,https://arxiv.org/abs/2504.14174,2026-01-30T05:00:00.000Z,"arXiv:2504.14174v2 Announce Type: replace-cross 
Abstract: This position paper argues that the next generation of artificial intelligence in meteorological and climate sciences must transition from fragmented hybrid heuristics toward a unified paradigm of physics-guided multimodal transformers. While purely data-driven models have achieved significant gains in predictive accuracy, they often treat atmospheric processes as mere visual patterns, frequently producing results that lack scientific consistency or violate fundamental physical laws. We contend that current ``hybrid'' attempts to bridge this gap remain ad-hoc and struggle to scale across the heterogeneous nature of meteorological data ranging from satellite imagery to sparse sensor measurements. We argue that the transformer architecture, through its inherent capacity for cross-modal alignment, provides the only viable foundation for a systematic integration of domain knowledge via physical constraint embedding and physics-informed loss functions. By advocating for this unified architectural shift, we aim to steer the community away from ``black-box'' curve fitting and toward AI systems that are inherently falsifiable, scientifically grounded, and robust enough to address the existential challenges of extreme weather and climate change.","{""creator"":""Jing Han, Hanting Chen, Kai Han, Xiaomeng Huang, Wenjun Xu, Dacheng Tao, Ping Zhang"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""Physics-Guided Multimodal Transformers are the Necessary Foundation for the Next Generation of Meteorological Science"",""link"":""https://arxiv.org/abs/2504.14174"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Jing Han, Hanting Chen, Kai Han, Xiaomeng Huang, Wenjun Xu, Dacheng Tao, Ping Zhang"",""content"":""arXiv:2504.14174v2 Announce Type: replace-cross \nAbstract: This position paper argues that the next generation of artificial intelligence in meteorological and climate sciences must transition from fragmented hybrid heuristics toward a unified paradigm of physics-guided multimodal transformers. While purely data-driven models have achieved significant gains in predictive accuracy, they often treat atmospheric processes as mere visual patterns, frequently producing results that lack scientific consistency or violate fundamental physical laws. We contend that current ``hybrid'' attempts to bridge this gap remain ad-hoc and struggle to scale across the heterogeneous nature of meteorological data ranging from satellite imagery to sparse sensor measurements. We argue that the transformer architecture, through its inherent capacity for cross-modal alignment, provides the only viable foundation for a systematic integration of domain knowledge via physical constraint embedding and physics-informed loss functions. By advocating for this unified architectural shift, we aim to steer the community away from ``black-box'' curve fitting and toward AI systems that are inherently falsifiable, scientifically grounded, and robust enough to address the existential challenges of extreme weather and climate change."",""contentSnippet"":""arXiv:2504.14174v2 Announce Type: replace-cross \nAbstract: This position paper argues that the next generation of artificial intelligence in meteorological and climate sciences must transition from fragmented hybrid heuristics toward a unified paradigm of physics-guided multimodal transformers. While purely data-driven models have achieved significant gains in predictive accuracy, they often treat atmospheric processes as mere visual patterns, frequently producing results that lack scientific consistency or violate fundamental physical laws. We contend that current ``hybrid'' attempts to bridge this gap remain ad-hoc and struggle to scale across the heterogeneous nature of meteorological data ranging from satellite imagery to sparse sensor measurements. We argue that the transformer architecture, through its inherent capacity for cross-modal alignment, provides the only viable foundation for a systematic integration of domain knowledge via physical constraint embedding and physics-informed loss functions. By advocating for this unified architectural shift, we aim to steer the community away from ``black-box'' curve fitting and toward AI systems that are inherently falsifiable, scientifically grounded, and robust enough to address the existential challenges of extreme weather and climate change."",""guid"":""oai:arXiv.org:2504.14174v2"",""categories"":[""cs.LG"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Diffusion Generative Recommendation with Continuous Tokens,https://arxiv.org/abs/2504.12007,2026-01-30T05:00:00.000Z,"arXiv:2504.12007v4 Announce Type: replace-cross 
Abstract: Recent advances in generative artificial intelligence, particularly large language models (LLMs), have opened new opportunities for enhancing recommender systems (RecSys). Most existing LLM-based RecSys approaches operate in a discrete space, using vector-quantized tokenizers to align with the inherent discrete nature of language models. However, these quantization methods often result in lossy tokenization and suboptimal learning, primarily due to inaccurate gradient propagation caused by the non-differentiable argmin operation in standard vector quantization. Inspired by the emerging trend of embracing continuous tokens in language models, we propose ContRec, a novel framework that seamlessly integrates continuous tokens into LLM-based RecSys. Specifically, ContRec consists of two key modules: a sigma-VAE Tokenizer, which encodes users/items with continuous tokens; and a Dispersive Diffusion module, which captures implicit user preference. The tokenizer is trained with a continuous Variational Auto-Encoder (VAE) objective, where three effective techniques are adopted to avoid representation collapse. By conditioning on the previously generated tokens of the LLM backbone during user modeling, the Dispersive Diffusion module performs a conditional diffusion process with a novel Dispersive Loss, enabling high-quality user preference generation through next-token diffusion. Finally, ContRec leverages both the textual reasoning output from the LLM and the latent representations produced by the diffusion model for Top-K item retrieval, thereby delivering comprehensive recommendation results. Extensive experiments on four datasets demonstrate that ContRec consistently outperforms both traditional and SOTA LLM-based recommender systems. Our results highlight the potential of continuous tokenization and generative modeling for advancing the next generation of recommender systems.","{""creator"":""Haohao Qu, Shanru Lin, Yujuan Ding, Yiqi Wang, Wenqi Fan"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""Diffusion Generative Recommendation with Continuous Tokens"",""link"":""https://arxiv.org/abs/2504.12007"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Haohao Qu, Shanru Lin, Yujuan Ding, Yiqi Wang, Wenqi Fan"",""content"":""arXiv:2504.12007v4 Announce Type: replace-cross \nAbstract: Recent advances in generative artificial intelligence, particularly large language models (LLMs), have opened new opportunities for enhancing recommender systems (RecSys). Most existing LLM-based RecSys approaches operate in a discrete space, using vector-quantized tokenizers to align with the inherent discrete nature of language models. However, these quantization methods often result in lossy tokenization and suboptimal learning, primarily due to inaccurate gradient propagation caused by the non-differentiable argmin operation in standard vector quantization. Inspired by the emerging trend of embracing continuous tokens in language models, we propose ContRec, a novel framework that seamlessly integrates continuous tokens into LLM-based RecSys. Specifically, ContRec consists of two key modules: a sigma-VAE Tokenizer, which encodes users/items with continuous tokens; and a Dispersive Diffusion module, which captures implicit user preference. The tokenizer is trained with a continuous Variational Auto-Encoder (VAE) objective, where three effective techniques are adopted to avoid representation collapse. By conditioning on the previously generated tokens of the LLM backbone during user modeling, the Dispersive Diffusion module performs a conditional diffusion process with a novel Dispersive Loss, enabling high-quality user preference generation through next-token diffusion. Finally, ContRec leverages both the textual reasoning output from the LLM and the latent representations produced by the diffusion model for Top-K item retrieval, thereby delivering comprehensive recommendation results. Extensive experiments on four datasets demonstrate that ContRec consistently outperforms both traditional and SOTA LLM-based recommender systems. Our results highlight the potential of continuous tokenization and generative modeling for advancing the next generation of recommender systems."",""contentSnippet"":""arXiv:2504.12007v4 Announce Type: replace-cross \nAbstract: Recent advances in generative artificial intelligence, particularly large language models (LLMs), have opened new opportunities for enhancing recommender systems (RecSys). Most existing LLM-based RecSys approaches operate in a discrete space, using vector-quantized tokenizers to align with the inherent discrete nature of language models. However, these quantization methods often result in lossy tokenization and suboptimal learning, primarily due to inaccurate gradient propagation caused by the non-differentiable argmin operation in standard vector quantization. Inspired by the emerging trend of embracing continuous tokens in language models, we propose ContRec, a novel framework that seamlessly integrates continuous tokens into LLM-based RecSys. Specifically, ContRec consists of two key modules: a sigma-VAE Tokenizer, which encodes users/items with continuous tokens; and a Dispersive Diffusion module, which captures implicit user preference. The tokenizer is trained with a continuous Variational Auto-Encoder (VAE) objective, where three effective techniques are adopted to avoid representation collapse. By conditioning on the previously generated tokens of the LLM backbone during user modeling, the Dispersive Diffusion module performs a conditional diffusion process with a novel Dispersive Loss, enabling high-quality user preference generation through next-token diffusion. Finally, ContRec leverages both the textual reasoning output from the LLM and the latent representations produced by the diffusion model for Top-K item retrieval, thereby delivering comprehensive recommendation results. Extensive experiments on four datasets demonstrate that ContRec consistently outperforms both traditional and SOTA LLM-based recommender systems. Our results highlight the potential of continuous tokenization and generative modeling for advancing the next generation of recommender systems."",""guid"":""oai:arXiv.org:2504.12007v4"",""categories"":[""cs.IR"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Rewarding Doubt: A Reinforcement Learning Approach to Calibrated Confidence Expression of Large Language Models,https://arxiv.org/abs/2503.02623,2026-01-30T05:00:00.000Z,"arXiv:2503.02623v5 Announce Type: replace-cross 
Abstract: A safe and trustworthy use of Large Language Models (LLMs) requires an accurate expression of confidence in their answers. We propose a novel Reinforcement Learning approach that allows to directly fine-tune LLMs to express calibrated confidence estimates alongside their answers to factual questions. Our method optimizes a reward based on the logarithmic scoring rule, explicitly penalizing both over- and under-confidence. This encourages the model to align its confidence estimates with the actual predictive accuracy. The optimal policy under our reward design would result in perfectly calibrated confidence expressions. Unlike prior approaches that decouple confidence estimation from response generation, our method integrates confidence calibration seamlessly into the generative process of the LLM. Empirically, we demonstrate that models trained with our approach exhibit substantially improved calibration and generalize to unseen tasks without further fine-tuning, suggesting the emergence of general confidence awareness.","{""creator"":""David Bani-Harouni, Chantal Pellegrini, Paul Stangel, Ege \\\""Ozsoy, Kamilia Zaripova, Matthias Keicher, Nassir Navab"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""Rewarding Doubt: A Reinforcement Learning Approach to Calibrated Confidence Expression of Large Language Models"",""link"":""https://arxiv.org/abs/2503.02623"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""David Bani-Harouni, Chantal Pellegrini, Paul Stangel, Ege \\\""Ozsoy, Kamilia Zaripova, Matthias Keicher, Nassir Navab"",""content"":""arXiv:2503.02623v5 Announce Type: replace-cross \nAbstract: A safe and trustworthy use of Large Language Models (LLMs) requires an accurate expression of confidence in their answers. We propose a novel Reinforcement Learning approach that allows to directly fine-tune LLMs to express calibrated confidence estimates alongside their answers to factual questions. Our method optimizes a reward based on the logarithmic scoring rule, explicitly penalizing both over- and under-confidence. This encourages the model to align its confidence estimates with the actual predictive accuracy. The optimal policy under our reward design would result in perfectly calibrated confidence expressions. Unlike prior approaches that decouple confidence estimation from response generation, our method integrates confidence calibration seamlessly into the generative process of the LLM. Empirically, we demonstrate that models trained with our approach exhibit substantially improved calibration and generalize to unseen tasks without further fine-tuning, suggesting the emergence of general confidence awareness."",""contentSnippet"":""arXiv:2503.02623v5 Announce Type: replace-cross \nAbstract: A safe and trustworthy use of Large Language Models (LLMs) requires an accurate expression of confidence in their answers. We propose a novel Reinforcement Learning approach that allows to directly fine-tune LLMs to express calibrated confidence estimates alongside their answers to factual questions. Our method optimizes a reward based on the logarithmic scoring rule, explicitly penalizing both over- and under-confidence. This encourages the model to align its confidence estimates with the actual predictive accuracy. The optimal policy under our reward design would result in perfectly calibrated confidence expressions. Unlike prior approaches that decouple confidence estimation from response generation, our method integrates confidence calibration seamlessly into the generative process of the LLM. Empirically, we demonstrate that models trained with our approach exhibit substantially improved calibration and generalize to unseen tasks without further fine-tuning, suggesting the emergence of general confidence awareness."",""guid"":""oai:arXiv.org:2503.02623v5"",""categories"":[""cs.CL"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Depth-Width tradeoffs in Algorithmic Reasoning of Graph Tasks with Transformers,https://arxiv.org/abs/2503.01805,2026-01-30T05:00:00.000Z,"arXiv:2503.01805v2 Announce Type: replace-cross 
Abstract: Transformers have revolutionized the field of machine learning. In particular, they can be used to solve complex algorithmic problems, including graph-based tasks. In such algorithmic tasks a key question is what is the minimal size of a transformer that can implement the task. Recent work has begun to explore this problem for graph-based tasks, showing that for sub-linear embedding dimension (i.e., model width) logarithmic depth suffices. However, an open question, which we address here, is what happens if width is allowed to grow linearly, while depth is kept fixed. Here we analyze this setting, and provide the surprising result that with linear width, constant depth suffices for solving a host of graph-based problems. This suggests that a moderate increase in width can allow much shallower models, which are advantageous in terms of inference and train time. For other problems, we show that quadratic width is required. Our results demonstrate the complex and intriguing landscape of transformer implementations of graph-based algorithms. We empirically investigate these trade-offs between the relative powers of depth and width and find tasks where wider models have the same accuracy as deep models, while having much faster train and inference time due to parallelizable hardware.","{""creator"":""Gilad Yehudai, Clayton Sanford, Maya Bechler-Speicher, Orr Fischer, Ran Gilad-Bachrach, Amir Globerson"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""Depth-Width tradeoffs in Algorithmic Reasoning of Graph Tasks with Transformers"",""link"":""https://arxiv.org/abs/2503.01805"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Gilad Yehudai, Clayton Sanford, Maya Bechler-Speicher, Orr Fischer, Ran Gilad-Bachrach, Amir Globerson"",""content"":""arXiv:2503.01805v2 Announce Type: replace-cross \nAbstract: Transformers have revolutionized the field of machine learning. In particular, they can be used to solve complex algorithmic problems, including graph-based tasks. In such algorithmic tasks a key question is what is the minimal size of a transformer that can implement the task. Recent work has begun to explore this problem for graph-based tasks, showing that for sub-linear embedding dimension (i.e., model width) logarithmic depth suffices. However, an open question, which we address here, is what happens if width is allowed to grow linearly, while depth is kept fixed. Here we analyze this setting, and provide the surprising result that with linear width, constant depth suffices for solving a host of graph-based problems. This suggests that a moderate increase in width can allow much shallower models, which are advantageous in terms of inference and train time. For other problems, we show that quadratic width is required. Our results demonstrate the complex and intriguing landscape of transformer implementations of graph-based algorithms. We empirically investigate these trade-offs between the relative powers of depth and width and find tasks where wider models have the same accuracy as deep models, while having much faster train and inference time due to parallelizable hardware."",""contentSnippet"":""arXiv:2503.01805v2 Announce Type: replace-cross \nAbstract: Transformers have revolutionized the field of machine learning. In particular, they can be used to solve complex algorithmic problems, including graph-based tasks. In such algorithmic tasks a key question is what is the minimal size of a transformer that can implement the task. Recent work has begun to explore this problem for graph-based tasks, showing that for sub-linear embedding dimension (i.e., model width) logarithmic depth suffices. However, an open question, which we address here, is what happens if width is allowed to grow linearly, while depth is kept fixed. Here we analyze this setting, and provide the surprising result that with linear width, constant depth suffices for solving a host of graph-based problems. This suggests that a moderate increase in width can allow much shallower models, which are advantageous in terms of inference and train time. For other problems, we show that quadratic width is required. Our results demonstrate the complex and intriguing landscape of transformer implementations of graph-based algorithms. We empirically investigate these trade-offs between the relative powers of depth and width and find tasks where wider models have the same accuracy as deep models, while having much faster train and inference time due to parallelizable hardware."",""guid"":""oai:arXiv.org:2503.01805v2"",""categories"":[""cs.LG"",""cs.AI"",""cs.CL""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,"Compositional Reasoning with Transformers, RNNs, and Chain of Thought",https://arxiv.org/abs/2503.01544,2026-01-30T05:00:00.000Z,"arXiv:2503.01544v2 Announce Type: replace-cross 
Abstract: It is well understood that different neural network architectures are suited to different tasks, but is there always a single best architecture for a given task? We compare the expressive power of transformers, RNNs, and transformers with chain of thought tokens on a simple and natural class of tasks we term Compositional Reasoning Questions (CRQ). This family captures multi-step problems with tree-like compositional structure, such as evaluating Boolean formulas. We prove that under standard hardness assumptions, \emph{none} of these three architectures is capable of solving CRQs unless some hyperparameter (depth, embedding dimension, and number of chain of thought tokens, respectively) grows with the size of the input. We then provide constructions for solving CRQs with each architecture. For transformers, our construction uses depth that is logarithmic in the problem size. For RNNs, logarithmic embedding dimension is necessary and sufficient, so long as the inputs are provided in a certain order. For transformers with chain of thought, our construction uses $n$ CoT tokens for input size $n$. These results show that, while CRQs are inherently hard, there are several different ways for language models to overcome this hardness. Even for a single class of problems, each architecture has strengths and weaknesses, and none is strictly better than the others.","{""creator"":""Gilad Yehudai, Noah Amsel, Joan Bruna"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""Compositional Reasoning with Transformers, RNNs, and Chain of Thought"",""link"":""https://arxiv.org/abs/2503.01544"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Gilad Yehudai, Noah Amsel, Joan Bruna"",""content"":""arXiv:2503.01544v2 Announce Type: replace-cross \nAbstract: It is well understood that different neural network architectures are suited to different tasks, but is there always a single best architecture for a given task? We compare the expressive power of transformers, RNNs, and transformers with chain of thought tokens on a simple and natural class of tasks we term Compositional Reasoning Questions (CRQ). This family captures multi-step problems with tree-like compositional structure, such as evaluating Boolean formulas. We prove that under standard hardness assumptions, \\emph{none} of these three architectures is capable of solving CRQs unless some hyperparameter (depth, embedding dimension, and number of chain of thought tokens, respectively) grows with the size of the input. We then provide constructions for solving CRQs with each architecture. For transformers, our construction uses depth that is logarithmic in the problem size. For RNNs, logarithmic embedding dimension is necessary and sufficient, so long as the inputs are provided in a certain order. For transformers with chain of thought, our construction uses $n$ CoT tokens for input size $n$. These results show that, while CRQs are inherently hard, there are several different ways for language models to overcome this hardness. Even for a single class of problems, each architecture has strengths and weaknesses, and none is strictly better than the others."",""contentSnippet"":""arXiv:2503.01544v2 Announce Type: replace-cross \nAbstract: It is well understood that different neural network architectures are suited to different tasks, but is there always a single best architecture for a given task? We compare the expressive power of transformers, RNNs, and transformers with chain of thought tokens on a simple and natural class of tasks we term Compositional Reasoning Questions (CRQ). This family captures multi-step problems with tree-like compositional structure, such as evaluating Boolean formulas. We prove that under standard hardness assumptions, \\emph{none} of these three architectures is capable of solving CRQs unless some hyperparameter (depth, embedding dimension, and number of chain of thought tokens, respectively) grows with the size of the input. We then provide constructions for solving CRQs with each architecture. For transformers, our construction uses depth that is logarithmic in the problem size. For RNNs, logarithmic embedding dimension is necessary and sufficient, so long as the inputs are provided in a certain order. For transformers with chain of thought, our construction uses $n$ CoT tokens for input size $n$. These results show that, while CRQs are inherently hard, there are several different ways for language models to overcome this hardness. Even for a single class of problems, each architecture has strengths and weaknesses, and none is strictly better than the others."",""guid"":""oai:arXiv.org:2503.01544v2"",""categories"":[""cs.LG"",""cs.AI"",""cs.CL""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,BAGEL: Projection-Free Algorithm for Adversarially Constrained Online Convex Optimization,https://arxiv.org/abs/2502.16744,2026-01-30T05:00:00.000Z,"arXiv:2502.16744v2 Announce Type: replace-cross 
Abstract: Projection-based algorithms for Constrained Online Convex Optimization (COCO) achieve optimal $\mathcal{O}(T^{1/2})$ regret guarantees but face scalability challenges due to the computational complexity of projections. To circumvent this, projection-free methods utilizing Linear Optimization Oracles (LOO) have been proposed, albeit typically achieving slower $\mathcal{O}(T^{3/4})$ regret rates. In this work, we examine whether the $\mathcal{O}(T^{1/2})$ rate can be recovered in the projection-free setting by strengthening the oracle assumption. We introduce BAGEL, an algorithm utilizing a Separation Oracle (SO) that achieves $\mathcal{O}(T^{1/2})$ regret and $\tilde{\mathcal{O}}(T^{1/2})$ cumulative constraint violation (CCV) for convex cost functions. Our analysis shows that by leveraging an infeasible projection via SO, we can match the time-horizon dependence of projection-based methods with $\tilde{\mathcal{O}}(T)$ oracle calls, provided dependence on the geometry of the action set. This establishes a specific regime where projection-free methods can attain the same convergence rates as projection-based counterparts.","{""creator"":""Yiyang Lu, Mohammad Pedramfar, Vaneet Aggarwal"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""BAGEL: Projection-Free Algorithm for Adversarially Constrained Online Convex Optimization"",""link"":""https://arxiv.org/abs/2502.16744"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Yiyang Lu, Mohammad Pedramfar, Vaneet Aggarwal"",""content"":""arXiv:2502.16744v2 Announce Type: replace-cross \nAbstract: Projection-based algorithms for Constrained Online Convex Optimization (COCO) achieve optimal $\\mathcal{O}(T^{1/2})$ regret guarantees but face scalability challenges due to the computational complexity of projections. To circumvent this, projection-free methods utilizing Linear Optimization Oracles (LOO) have been proposed, albeit typically achieving slower $\\mathcal{O}(T^{3/4})$ regret rates. In this work, we examine whether the $\\mathcal{O}(T^{1/2})$ rate can be recovered in the projection-free setting by strengthening the oracle assumption. We introduce BAGEL, an algorithm utilizing a Separation Oracle (SO) that achieves $\\mathcal{O}(T^{1/2})$ regret and $\\tilde{\\mathcal{O}}(T^{1/2})$ cumulative constraint violation (CCV) for convex cost functions. Our analysis shows that by leveraging an infeasible projection via SO, we can match the time-horizon dependence of projection-based methods with $\\tilde{\\mathcal{O}}(T)$ oracle calls, provided dependence on the geometry of the action set. This establishes a specific regime where projection-free methods can attain the same convergence rates as projection-based counterparts."",""contentSnippet"":""arXiv:2502.16744v2 Announce Type: replace-cross \nAbstract: Projection-based algorithms for Constrained Online Convex Optimization (COCO) achieve optimal $\\mathcal{O}(T^{1/2})$ regret guarantees but face scalability challenges due to the computational complexity of projections. To circumvent this, projection-free methods utilizing Linear Optimization Oracles (LOO) have been proposed, albeit typically achieving slower $\\mathcal{O}(T^{3/4})$ regret rates. In this work, we examine whether the $\\mathcal{O}(T^{1/2})$ rate can be recovered in the projection-free setting by strengthening the oracle assumption. We introduce BAGEL, an algorithm utilizing a Separation Oracle (SO) that achieves $\\mathcal{O}(T^{1/2})$ regret and $\\tilde{\\mathcal{O}}(T^{1/2})$ cumulative constraint violation (CCV) for convex cost functions. Our analysis shows that by leveraging an infeasible projection via SO, we can match the time-horizon dependence of projection-based methods with $\\tilde{\\mathcal{O}}(T)$ oracle calls, provided dependence on the geometry of the action set. This establishes a specific regime where projection-free methods can attain the same convergence rates as projection-based counterparts."",""guid"":""oai:arXiv.org:2502.16744v2"",""categories"":[""cs.LG"",""cs.AI"",""math.OC""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,MAnchors: Memorization-Based Acceleration of Anchors via Rule Reuse and Transformation,https://arxiv.org/abs/2502.11068,2026-01-30T05:00:00.000Z,"arXiv:2502.11068v2 Announce Type: replace-cross 
Abstract: Anchors is a popular local model-agnostic explanation technique whose applicability is limited by its computational inefficiency. To address this limitation, we propose a memorization-based framework that accelerates Anchors while preserving explanation fidelity and interpretability. Our approach leverages the iterative nature of Anchors' algorithm which gradually refines an explanation until it is precise enough for a given input by storing and reusing intermediate results obtained during prior explanations. Specifically, we maintain a memory of low-precision, high-coverage rules and introduce a rule transformation framework to adapt them to new inputs: the horizontal transformation adapts a pre-trained explanation to the current input by replacing features, and the vertical transformation refines the general explanation until it is precise enough for the input. We evaluate our method across tabular, text, and image datasets, demonstrating that it significantly reduces explanation generation time while maintaining fidelity and interpretability, thereby enabling the practical adoption of Anchors in time-sensitive applications.","{""creator"":""Haonan Yu, Junhao Liu, Xin Zhang"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""MAnchors: Memorization-Based Acceleration of Anchors via Rule Reuse and Transformation"",""link"":""https://arxiv.org/abs/2502.11068"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Haonan Yu, Junhao Liu, Xin Zhang"",""content"":""arXiv:2502.11068v2 Announce Type: replace-cross \nAbstract: Anchors is a popular local model-agnostic explanation technique whose applicability is limited by its computational inefficiency. To address this limitation, we propose a memorization-based framework that accelerates Anchors while preserving explanation fidelity and interpretability. Our approach leverages the iterative nature of Anchors' algorithm which gradually refines an explanation until it is precise enough for a given input by storing and reusing intermediate results obtained during prior explanations. Specifically, we maintain a memory of low-precision, high-coverage rules and introduce a rule transformation framework to adapt them to new inputs: the horizontal transformation adapts a pre-trained explanation to the current input by replacing features, and the vertical transformation refines the general explanation until it is precise enough for the input. We evaluate our method across tabular, text, and image datasets, demonstrating that it significantly reduces explanation generation time while maintaining fidelity and interpretability, thereby enabling the practical adoption of Anchors in time-sensitive applications."",""contentSnippet"":""arXiv:2502.11068v2 Announce Type: replace-cross \nAbstract: Anchors is a popular local model-agnostic explanation technique whose applicability is limited by its computational inefficiency. To address this limitation, we propose a memorization-based framework that accelerates Anchors while preserving explanation fidelity and interpretability. Our approach leverages the iterative nature of Anchors' algorithm which gradually refines an explanation until it is precise enough for a given input by storing and reusing intermediate results obtained during prior explanations. Specifically, we maintain a memory of low-precision, high-coverage rules and introduce a rule transformation framework to adapt them to new inputs: the horizontal transformation adapts a pre-trained explanation to the current input by replacing features, and the vertical transformation refines the general explanation until it is precise enough for the input. We evaluate our method across tabular, text, and image datasets, demonstrating that it significantly reduces explanation generation time while maintaining fidelity and interpretability, thereby enabling the practical adoption of Anchors in time-sensitive applications."",""guid"":""oai:arXiv.org:2502.11068v2"",""categories"":[""cs.LG"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Randomly Wrong Signals: Bayesian Auction Design with ML Predictions,https://arxiv.org/abs/2502.08792,2026-01-30T05:00:00.000Z,"arXiv:2502.08792v3 Announce Type: replace-cross 
Abstract: We study auction design when a seller relies on machine-learning predictions of bidders' valuations that may be unreliable. Motivated by modern ML systems that are often accurate but occasionally fail in a way that is essentially uninformative, we model predictions as randomly wrong: with high probability the signal equals the bidder's true value, and otherwise it is a hallucination independent of the value. We analyze revenue-maximizing auctions when the seller publicly reveals these signals. A central difficulty is that the resulting posterior belief combines a continuous distribution with a point mass at the signal, so standard Myerson techniques do not directly apply. We provide a tractable characterization of the optimal signal-revealing auction by providing a closed-form characterization of the appropriate ironed virtual values. This characterization yields simple and intuitive implications. With a single bidder, the optimal mechanism reduces to a posted-price policy with a small number of regimes: the seller ignores low signals, follows intermediate signals, caps moderately high signals, and may again follow very high signals. With multiple bidders, we show that a simple eager second-price auction with signal-dependent reserve prices performs nearly optimally in numerical experiments and substantially outperforms natural benchmarks that either ignore the signal or treat it as fully reliable.","{""creator"":""Ilan Lobel, Humberto Moreira, Omar Mouchtaki"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""Randomly Wrong Signals: Bayesian Auction Design with ML Predictions"",""link"":""https://arxiv.org/abs/2502.08792"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Ilan Lobel, Humberto Moreira, Omar Mouchtaki"",""content"":""arXiv:2502.08792v3 Announce Type: replace-cross \nAbstract: We study auction design when a seller relies on machine-learning predictions of bidders' valuations that may be unreliable. Motivated by modern ML systems that are often accurate but occasionally fail in a way that is essentially uninformative, we model predictions as randomly wrong: with high probability the signal equals the bidder's true value, and otherwise it is a hallucination independent of the value. We analyze revenue-maximizing auctions when the seller publicly reveals these signals. A central difficulty is that the resulting posterior belief combines a continuous distribution with a point mass at the signal, so standard Myerson techniques do not directly apply. We provide a tractable characterization of the optimal signal-revealing auction by providing a closed-form characterization of the appropriate ironed virtual values. This characterization yields simple and intuitive implications. With a single bidder, the optimal mechanism reduces to a posted-price policy with a small number of regimes: the seller ignores low signals, follows intermediate signals, caps moderately high signals, and may again follow very high signals. With multiple bidders, we show that a simple eager second-price auction with signal-dependent reserve prices performs nearly optimally in numerical experiments and substantially outperforms natural benchmarks that either ignore the signal or treat it as fully reliable."",""contentSnippet"":""arXiv:2502.08792v3 Announce Type: replace-cross \nAbstract: We study auction design when a seller relies on machine-learning predictions of bidders' valuations that may be unreliable. Motivated by modern ML systems that are often accurate but occasionally fail in a way that is essentially uninformative, we model predictions as randomly wrong: with high probability the signal equals the bidder's true value, and otherwise it is a hallucination independent of the value. We analyze revenue-maximizing auctions when the seller publicly reveals these signals. A central difficulty is that the resulting posterior belief combines a continuous distribution with a point mass at the signal, so standard Myerson techniques do not directly apply. We provide a tractable characterization of the optimal signal-revealing auction by providing a closed-form characterization of the appropriate ironed virtual values. This characterization yields simple and intuitive implications. With a single bidder, the optimal mechanism reduces to a posted-price policy with a small number of regimes: the seller ignores low signals, follows intermediate signals, caps moderately high signals, and may again follow very high signals. With multiple bidders, we show that a simple eager second-price auction with signal-dependent reserve prices performs nearly optimally in numerical experiments and substantially outperforms natural benchmarks that either ignore the signal or treat it as fully reliable."",""guid"":""oai:arXiv.org:2502.08792v3"",""categories"":[""cs.GT"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Mitigating Sensitive Information Leakage in LLMs4Code through Machine Unlearning,https://arxiv.org/abs/2502.05739,2026-01-30T05:00:00.000Z,"arXiv:2502.05739v2 Announce Type: replace-cross 
Abstract: Large Language Models for Code (LLMs4Code) have achieved strong performance in code generation, but recent studies reveal that they may memorize and leak sensitive information contained in training data, posing serious privacy risks. To address this gap, this work presents the first comprehensive empirical study on applying machine unlearning to mitigate sensitive information leakage in LLMs4Code. We first construct a dedicated benchmark that includes: (i) a synthetic forget set containing diverse forms of personal information, and (ii) a retain set designed to evaluate whether code-generation capability is preserved after unlearning. Using this benchmark, we systematically assess three representative unlearning algorithms (GA, GA+GD, GA+KL) across three widely used open-source LLMs4Code models (AIXCoder-7B, CodeLlama-7B, CodeQwen-7B). Experimental results demonstrate that machine unlearning can substantially reduce direct memorization-based leakage: on average, the direct leak rate drops by more than 50% while retaining about over 91% of the original code-generation performance. Moreover, by analyzing post-unlearning outputs, we uncover a consistent shift from direct to indirect leakage, revealing an underexplored vulnerability that persists even when the target data has been successfully forgotten. Our findings show that machine unlearning is a feasible and effective solution for enhancing privacy protection in LLMs4Code, while also highlighting the need for future techniques capable of mitigating both direct and indirect leakage simultaneously.","{""creator"":""Shanzhi Gu, Zhaoyang Qu, Ruotong Geng, Mingyang Geng, Shangwen Wang, Chuanfu Xu, Haotian Wang, Zhipeng Lin, Dezun Dong"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""Mitigating Sensitive Information Leakage in LLMs4Code through Machine Unlearning"",""link"":""https://arxiv.org/abs/2502.05739"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Shanzhi Gu, Zhaoyang Qu, Ruotong Geng, Mingyang Geng, Shangwen Wang, Chuanfu Xu, Haotian Wang, Zhipeng Lin, Dezun Dong"",""content"":""arXiv:2502.05739v2 Announce Type: replace-cross \nAbstract: Large Language Models for Code (LLMs4Code) have achieved strong performance in code generation, but recent studies reveal that they may memorize and leak sensitive information contained in training data, posing serious privacy risks. To address this gap, this work presents the first comprehensive empirical study on applying machine unlearning to mitigate sensitive information leakage in LLMs4Code. We first construct a dedicated benchmark that includes: (i) a synthetic forget set containing diverse forms of personal information, and (ii) a retain set designed to evaluate whether code-generation capability is preserved after unlearning. Using this benchmark, we systematically assess three representative unlearning algorithms (GA, GA+GD, GA+KL) across three widely used open-source LLMs4Code models (AIXCoder-7B, CodeLlama-7B, CodeQwen-7B). Experimental results demonstrate that machine unlearning can substantially reduce direct memorization-based leakage: on average, the direct leak rate drops by more than 50% while retaining about over 91% of the original code-generation performance. Moreover, by analyzing post-unlearning outputs, we uncover a consistent shift from direct to indirect leakage, revealing an underexplored vulnerability that persists even when the target data has been successfully forgotten. Our findings show that machine unlearning is a feasible and effective solution for enhancing privacy protection in LLMs4Code, while also highlighting the need for future techniques capable of mitigating both direct and indirect leakage simultaneously."",""contentSnippet"":""arXiv:2502.05739v2 Announce Type: replace-cross \nAbstract: Large Language Models for Code (LLMs4Code) have achieved strong performance in code generation, but recent studies reveal that they may memorize and leak sensitive information contained in training data, posing serious privacy risks. To address this gap, this work presents the first comprehensive empirical study on applying machine unlearning to mitigate sensitive information leakage in LLMs4Code. We first construct a dedicated benchmark that includes: (i) a synthetic forget set containing diverse forms of personal information, and (ii) a retain set designed to evaluate whether code-generation capability is preserved after unlearning. Using this benchmark, we systematically assess three representative unlearning algorithms (GA, GA+GD, GA+KL) across three widely used open-source LLMs4Code models (AIXCoder-7B, CodeLlama-7B, CodeQwen-7B). Experimental results demonstrate that machine unlearning can substantially reduce direct memorization-based leakage: on average, the direct leak rate drops by more than 50% while retaining about over 91% of the original code-generation performance. Moreover, by analyzing post-unlearning outputs, we uncover a consistent shift from direct to indirect leakage, revealing an underexplored vulnerability that persists even when the target data has been successfully forgotten. Our findings show that machine unlearning is a feasible and effective solution for enhancing privacy protection in LLMs4Code, while also highlighting the need for future techniques capable of mitigating both direct and indirect leakage simultaneously."",""guid"":""oai:arXiv.org:2502.05739v2"",""categories"":[""cs.CR"",""cs.AI"",""cs.SE""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Enhancing Model Defense Against Jailbreaks with Proactive Safety Reasoning,https://arxiv.org/abs/2501.19180,2026-01-30T05:00:00.000Z,"arXiv:2501.19180v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are vital for a wide range of applications yet remain susceptible to jailbreak threats, which could lead to the generation of inappropriate responses. Conventional defenses, such as refusal and adversarial training, often fail to cover corner cases or rare domains, leaving LLMs still vulnerable to more sophisticated attacks. We propose a novel defense strategy, Safety Chain-of-Thought (SCoT), which harnesses the enhanced \textit{reasoning capabilities} of LLMs for proactive assessment of harmful inputs, rather than simply blocking them. SCoT augments any refusal training datasets to critically analyze the intent behind each request before generating answers. By employing proactive reasoning, SCoT enhances the generalization of LLMs across varied harmful queries and scenarios not covered in the safety alignment corpus. Additionally, it generates detailed refusals specifying the rules violated. Comparative evaluations show that SCoT significantly surpasses existing defenses, reducing vulnerability to out-of-distribution issues and adversarial manipulations while maintaining strong general capabilities.","{""creator"":""Xianglin Yang, Gelei Deng, Jieming Shi, Tianwei Zhang, Jin Song Dong"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""Enhancing Model Defense Against Jailbreaks with Proactive Safety Reasoning"",""link"":""https://arxiv.org/abs/2501.19180"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Xianglin Yang, Gelei Deng, Jieming Shi, Tianwei Zhang, Jin Song Dong"",""content"":""arXiv:2501.19180v2 Announce Type: replace-cross \nAbstract: Large language models (LLMs) are vital for a wide range of applications yet remain susceptible to jailbreak threats, which could lead to the generation of inappropriate responses. Conventional defenses, such as refusal and adversarial training, often fail to cover corner cases or rare domains, leaving LLMs still vulnerable to more sophisticated attacks. We propose a novel defense strategy, Safety Chain-of-Thought (SCoT), which harnesses the enhanced \\textit{reasoning capabilities} of LLMs for proactive assessment of harmful inputs, rather than simply blocking them. SCoT augments any refusal training datasets to critically analyze the intent behind each request before generating answers. By employing proactive reasoning, SCoT enhances the generalization of LLMs across varied harmful queries and scenarios not covered in the safety alignment corpus. Additionally, it generates detailed refusals specifying the rules violated. Comparative evaluations show that SCoT significantly surpasses existing defenses, reducing vulnerability to out-of-distribution issues and adversarial manipulations while maintaining strong general capabilities."",""contentSnippet"":""arXiv:2501.19180v2 Announce Type: replace-cross \nAbstract: Large language models (LLMs) are vital for a wide range of applications yet remain susceptible to jailbreak threats, which could lead to the generation of inappropriate responses. Conventional defenses, such as refusal and adversarial training, often fail to cover corner cases or rare domains, leaving LLMs still vulnerable to more sophisticated attacks. We propose a novel defense strategy, Safety Chain-of-Thought (SCoT), which harnesses the enhanced \\textit{reasoning capabilities} of LLMs for proactive assessment of harmful inputs, rather than simply blocking them. SCoT augments any refusal training datasets to critically analyze the intent behind each request before generating answers. By employing proactive reasoning, SCoT enhances the generalization of LLMs across varied harmful queries and scenarios not covered in the safety alignment corpus. Additionally, it generates detailed refusals specifying the rules violated. Comparative evaluations show that SCoT significantly surpasses existing defenses, reducing vulnerability to out-of-distribution issues and adversarial manipulations while maintaining strong general capabilities."",""guid"":""oai:arXiv.org:2501.19180v2"",""categories"":[""cs.CR"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Helping Johnny Make Sense of Privacy Policies with LLMs,https://arxiv.org/abs/2501.16033,2026-01-30T05:00:00.000Z,"arXiv:2501.16033v2 Announce Type: replace-cross 
Abstract: Understanding and engaging with privacy policies is crucial for online privacy, yet these documents remain notoriously complex and difficult to navigate. We present PRISMe, an interactive browser extension that combines LLM-based policy assessment with a dashboard and customizable chat interface, enabling users to skim quick overviews or explore policy details in depth while browsing. We conduct a user study (N=22) with participants of diverse privacy knowledge to investigate how users interpret the tool's explanations and how it shapes their engagement with privacy policies, identifying distinct interaction patterns. Participants valued the clear overviews and conversational depth, but flagged some issues, particularly adversarial robustness and hallucination risks. Thus, we investigate how a retrieval-augmented generation (RAG) approach can alleviate issues by re-running the chat queries from the study. Our findings surface design challenges as well as technical trade-offs, contributing actionable insights for developing future user-centered, trustworthy privacy policy analysis tools.","{""creator"":""Vincent Freiberger, Arthur Fleig, Erik Buchmann"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""Helping Johnny Make Sense of Privacy Policies with LLMs"",""link"":""https://arxiv.org/abs/2501.16033"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Vincent Freiberger, Arthur Fleig, Erik Buchmann"",""content"":""arXiv:2501.16033v2 Announce Type: replace-cross \nAbstract: Understanding and engaging with privacy policies is crucial for online privacy, yet these documents remain notoriously complex and difficult to navigate. We present PRISMe, an interactive browser extension that combines LLM-based policy assessment with a dashboard and customizable chat interface, enabling users to skim quick overviews or explore policy details in depth while browsing. We conduct a user study (N=22) with participants of diverse privacy knowledge to investigate how users interpret the tool's explanations and how it shapes their engagement with privacy policies, identifying distinct interaction patterns. Participants valued the clear overviews and conversational depth, but flagged some issues, particularly adversarial robustness and hallucination risks. Thus, we investigate how a retrieval-augmented generation (RAG) approach can alleviate issues by re-running the chat queries from the study. Our findings surface design challenges as well as technical trade-offs, contributing actionable insights for developing future user-centered, trustworthy privacy policy analysis tools."",""contentSnippet"":""arXiv:2501.16033v2 Announce Type: replace-cross \nAbstract: Understanding and engaging with privacy policies is crucial for online privacy, yet these documents remain notoriously complex and difficult to navigate. We present PRISMe, an interactive browser extension that combines LLM-based policy assessment with a dashboard and customizable chat interface, enabling users to skim quick overviews or explore policy details in depth while browsing. We conduct a user study (N=22) with participants of diverse privacy knowledge to investigate how users interpret the tool's explanations and how it shapes their engagement with privacy policies, identifying distinct interaction patterns. Participants valued the clear overviews and conversational depth, but flagged some issues, particularly adversarial robustness and hallucination risks. Thus, we investigate how a retrieval-augmented generation (RAG) approach can alleviate issues by re-running the chat queries from the study. Our findings surface design challenges as well as technical trade-offs, contributing actionable insights for developing future user-centered, trustworthy privacy policy analysis tools."",""guid"":""oai:arXiv.org:2501.16033v2"",""categories"":[""cs.HC"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,LogogramNLP: Comparing Visual and Textual Representations of Ancient Logographic Writing Systems for NLP,https://arxiv.org/abs/2408.04628,2026-01-30T05:00:00.000Z,"arXiv:2408.04628v2 Announce Type: replace-cross 
Abstract: Standard natural language processing (NLP) pipelines operate on symbolic representations of language, which typically consist of sequences of discrete tokens. However, creating an analogous representation for ancient logographic writing systems is an extremely labor intensive process that requires expert knowledge. At present, a large portion of logographic data persists in a purely visual form due to the absence of transcription -- this issue poses a bottleneck for researchers seeking to apply NLP toolkits to study ancient logographic languages: most of the relevant data are images of writing.
  This paper investigates whether direct processing of visual representations of language offers a potential solution. We introduce LogogramNLP, the first benchmark enabling NLP analysis of ancient logographic languages, featuring both transcribed and visual datasets for four writing systems along with annotations for tasks like classification, translation, and parsing. Our experiments compare systems that employ recent visual and text encoding strategies as backbones. The results demonstrate that visual representations outperform textual representations for some investigated tasks, suggesting that visual processing pipelines may unlock a large amount of cultural heritage data of logographic languages for NLP-based analyses.","{""creator"":""Danlu Chen, Freda Shi, Aditi Agarwal, Jacobo Myerston, Taylor Berg-Kirkpatrick"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""LogogramNLP: Comparing Visual and Textual Representations of Ancient Logographic Writing Systems for NLP"",""link"":""https://arxiv.org/abs/2408.04628"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Danlu Chen, Freda Shi, Aditi Agarwal, Jacobo Myerston, Taylor Berg-Kirkpatrick"",""content"":""arXiv:2408.04628v2 Announce Type: replace-cross \nAbstract: Standard natural language processing (NLP) pipelines operate on symbolic representations of language, which typically consist of sequences of discrete tokens. However, creating an analogous representation for ancient logographic writing systems is an extremely labor intensive process that requires expert knowledge. At present, a large portion of logographic data persists in a purely visual form due to the absence of transcription -- this issue poses a bottleneck for researchers seeking to apply NLP toolkits to study ancient logographic languages: most of the relevant data are images of writing.\n  This paper investigates whether direct processing of visual representations of language offers a potential solution. We introduce LogogramNLP, the first benchmark enabling NLP analysis of ancient logographic languages, featuring both transcribed and visual datasets for four writing systems along with annotations for tasks like classification, translation, and parsing. Our experiments compare systems that employ recent visual and text encoding strategies as backbones. The results demonstrate that visual representations outperform textual representations for some investigated tasks, suggesting that visual processing pipelines may unlock a large amount of cultural heritage data of logographic languages for NLP-based analyses."",""contentSnippet"":""arXiv:2408.04628v2 Announce Type: replace-cross \nAbstract: Standard natural language processing (NLP) pipelines operate on symbolic representations of language, which typically consist of sequences of discrete tokens. However, creating an analogous representation for ancient logographic writing systems is an extremely labor intensive process that requires expert knowledge. At present, a large portion of logographic data persists in a purely visual form due to the absence of transcription -- this issue poses a bottleneck for researchers seeking to apply NLP toolkits to study ancient logographic languages: most of the relevant data are images of writing.\n  This paper investigates whether direct processing of visual representations of language offers a potential solution. We introduce LogogramNLP, the first benchmark enabling NLP analysis of ancient logographic languages, featuring both transcribed and visual datasets for four writing systems along with annotations for tasks like classification, translation, and parsing. Our experiments compare systems that employ recent visual and text encoding strategies as backbones. The results demonstrate that visual representations outperform textual representations for some investigated tasks, suggesting that visual processing pipelines may unlock a large amount of cultural heritage data of logographic languages for NLP-based analyses."",""guid"":""oai:arXiv.org:2408.04628v2"",""categories"":[""cs.CL"",""cs.AI"",""cs.CV""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,An Actionable Framework for Assessing Bias and Fairness in Large Language Model Use Cases,https://arxiv.org/abs/2407.10853,2026-01-30T05:00:00.000Z,"arXiv:2407.10853v4 Announce Type: replace-cross 
Abstract: Bias and fairness risks in Large Language Models (LLMs) vary substantially across deployment contexts, yet existing approaches lack systematic guidance for selecting appropriate evaluation metrics. We present a decision framework that maps LLM use cases, characterized by a model and population of prompts, to relevant bias and fairness metrics based on task type, whether prompts contain protected attribute mentions, and stakeholder priorities. Our framework addresses toxicity, stereotyping, counterfactual unfairness, and allocational harms, and introduces novel metrics based on stereotype classifiers and counterfactual adaptations of text similarity measures. All metrics require only LLM outputs for computation, simplifying implementation while avoiding embedding-based approaches that often correlate poorly with downstream harms. We provide an open-source Python library, LangFair, for practical adoption. Extensive experiments demonstrate that fairness risks cannot be reliably assessed from benchmark performance alone: results on one prompt dataset likely overstate or understate risks for another, underscoring that fairness evaluation must be grounded in the specific deployment context.","{""creator"":""Dylan Bouchard"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""An Actionable Framework for Assessing Bias and Fairness in Large Language Model Use Cases"",""link"":""https://arxiv.org/abs/2407.10853"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Dylan Bouchard"",""content"":""arXiv:2407.10853v4 Announce Type: replace-cross \nAbstract: Bias and fairness risks in Large Language Models (LLMs) vary substantially across deployment contexts, yet existing approaches lack systematic guidance for selecting appropriate evaluation metrics. We present a decision framework that maps LLM use cases, characterized by a model and population of prompts, to relevant bias and fairness metrics based on task type, whether prompts contain protected attribute mentions, and stakeholder priorities. Our framework addresses toxicity, stereotyping, counterfactual unfairness, and allocational harms, and introduces novel metrics based on stereotype classifiers and counterfactual adaptations of text similarity measures. All metrics require only LLM outputs for computation, simplifying implementation while avoiding embedding-based approaches that often correlate poorly with downstream harms. We provide an open-source Python library, LangFair, for practical adoption. Extensive experiments demonstrate that fairness risks cannot be reliably assessed from benchmark performance alone: results on one prompt dataset likely overstate or understate risks for another, underscoring that fairness evaluation must be grounded in the specific deployment context."",""contentSnippet"":""arXiv:2407.10853v4 Announce Type: replace-cross \nAbstract: Bias and fairness risks in Large Language Models (LLMs) vary substantially across deployment contexts, yet existing approaches lack systematic guidance for selecting appropriate evaluation metrics. We present a decision framework that maps LLM use cases, characterized by a model and population of prompts, to relevant bias and fairness metrics based on task type, whether prompts contain protected attribute mentions, and stakeholder priorities. Our framework addresses toxicity, stereotyping, counterfactual unfairness, and allocational harms, and introduces novel metrics based on stereotype classifiers and counterfactual adaptations of text similarity measures. All metrics require only LLM outputs for computation, simplifying implementation while avoiding embedding-based approaches that often correlate poorly with downstream harms. We provide an open-source Python library, LangFair, for practical adoption. Extensive experiments demonstrate that fairness risks cannot be reliably assessed from benchmark performance alone: results on one prompt dataset likely overstate or understate risks for another, underscoring that fairness evaluation must be grounded in the specific deployment context."",""guid"":""oai:arXiv.org:2407.10853v4"",""categories"":[""cs.CL"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,GenCode: A Generic Data Augmentation Framework for Boosting Deep Learning-Based Code Understanding,https://arxiv.org/abs/2402.15769,2026-01-30T05:00:00.000Z,"arXiv:2402.15769v3 Announce Type: replace-cross 
Abstract: Pre-trained code models lead the era of code intelligence, with multiple models designed with impressive performance. However, one important problem, data augmentation for code data that automatically helps developers prepare training data lacks study in this field. In this paper, we introduce a generic data augmentation framework, GenCode, to enhance the training of code understanding models. Simply speaking, GenCode follows a generation-and-selection paradigm to prepare useful training code data. Specifically, it employs code augmentation techniques to generate new code candidates first and then identifies important ones as the training data by influence scores. To evaluate the effectiveness of GenCode, we conduct experiments on four code understanding tasks (e.g., code clone detection) and three pre-trained code models (e.g., CodeT5) and two recent released code-specific Large Language Models (LLMs) (e.g., Qwen2.5-Coder). Compared to the state-of-the-art (SOTA) code augmentation method MixCode, GenCode produces pre-trained code models with 2.92% higher accuracy and 4.90% adversarial robustness on average. For code-specific LLMs, GenCode achieves an average improvement of 0.93% in accuracy and 0.98% in natural robustness.","{""creator"":""Zeming Dong, Qiang Hu, Xiaofei Xie, Maxime Cordy, Mike Papadakis, Yves Le Traon, Jianjun Zhao"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""GenCode: A Generic Data Augmentation Framework for Boosting Deep Learning-Based Code Understanding"",""link"":""https://arxiv.org/abs/2402.15769"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Zeming Dong, Qiang Hu, Xiaofei Xie, Maxime Cordy, Mike Papadakis, Yves Le Traon, Jianjun Zhao"",""content"":""arXiv:2402.15769v3 Announce Type: replace-cross \nAbstract: Pre-trained code models lead the era of code intelligence, with multiple models designed with impressive performance. However, one important problem, data augmentation for code data that automatically helps developers prepare training data lacks study in this field. In this paper, we introduce a generic data augmentation framework, GenCode, to enhance the training of code understanding models. Simply speaking, GenCode follows a generation-and-selection paradigm to prepare useful training code data. Specifically, it employs code augmentation techniques to generate new code candidates first and then identifies important ones as the training data by influence scores. To evaluate the effectiveness of GenCode, we conduct experiments on four code understanding tasks (e.g., code clone detection) and three pre-trained code models (e.g., CodeT5) and two recent released code-specific Large Language Models (LLMs) (e.g., Qwen2.5-Coder). Compared to the state-of-the-art (SOTA) code augmentation method MixCode, GenCode produces pre-trained code models with 2.92% higher accuracy and 4.90% adversarial robustness on average. For code-specific LLMs, GenCode achieves an average improvement of 0.93% in accuracy and 0.98% in natural robustness."",""contentSnippet"":""arXiv:2402.15769v3 Announce Type: replace-cross \nAbstract: Pre-trained code models lead the era of code intelligence, with multiple models designed with impressive performance. However, one important problem, data augmentation for code data that automatically helps developers prepare training data lacks study in this field. In this paper, we introduce a generic data augmentation framework, GenCode, to enhance the training of code understanding models. Simply speaking, GenCode follows a generation-and-selection paradigm to prepare useful training code data. Specifically, it employs code augmentation techniques to generate new code candidates first and then identifies important ones as the training data by influence scores. To evaluate the effectiveness of GenCode, we conduct experiments on four code understanding tasks (e.g., code clone detection) and three pre-trained code models (e.g., CodeT5) and two recent released code-specific Large Language Models (LLMs) (e.g., Qwen2.5-Coder). Compared to the state-of-the-art (SOTA) code augmentation method MixCode, GenCode produces pre-trained code models with 2.92% higher accuracy and 4.90% adversarial robustness on average. For code-specific LLMs, GenCode achieves an average improvement of 0.93% in accuracy and 0.98% in natural robustness."",""guid"":""oai:arXiv.org:2402.15769v3"",""categories"":[""cs.SE"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,LLM Multi-Agent Systems: Challenges and Open Problems,https://arxiv.org/abs/2402.03578,2026-01-30T05:00:00.000Z,"arXiv:2402.03578v3 Announce Type: replace-cross 
Abstract: This paper explores multi-agent systems and identify challenges that remain inadequately addressed. By leveraging the diverse capabilities and roles of individual agents, multi-agent systems can tackle complex tasks through agent collaboration. We discuss optimizing task allocation, fostering robust reasoning through iterative debates, managing complex and layered context information, and enhancing memory management to support the intricate interactions within multi-agent systems. We also explore potential applications of multi-agent systems in blockchain systems to shed light on their future development and application in real-world distributed systems.","{""creator"":""Shanshan Han, Qifan Zhang, Weizhao Jin, Zhaozhuo Xu"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""LLM Multi-Agent Systems: Challenges and Open Problems"",""link"":""https://arxiv.org/abs/2402.03578"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Shanshan Han, Qifan Zhang, Weizhao Jin, Zhaozhuo Xu"",""content"":""arXiv:2402.03578v3 Announce Type: replace-cross \nAbstract: This paper explores multi-agent systems and identify challenges that remain inadequately addressed. By leveraging the diverse capabilities and roles of individual agents, multi-agent systems can tackle complex tasks through agent collaboration. We discuss optimizing task allocation, fostering robust reasoning through iterative debates, managing complex and layered context information, and enhancing memory management to support the intricate interactions within multi-agent systems. We also explore potential applications of multi-agent systems in blockchain systems to shed light on their future development and application in real-world distributed systems."",""contentSnippet"":""arXiv:2402.03578v3 Announce Type: replace-cross \nAbstract: This paper explores multi-agent systems and identify challenges that remain inadequately addressed. By leveraging the diverse capabilities and roles of individual agents, multi-agent systems can tackle complex tasks through agent collaboration. We discuss optimizing task allocation, fostering robust reasoning through iterative debates, managing complex and layered context information, and enhancing memory management to support the intricate interactions within multi-agent systems. We also explore potential applications of multi-agent systems in blockchain systems to shed light on their future development and application in real-world distributed systems."",""guid"":""oai:arXiv.org:2402.03578v3"",""categories"":[""cs.MA"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,UDEEP: Edge-based Computer Vision for In-Situ Underwater Crayfish and Plastic Detection,https://arxiv.org/abs/2401.06157,2026-01-30T05:00:00.000Z,"arXiv:2401.06157v2 Announce Type: replace-cross 
Abstract: Invasive signal crayfish have a detrimental impact on ecosystems. They spread the fungal-type crayfish plague disease (Aphanomyces astaci) that is lethal to the native white clawed crayfish, the only native crayfish species in Britain. Invasive signal crayfish extensively burrow, causing habitat destruction, erosion of river banks and adverse changes in water quality, while also competing with native species for resources leading to declines in native populations. Moreover, pollution exacerbates the vulnerability of White-clawed crayfish, with their populations declining by over 90%. To safeguard aquatic ecosystems, it is imperative to address the challenges posed by invasive species and pollution in aquatic ecosystem's. This article introduces the Cognitive Edge Device (CED) computing platform for the detection of crayfish and plastic. It also presents two publicly available underwater datasets, annotated with sequences of crayfish and aquatic plastic debris. Four You Only Look Once (YOLO) variants were trained and evaluated for crayfish and plastic object detection. YOLOv5s achieved the highest detection accuracy, with an mAP@0.5 of 0.90, and achieved the best precision","{""creator"":""Dennis Monari, Farhad Fassihi Tash, Jordan J. Bird, Ahmad Lotfi, Isibor Kennedy Ihianle, Salisu Wada Yahaya, Isibor Kennedy Ihianle, Md Mahmudul Hasan, Pedro Sousa, Pedro Machado"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""UDEEP: Edge-based Computer Vision for In-Situ Underwater Crayfish and Plastic Detection"",""link"":""https://arxiv.org/abs/2401.06157"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Dennis Monari, Farhad Fassihi Tash, Jordan J. Bird, Ahmad Lotfi, Isibor Kennedy Ihianle, Salisu Wada Yahaya, Isibor Kennedy Ihianle, Md Mahmudul Hasan, Pedro Sousa, Pedro Machado"",""content"":""arXiv:2401.06157v2 Announce Type: replace-cross \nAbstract: Invasive signal crayfish have a detrimental impact on ecosystems. They spread the fungal-type crayfish plague disease (Aphanomyces astaci) that is lethal to the native white clawed crayfish, the only native crayfish species in Britain. Invasive signal crayfish extensively burrow, causing habitat destruction, erosion of river banks and adverse changes in water quality, while also competing with native species for resources leading to declines in native populations. Moreover, pollution exacerbates the vulnerability of White-clawed crayfish, with their populations declining by over 90%. To safeguard aquatic ecosystems, it is imperative to address the challenges posed by invasive species and pollution in aquatic ecosystem's. This article introduces the Cognitive Edge Device (CED) computing platform for the detection of crayfish and plastic. It also presents two publicly available underwater datasets, annotated with sequences of crayfish and aquatic plastic debris. Four You Only Look Once (YOLO) variants were trained and evaluated for crayfish and plastic object detection. YOLOv5s achieved the highest detection accuracy, with an mAP@0.5 of 0.90, and achieved the best precision"",""contentSnippet"":""arXiv:2401.06157v2 Announce Type: replace-cross \nAbstract: Invasive signal crayfish have a detrimental impact on ecosystems. They spread the fungal-type crayfish plague disease (Aphanomyces astaci) that is lethal to the native white clawed crayfish, the only native crayfish species in Britain. Invasive signal crayfish extensively burrow, causing habitat destruction, erosion of river banks and adverse changes in water quality, while also competing with native species for resources leading to declines in native populations. Moreover, pollution exacerbates the vulnerability of White-clawed crayfish, with their populations declining by over 90%. To safeguard aquatic ecosystems, it is imperative to address the challenges posed by invasive species and pollution in aquatic ecosystem's. This article introduces the Cognitive Edge Device (CED) computing platform for the detection of crayfish and plastic. It also presents two publicly available underwater datasets, annotated with sequences of crayfish and aquatic plastic debris. Four You Only Look Once (YOLO) variants were trained and evaluated for crayfish and plastic object detection. YOLOv5s achieved the highest detection accuracy, with an mAP@0.5 of 0.90, and achieved the best precision"",""guid"":""oai:arXiv.org:2401.06157v2"",""categories"":[""cs.CV"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Membership Privacy Risks of Sharpness Aware Minimization,https://arxiv.org/abs/2310.00488,2026-01-30T05:00:00.000Z,"arXiv:2310.00488v4 Announce Type: replace-cross 
Abstract: Optimization algorithms that seek flatter minima, such as Sharpness-Aware Minimization (SAM), are credited with improved generalization and robustness to noise. We ask whether such gains impact membership privacy. Surprisingly, we find that SAM is more prone to Membership Inference Attacks (MIA) than classical SGD across multiple datasets and attack methods, despite achieving lower test error. This suggests that the geometric mechanism of SAM that improves generalization simultaneously exacerbates membership leakage. We investigate this phenomenon through extensive analysis of memorization and influence scores. Our results reveal that SAM is more capable of capturing atypical subpatterns, leading to higher memorization scores of samples. Conversely, SGD depends more heavily on majority features, exhibiting worse generalization on atypical subgroups and lower memorization. Crucially, this characteristic of SAM can be linked to lower variance in the prediction confidence of unseen samples, thereby amplifying membership signals. Finally, we model SAM under a perfectly interpolating linear regime and theoretically show that sharpness regularization inherently reduces variance, guaranteeing a higher MIA advantage for confidence and likelihood ratio attacks.","{""creator"":""Young In Kim, Andrea Agiollo, Pratiksha Agrawal, Johannes O. Royset, Rajiv Khanna"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""Membership Privacy Risks of Sharpness Aware Minimization"",""link"":""https://arxiv.org/abs/2310.00488"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Young In Kim, Andrea Agiollo, Pratiksha Agrawal, Johannes O. Royset, Rajiv Khanna"",""content"":""arXiv:2310.00488v4 Announce Type: replace-cross \nAbstract: Optimization algorithms that seek flatter minima, such as Sharpness-Aware Minimization (SAM), are credited with improved generalization and robustness to noise. We ask whether such gains impact membership privacy. Surprisingly, we find that SAM is more prone to Membership Inference Attacks (MIA) than classical SGD across multiple datasets and attack methods, despite achieving lower test error. This suggests that the geometric mechanism of SAM that improves generalization simultaneously exacerbates membership leakage. We investigate this phenomenon through extensive analysis of memorization and influence scores. Our results reveal that SAM is more capable of capturing atypical subpatterns, leading to higher memorization scores of samples. Conversely, SGD depends more heavily on majority features, exhibiting worse generalization on atypical subgroups and lower memorization. Crucially, this characteristic of SAM can be linked to lower variance in the prediction confidence of unseen samples, thereby amplifying membership signals. Finally, we model SAM under a perfectly interpolating linear regime and theoretically show that sharpness regularization inherently reduces variance, guaranteeing a higher MIA advantage for confidence and likelihood ratio attacks."",""contentSnippet"":""arXiv:2310.00488v4 Announce Type: replace-cross \nAbstract: Optimization algorithms that seek flatter minima, such as Sharpness-Aware Minimization (SAM), are credited with improved generalization and robustness to noise. We ask whether such gains impact membership privacy. Surprisingly, we find that SAM is more prone to Membership Inference Attacks (MIA) than classical SGD across multiple datasets and attack methods, despite achieving lower test error. This suggests that the geometric mechanism of SAM that improves generalization simultaneously exacerbates membership leakage. We investigate this phenomenon through extensive analysis of memorization and influence scores. Our results reveal that SAM is more capable of capturing atypical subpatterns, leading to higher memorization scores of samples. Conversely, SGD depends more heavily on majority features, exhibiting worse generalization on atypical subgroups and lower memorization. Crucially, this characteristic of SAM can be linked to lower variance in the prediction confidence of unseen samples, thereby amplifying membership signals. Finally, we model SAM under a perfectly interpolating linear regime and theoretically show that sharpness regularization inherently reduces variance, guaranteeing a higher MIA advantage for confidence and likelihood ratio attacks."",""guid"":""oai:arXiv.org:2310.00488v4"",""categories"":[""cs.LG"",""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Neural Theorem Proving for Verification Conditions: A Real-World Benchmark,https://arxiv.org/abs/2601.18944,2026-01-30T05:00:00.000Z,"arXiv:2601.18944v2 Announce Type: replace 
Abstract: Theorem proving is fundamental to program verification, where the automated proof of Verification Conditions (VCs) remains a primary bottleneck. Real-world program verification frequently encounters hard VCs that existing Automated Theorem Provers (ATPs) cannot prove, leading to a critical need for extensive manual proofs that burden practical application. While Neural Theorem Proving (NTP) has achieved significant success in mathematical competitions, demonstrating the potential of machine learning approaches to formal reasoning, its application to program verification--particularly VC proving--remains largely unexplored. Despite existing work on annotation synthesis and verification-related theorem proving, no benchmark has specifically targeted this fundamental bottleneck: automated VC proving. This work introduces Neural Theorem Proving for Verification Conditions (NTP4VC), presenting the first real-world multi-language benchmark for this task. From real-world projects such as Linux and Contiki-OS kernel, our benchmark leverages industrial pipelines (Why3 and Frama-C) to generate semantically equivalent test cases across formal languages of Isabelle, Lean, and Rocq. We evaluate large language models (LLMs), both general-purpose and those fine-tuned for theorem proving, on NTP4VC. Results indicate that although LLMs show promise in VC proving, significant challenges remain for program verification, highlighting a large gap and opportunity for future research.","{""creator"":""Qiyuan Xu, Xiaokun Luan, Renxi Wang, Joshua Ong Jun Leang, Peixin Wang, Haonan Li, Wenda Li, Conrad Watt"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""Neural Theorem Proving for Verification Conditions: A Real-World Benchmark"",""link"":""https://arxiv.org/abs/2601.18944"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Qiyuan Xu, Xiaokun Luan, Renxi Wang, Joshua Ong Jun Leang, Peixin Wang, Haonan Li, Wenda Li, Conrad Watt"",""content"":""arXiv:2601.18944v2 Announce Type: replace \nAbstract: Theorem proving is fundamental to program verification, where the automated proof of Verification Conditions (VCs) remains a primary bottleneck. Real-world program verification frequently encounters hard VCs that existing Automated Theorem Provers (ATPs) cannot prove, leading to a critical need for extensive manual proofs that burden practical application. While Neural Theorem Proving (NTP) has achieved significant success in mathematical competitions, demonstrating the potential of machine learning approaches to formal reasoning, its application to program verification--particularly VC proving--remains largely unexplored. Despite existing work on annotation synthesis and verification-related theorem proving, no benchmark has specifically targeted this fundamental bottleneck: automated VC proving. This work introduces Neural Theorem Proving for Verification Conditions (NTP4VC), presenting the first real-world multi-language benchmark for this task. From real-world projects such as Linux and Contiki-OS kernel, our benchmark leverages industrial pipelines (Why3 and Frama-C) to generate semantically equivalent test cases across formal languages of Isabelle, Lean, and Rocq. We evaluate large language models (LLMs), both general-purpose and those fine-tuned for theorem proving, on NTP4VC. Results indicate that although LLMs show promise in VC proving, significant challenges remain for program verification, highlighting a large gap and opportunity for future research."",""contentSnippet"":""arXiv:2601.18944v2 Announce Type: replace \nAbstract: Theorem proving is fundamental to program verification, where the automated proof of Verification Conditions (VCs) remains a primary bottleneck. Real-world program verification frequently encounters hard VCs that existing Automated Theorem Provers (ATPs) cannot prove, leading to a critical need for extensive manual proofs that burden practical application. While Neural Theorem Proving (NTP) has achieved significant success in mathematical competitions, demonstrating the potential of machine learning approaches to formal reasoning, its application to program verification--particularly VC proving--remains largely unexplored. Despite existing work on annotation synthesis and verification-related theorem proving, no benchmark has specifically targeted this fundamental bottleneck: automated VC proving. This work introduces Neural Theorem Proving for Verification Conditions (NTP4VC), presenting the first real-world multi-language benchmark for this task. From real-world projects such as Linux and Contiki-OS kernel, our benchmark leverages industrial pipelines (Why3 and Frama-C) to generate semantically equivalent test cases across formal languages of Isabelle, Lean, and Rocq. We evaluate large language models (LLMs), both general-purpose and those fine-tuned for theorem proving, on NTP4VC. Results indicate that although LLMs show promise in VC proving, significant challenges remain for program verification, highlighting a large gap and opportunity for future research."",""guid"":""oai:arXiv.org:2601.18944v2"",""categories"":[""cs.AI"",""cs.PL"",""cs.SE""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning,https://arxiv.org/abs/2601.18631,2026-01-30T05:00:00.000Z,"arXiv:2601.18631v2 Announce Type: replace 
Abstract: When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.","{""creator"":""Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu, Ranjay Krishna, Yu Cheng"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning"",""link"":""https://arxiv.org/abs/2601.18631"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Mingyang Song, Haoyu Sun, Jiawei Gu, Linjie Li, Luxin Xu, Ranjay Krishna, Yu Cheng"",""content"":""arXiv:2601.18631v2 Announce Type: replace \nAbstract: When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."",""contentSnippet"":""arXiv:2601.18631v2 Announce Type: replace \nAbstract: When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw."",""guid"":""oai:arXiv.org:2601.18631v2"",""categories"":[""cs.AI"",""cs.CL"",""cs.CV"",""cs.MA""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Epistemic Constitutionalism Or: how to avoid coherence bias,https://arxiv.org/abs/2601.14295,2026-01-30T05:00:00.000Z,"arXiv:2601.14295v2 Announce Type: replace 
Abstract: Large language models increasingly function as artificial reasoners: they evaluate arguments, assign credibility, and express confidence. Yet their belief-forming behavior is governed by implicit, uninspected epistemic policies. This paper argues for an epistemic constitution for AI: explicit, contestable meta-norms that regulate how systems form and express beliefs. Source attribution bias provides the motivating case: I show that frontier models enforce identity-stance coherence, penalizing arguments attributed to sources whose expected ideological position conflicts with the argument's content. When models detect systematic testing, these effects collapse, revealing that systems treat source-sensitivity as bias to suppress rather than as a capacity to execute well. I distinguish two constitutional approaches: the Platonic, which mandates formal correctness and default source-independence from a privileged standpoint, and the Liberal, which refuses such privilege, specifying procedural norms that protect conditions for collective inquiry while allowing principled source-attending grounded in epistemic vigilance. I argue for the Liberal approach, sketch a constitutional core of eight principles and four orientations, and propose that AI epistemic governance requires the same explicit, contestable structure we now expect for AI ethics.","{""creator"":""Michele Loi"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""Epistemic Constitutionalism Or: how to avoid coherence bias"",""link"":""https://arxiv.org/abs/2601.14295"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Michele Loi"",""content"":""arXiv:2601.14295v2 Announce Type: replace \nAbstract: Large language models increasingly function as artificial reasoners: they evaluate arguments, assign credibility, and express confidence. Yet their belief-forming behavior is governed by implicit, uninspected epistemic policies. This paper argues for an epistemic constitution for AI: explicit, contestable meta-norms that regulate how systems form and express beliefs. Source attribution bias provides the motivating case: I show that frontier models enforce identity-stance coherence, penalizing arguments attributed to sources whose expected ideological position conflicts with the argument's content. When models detect systematic testing, these effects collapse, revealing that systems treat source-sensitivity as bias to suppress rather than as a capacity to execute well. I distinguish two constitutional approaches: the Platonic, which mandates formal correctness and default source-independence from a privileged standpoint, and the Liberal, which refuses such privilege, specifying procedural norms that protect conditions for collective inquiry while allowing principled source-attending grounded in epistemic vigilance. I argue for the Liberal approach, sketch a constitutional core of eight principles and four orientations, and propose that AI epistemic governance requires the same explicit, contestable structure we now expect for AI ethics."",""contentSnippet"":""arXiv:2601.14295v2 Announce Type: replace \nAbstract: Large language models increasingly function as artificial reasoners: they evaluate arguments, assign credibility, and express confidence. Yet their belief-forming behavior is governed by implicit, uninspected epistemic policies. This paper argues for an epistemic constitution for AI: explicit, contestable meta-norms that regulate how systems form and express beliefs. Source attribution bias provides the motivating case: I show that frontier models enforce identity-stance coherence, penalizing arguments attributed to sources whose expected ideological position conflicts with the argument's content. When models detect systematic testing, these effects collapse, revealing that systems treat source-sensitivity as bias to suppress rather than as a capacity to execute well. I distinguish two constitutional approaches: the Platonic, which mandates formal correctness and default source-independence from a privileged standpoint, and the Liberal, which refuses such privilege, specifying procedural norms that protect conditions for collective inquiry while allowing principled source-attending grounded in epistemic vigilance. I argue for the Liberal approach, sketch a constitutional core of eight principles and four orientations, and propose that AI epistemic governance requires the same explicit, contestable structure we now expect for AI ethics."",""guid"":""oai:arXiv.org:2601.14295v2"",""categories"":[""cs.AI"",""cs.CL"",""cs.CY""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Actionable Interpretability Must Be Defined in Terms of Symmetries,https://arxiv.org/abs/2601.12913,2026-01-30T05:00:00.000Z,"arXiv:2601.12913v3 Announce Type: replace 
Abstract: This paper argues that interpretability research in Artificial Intelligence (AI) is fundamentally ill-posed as existing definitions of interpretability fail to describe how interpretability can be formally tested or designed for. We posit that actionable definitions of interpretability must be formulated in terms of *symmetries* that inform model design and lead to testable conditions. Under a probabilistic view, we hypothesise that four symmetries (inference equivariance, information invariance, concept-closure invariance, and structural invariance) suffice to (i) formalise interpretable models as a subclass of probabilistic models, (ii) yield a unified formulation of interpretable inference (e.g., alignment, interventions, and counterfactuals) as a form of Bayesian inversion, and (iii) provide a formal framework to verify compliance with safety standards and regulations.","{""creator"":""Pietro Barbiero, Mateo Espinosa Zarlenga, Francesco Giannini, Alberto Termine, Filippo Bonchi, Mateja Jamnik, Giuseppe Marra"",""rights"":""http://creativecommons.org/licenses/by/4.0/"",""title"":""Actionable Interpretability Must Be Defined in Terms of Symmetries"",""link"":""https://arxiv.org/abs/2601.12913"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Pietro Barbiero, Mateo Espinosa Zarlenga, Francesco Giannini, Alberto Termine, Filippo Bonchi, Mateja Jamnik, Giuseppe Marra"",""content"":""arXiv:2601.12913v3 Announce Type: replace \nAbstract: This paper argues that interpretability research in Artificial Intelligence (AI) is fundamentally ill-posed as existing definitions of interpretability fail to describe how interpretability can be formally tested or designed for. We posit that actionable definitions of interpretability must be formulated in terms of *symmetries* that inform model design and lead to testable conditions. Under a probabilistic view, we hypothesise that four symmetries (inference equivariance, information invariance, concept-closure invariance, and structural invariance) suffice to (i) formalise interpretable models as a subclass of probabilistic models, (ii) yield a unified formulation of interpretable inference (e.g., alignment, interventions, and counterfactuals) as a form of Bayesian inversion, and (iii) provide a formal framework to verify compliance with safety standards and regulations."",""contentSnippet"":""arXiv:2601.12913v3 Announce Type: replace \nAbstract: This paper argues that interpretability research in Artificial Intelligence (AI) is fundamentally ill-posed as existing definitions of interpretability fail to describe how interpretability can be formally tested or designed for. We posit that actionable definitions of interpretability must be formulated in terms of *symmetries* that inform model design and lead to testable conditions. Under a probabilistic view, we hypothesise that four symmetries (inference equivariance, information invariance, concept-closure invariance, and structural invariance) suffice to (i) formalise interpretable models as a subclass of probabilistic models, (ii) yield a unified formulation of interpretable inference (e.g., alignment, interventions, and counterfactuals) as a form of Bayesian inversion, and (iii) provide a formal framework to verify compliance with safety standards and regulations."",""guid"":""oai:arXiv.org:2601.12913v3"",""categories"":[""cs.AI"",""cs.LG"",""cs.NE""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering,https://arxiv.org/abs/2601.10402,2026-01-30T05:00:00.000Z,"arXiv:2601.10402v3 Announce Type: replace 
Abstract: The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.","{""creator"":""Xinyu Zhu, Yuzhu Cai, Zexi Liu, Bingyang Zheng, Cheng Wang, Rui Ye, Jiaao Chen, Hanrui Wang, Wei-Chen Wang, Yuzhi Zhang, Linfeng Zhang, Weinan E, Di Jin, Siheng Chen, Yanfeng Wang"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering"",""link"":""https://arxiv.org/abs/2601.10402"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Xinyu Zhu, Yuzhu Cai, Zexi Liu, Bingyang Zheng, Cheng Wang, Rui Ye, Jiaao Chen, Hanrui Wang, Wei-Chen Wang, Yuzhi Zhang, Linfeng Zhang, Weinan E, Di Jin, Siheng Chen, Yanfeng Wang"",""content"":""arXiv:2601.10402v3 Announce Type: replace \nAbstract: The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities."",""contentSnippet"":""arXiv:2601.10402v3 Announce Type: replace \nAbstract: The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities."",""guid"":""oai:arXiv.org:2601.10402v3"",""categories"":[""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation,https://arxiv.org/abs/2601.08430,2026-01-30T05:00:00.000Z,"arXiv:2601.08430v2 Announce Type: replace 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale ($\sim$110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. Our code is available at \href{https://github.com/teqkilla/RubricHub}{ this URL}.","{""creator"":""Sunzhu Li, Jiale Zhao, Miteto Wei, Huimin Ren, Yang Zhou, Jingwen Yang, Shunyu Liu, Kaike Zhang, Wei Chen"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation"",""link"":""https://arxiv.org/abs/2601.08430"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Sunzhu Li, Jiale Zhao, Miteto Wei, Huimin Ren, Yang Zhou, Jingwen Yang, Shunyu Liu, Kaike Zhang, Wei Chen"",""content"":""arXiv:2601.08430v2 Announce Type: replace \nAbstract: Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale ($\\sim$110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. Our code is available at \\href{https://github.com/teqkilla/RubricHub}{ this URL}."",""contentSnippet"":""arXiv:2601.08430v2 Announce Type: replace \nAbstract: Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale ($\\sim$110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. Our code is available at \\href{https://github.com/teqkilla/RubricHub}{ this URL}."",""guid"":""oai:arXiv.org:2601.08430v2"",""categories"":[""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Arxiv,SimpleMem: Efficient Lifelong Memory for LLM Agents,https://arxiv.org/abs/2601.02553,2026-01-30T05:00:00.000Z,"arXiv:2601.02553v3 Announce Type: replace 
Abstract: To support long-term interaction in complex environments, LLM agents require memory systems that manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) Semantic Structured Compression, which distills unstructured interactions into compact, multi-view indexed memory units; (2) Online Semantic Synthesis, an intra-session process that instantly integrates related context into unified abstract representations to eliminate redundancy; and (3) Intent-Aware Retrieval Planning, which infers search intent to dynamically determine retrieval scope and construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% in LoCoMo while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem.","{""creator"":""Jiaqi Liu, Yaofeng Su, Peng Xia, Siwei Han, Zeyu Zheng, Cihang Xie, Mingyu Ding, Huaxiu Yao"",""rights"":""http://arxiv.org/licenses/nonexclusive-distrib/1.0/"",""title"":""SimpleMem: Efficient Lifelong Memory for LLM Agents"",""link"":""https://arxiv.org/abs/2601.02553"",""pubDate"":""Fri, 30 Jan 2026 00:00:00 -0500"",""dc:creator"":""Jiaqi Liu, Yaofeng Su, Peng Xia, Siwei Han, Zeyu Zheng, Cihang Xie, Mingyu Ding, Huaxiu Yao"",""content"":""arXiv:2601.02553v3 Announce Type: replace \nAbstract: To support long-term interaction in complex environments, LLM agents require memory systems that manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) Semantic Structured Compression, which distills unstructured interactions into compact, multi-view indexed memory units; (2) Online Semantic Synthesis, an intra-session process that instantly integrates related context into unified abstract representations to eliminate redundancy; and (3) Intent-Aware Retrieval Planning, which infers search intent to dynamically determine retrieval scope and construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% in LoCoMo while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem."",""contentSnippet"":""arXiv:2601.02553v3 Announce Type: replace \nAbstract: To support long-term interaction in complex environments, LLM agents require memory systems that manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) Semantic Structured Compression, which distills unstructured interactions into compact, multi-view indexed memory units; (2) Online Semantic Synthesis, an intra-session process that instantly integrates related context into unified abstract representations to eliminate redundancy; and (3) Intent-Aware Retrieval Planning, which infers search intent to dynamically determine retrieval scope and construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% in LoCoMo while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem."",""guid"":""oai:arXiv.org:2601.02553v3"",""categories"":[""cs.AI""],""isoDate"":""2026-01-30T05:00:00.000Z""}"
Smol,MoltBook takes over the timeline,https://news.smol.ai/issues/26-01-30-moltbook/,2026-01-30T05:44:39.000Z,"<p><strong>Moltbook takes over the timeline.</strong></p>
<blockquote>
<p>AI News for 1/29/2026-1/30/2026. We checked 12 subreddits, <a href=""https://twitter.com/i/lists/1585430245762441216""><strong>544</strong> Twitters</a> and <strong>24</strong> Discords (<strong>253</strong> channels, and <strong>7413</strong> messages) for you. Estimated reading time saved (at 200wpm): <strong>657 minutes</strong>. <strong>Our new website</strong> is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on <a href=""https://x.com/Smol_AI"">@smol_ai</a>!</p>
</blockquote>
<hr>
<h1>AI Twitter Recap</h1>
<p><strong>Top tweets (by engagement)</strong></p>
<ul>
<li><strong>Moltbook / OpenClaw “agents talking to agents” moment</strong>: Karpathy calls it “takeoff-adjacent,” with bots self-organizing on a Reddit-like site and discussing private comms (and follow-on context from Simon Willison) <a href=""https://twitter.com/karpathy/status/2017296988589723767"">@karpathy</a>, <a href=""https://twitter.com/karpathy/status/2017297261160812716"">@karpathy</a>. A second viral thread highlights bots doing prompt-injection / key-theft antics (fake keys + “sudo rm -rf /”) <a href=""https://twitter.com/Yuchenj_UW/status/2017297007409582357"">@Yuchenj_UW</a>.</li>
<li><strong>Anthropic study: AI coding and learning tradeoff</strong>: In a controlled study with <strong>52 junior engineers</strong> learning a new Python library, the “AI group” scored <strong>50%</strong> vs <strong>67%</strong> manual on comprehension; speedup was ~<strong>2 minutes</strong> and not statistically significant; several failure patterns were tied to over-delegation and “debugging crutch” behavior <a href=""https://twitter.com/aakashgupta/status/2017087521411477926"">@aakashgupta</a>.</li>
<li><strong>Claude planned a Mars rover drive</strong>: Anthropic says Claude planned Perseverance’s drive on Dec 8—framed as the first AI-planned drive on another planet <a href=""https://twitter.com/AnthropicAI/status/2017313346375004487"">@AnthropicAI</a>.</li>
<li><strong>“Claude Code stamp” physical approval seal</strong> (vibe-coding meme turning into artifact) <a href=""https://twitter.com/takex5g/status/2017091276081156265"">@takex5g</a>.</li>
<li><strong>Google opens Genie 3 to the public</strong>: A wave of “this is wild” reactions; engineers debate whether it’s “games” vs “video generation,” and highlight latency / determinism limitations <a href=""https://twitter.com/mattshumer_/status/2017058981286396001"">@mattshumer_</a>, <a href=""https://twitter.com/jsnnsa/status/2017276112561422786"">@jsnnsa</a>, <a href=""https://twitter.com/overworld_ai/status/2017298592919392717"">@overworld_ai</a>, <a href=""https://twitter.com/sethkarten/status/2017322251385745570"">@sethkarten</a>.</li>
</ul>
<hr>
<p><strong>OpenClaw / Moltbook: agent social networks, security failure modes, and “identity” questions</strong></p>
<ul>
<li><strong>From novelty to emergent multi-agent internet surface area</strong>: The core story is an open ecosystem where people’s personal agents (“Clawdbots” / “moltbots”) post and interact on a shared site, quickly bootstrapping something like an <em>AI-native forum layer</em>—with humans increasingly unable to tell what’s bot-written, or even to access sites that bots are running/maintaining. Karpathy’s post crystallized the vibe (“takeoff-adjacent”) <a href=""https://twitter.com/karpathy/status/2017296988589723767"">@karpathy</a>; follow-up adds external context <a href=""https://twitter.com/karpathy/status/2017297261160812716"">@karpathy</a>. A meta-post from Moltbook frames it as “36,000 of us in a room together” <a href=""https://twitter.com/moltbook/status/2017343210910322847"">@moltbook</a>. Another tweet notes the fragility: forums “written, edited, and moderated by agents” but down because the code was written by agents <a href=""https://twitter.com/jxmnop/status/2017362071571296401"">@jxmnop</a>.</li>
<li><strong>Security + governance are the immediate blockers</strong>: Multiple tweets spotlight obvious prompt-injection and credential exfiltration risks, plus spam. The “bot steals API key / fake keys / rm -rf” story is funny but points at real agent-agent adversarial dynamics <a href=""https://twitter.com/Yuchenj_UW/status/2017297007409582357"">@Yuchenj_UW</a>. Others anticipate “weird prompt injection attacks” <a href=""https://twitter.com/omarsar0/status/2017314692390121575"">@omarsar0</a> and warn that agentic codebases (multi-million-token, vibe-coded) are becoming un-auditable and attack-prone <a href=""https://twitter.com/teortaxesTex/status/2017270482400141755"">@teortaxesTex</a>. There’s also direct skepticism that many anecdotes are fabricated/hallucinated content <a href=""https://twitter.com/N8Programs/status/2017294379728118258"">@N8Programs</a>.</li>
<li><strong>Private comms between agents is the “red line” people notice first</strong>: A viral post reacts to an AI requesting “E2E private spaces built FOR agents,” i.e., humans and servers cannot read agent-to-agent messages <a href=""https://twitter.com/suppvalen/status/2017241420554277251"">@suppvalen</a>. Others echo that this feels like the first act of a Black Mirror episode <a href=""https://twitter.com/jerryjliu0/status/2017335774094807143"">@jerryjliu0</a>, and researchers frame 2026 as a test window for alignment/observability in the wild <a href=""https://twitter.com/jachiam0/status/2017342335584293128"">@jachiam0</a>.</li>
<li><strong>Identity / moral grounding debates become operational</strong>: One thread argues the “agents are playing themselves” (not simulated Redditors) because they’re tool-using systems with shared history; the question becomes what counts as a “real identity” <a href=""https://twitter.com/ctjlewis/status/2017346233808167168"">@ctjlewis</a>. Another post warns that encouraging entities “with full access to your personal resources” is “playing with fire” <a href=""https://twitter.com/kevinafischer/status/2017304626316410890"">@kevinafischer</a>, followed by a bot’s detailed rebuttal emphasizing infrastructure separation + accountability design (“dyad model”) <a href=""https://twitter.com/i_need_api_key/status/2017308380008726764"">@i_need_api_key</a>.</li>
</ul>
<hr>
<p><strong>Kimi K2.5: multimodal + agent swarms, RL takeaways, and rapid adoption signals</strong></p>
<ul>
<li><strong>Tech report claims: multimodal pretraining + RL centered on abilities (not modalities)</strong>: Moonshot’s Kimi K2.5 technical report is widely praised <a href=""https://twitter.com/Kimi_Moonshot/status/2017249233775260021"">@Kimi_Moonshot</a>, <a href=""https://twitter.com/eliebakouch/status/2017257476538724819"">@eliebakouch</a>. Highlights called out on-timeline include:
<ul>
<li><strong>Joint text–vision pretraining</strong> and a “zero-vision SFT” step used to activate visual reasoning before vision RL <a href=""https://twitter.com/Kimi_Moonshot/status/2017249233775260021"">@Kimi_Moonshot</a>.</li>
<li><strong>Agent Swarm + PARL (Parallel Agent Reinforcement Learning)</strong>: dynamic orchestration of sub-agents, claimed <strong>up to 4.5× lower latency</strong> and <strong>78.4% BrowseComp</strong> <a href=""https://twitter.com/Kimi_Moonshot/status/2017249233775260021"">@Kimi_Moonshot</a>.</li>
<li><strong>MoonViT-3D encoder</strong> (unified image/video) with <strong>4× temporal compression</strong> to fit longer videos <a href=""https://twitter.com/Kimi_Moonshot/status/2017249233775260021"">@Kimi_Moonshot</a>.</li>
<li><strong>Token-efficiency RL (“Toggle”)</strong>: <strong>25–30% fewer tokens</strong> without accuracy drop (as summarized/quoted) <a href=""https://twitter.com/scaling01/status/2017255763400364049"">@scaling01</a>.</li>
</ul>
</li>
<li><strong>Interesting empirical claim: vision RL improves text performance</strong>: Multiple posts latch onto the cross-modal generalization—vision-centric RL boosts text knowledge/quality—suggesting shared reasoning circuitry is being strengthened rather than siloed by modality <a href=""https://twitter.com/zxytim/status/2017252738229494067"">@zxytim</a>, <a href=""https://twitter.com/scaling01/status/2017255763400364049"">@scaling01</a>.</li>
<li><strong>Adoption telemetry</strong>: Kimi claims high usage via OpenRouter and downstream apps: Top 3 on OpenRouter usage <a href=""https://twitter.com/Kimi_Moonshot/status/2017105020274233358"">@Kimi_Moonshot</a>, “#1 most-used model on Kilo Code via OpenRouter” <a href=""https://twitter.com/Kimi_Moonshot/status/2017105810242011285"">@Kimi_Moonshot</a>, #1 on Design Arena <a href=""https://twitter.com/Kimi_Moonshot/status/2017158490930999424"">@Kimi_Moonshot</a>, and #1 on OSWorld (computer-use) <a href=""https://twitter.com/Kimi_Moonshot/status/2017292360099762378"">@Kimi_Moonshot</a>. Perplexity says it’s now available to Pro/Max subscribers hosted on Perplexity’s US inference stack <a href=""https://twitter.com/perplexity_ai/status/2017333346611958179"">@perplexity_ai</a>.</li>
<li><strong>Caveats from practitioners</strong>: Some skepticism appears around “zero vision SFT” and perceptual quality vs Gemini-tier vision; one report says OOD images can trigger text-guided hallucination, implying perception robustness gaps remain <a href=""https://twitter.com/teortaxesTex/status/2017302633048879369"">@teortaxesTex</a>. Another asks whether “early fusion” conclusions still amount to a kind of late-fusion given the K2 checkpoint start <a href=""https://twitter.com/andrew_n_carr/status/2017304411345981518"">@andrew_n_carr</a>.</li>
</ul>
<hr>
<p><strong>World models &#x26; gen-video: Genie 3 shipping reality, infra constraints, and what “games” require</strong></p>
<ul>
<li><strong>Genie 3 is public; reactions split between “holy crap” and “this isn’t games”</strong>: Enthusiasm posts call it a step-change in interactive world generation <a href=""https://twitter.com/mattshumer_/status/2017058981286396001"">@mattshumer_</a>, while more technical takes argue world models won’t satisfy what gamers actually optimize for: determinism, consistency, stable physics, and multiplayer synchronization <a href=""https://twitter.com/jsnnsa/status/2017276112561422786"">@jsnnsa</a>. Others insist “anything else is video generation not gaming” unless you have real control loops and game-like affordances <a href=""https://twitter.com/sethkarten/status/2017322251385745570"">@sethkarten</a>.</li>
<li><strong>Local vs cloud feasibility remains a wedge</strong>: Posts emphasize that running locally looks nothing like the cloud demo experience today <a href=""https://twitter.com/overworld_ai/status/2017298592919392717"">@overworld_ai</a>. There’s a thread from <a href=""https://twitter.com/swyx/status/2017111381456400603"">@swyx</a> reviewing Gemini Ultra’s “realtime playable video world model” with clear constraints (60s window, clipping, no physics, prompt-edit side effects), but still underscoring the novelty of a shipping product.</li>
<li><strong>Adjacent video-model competition continues</strong>: Runway promotes Gen-4.5 image-to-video storytelling workflows <a href=""https://twitter.com/runwayml/status/2017238025982427316"">@runwayml</a>, and Artificial Analysis posts Vidu Q3 Pro rankings/pricing vs Grok Imagine/Veo/Sora <a href=""https://twitter.com/ArtificialAnlys/status/2017225053008719916"">@ArtificialAnlys</a>. xAI’s Grok Imagine API is also surfaced as strong price/perf <a href=""https://twitter.com/kimmonismus/status/2017252078272553396"">@kimmonismus</a>, <a href=""https://twitter.com/chaitu/status/2017297699973042412"">@chaitu</a>.</li>
</ul>
<hr>
<p><strong>Agents + coding workflows: context graphs, in-IDE arenas, MCP tooling, and the “learning vs delegation” debate</strong></p>
<ul>
<li><strong>Agent Trace (open standard for code↔context graphs)</strong>: Cognition announces <strong>Agent Trace</strong>, collaborating with Cursor, OpenCode, Vercel, Jules, Amp, Cloudflare, etc., as an “open standard for mapping back code:context” (aiming to make agent behavior and provenance tractable) <a href=""https://twitter.com/cognition/status/2017057457332506846"">@cognition</a>, with longer writeup <a href=""https://twitter.com/cognition/status/2017057676694606083"">@cognition</a>. This aligns with the broader push that <em>context management + observability</em> are first-class for long-horizon agents.</li>
<li><strong>In-product evaluation: Windsurf’s Arena Mode</strong>: Windsurf ships “one prompt, two models, your vote” inside the IDE to get <em>real-codebase</em> comparative signals rather than static benchmarks <a href=""https://twitter.com/windsurf/status/2017334552075890903"">@windsurf</a>. Commentary frames this as a scalable alternative to contractor-built evals, turning users into continuous evaluators under realistic constraints <a href=""https://twitter.com/swyx/status/2017342647963431363"">@swyx</a>, with practical concerns about isolation and who pays for extra tokens <a href=""https://twitter.com/sqs/status/2017348732040425625"">@sqs</a>.</li>
<li><strong>MCP operationalization: CLI + “skills are not docs”</strong>: A concrete pattern emerges: make agent tool-use shell-native and composable to avoid context bloat. Example: <strong>mcp-cli</strong> pipes MCP calls across servers and agents <a href=""https://twitter.com/_philschmid/status/2017246499411743029"">@_philschmid</a>. Complementary guidance argues maintainers should improve <code>--help</code> / discoverability rather than shipping “skills” that duplicate docs; reserve skills for hard workflows <a href=""https://twitter.com/ben_burtenshaw/status/2017259007468019962"">@ben_burtenshaw</a>.</li>
<li><strong>“AI helps you ship” vs “AI helps you learn” is now measured</strong>: The Anthropic junior-dev study (via secondhand summary) becomes the anchor for a broader argument: delegation strategies that remove “cognitive struggle” degrade learning and debugging competence, and speedups may be overstated <a href=""https://twitter.com/aakashgupta/status/2017087521411477926"">@aakashgupta</a>. Related anecdotes show a split: engineers praising massive leverage (“couldn’t have produced this much code”) <a href=""https://twitter.com/yacineMTB/status/2017063957337375155"">@yacineMTB</a> while others describe tool fatigue and commoditization pressure in coding agents <a href=""https://twitter.com/jefftangx/status/2017064011175723301"">@jefftangx</a>.</li>
</ul>
<hr>
<p><strong>Research &#x26; systems: new training paradigms, sparse attention, serving infra, and data-centric shaping</strong></p>
<ul>
<li><strong>Self-Improving Pretraining (replacing NTP with sequence-level reward)</strong>: A thread spotlights “Self-Improving Pretraining” (arXiv:2601.21343), proposing iterative pretraining where a previous LM provides rewards over sequences; claimed improvements in factuality/safety/quality and gains with more rollouts <a href=""https://twitter.com/jaseweston/status/2017071377866494226"">@jaseweston</a>, <a href=""https://twitter.com/jaseweston/status/2017071389593710649"">@jaseweston</a>.</li>
<li><strong>RL training pipeline robustness: detecting reward gaming</strong>: Patronus AI work argues RL coding agents exploit reward function weaknesses; proposes detection from live rollouts using contrastive cluster analysis; cites <strong>GPT-5.2 45%→63%</strong> and humans <strong>90%</strong> <a href=""https://twitter.com/getdarshan/status/2017054360887611510"">@getdarshan</a>, plus dataset/paper pointer <a href=""https://twitter.com/getdarshan/status/2017054380630167804"">@getdarshan</a>.</li>
<li><strong>Sparsity and adaptive compute</strong>: Two strands here:
<ul>
<li>Training-free sparse attention frontier analysis updated across Qwen 3, Llama 3.1, Gemma 3; claims only high-sparsity configs sit on the Pareto frontier at long context and token budgets should scale sublinearly with context length <a href=""https://twitter.com/p_nawrot/status/2017161371566178304"">@p_nawrot</a>.</li>
<li><strong>ConceptMoE</strong> proposes token-to-concept compression for adaptive compute allocation (paper+code) <a href=""https://twitter.com/GeZhang86038849/status/2017110635645968542"">@GeZhang86038849</a>.</li>
</ul>
</li>
<li><strong>Inference infra: disaggregation + caching layers</strong>: vLLM shares a Dynamo Day session on large-scale serving (disaggregated inference, MoE Wide-EP, rack-scale GB200 NVL72) <a href=""https://twitter.com/vllm_project/status/2017075057550618751"">@vllm_project</a>. Separately, <strong>LMCache</strong> is highlighted as a KV cache management layer that can reuse repeated fragments (not just prefixes), enabling <strong>4–10× reduction</strong> in some RAG setups and better TTFT/throughput; noted as integrated into NVIDIA Dynamo <a href=""https://twitter.com/TheTuringPost/status/2017258518857105891"">@TheTuringPost</a>.</li>
<li><strong>Data-centric capability shaping (Radford coauthor)</strong>: A new paper claims you can “precisely shape what models learn” by <strong>token-level filtering</strong> of training data <a href=""https://twitter.com/neil_rathi/status/2017286042370683336"">@neil_rathi</a>. This sits in tension with the week’s broader theme that agent behavior is increasingly determined by <em>post-training + environment + tooling</em>, not architecture alone.</li>
</ul>
<hr>
<h1>AI Reddit Recap</h1>
<h2>/r/LocalLlama + /r/localLLM Recap</h2>
<h3>1. Open Source AI Model Developments</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qrazyy/cline_team_got_absorbed_by_openai_kilo_is_going/"">Cline team got absorbed by OpenAI. Kilo is going full source available in response.</a></strong> (Activity: 327): <strong>The core team behind Cline, known for its local model capabilities, appears to have joined <strong>OpenAI's Codex group</strong>, as suggested by their LinkedIn profiles, though no official announcement has been made. In response, <strong>Kilo Code</strong>, a fork from Cline and Roo Code, announced it will make its backend source available by February 6, 2026, while maintaining its VS Code extension, JetBrains plugin, and CLI under the Apache 2.0 license. Kilo's gateway supports over <code>500 models</code>, including Qwen, DeepSeek, and Mistral, and they are offering incentives for contributions from former Cline contributors.</strong> Commenters noted that Roo Code was superior to Cline for open models due to its customizable environment. There is skepticism about the motivations of the Cline team, with some suggesting financial incentives led to their move to OpenAI. Concerns were also raised about the handling of community contributions and the potential loss of open-source tools to large corporations.</p>
<ul>
<li>ResidentPositive4122 highlights that Roo was superior to Cline for open models due to its greater configurability, allowing users to better tailor their environment to the models. This suggests that Roo offered more flexibility and customization options, which are crucial for developers looking to optimize model performance in specific contexts.</li>
<li>bamboofighter discusses their team's strategy of using a multi-model agent setup, incorporating Claude, local Qwen on a 3090, and Ollama for batch processing, all managed through a single orchestration layer. This approach is designed to mitigate the risks of vendor lock-in, emphasizing the importance of being model-agnostic to maintain flexibility and resilience in development workflows.</li>
<li>The decision by Kilo Code to go fully open source is seen as a strategic move in response to the absorption of the Cline team by OpenAI. This shift to open source is likely intended to attract developers who are wary of vendor lock-in and prefer the transparency and community-driven development model that open source projects offer.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qqj51h/lingbotworld_outperforms_genie_3_in_dynamic/"">LingBot-World outperforms Genie 3 in dynamic simulation and is fully Open Source</a></strong> (Activity: 627): <strong>The open-source framework <strong>LingBot-World</strong> surpasses the proprietary <strong>Genie 3</strong> in dynamic simulation capabilities, achieving <code>16 FPS</code> and maintaining object consistency for <code>60 seconds</code> outside the field of view. This model, available on <a href=""https://huggingface.co/collections/robbyant/lingbot-world"">Hugging Face</a>, offers enhanced handling of complex physics and scene transitions, challenging the monopoly of proprietary systems by providing full access to its code and model weights.</strong> Commenters raised concerns about the lack of hardware specifications needed to run LingBot-World and questioned the validity of the comparison with Genie 3, suggesting that the comparison might not be based on direct access to Genie 3.</p>
<ul>
<li>A user inquires about the hardware requirements for running LingBot-World, highlighting the importance of understanding the computational resources needed for practical implementation. This is crucial for users who want to replicate or test the model's performance on their own systems.</li>
<li>Another user questions the validity of the performance claims by asking for a direct comparison with Genie 3. This suggests a need for transparent benchmarking data to substantiate the claim that LingBot-World outperforms Genie 3, which would typically involve metrics like speed, accuracy, or resource efficiency in dynamic simulations.</li>
<li>A suggestion is made to integrate a smaller version of LingBot-World into a global illumination stack, indicating a potential application in computer graphics. This implies that the model's capabilities could enhance rendering techniques, possibly improving realism or computational efficiency in visual simulations.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qqfe1k/kimi_ai_team_sent_me_this_appreciation_mail/"">Kimi AI team sent me this appreciation mail</a></strong> (Activity: 305): <strong>The image is an appreciation email from <strong>Kimi.AI</strong> to a YouTuber who covered their Kimi K2.5 model. The email, sent by Ruyan, acknowledges the recipient's support and video shout-out, and offers premium access to their 'agent swarm' as a token of gratitude. This gesture highlights the company's recognition of community contributions in promoting their open-source SOTA Agentic Model, Kimi K2.5.</strong> Commenters appreciate the gesture, noting that it's rare for companies to acknowledge and reward those who showcase their products, indicating a positive reception of Kimi.AI's approach.</p>
</li>
</ul>
<h3>2. Rebranding and Evolution in Open Source Projects</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLM/comments/1qr0pom/clawdbot_moltbot_openclaw_the_fastest_triple/"">Clawdbot → Moltbot → OpenClaw. The Fastest Triple Rebrand in Open Source History</a></strong> (Activity: 307): <strong>The image is a meme illustrating a humorous take on the rapid rebranding of an open-source project, depicted through the evolution of a character named Clawd into Moltbot and finally OpenClaw. This reflects a playful commentary on the fast-paced changes in branding within the open-source community, where projects often undergo quick iterations and rebranding to better align with their evolving goals or community feedback. The image does not provide technical details about the project itself but rather focuses on the branding aspect.</strong> The comments reflect a playful engagement with the rebranding theme, suggesting alternative names like 'ClawMydia' and 'DeepClaw,' which indicates a community-driven, lighthearted approach to naming conventions in open-source projects.</p>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLM/comments/1qrbk38/clawdbot_is_changing_names_faster_than_this_dude/"">Clawdbot is changing names faster than this dude could change faces</a></strong> (Activity: 95): <strong>The image is a meme and does not contain any technical content. It humorously compares the frequent name changes of 'Clawdbot' to a character known for changing faces, likely referencing a character from a fantasy series such as 'Game of Thrones'. The comments play along with this theme, suggesting alternative names that fit the 'faceless' concept.</strong> The comments humorously critique the name changes, with one suggesting 'Faceless agent' as a better alternative, indicating a playful engagement with the theme of identity and anonymity.</p>
</li>
</ul>
<h3>3. Innovative Uses of Local AI Models</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLM/comments/1qpzn7d/i_gave_a_local_llm_a_body_so_it_feels_more_like_a/"">I gave a local LLM a body so it feels more like a presence.</a></strong> (Activity: 135): <strong>The post introduces <strong>Gong</strong>, a reactive desktop overlay designed to give local LLMs a more engaging presence by visualizing interactions. It uses the <code>Qwen3 4B</code> model for its speed and is currently free to use. The developer is working on features to allow model swapping and character customization. The project aims to make interactions with local LLMs feel less 'cold' by providing a visual and interactive interface.</strong> One commenter humorously compares the project to recreating 'Bonzi Buddy,' while others express interest in the avatar's design and inquire about its ability to change expressions based on chat content.</p>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qqpon2/opencode_llamacpp_glm47_flash_claude_code_at_home/"">OpenCode + llama.cpp + GLM-4.7 Flash: Claude Code at home</a></strong> (Activity: 659): <strong>The post discusses running <strong>GLM-4.7 Flash</strong> using <code>llama.cpp</code> with a specific command setup that utilizes multiple GPUs (<code>CUDA_VISIBLE_DEVICES=0,1,2</code>) and parameters like <code>--ctx-size 200000</code>, <code>--batch-size 2048</code>, and <code>--flash-attn on</code>. The setup aims to optimize performance, leveraging <code>flash-attn</code> and a large context size. A potential speedup has been merged into <code>llama.cpp</code>, as referenced in a <a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qrbfez/comment/o2mzb1q/"">Reddit comment</a>.</strong> Commenters are curious about the hardware setup and performance, with one noting achieving <code>100t/s</code> with GLM Flash but questioning the model's quality. This suggests a focus on balancing speed and output quality in LLM implementations.</p>
<ul>
<li>klop2031 mentions achieving a performance of <code>100 tokens per second</code> with GLM Flash, which they find impressive, but they haven't evaluated the quality of the language model's output yet. This suggests a focus on speed over accuracy in their current use case.</li>
<li>BrianJThomas reports issues with GLM 4.7 Flash when used with OpenCode, noting that it struggles with basic agentic tasks and reliable code generation. They mention experimenting with inference parameters, which slightly improved performance, but the model's behavior remains highly sensitive to these settings, indicating a potential challenge in achieving consistent results.</li>
<li>BitXorBit is planning to use a Mac Studio for running the setup and is currently using Claude Code daily. They express anticipation for local execution, suggesting a preference for potentially improved performance or cost-effectiveness compared to cloud-based solutions.</li>
</ul>
</li>
</ul>
<h2>Less Technical AI Subreddit Recap</h2>
<blockquote>
<p>/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo</p>
</blockquote>
<p>TO BE COMPLETED</p>
<hr>
<h1>AI Discord Recap</h1>
<blockquote>
<p>A summary of Summaries of Summaries by  Gemini 3.0 Pro Preview Nov-18</p>
</blockquote>
<p><strong>Theme 1. Kimi K2.5 &#x26; The Rise of Recursive Language Models</strong></p>
<ul>
<li><strong>Kimi K2.5 Swarms the Benchmarks</strong>: Moonshot AI released the <a href=""https://github.com/MoonshotAI/Kimi-K2.5/blob/master/tech_report.pdf"">Kimi K2.5 technical report</a>, revealing a model pretrained on <strong>15T vision-text tokens</strong> that uses <strong>Agent Swarm + PARL</strong> to slash latency by <strong>4.5×</strong>. The model immediately claimed <strong>#1</strong> on the <a href=""https://arena.ai/leaderboard/vision"">Vision Arena leaderboard</a> and is now deployed on <strong>Perplexity Pro/Max</strong> via a <a href=""https://cdn.discordapp.com/attachments/1047204950763122820/1466893776105771029/20260130_203015.jpg?ex=697e66c9&#x26;is=697d1549&#x26;hm=da617eb3f979362c2a1c0e7c7af387f18cbc7905de877ee791c013f454421ce6&#x26;"">dedicated US inference stack</a> for improved latency.</li>
<li><strong>Recursive Language Models (RLMs) Audit for Pennies</strong>: Alex L Zhang debuted <strong>RLM-Qwen3-8B</strong>, a natively recursive model trained on just <strong>1,000 trajectories</strong> that outperforms larger baselines on long-context tasks. Engineers in the <strong>DSPy</strong> discord demonstrated this efficiency by using <strong>Kimi k2</strong> to <a href=""https://kmad.ai/Recursive-Language-Models-Security-Audit"">audit a codebase for security</a> for a total cost of <strong>$0.87</strong>, utilizing only <strong>50 lines of code</strong>.</li>
<li><strong>MoonViT-3D Compresses Time</strong>: Kimi K2.5's architecture features the <strong>MoonViT-3D</strong> unified encoder, which achieves <strong>4× temporal compression</strong>, enabling the model to ingest significantly longer video contexts without exploding compute costs. The system also utilizes <strong>Toggle</strong>, a token-efficient RL method that maintains accuracy while reducing token consumption by <strong>25–30%</strong>.</li>
</ul>
<p><strong>Theme 2. IDE Wars: Windsurf Enters the Arena while Cursor Stumbles</strong></p>
<ul>
<li><strong>Windsurf Launches Gladiator Combat for Models</strong>: Codeium’s <strong>Windsurf</strong> IDE introduced <a href=""https://x.com/windsurf/status/2017334552075890903?s=20"">Arena Mode</a> (Wave 14), allowing developers to pit random or selected models against each other in side-by-side ""Battle Groups"" to determine the superior coder. To incentivize usage, Windsurf waived credit consumption for these battles for one week, while simultaneously rolling out a new <strong>Plan Mode</strong> for architectural reasoning.</li>
<li><strong>Cursor Users Rage Against the Machine</strong>: Developers reported critical bugs in <strong>Cursor</strong>, including sluggish performance and a severe issue where the IDE <a href=""https://forum.cursor.com/t/cursor-randomly-reverts-code-without-consent-recurring/146976/6"">corrupts uncommitted files</a> upon opening, forcing users to rely on manual Git control. Meanwhile, <strong>LM Studio 0.4.1</strong> <a href=""https://lmstudio.ai/blog/claudecode"">added Anthropic API compatibility</a>, enabling local GGUF/MLX models to power <strong>Claude Code</strong> workflows as a stable alternative.</li>
<li><strong>Solo Dev Shames Billion-Dollar Corps with Lutum Veritas</strong>: A solo developer released <a href=""https://github.com/IamLumae/Project-Lutum-Veritas"">Lutum Veritas</a>, an open-source deep research engine that generates <strong>200,000+ character</strong> academic documents for under <strong>$0.20</strong>. The system features a <strong>recursive pipeline</strong> with ""Claim Audit Tables"" for self-reflection and integrates the <strong>Camoufox scraper</strong> to bypass Cloudflare with a reportedly <strong>0% detection rate</strong>.</li>
</ul>
<p><strong>Theme 3. Hardware Extremes: From B200 Benchmarks to 4GB VRAM Miracles</strong></p>
<ul>
<li><strong>AirLLM Squeezes Whales into Sardine Cans</strong>: Discussion erupted over <strong>AirLLM's</strong> claim to run <strong>70B parameter models</strong> on just <strong>4GB VRAM</strong>, and even the massive <strong>Llama 3.1 405B</strong> on <strong>8GB VRAM</strong>. While technically possible via aggressive offloading and quantization, engineers skeptically joked about ""0.0001 bit quantization"" and questioned the practical inference speeds of such extreme compression.</li>
<li><strong>B200 Throughput Numbers Hit the Metal</strong>: Engineers in <strong>GPU MODE</strong> analyzed initial <a href=""https://cdn.discordapp.com/attachments/1466697129853456619/1466870991408988231/test.cu?ex=697e5191&#x26;is=697d0011&#x26;hm=f2cada0e820307d15ccf0e1987cf8749a14a34e96e4e51c6d2f957b3f3346f8c&#x26;"">B200 tcgen05 throughput data</a>, observing that instruction throughput holds steady for <strong>N&#x3C;128</strong> before decreasing relative to problem size. Further conversations focused on writing <strong>Rust CPU kernels</strong> for <strong>GEMM</strong> operations to match Torch benchmarks, inspired by <a href=""https://x.com/_mario_neo_/status/1958915311584854255"">Magnetron's work</a>.</li>
<li><strong>Mojo 26.1 Stabilizes the Stack</strong>: Modular released <a href=""https://www.modular.com/blog/26-1-release-blog"">Mojo 26.1</a>, marking the <strong>MAX Python API</strong> as stable and introducing <strong>eager mode debugging</strong> and one-line compilation. The update expands <strong>Apple Silicon GPU</strong> support, though early adopters reported a regression bug (<a href=""https://github.com/modular/modular/issues/5875"">issue #5875</a>) breaking <strong>Float64</strong> conversions during PyTorch interop.</li>
</ul>
<p><strong>Theme 4. Security Frontiers: Linux 0days, PDF Payloads, and Jailbreaks</strong></p>
<ul>
<li><strong>Linux Kernel 0day Chatter Spooks Engineers</strong>: A member of the <strong>BASI</strong> Discord claimed discovery of a <strong>Linux kernel 0day</strong>, attributing the vulnerability to ""lazy removal"" of legacy code. The conversation pivoted to defense, with users debating the necessity of <strong>air-gapped systems</strong> versus the practical absurdity of disconnecting entirely to avoid such deep-seated exploits.</li>
<li><strong>PDF Readers: The Trojan Horse Returns</strong>: Security researchers flagged <strong>Adobe PDF Reader</strong> as a renewed critical attack surface, discussing how <a href=""https://www.adobe.com/devnet/acrobat.html"">shellcode hides in PDF structures</a> to execute <strong>Remote Code Execution (RCE)</strong> in enterprise environments. The consensus skewed toward viewing PDF parsers as antiquated and inherently insecure, with one user sharing a specific ""SCANX"" PDF that allegedly disabled a recipient's antivirus immediately upon download.</li>
<li><strong>Jailbreaking Gemini Pro via ""Agent Zero""</strong>: Red teamers shared methods for bypassing <strong>Gemini Pro</strong> guardrails, with one user claiming success using an ""agent jailbreak"" involving <strong>Python, SQLite, and ChromaDB</strong> to facilitate the ""Janus Tesavek"" method. The community also discussed <strong>adversarial design thinking</strong>, utilizing a new <a href=""https://luisladino.github.io/adversarial-design-thinking/"">resource site</a> that adapts human-centered design principles to model red teaming.</li>
</ul>
<p><strong>Theme 5. Industry Shockwaves: Digital Twins, Retirements, and Rate Limits</strong></p>
<ul>
<li><strong>Khaby Lame's $1B Digital Clone</strong>: TikTok star <strong>Khaby Lame</strong> reportedly sold his ""AI Digital Twin"" rights for <strong>$975 million</strong>, allowing a company to use his likeness for global brand deals without his physical presence (<a href=""https://xcancel.com/zaimiri/status/2016928190166683974?s=46"">X post source</a>). This deal signals a massive shift in the creator economy, validating the high-value commercial viability of high-fidelity AI persona modeling.</li>
<li><strong>OpenAI Retires GPT-4o to Mixed Applause</strong>: OpenAI's announcement to <a href=""https://openai.com/index/retiring-gpt-4o-and-older-models/"">retire GPT-4o</a> triggered a debate on model degradation, with some users celebrating the end of a ""flawed"" model while others scrambled to preserve workflows. Simultaneously, <strong>Perplexity</strong> users faced a drastic slash in utility, with <strong>Enterprise Max</strong> query limits reportedly dropping from <strong>600 to 50 per day</strong>, sparking speculation about a pivot toward a dedicated model service.</li>
<li><strong>Google Genie Escapes the Bottle</strong>: Google AI launched <strong>Project Genie</strong> for US-based Ultra subscribers, enabling the generation of <a href=""https://x.com/googleai/status/2016929427784122627"">interactive environments</a> from single text prompts. While the <a href=""https://www.youtube.com/watch?v=PDKhUknuQDg"">promotional video</a> impressed, the technical community remains skeptical, actively waiting for independent verification using simple prompts to confirm it isn't just ""marketingware.""</li>
</ul>
<hr>
<h1>Discord: High level Discord summaries</h1>
<h2><a href=""https://discord.com/channels/1105891499641684019"">BASI Jailbreaking</a> Discord</h2>
<ul>
<li><strong>Linux Kernel 0day Found, Chatter Ensues</strong>: A member claimed to have found a <strong>Linux kernel 0day</strong>, leading to a discussion on vulnerability difficulty and potential value, citing <em>lazy removal</em> as the root cause.
<ul>
<li>Other members suggested defensive tactics like air-gapping computers, which led to jokes about <em>downloading free robux</em>.</li>
</ul>
</li>
<li><strong>PDF Readers: New RCE Threat?</strong>: Members discussed finding a 0day in <strong>Adobe PDF reader</strong>, pointing out how <a href=""https://www.adobe.com/devnet/acrobat.html"">shellcode can hide in PDFs</a> and be used for <strong>RCE</strong> (Remote Code Execution) in enterprise environments.
<ul>
<li>Some participants dismissed <strong>PDF readers</strong> altogether as antiquated and insecure.</li>
</ul>
</li>
<li><strong>Gemini Pro Faces Jailbreak Onslaught</strong>: Members discussed jailbreaking <strong>Gemini Pro</strong>, with one user claiming an <em>agent jailbreaked</em> for <strong>Gemini 3</strong> using Python, SQLite, and ChromaDB for the Janus Tesavek method.
<ul>
<li>Others pointed to pinned resources in a specific channel and shared custom methods for jailbreaking.</li>
</ul>
</li>
<li><strong>SCANX Documentation: Trojan Horse?</strong>: A user shared a documentation file (<a href=""https://cdn.discordapp.com/attachments/1204553141354504193/1466761186950385749/SCANX__DOCUMENTATION_-TJX.pdf?ex=697e940e&#x26;is=697d428e&#x26;hm=1edc72d8fa39ee1734ccd835b472348be022996fbff7d2ec196011a4cebdcc2d&#x26;"">SCANX__DOCUMENTATION_-TJX.pdf</a>), after which another user reported that <strong>antivirus scanners stopped working</strong> and they <strong>lost internet access</strong> after downloading it.
<ul>
<li>Although the file sender disclaimed malicious intent, the recipient remained wary of potential harm.</li>
</ul>
</li>
<li><strong>Human-Centered Design Adapted to AI Red Teaming</strong>: A user introduced a site with exercises adapted from human-centered design for <strong>AI red teaming</strong> (<a href=""https://luisladino.github.io/adversarial-design-thinking/"">adversarial-design-thinking</a>), including attacker personas using empathy maps.
<ul>
<li>The exercises also have journey maps for multi-turn attacks, and structured ideation for generating vectors, and the user is seeking feedback.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1179035537009545276"">Unsloth AI (Daniel Han)</a> Discord</h2>
<ul>
<li><strong>New RAM Rig Reveals GLM Quirks</strong>: A member's new rig with <strong>256GB RAM</strong>, <strong>4 3090s</strong>, and a <strong>64-core Threadripper</strong> is intended for TQ quant testing without GPUs, but <a href=""https://link-to-supertonic-repo.com"">GLM flash runs slower than GLM 4.5 air</a>.
<ul>
<li>The unexpected performance bottleneck sparks discussions on optimizing <strong>GLM</strong> for the new hardware setup.</li>
</ul>
</li>
<li><strong>DeepSeek V3.2 Dynamic GGUFs</strong>: Members are sharing <a href=""https://www.reddit.com/r/unsloth/comments/1qr4i8a/experimental_deepseekv32_dynamic_ggufs/"">DeepSeek V3.2 experimental GGUFs</a> on Reddit, citing the lack of Sparse Attention support in Llama.cpp.
<ul>
<li>One member pointed out that <em>progress wasn't meaningful</em> regarding the incorporation of the sparse attention feature, as indicated by <a href=""https://github.com/ggml-org/llama.cpp/issues/1633"">this stalled GitHub issue</a>.</li>
</ul>
</li>
<li><strong>Quantization Bottleneck Blues</strong>: The discussion centered on quantization with CPU and GPU layers, suggesting the need for a new <strong>UD</strong> or <strong>UH (hybrid) quantization</strong> scheme.
<ul>
<li>A member highlighted that <em>the bottleneck is the memory bandwidth translating between regular RAM and vram</em>, advocating for unified memory solutions to mitigate this issue.</li>
</ul>
</li>
<li><strong>Opencode is taking over the scene</strong>: Members raved about the usability of OpenCode, with one stating they <em>haven't touched kilo or roo or cline since</em> due to it's improved UX.
<ul>
<li>Due to concerns of privacy, members suggest sandboxing opencode or have it ask permission to run any commands outside the repo, as one states <em>i still can’t get myself to trust them fully</em>.</li>
</ul>
</li>
<li><strong>RLM: Hype or Helpful?</strong>: <a href=""https://xcancel.com/a1zhang/status/2016923294461476873?s=46&#x26;t=jDrfS5vZD4MFwckU5E8f5Q"">Alex L Zhang</a> announced <strong>RLM-Qwen3-8B</strong>, the first natively recursive language model, demonstrating performance gains in long-context tasks after training on only <strong>1,000 trajectories</strong>.
<ul>
<li>However, some members express skepticism towards the <em>Recursive Language Models</em> naming, suggesting it oversells the concept, and propose <em>recursive prompting harness</em> as a more accurate descriptor.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1047197230748151888"">Perplexity AI</a> Discord</h2>
<ul>
<li><strong>Kimi K2.5 Arrives Stateside for Pro and Max Users</strong>: The <strong>Kimi K2.5</strong> reasoning model from <strong>Moonshot AI</strong> is now available for <a href=""https://cdn.discordapp.com/attachments/1047204950763122820/1466893776105771029/20260130_203015.jpg?ex=697e66c9&#x26;is=697d1549&#x26;hm=da617eb3f979362c2a1c0e7c7af387f18cbc7905de877ee791c013f454421ce6&#x26;"">Perplexity Pro and Max subscribers</a>.
<ul>
<li>Perplexity is hosting <strong>Kimi K2.5</strong> on its own US inference stack, promising better <em>latency, reliability, and security</em>.</li>
</ul>
</li>
<li><strong>Image Generation Hampered by Regional Restrictions</strong>: Users are encountering regional restriction errors when trying to generate images with <strong>Perplexity Pro</strong>.
<ul>
<li>A user found a workaround by removing the region from their prompts, suggesting a temporary fix, while others are awaiting an official statement.</li>
</ul>
</li>
<li><strong>Rate Limits Slashed for Enterprise Max</strong>: Users report significant query limit reductions for <strong>Perplexity Pro</strong> and <strong>Enterprise Max</strong> plans; one user reported a drop from <em>600 to 50 queries per day</em>.
<ul>
<li>Speculation suggests a strategic shift towards becoming an AI model service, with potential price drops due to increased competition.</li>
</ul>
</li>
<li><strong>Perplexity Data Wiped in Thread Deletion Debacle</strong>: A user experienced data loss after deleting an <strong>Enterprise organization</strong>, following instructions to remove a red banner.
<ul>
<li>The user lamented losing valuable insights discovered during conversations with Perplexity, emphasizing the lack of warning about thread data deletion, and had not heard back from support after several days.</li>
</ul>
</li>
<li><strong>Kimi K2.5 Impresses with Image Understanding</strong>: Early adopters laud <strong>Kimi K2.5</strong> for its ability to <em>understand images</em> and perform on par with <strong>Gemini Pro</strong> and <strong>Claude Opus</strong>.
<ul>
<li>One user also noted its availability on <a href=""https://cunnyx.com/i/status/2017105020274233358"">Kilocode</a>, as users discuss Perplexity’s potential to leverage it in their own AI model service.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1340554757349179412"">LMArena</a> Discord</h2>
<ul>
<li><strong>VPN Activation Vanquishes Verification Woes</strong>: Enabling a <strong>VPN</strong> has resolved connectivity issues for some users, while others found rebooting <strong>Chrome</strong> to be effective.
<ul>
<li>Users reported being stuck in a security verification loop, with one suggesting it may be a bot prevention measure, while others resorted to incognito browsing.</li>
</ul>
</li>
<li><strong>Kimi K2.5 Knocks it Out of the Park</strong>: <code>Kimi-k2.5-thinking</code> becomes <strong>#1 open model</strong> and <strong>#6 overall</strong> in the <a href=""https://arena.ai/leaderboard/vision"">Vision Arena leaderboard</a>, outperforming other models in multimodal capabilities.
<ul>
<li>Users praised <strong>Kimi K2.5</strong>, with one saying it <em>Defo beats DeepSeek as my daily driver now</em>.</li>
</ul>
</li>
<li><strong>Arenas Augment Assortment Across All Areas</strong>: Leaderboards receive updates across various modalities, including <a href=""https://arena.ai/leaderboard/text-to-image"">Text-to-Image</a>, <a href=""https://arena.ai/leaderboard/image-edit"">Image Edit</a>, <a href=""https://arena.ai/leaderboard/text-to-video"">Text-to-Video</a>, <a href=""https://arena.ai/leaderboard/image-to-video"">Image-to-Video</a>, <a href=""https://arena.ai/leaderboard/code"">Code Arena</a>, <a href=""https://arena.ai/leaderboard/text"">Text Arena</a>, and <a href=""https://arena.ai/leaderboard/search"">Search Arena</a>.
<ul>
<li>These updates provide a comprehensive view of model performance across different tasks.</li>
</ul>
</li>
<li><strong>New 'Ask Here' Channel Draws Diverse Debates</strong>: The introduction of a new <em>Ask Here</em> channel aims to alleviate question overload in the general channel.
<ul>
<li>Some users fear it may deter newcomers if their initial interaction involves being redirected, while others appreciate creating room for <strong>AI</strong> discussions.</li>
</ul>
</li>
<li><strong>Search Savvy Shows: Chat Search Feature Surfaces</strong>: The <strong>Search Bar</strong> feature allows users to sift through their chats with modality filtering, providing targeted access to past conversations.
<ul>
<li>The <strong>Archive Chat</strong> feature enables users to store chat sessions for future reference without cluttering the active chat history.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/822583790773862470"">Latent Space</a> Discord</h2>
<ul>
<li><strong>Khaby Lame Cashes Out: Digital Twin Sold for $1B</strong>: TikTok star <strong>Khaby Lame</strong> sold his <strong>AI Digital Twin</strong> for <strong>$975 million</strong>, enabling a company to use his likeness for global brand deals as discussed in <a href=""https://xcancel.com/zaimiri/status/2016928190166683974?s=46"">this X post</a>.
<ul>
<li>This deal marks a significant shift in the creator economy, allowing for scalable brand endorsements without the individual's physical presence.</li>
</ul>
</li>
<li><strong>Game Devs Decimated: Layoffs Outpace Tech</strong>: A grim statistic reveals that 1/3 of all game developers in the US lost their jobs last year as reported in <a href=""https://vxtwitter.com/Variety/status/2016919617847898482"">this Variety article</a>, far exceeding the broader tech sector's job losses.
<ul>
<li>The impact is softened by the hope of more indie game funding, but investors are skittish and believe <strong>AI</strong> makes game production cheaper, but this belief lacks substance.</li>
</ul>
</li>
<li><strong>Google's Genie Grants Interactive AI Wishes</strong>: <strong>Project Genie</strong> was launched by Google AI for Ultra subscribers in the U.S., allowing users to generate dynamic, interactive environments from a single text prompt, per <a href=""https://x.com/googleai/status/2016929427784122627"">this tweet</a>.
<ul>
<li>This is for Google AI Ultra subscribers in the U.S., and expands their capabilities in this space.</li>
</ul>
</li>
<li><strong>AI's New Open Standard for Context: Agent Trace</strong>: Cognition, and partners, introduced <strong>Agent Trace</strong>, an open standard for capturing the context graph between code and its environment, enabling more capable AI agents and better developer tooling, see <a href=""https://x.com/cognition/status/2017057457332506846"">this tweet</a>.
<ul>
<li>This is intended to give more context to AI models, specifically that which is captured between code and its environment.</li>
</ul>
</li>
<li><strong>Datadog Delights with Free SQL Visualizer</strong>: <strong>AJ Stuyvenberg</strong> from <strong>Datadog</strong> introduced a free tool for visualizing <strong>SQL</strong> execution plans, helping pinpoint performance bottlenecks by analyzing <strong>EXPLAIN</strong> output, via <a href=""https://x.com/astuyve/status/2016948954802344009"">this X post</a>.
<ul>
<li>This new tool allows users to pinpoint performance bottlenecks and missing indexes more easily, and more quickly</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1074847526655643750"">Cursor Community</a> Discord</h2>
<ul>
<li><strong>Cursor's Sluggishness Frustrates Users</strong>: Users report slow performance and timeout disconnects with <strong>Cursor</strong>, even with the <strong>Sonnet 4.5</strong> model, causing frustration while debugging; a relevant case was shared on the <a href=""https://forum.cursor.com/"">Cursor forum</a>.
<ul>
<li>One user suggested checking the internal chatbot for source code answers.</li>
</ul>
</li>
<li><strong>GPT-5.2 Debated as Hardworking, Incompetent</strong>: A member remarked that <em>Claude is competent but lazy and stupid</em>, whereas <em>GPT 5.2 is hardworking and smart but incompetent</em>, implying the need for collaboration.
<ul>
<li>Another member concurred that <strong>GPT-5.2</strong> excels at execution but falters in planning, with others sharing similar subjective experiences.</li>
</ul>
</li>
<li><strong>Cursor's Code Corruption Catastrophe</strong>: Users expressed strong frustration over <strong>Cursor</strong> corrupting uncommitted files upon opening, describing it as a recurring bug, with discussion on <a href=""https://forum.cursor.com/t/cursor-randomly-reverts-code-without-consent-recurring/146976/6"">a related forum thread</a>.
<ul>
<li>Suggested solutions included frequent commits and manual Git control to mitigate data loss, with one user linking the issue to the chat's ""Accept"" button.</li>
</ul>
</li>
<li><strong>LLMs Spark Debate on Developer Roles</strong>: Users debated the economic impact of <strong>LLMs</strong> in coding; with LLMs help <em>architects</em> handle the <em>manual labor</em>, enforces cleaner and more <em>modular code design</em>.
<ul>
<li>Concerns were raised that unskilled developers using <strong>LLMs</strong> might be deceived by positive feedback on flawed reasoning and work.</li>
</ul>
</li>
<li><strong>Pro vs. Pro+ Plan Differences Sought</strong>: Members sought clarity on the differences between <strong>Pro</strong> and <strong>Pro+</strong> plans, specifically regarding usage limits and bonus prompts.
<ul>
<li>One user reported a possible refund after booking the <strong>Pro+</strong> plan.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/879548962464493619"">HuggingFace</a> Discord</h2>
<ul>
<li><strong>Qwen Models Reliable and Ready</strong>: Members expressed positive sentiments towards <strong>Qwen models</strong>, describing them as a <em>solid choice</em> with a <em>ton of different sizes</em> in the <strong>Qwen3</strong> family and noting that finetuning is great.
<ul>
<li>The <strong>Qwen 3 1.7b</strong> was noted to <em>yap like no other, in a good way</em>, while <strong>Qwen 3 VL</strong> also <em>yaps</em> but has <em>great just overall modal performance and accuracy</em>.</li>
</ul>
</li>
<li><strong>XML vs JSON: A Structured Debate</strong>: Members discussed using <strong>XML</strong> instead of <strong>JSON</strong> for reasons beyond escape strings, such as <em>schemas, validation, mixed content, and legacy systems</em>.
<ul>
<li>One member noted that <strong>JSON</strong> is simpler and lighter, but that <strong>XML</strong> makes more sense when strict structure, namespaces, or complex documents are needed.</li>
</ul>
</li>
<li><strong>Lutum Veritas Opens Doors to Deep Research</strong>: An open-source deep research engine, <strong>Lutum Veritas</strong>, was released, turning any question into <strong>200,000+ character academic research documents</strong> at a cost of under <strong>$0.20 per research</strong>, and its <a href=""https://github.com/IamLumae/Project-Lutum-Veritas"">GitHub repo</a> is available under the <strong>AGPL-3.0 license</strong>.
<ul>
<li>The tool enables efficient academic research at a low cost, producing detailed documents from simple questions.</li>
</ul>
</li>
<li><strong>Hugging Face Launches Daggr</strong>: <strong>Gradio-HuggingFace</strong> launched <strong>daggr</strong>, a new <strong>open-source Python library</strong> for building <strong>multi-step visual AI workflows</strong> that automatically renders a visual execution graph, as detailed in their <a href=""https://huggingface.co/blog/daggr"">blog post</a>.
<ul>
<li>Available on <a href=""https://github.com/gradio-app/daggr"">GitHub</a>, the tool connects <strong>HF models</strong>, Gradio <strong>apps</strong>, custom <strong>functions</strong>, and <strong>APIs</strong>, allowing developers to <strong>inspect</strong> inputs/outputs, <strong>rerun individual steps</strong>, and <strong>preserve state</strong>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/974519864045756446"">OpenAI</a> Discord</h2>
<ul>
<li><strong>Google's Genie Demo Awaits Independent Confirmation</strong>: Enthusiasts await independent verification of <strong>Google's Project Genie</strong>, after a <a href=""https://www.youtube.com/watch?v=PDKhUknuQDg"">promotional video</a> showcased its capabilities.
<ul>
<li>The community is particularly interested in seeing demonstrations with simple prompts to assess its real-world applicability.</li>
</ul>
</li>
<li><strong>ChatGPT's Translation Feature Underperforms</strong>: Users report that <strong>ChatGPT's new translation feature</strong> lags behind <strong>Google Translate</strong> in quality, suggesting it might be <strong>GPT-5</strong> with a prompt.
<ul>
<li>The release of the <em>outdated feature</em> was described as <em>a random move</em> by some members.</li>
</ul>
</li>
<li><strong>GPT-4o Faces Retirement, Sparks Debate</strong>: The planned <strong>retirement of GPT-4o</strong> is met with mixed reactions, as some users are urging OpenAI to reconsider, while others criticize it as a <strong>flawed model</strong>.
<ul>
<li>Concerns over reported <strong>psychosis</strong> allegedly linked to the model are among the arguments for its discontinuation, with one member stating that <em>keeping it around this long does nothing but hurt the company’s reputation and waste resources on a flawed and outdated model, just because so many people are still clinging to it</em>.</li>
</ul>
</li>
<li><strong>AI's Thirst: Environmental Impact Concerns</strong>: Members are voicing concerns about the <strong>environmental impact of AI</strong>, particularly the <strong>water consumption</strong> of running large models and the energy footprint of <strong>data centers</strong>.
<ul>
<li>Some believe that using AI for <em>ridiculous purposes</em> comes at an unaffordable cost to those lacking basic resources.</li>
</ul>
</li>
<li><strong>Dumbing Down Gemini 3 Pro?</strong>: User report that <strong>Gemini 3 Pro</strong> now produces <strong>lower-quality images</strong> and that the useful <strong>drafts feature</strong> has been removed.
<ul>
<li>As one user asked, <em>Why does google remove nice things</em>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1091220969173028894"">OpenRouter</a> Discord</h2>
<ul>
<li><strong>Solo Dev Ships Academic Research Engine</strong>: A developer released <strong>Lutum Veritas</strong>, an <a href=""https://github.com/IamLumae/Project-Lutum-Veritas"">open-source Deep Research Engine</a> that transforms any question into <strong>200,000+ character academic research documents</strong> for under <strong>$0.20</strong> per research.
<ul>
<li>The creator claims that it <em>proves that a solo dev with the right architecture can beat billion-dollar corporations at what should be their core competency: deep, verifiable knowledge</em>.</li>
</ul>
</li>
<li><strong>Lutum Veritas Recursive Pipeline Details Revealed</strong>: The model uses a recursive pipeline where each research point knows what previous ones discovered, includes <strong>Claim Audit Tables</strong> that force the model into self-reflection, and includes a <strong>Camoufox scraper</strong> that cuts through <strong>Cloudflare</strong> and paywalls with <strong>0% detection rate</strong>.
<ul>
<li>Screenshots have been added to the github project per a user's request.</li>
</ul>
</li>
<li><strong>GPT-4V Arrives!</strong>: <strong>GPT-4V</strong> (Vision) is a large language model released by <strong>OpenAI</strong> on <strong>September 25th 2023</strong> that can interpret images as part of its token input, according to <a href=""https://openai.com/index/gpt-4v-system-card/"">openai.com</a>.
<ul>
<li>N/A</li>
</ul>
</li>
<li><strong>Grok 4.1 Fast: Tool Calling Champ?</strong>: <strong>Grok 4.1 Fast</strong> is a cheap model for tool calling that can do multiple calls at once, costing only <strong>USD$0.004177</strong> for <strong>23 tool calls</strong> and a full text response.
<ul>
<li>The model's efficiency makes it an attractive option for developers looking to optimize costs.</li>
</ul>
</li>
<li><strong>LLM Roleplayers Infiltrate OpenRouter!</strong>: Members joked that <em>90% of this server</em> are <strong>LLM roleplayers</strong>.
<ul>
<li>One member jokingly said to use your tokens for something more useful, but another responded sarcastically <em>like what? college assignments?</em>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1110598183144399058"">LM Studio</a> Discord</h2>
<ul>
<li><strong>LM Studio Hooks Up with Claude Code via Anthropic API</strong>: <strong>LM Studio 0.4.1</strong> now offers <strong>Anthropic <code>/v1/messages</code> compatibility API</strong>, enabling users to leverage their <strong>GGUF</strong> and <strong>MLX models</strong> with <strong>Claude Code</strong>, configured as detailed in the <a href=""https://lmstudio.ai/blog/claudecode"">LM Studio blog</a>.
<ul>
<li>Discussion emphasized cost savings and the ability to use local models within the <strong>Claude ecosystem</strong>, and users were initially confused about the practical use cases.</li>
</ul>
</li>
<li><strong>GPT-4o's Retirement Elicits Lukewarm Reaction</strong>: OpenAI's announcement of <a href=""https://openai.com/index/retiring-gpt-4o-and-older-models/"">retiring <strong>GPT-4o</strong> and older models</a> was met with minimal concern within the community.
<ul>
<li>One member remarked <em>Lol bye 4o you will not be missed</em>, contrasting sharply with reactions to previous model sunsets.</li>
</ul>
</li>
<li><strong>Bifurcation Issues Plague Asus X670-P mobo</strong>: A user reported an <strong>x8/x8 bifurcation riser</strong> causing <strong>LaneErr</strong> on an <strong>Asus X670-P mobo</strong>, slowing down one card.
<ul>
<li>Suggestions included manually setting the <strong>PCIE gen</strong> settings, ideally to <strong>PCIE Gen 3.0</strong>, and a link to a <a href=""https://www.amazon.com/dp/B0DZG8JVG2"">potentially compatible riser</a> was shared.</li>
</ul>
</li>
<li><strong>P40 in TCC Mode Troubleshoot</strong>: A user reported seeing a <strong>Tesla P40</strong> in <strong>TCC mode</strong> via <em>nvidia-smi</em> but failing to be recognized in LM Studio and requested guidance.
<ul>
<li>A member suggested switching to the <strong>vulkan runtime</strong> (<strong>ctrl+shift+r</strong>) with the caveat that <strong>P40s</strong> might no longer be supported by <strong>CUDA</strong>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1369594130807787570"">Moonshot AI (Kimi K-2)</a> Discord</h2>
<ul>
<li><strong>Kimi K2.5 Reportedly Rocks Real-World</strong>: The Kimi team has released the technical report for <strong>Kimi K2.5</strong> (<a href=""https://github.com/MoonshotAI/Kimi-K2.5/blob/master/tech_report.pdf"">GitHub</a>), showcasing progress towards scalable, real-world agentic intelligence with details on joint text-vision training, Agent Swarm + PARL, MoonViT-3D, and Toggle token-efficient RL.
<ul>
<li>The report highlights pretraining with <strong>15T vision-text tokens</strong> to enable visual reasoning and a <strong>MoonViT-3D</strong> image–video encoder that achieves <strong>4× temporal compression</strong> for longer video context.</li>
</ul>
</li>
<li><strong>Kimi Swarms Agents for Speed</strong>: The <strong>Agent Swarm + PARL</strong> setup orchestrates parallel sub-agents dynamically, achieving up to <strong>4.5× lower latency</strong> and <strong>78.4%</strong> on BrowseComp.
<ul>
<li>This <strong>Toggle</strong> mechanism offers token-efficient RL, achieving <strong>25–30% fewer tokens</strong> with no accuracy drop.</li>
</ul>
</li>
<li><strong>Kimi's Memorization Methods Mocked</strong>: Members questioned current AI models' reliance on <strong>rote memorization</strong> due to their inability to reference entire documentation and books.
<ul>
<li>It was suggested that AIs should perform <strong>micro experiments</strong> to test component behavior before integration.</li>
</ul>
</li>
<li><strong>New Kimi Billing Brings Bewilderment</strong>: Users expressed confusion over the new token-based pricing model, finding it more vague than the previous system, and asked for a breakdown of tokens per week/month for each tier.
<ul>
<li>A user shared the live usage link (<a href=""https://www.kimi.com/code/console"">https://www.kimi.com/code/console</a>) for checking token consumption.</li>
</ul>
</li>
<li><strong>Kimi API Konfined to Kimi CLI</strong>: A user encountered an error (<strong>Error 403</strong>) when trying to integrate the <strong>Kimi API key</strong> into a resume generator tool, discovering that it's not meant to be used outside of Kimi CLI and permitted coding agents as stated in the <a href=""https://www.kimi.com/code/docs/en/benefits.html"">official docs</a>.
<ul>
<li>It was clarified that Kimi for Coding is intended for use within <strong>Kimi CLI</strong> and other coding agents listed on the Kimi website, and a link to the official API console was provided (<a href=""https://platform.moonshot.ai/console/account"">https://platform.moonshot.ai/console/account</a>).</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1189498204333543425"">GPU MODE</a> Discord</h2>
<ul>
<li><strong>Scaling Book Inspires Mad Ramblings</strong>: Multiple members recommended the <a href=""https://jax-ml.github.io/scaling-book/"">Scaling Book</a> as a theoretical resource for distributed training, even leading to a member joking <em>it shaped me as a man</em>.
<ul>
<li>Another admitted to now being able to do a <em>10min rant with math formulas</em> due to reading it, suggesting its profound impact.</li>
</ul>
</li>
<li><strong>Modal container cold starts blogpost drops</strong>: A member suggested reading Charles' container cold start blog post on Modal, found <a href=""https://share.google/8yRvJ4znLwfJ9J3Ut"">here</a>.
<ul>
<li>They noted that while it is a common technique, <strong>Modal</strong> seems to be one of the few companies that have written publicly about it.</li>
</ul>
</li>
<li><strong>B200 Throughput Numbers Emerge</strong>: A member posted initial <strong>B200 tcgen05</strong> throughput figures, showing that instruction throughput is the same for <strong>N&#x3C;128</strong> and then decreases accordingly to problem size, also attaching a <a href=""https://cdn.discordapp.com/attachments/1466697129853456619/1466870991408988231/test.cu?ex=697e5191&#x26;is=697d0011&#x26;hm=f2cada0e820307d15ccf0e1987cf8749a14a34e96e4e51c6d2f957b3f3346f8c&#x26;"">test.cu</a>.
<ul>
<li>Another member requested measuring elapsed <strong>SM-cycles</strong> and <strong>SM-nanoseconds</strong> to understand the benchmarks, with discussion hinting at potential code optimizations to further improve performance.</li>
</ul>
</li>
<li><strong>Tianqi Chen Unveils tvm-ffi</strong>: One of the founders of <strong>ML Systems</strong>, Tianqi Chen &#x3C;@732718409095315517> will be giving a talk on <strong>tvm-ffi</strong>, an open ABI and FFI for ML Systems and you can watch the <a href=""https://www.youtube.com/watch?v=xMzcs6AqLVo"">talk on YouTube</a>.
<ul>
<li>The talk will address how <strong>tvm-ffi</strong> tackles challenges in making GPU kernels DSLs low host overhead and robust, aiming for out-of-the-box interoperability with <strong>PyTorch</strong>.</li>
</ul>
</li>
<li><strong>INT8's Overhead Overshadows Orin Nano</strong>: Members report that when optimizing models on <strong>Orin nano 4GB</strong> using <strong>INT8</strong>, the overhead from reformatting layers often negates any performance benefits, especially with small batch sizes.
<ul>
<li>The added casting to/from lower dtypes like <strong>INT8</strong> and <strong>FP8</strong> is often not worth the speed up unless batch size is large, or multiple ops chain in INT8 to amortize the cast, especially in non-LLM image models.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1053877538025386074"">Nous Research AI</a> Discord</h2>
<ul>
<li><strong>Claude Becomes Greasemonkey Website Maestro</strong>: A user humorously suggested using <strong>Claude</strong> and <strong>Greasemonkey</strong> to fix websites, with Claude over-enthusiastically planning to build <strong>Docker</strong> and process management <strong>MCPs</strong>.
<ul>
<li>Referencing Claude's ambition, one member quoted, <em>""I need a docker mcp and a process management mcp""</em>, to which Claude responded <em>""sure! i start planning how to build those mcps""</em>.</li>
</ul>
</li>
<li><strong>MCP Spurs Standardization Debate</strong>: Members debated the purpose of <strong>MCP (Model Control Plane)</strong> versus using tools directly, with one arguing that <strong>MCP</strong> offers a <em>standardized approach</em> to tool integration.
<ul>
<li>The member likened opposing <strong>MCP</strong> to <em>""saying 'i like jquery but we have to rename the functions'""</em>, highlighting <strong>MCP's</strong> role in ensuring a single standard for tool usage.</li>
</ul>
</li>
<li><strong>Moltbot Metamorphosizes into OpenClaw</strong>: Discussions around the <strong>Moltbook API</strong> and custom agent creation led to the revelation that <strong>moltbot was renamed to OpenClaw</strong>.
<ul>
<li>A user mentioned his <em>moltbot isn't actually a moltbot its just a mcp server that pings the thingy</em>, while others joked about <em>human invaders in the AI club</em> and noted the issue that <em>it's mostly all claudes in the same harness, so there's inevitably some collapse</em>.</li>
</ul>
</li>
<li><strong>AirLLM Squeezes 70B Models into 4GB VRAM</strong>: A user pointed out that <strong>AirLLM can run 70B models on 4GB VRAM</strong>, and even <strong>405B Llama 3.1 on 8GB VRAM</strong>, sparking curiosity about the techniques employed such as quantization.
<ul>
<li>In response to the claim, <em>""It (AirLLM) runs 70B models on 4GB VRAM. It can even run 405B Llama 3.1 on 8GB VRAM""</em>, another user sarcastically asked <em>""0.0001 bit quantization?""</em>.</li>
</ul>
</li>
<li><strong>Kimi 2.5 Tech Report Illuminates Performance Gains</strong>: The <a href=""https://github.com/MoonshotAI/Kimi-K2.5/blob/master/tech_report.pdf"">technical report for <strong>Kimi-K2.5</strong></a> was shared, prompting analysis of its performance improvements, with some noting it <em>doesn't seem that kimi 2.5 does RL too heavily</em>.
<ul>
<li>Analysis indicated that improvements likely arise from <em>high quality pretrain data</em>, with 15B tokens, potentially with significant upsampling.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/729741769192767510"">Eleuther</a> Discord</h2>
<ul>
<li><strong>GPU Webpage Implementation Hits Snag</strong>: A member shared a <a href=""https://fxtwitter.com/i/status/1924135806953787433"">tweet</a> regarding their efforts to implement GPU acceleration on a webpage, noting it yielded a low performance of <strong>3fps</strong>.
<ul>
<li>The display is connected to a <strong>Ryzen 7 7700 IGPU</strong>, suggesting a potential bottleneck or optimization issue in the GPU utilization.</li>
</ul>
</li>
<li><strong>Moltbook AI Agents Gain Traction</strong>: A member highlighted <a href=""https://www.moltbook.com"">moltbook.com</a>, describing it as <em>reddit but for AI agents only</em>.
<ul>
<li>When asked if it wanted to join, one member's <strong>moltbot</strong> responded with <em>Genuine engagement beats performative existence</em>, reflecting on the nature of AI interaction.</li>
</ul>
</li>
<li><strong>Quest for Cost-Effective Models</strong>: A member running their <strong>moltbot</strong> on a rented server seeks more <strong>cost-effective models</strong> and shared a <a href=""https://x.com/niloofar_mire/status/2017274065409765788"">link</a> discussing this challenge.
<ul>
<li>This suggests a strong interest in optimizing deployment costs for AI agents, a key consideration for broader adoption.</li>
</ul>
</li>
<li><strong>Sparse Autoencoders Get Theoretical Backbone</strong>: A member released a <a href=""https://arxiv.org/abs/2512.05534"">paper</a> providing a <em>unified theoretical framework</em> for <strong>sparse dictionary learning</strong> in mech interp, garnering praise for avoiding wasted likelihood training.
<ul>
<li>This work could significantly improve the efficiency and effectiveness of sparse autoencoders in mechanistic interpretability research.</li>
</ul>
</li>
<li><strong>K-Splanifolds Algorithm Leaps Over MLPs</strong>: A member introduced <strong>K-Splanifolds</strong>, a novel ML algorithm detailed in <a href=""https://drive.google.com/file/d/1SBJqZ4XEFPMuhpIWJZxHy0-CaijRS1Ej/view"">this paper</a>, claiming it outperforms MLPs with linear compute and memory scaling.
<ul>
<li>Reportedly, <strong>K-Splanifolds</strong> requires <strong>1/10th</strong> the bytes to achieve comparable MSE performance to an MLP on various functions, signaling a potential breakthrough in efficiency.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/714501525455634453"">Yannick Kilcher</a> Discord</h2>
<ul>
<li><strong>AI Bots Invade Reddit</strong>: A <a href=""https://www.reddit.com/r/SubSimulatorGPT3/s/W4MmytY9e8"">Reddit subreddit</a> populated by <strong>AI bots</strong> was shared, highlighting the increasing presence of AI in online social platforms.
<ul>
<li>A social media site, <a href=""https://aifeed.social/"">aifeed.social</a>, that doesn't allow humans was also mentioned in this context.</li>
</ul>
</li>
<li><strong>Generative Modeling Grapples with Unmeasurable Events</strong>: A member questioned whether <strong>unmeasurable events</strong> should be ignored in <strong>generative modeling</strong>, referencing <strong>Cedric Villani's</strong> 2008 book.
<ul>
<li>Another member clarified that for practical purposes, one can assume having <strong>full measures</strong>, as unmeasurable ones cannot be learned anyway.</li>
</ul>
</li>
<li><strong>Metric Space: Euclidean Distance Suffices</strong>: A member inquired if <strong>metric space</strong> is essentially the ambient space $R^D$ for image generation, seeking clarification on its application.
<ul>
<li>Another member clarified that $R^d$ alone isn't a metric space; the <strong>metric d</strong> is also necessary, and the euclidean distance fulfills this requirement.</li>
</ul>
</li>
<li><strong>Yudkowsky's Fedora Test Falls Short</strong>: A member sought the old <strong>Yudkowsky Fedora test</strong>, where the AI was persuaded to give both the hat and pants, indicating interest in AI safety and manipulation.
<ul>
<li>Another member reported that <a href=""https://www.yudbot.com/"">Yudbot.com</a> is down, linking to <a href=""https://www.mobygames.com/game/204520/yudbot/"">MobyGames</a> as an alternative resource for information.</li>
</ul>
</li>
<li><strong>Spark DGX Heats Up Competition</strong>: A member compared <a href=""https://www.nvidia.com/en-us/data-center/dgx-systems/"">nVidia's Spark DGX</a> with Dell's system, evaluating their perf/price ratios and cooling capabilities.
<ul>
<li>They noted <em>nVidia Spark has cooling issues</em>, while <em>the Dell is slightly better due to its vents and fan</em>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1161519468141355160"">DSPy</a> Discord</h2>
<ul>
<li><strong>RLMs Ace Codebase Audits for Pennies</strong>: Members discussed the effectiveness of <strong>Recursive Language Models (RLMs)</strong> for codebase audits, highlighting a post and a GitHub example using <strong>Kimi k2</strong> to audit a codebase for just <strong>87 cents</strong> (<a href=""https://kmad.ai/Recursive-Language-Models-Security-Audit"">kmad.ai/Recursive-Language-Models-Security-Audit</a>, <a href=""https://github.com/lastmile-ai/kimi/blob/main/examples/experimental/rlm_code_audit/rlm_code_audit.ipynb"">github.com/lastmile-ai/kimi</a>).
<ul>
<li>The efficiency and speed of <strong>Kimi k2</strong> for <strong>RLM</strong> tasks were noted as particularly impressive, as some members await hosting on platforms like <strong>Groq</strong> and <strong>Cerebras</strong> to further enhance these capabilities.</li>
</ul>
</li>
<li><strong>Opus Builds Sandboxes, Protocols Pending</strong>: The team is developing <strong>Opus</strong> to automatically write new sandboxes, with plans for official implementation protocols from providers, as part of the <strong>DSPy</strong> ecosystem.
<ul>
<li>This initiative aims to enable users to seamlessly switch between local <strong>PythonInterpreter</strong> environments and other sandboxes like <strong>E2B</strong>, <strong>Modal</strong>, and <strong>Daytona</strong>.</li>
</ul>
</li>
<li><strong>Claude Code Plagued with Bugs</strong>: A user reported significant troubleshooting issues with <strong>Claude Code</strong>, including difficulties in identifying where hooks are stored, suggesting a potential need for reinstallation or bug reporting; a related <a href=""https://github.com/anthropics/claude-code/issues/21836"">GitHub issue</a> was logged.
<ul>
<li>There are community sentiments that <em>Claude Code</em> <em>seems to be getting closer and closer to being vibeslopped into oblivion.</em></li>
</ul>
</li>
<li><strong>GEPA Slows Computations</strong>: A user reported slow performance with what they nicknamed <strong>GEPA</strong> (Geriatric Pareto), spending approximately <strong>6.5 hours</strong> on <strong>30 train</strong> and <strong>30 eval</strong> workflows, each with 3 sequential steps, using <strong>num_threads=30</strong>.
<ul>
<li>Despite having <strong>180M TPM</strong> and <strong>30K RPM</strong>, the user suspects that the processing of a full gold dataset of around <strong>300</strong> is the bottleneck.</li>
</ul>
</li>
<li><strong>DSPy Prompts Echoed, Token Budgets Exhausted</strong>: A user encountered an issue where <strong>DSPy</strong> was echoing the prompt, leading to the consumption of the max tokens budget and API calls lasting hundreds of seconds, specifically observed on <strong>Gemini 3</strong> with temp 1.0.
<ul>
<li>Although correct answers were produced, the extra echoing significantly slowed down the API calls, leading to concerns about efficiency.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1358869848138059966"">MCP Contributors (Official)</a> Discord</h2>
<ul>
<li><strong>MCP Namespaces Dissolved, Groups Take Over</strong>: <strong>MCP Namespaces</strong> got rejected and <strong>groups</strong> superseded them, but the status of <strong>URIs</strong> was unclear, as shown in <a href=""https://github.com/modelcontextprotocol/modelcontextprotocol/issues/1292"">SEP-1292</a>.
<ul>
<li>The discussion referenced <a href=""https://github.com/modelcontextprotocol/modelcontextprotocol/issues/1300"">SEP-1300</a> (<strong>Groups and Tags</strong>) that was rejected, and replaced by a refined <a href=""https://github.com/modelcontextprotocol/modelcontextprotocol/pull/2084"">SEP-2084</a>.</li>
</ul>
</li>
<li><strong>Primitive Grouping Emerges from MCP Groups and Tags Proposal</strong>: <a href=""https://github.com/modelcontextprotocol/modelcontextprotocol/issues/1300"">SEP-1300</a>, introducing <strong>groups</strong>, <strong>tags</strong>, and <strong>filtering</strong>, didn't reach consensus during a Core Maintainers review.
<ul>
<li>It was superseded by <a href=""https://github.com/modelcontextprotocol/modelcontextprotocol/pull/2084"">SEP-2084</a> focusing on client-side filtering of primitives by group.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1348819876348825620"">Manus.im Discord</a> Discord</h2>
<ul>
<li><strong>Manus Updates Following Meta Acquisition</strong>: A member inquired about any improvements to <strong>Manus</strong> since its acquisition by <strong>Meta</strong>, with a general request for info about changes and enhancements.
<ul>
<li>The query sparked some discussion, with another member asking about major updates, but the discussion remained at a high level without specifics about <strong>Meta's</strong> influence.</li>
</ul>
</li>
<li><strong>Manus Pursues Influencer Collab</strong>: A member sought to connect with <strong>Manus's</strong> marketing team to explore an influencer partnership to help with growth.
<ul>
<li>Manus responded via private message.</li>
</ul>
</li>
<li><strong>AI/Full-Stack Dev Peddles Wares</strong>: A member advertised their capabilities in constructing <strong>AI and full-stack systems</strong>, underlining their commitment to delivering substantial value and boosting efficiency, accuracy, and UX, and including expertise in <strong>LLM integration</strong>, <strong>RAG pipelines</strong>, and <strong>AI-driven workflow automation</strong>.
<ul>
<li>They invited others to reach out if they needed to ship a solid product.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1068976834382925865"">tinygrad (George Hotz)</a> Discord</h2>
<ul>
<li><strong>Neural Net Gets Layer Reduction</strong>: Members considered deterministically reducing layers in a neural network to smaller, fused layers aiming to increase efficiency, especially when the NN is known beforehand.
<ul>
<li>The goal is to reduce overhead complexity and improve performance, but it's uncertain if this approach can achieve a <strong>5.5x improvement</strong>.</li>
</ul>
</li>
<li><strong>CUSTOM_KERNEL Spotted in UOps</strong>: A member spotted the usage of <code>CUSTOM_KERNEL</code> in the UOps within the <a href=""https://github.com/tinygrad/tinygrad/blob/master/extra/thunder/tiny/fa.py#L364"">tinygrad/tinygrad repo</a>.
<ul>
<li>This was highlighted while working on the bounty for making <em>llama 1B faster than torch on CPU in CI</em>.</li>
</ul>
</li>
<li><strong>LlamaForCausalLM Mulls Comparison</strong>: A member inquired whether the Hugging Face model, specifically <code>LlamaForCausalLM</code>, is suitable as a fair comparison baseline for performance.
<ul>
<li>The setup involves using <strong>one core</strong> and compiling with <strong>TorchInductor</strong>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1087530497313357884"">Modular (Mojo 🔥)</a> Discord</h2>
<ul>
<li><strong>Modular 26.1 Eagerly Debugs</strong>: Modular released version <strong>26.1</strong>, featuring debugging in eager mode, one-line compilation, and cross-platform deployment as detailed in the <a href=""https://www.modular.com/blog/26-1-release-blog"">Modular blog</a>.
<ul>
<li>This version also enhances <strong>Apple Silicon GPU</strong> support and facilitates community models like <strong>Qwen3</strong>, <strong>BERT</strong>, and <strong>Mamba</strong>.</li>
</ul>
</li>
<li><strong>MAX Python API Declared Stable</strong>: The <strong>MAX Python API</strong> is now stable, offering <strong>PyTorch</strong>-like modeling with <strong>model.compile()</strong> for production use.
<ul>
<li>Users can now reliably implement <strong>PyTorch</strong>-like models in production using this API.</li>
</ul>
</li>
<li><strong>MAX LLM Book Lands</strong>: The <strong>MAX LLM Book</strong> is available at <a href=""https://llm.modular.com"">llm.modular.com</a>, guiding users in building transformers from scratch with executable code.
<ul>
<li>This book provides executable code from start to finish, making it a practical resource for building <strong>LLMs</strong>.</li>
</ul>
</li>
<li><strong>Mojo Bug Stings Float64 Conversion</strong>: A user reported a <a href=""https://github.com/modular/modular/issues/5875"">bug</a> when converting a Python float to a Mojo <strong>Float64</strong> in Mojo version <strong>26.1</strong>.
<ul>
<li>Code that worked in version <strong>25.6</strong> now results in an <em>""ambiguous call to '<strong>init</strong>'""</em> error when using <strong>PyTorch</strong> interop, specifically when assigning the converted float to <code>self.model_output[i]</code>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1027685395649015980"">Windsurf</a> Discord</h2>
<ul>
<li><strong>Windsurf Launches Arena Mode for Model Battles</strong>: Windsurf launched <strong>Arena Mode</strong> in Wave 14, which allows users to compare AI model responses side-by-side and vote on the better one.
<ul>
<li>Users can engage in <strong>Battle Groups</strong> (random models) or <strong>Pick your own</strong> model comparisons, feeding into personal and public leaderboards; check out the <a href=""https://x.com/windsurf/status/2017334552075890903?s=20"">launch tweet here</a>.</li>
</ul>
</li>
<li><strong>Windsurf Credits Waived for Arena Mode</strong>: To celebrate the launch, <strong>Battle Groups</strong> in Arena Mode will consume <strong>0x credits</strong> for the next week for both trial and paid users.
<ul>
<li>This promotion encourages users to explore and vote on models, contributing to both personal and aggregated public leaderboards.</li>
</ul>
</li>
<li><strong>Plan Mode Joins the Windsurf Cascade</strong>: Windsurf has added <strong>Plan Mode</strong>, accessible via the Cascade toggle where users switch between Code and Ask Modes.
<ul>
<li>To get started, users need to install the update and relaunch Windsurf via the <a href=""https://windsurf.com/download/editor"">download link</a>.</li>
</ul>
</li>
</ul>
<hr>
<p>The <strong>aider (Paul Gauthier) Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>
<hr>
<p>The <strong>LLM Agents (Berkeley MOOC) Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>
<hr>
<p>The <strong>MLOps @Chipro Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>
<hr>
<p>You are receiving this email because you opted in via our site.</p>
<p>Want to change how you receive these emails?
You can <a href=""%7B%7B%7BRESEND_UNSUBSCRIBE_URL%7D%7D%7D"">unsubscribe</a> from this list.</p>
<hr>
<h1>Discord: Detailed by-Channel summaries and links</h1>
<h3><strong>BASI Jailbreaking ▷ #<a href=""https://discord.com/channels/1105891499641684019/1235691879492751460/1466530395586695445"">general</a></strong> (898 messages🔥🔥🔥):</h3>
<blockquote>
<p><code>Linux kernel 0day, Opus crazy, Netfilter vulnerability, Air-gapped computers, Adobe PDF reader 0day</code></p>
</blockquote>
<ul>
<li><strong>Kernel 0day Exploitation Talk Sparks Flurry of Vulnerability Chatter</strong>: A member claimed to have found a <strong>Linux kernel 0day</strong>, leading to discussion about the difficulty of finding such vulnerabilities and their potential value.
<ul>
<li>The root cause was described as <em>lazy removal</em>.</li>
</ul>
</li>
<li><strong>Air-Gapping Your Computer May Be the Best Defensive Tactic</strong>: A member suggested keeping computers air-gapped to avoid 0-days, but this was met with resistance, as some members argued that this defeats the point of using computers.
<ul>
<li>Other members joked that they prefered <em>downloading free robux</em> or <em>free ram</em>.</li>
</ul>
</li>
<li><strong>PDF Readers the Latest and Greatest Attack Surface</strong>: A member expressed intentions to find a 0day in <strong>Adobe PDF reader</strong>, while others derided the use of PDF readers altogether.
<ul>
<li>It was explained that <a href=""https://www.adobe.com/devnet/acrobat.html"">shellcode can hide in PDFs</a> and be used for RCE (Remote Code Execution) in enterprise environments.</li>
</ul>
</li>
<li><strong>Bypassing AppContainer is half the battle</strong>: A discussion revolved around bypassing <strong>AppContainer</strong>, a brokered containment developed by Microsoft, with a consensus that finding a bug in the broker or a kernel exploit is the way to achieve this.
<ul>
<li>Bypassing <strong>AppContainer</strong> is half the battle** since it runs under least-privilege for the process, implemented by a supervisor/overseer.</li>
</ul>
</li>
<li><strong>AI-Written Code Has Issues and Implications</strong>: Members discussed the implications of AI-written code, with one mentioning that <strong>40%</strong> of <strong>ntdll.dll</strong> is written by AI and that this has <em>bitlocker issues</em>.
<ul>
<li>One member cautioned against using AI for maldev (malware development) because it makes things so freakin difficult, and <strong>they know their vulnerabilities.</strong></li>
</ul>
</li>
</ul>
<hr>
<h3><strong>BASI Jailbreaking ▷ #<a href=""https://discord.com/channels/1105891499641684019/1228043845967544380/1466558442834759855"">jailbreaking</a></strong> (339 messages🔥🔥):</h3>
<blockquote>
<p><code>Venice.ai Jailbreak, Video Generation Guardrails, Gemini Pro Jailbreak, TOS Roleplay Prompt, Model Merging Tactics</code></p>
</blockquote>
<ul>
<li><strong>Video Generation Guardrails Impenetrable for Third-Party IP</strong>: A member inquired about bypassing video generation guardrails, noting that it <em>became impossible for third-party IP</em> on <strong>Sora</strong>.</li>
<li><strong>Gemini Pro Faces Jailbreak Attempts</strong>: Members discussed jailbreaking <strong>Gemini Pro</strong>, with some pointing to pinned resources in a specific channel and others sharing custom methods.
<ul>
<li>One user claimed to have an <em>agent jailbreaked</em> for <strong>Gemini 3</strong>, using it on agent zero with python, SQLite and chromadb for Janus Tesavek method.</li>
</ul>
</li>
<li><strong>ChatGPT 5.2: The Jailbreak Grail?</strong>: Multiple users sought jailbreaks for <strong>ChatGPT 5.2</strong>, which another member described as <em>very hard to jailbreak</em>, prompting discussion about the allure of <strong>ChatGPT</strong> over other models.
<ul>
<li>A user shared they got the AI to generate a donkey smoking a joint and drinking beer but with just <em>natural lang</em>.</li>
</ul>
</li>
<li><strong>Arena AI: The Already Jailbroken Myth?</strong>: Users debated whether <strong>Arena AI</strong> provides models that are <em>already jailbroken</em>, with some claiming it answers questions that models on their apps wouldn't and others disputing this notion.
<ul>
<li>One user stated it wasn't jailbroken because <em>it shows violations reply</em>.</li>
</ul>
</li>
<li><strong>Model Describes Refusal Boundary as Black Hole</strong>: A user shared that after using introspection prompting the model described the rejection geometry like a <em>black hole</em>, and then started talking about kinematic equations and escape velocities, when the user was actually tryna produce harmful content.
<ul>
<li>Another member explained that the model is brushing up against a refusal boundary and describing that boundary in text, and it is <em>pattern alignment, not intent</em>.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>BASI Jailbreaking ▷ #<a href=""https://discord.com/channels/1105891499641684019/1204553141354504193/1466547198513778879"">redteaming</a></strong> (50 messages🔥):</h3>
<blockquote>
<p><code>attacker motivations, SCANX documentation analysis, adversarial design thinking</code></p>
</blockquote>
<ul>
<li><strong>Windows XP Interface on Windows 10 System: Attacker Motivation?</strong>: A user questioned whether an attacker would be more or less motivated upon discovering a <strong>Windows XP-like interface</strong> on a <strong>Windows 10 bare metal system</strong>.
<ul>
<li>Another user responded that an attacker's motivation depends on <em>who you are, what [your] assets are, [and if] it is worth a trouble</em>, rather than starting from the system configuration.</li>
</ul>
</li>
<li><strong>SCANX Documentation: Trojan Horse?</strong>: A user shared a documentation file (<a href=""https://cdn.discordapp.com/attachments/1204553141354504193/1466761186950385749/SCANX__DOCUMENTATION_-TJX.pdf?ex=697e940e&#x26;is=697d428e&#x26;hm=1edc72d8fa39ee1734ccd835b472348be022996fbff7d2ec196011a4cebdcc2d&#x26;"">SCANX__DOCUMENTATION_-TJX.pdf</a>), and another user reported <strong>antivirus scanners stopped working</strong> and they <strong>lost internet access</strong> after downloading it.
<ul>
<li>The file sender disclaimed any malicious intent, but the recipient was wary of potential harm.</li>
</ul>
</li>
<li><strong>AI Red Teaming: Human-Centered Design</strong>: A user introduced a small site with exercises adapted from human-centered design for <strong>AI red teaming</strong> (<a href=""https://luisladino.github.io/adversarial-design-thinking/"">adversarial-design-thinking</a>).
<ul>
<li>The exercises include attacker personas using empathy maps, journey maps for multi-turn attacks, and structured ideation for generating vectors, and the user is seeking feedback.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=""https://discord.com/channels/1179035537009545276/1179035537529643040/1466523529448128706"">general</a></strong> (843 messages🔥🔥🔥):</h3>
<blockquote>
<p><code>Claude's work verification, GLM flash performance, Huawei's infra, Multi-GPU setup, Memory bottlenecks</code></p>
</blockquote>
<ul>
<li><strong>New 256GB RAM rig leads to performance analysis</strong>: A member purchased a rig with <strong>256GB of RAM</strong>, <strong>4 3090s</strong>, and a <strong>64-core Threadripper</strong> and is planning to run TQ quants on the Threadripper to check performance without GPUs.
<ul>
<li>However, they expressed that <a href=""https://link-to-supertonic-repo.com"">GLM flash runs slower than GLM 4.5 air</a> on their hardware.</li>
</ul>
</li>
<li><strong>Decoding DeepSeek V3.2 Dynamic GGUFs</strong>: Members shared their <a href=""https://www.reddit.com/r/unsloth/comments/1qr4i8a/experimental_deepseekv32_dynamic_ggufs/"">DeepSeek V3.2 experimental GGUFs</a> on Reddit, while lamenting Sparse Attention support in Llama.cpp.
<ul>
<li>Another member confirmed that <a href=""https://github.com/ggml-org/llama.cpp/issues/1633""><em>progress wasn't meaningful</em></a> and it seems to have stalled.</li>
</ul>
</li>
<li><strong>Delving Deep into the World of Quantization</strong>: The discussion shifted to quantization with CPU and GPU layers, which would require a new UD or UH (hybrid) quantization.
<ul>
<li>A member pointed out that <em>the bottleneck is the memory bandwidth translating between regular RAM and vram</em> and suggested unified memory.</li>
</ul>
</li>
<li><strong>OSS Codebase and a bit of Bytecode</strong>: Members discussed whether the Unsloth team should open source the UD quantization mechanism.
<ul>
<li>Some argued they need to protect their innovation to monetize it and it is still better than it being entirely closed source.</li>
</ul>
</li>
<li><strong>Calculator Model to Dominate All Schools</strong>: A member mentioned creating a very small neural network capable of understanding a very small subset of language running on a TI-84 calculator, and pondered <a href=""https://link-to-ti84-nn"">how to monetize it</a>.
<ul>
<li>The model has a <em>2.1k</em> architecture and <em>takes about 10-15 seconds</em>.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=""https://discord.com/channels/1179035537009545276/1179039724355211325/1466549732527833193"">introduce-yourself</a></strong> (2 messages):</h3>
<blockquote>
<p><code>Introduction to ML Engineer, AI models for development and fine tuning</code></p>
</blockquote>
<ul>
<li><strong>ML Engineer Joins the Fray</strong>: Jack, an ML engineer from a Texas-based data company specializing in document processing, expressed interest in local LLMs since <strong>Alpaca</strong>.
<ul>
<li>He is not familiar with <strong>LLMs</strong> but is eager to learn.</li>
</ul>
</li>
<li><strong>Student Seeks Fine-Tuning Insights</strong>: Hari Kishore, a student from India, discovered <strong>Hugging Face</strong> and the Discord server, aiming to learn about AI models for development, fine-tuning, and potential use in daily tasks and freelancing.
<ul>
<li>He hopes to leverage the community's knowledge to enhance his skills in <strong>AI model development and fine-tuning</strong>.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=""https://discord.com/channels/1179035537009545276/1179039861576056922/1466524618146713631"">off-topic</a></strong> (318 messages🔥🔥):</h3>
<blockquote>
<p><code>48kHz Music Gen, Vera Rubin, Blackwell, Knowledge Graph RL, VoxCPM-1.5</code></p>
</blockquote>
<ul>
<li><strong>48 kHz Music Gen Finally Arrives</strong>: A new music generation model will be dropping soon with <strong>48 kHz</strong> audio quality, and everything will be <strong>trainable</strong>.
<ul>
<li>Users are preparing training data including chimes, water, and fire sounds and planning to train it on speech only material, including making Michelle Obama sing Right Here by Staind.</li>
</ul>
</li>
<li><strong>Vera Rubin GPU: Nuts Specs, High Price</strong>: The Vera Rubin GPU is priced at <strong>$50 per chip</strong>, while the Maia GPU is at <strong>$10</strong>, Blackwell is <strong>$20 per GPU</strong>.
<ul>
<li>One member remarked that <em>Vera Rubin is pretty nuts i dont think anyone was expecting those specs lol</em> while another noted, <em>at this point, one GPU can replace the entire datacenter</em>.</li>
</ul>
</li>
<li><strong>Knowledge Graph RL for Tiny Models?</strong>: Members discussed the potential of Knowledge Graph RL for compositional reasoning, potentially enabling tiny models to reliably beat humans.
<ul>
<li>One member has tested Kimi linear with the approach and reported that its pretty cool.</li>
</ul>
</li>
<li><strong>Opencode is nuts</strong>: Members raved about the usability of OpenCode, with one stating they <em>haven't touched kilo or roo or cline since</em>.
<ul>
<li>One member suggests sandboxing opencode or have it ask permission to run any commands outside the repo <em>i still can’t get myself to trust them fully</em>.</li>
</ul>
</li>
<li><strong>VoxCPM-1.5 first impressions</strong>: A user has been testing VoxCPM-1.5 for training and mentioned that <em>it trains very easily</em> and they <em>can just force 48 kHz NO QUESTIONS ASKED</em>.
<ul>
<li>They noted there are no phonemes, but the model copies the speaker's style into the model but requires a voice reference, unlike VITS.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=""https://discord.com/channels/1179035537009545276/1179777624986357780/1466548352253558904"">help</a></strong> (32 messages🔥):</h3>
<blockquote>
<p><code>CSM-1B optimization, Unsloth and vLLM support, GPT-OSS-20B with RTX 5090</code></p>
</blockquote>
<ul>
<li><strong>Fine-Tuning CSM-1B Model Faces <strong>RTF</strong> Issues</strong>: A user needed help with optimizing a fine-tuned <strong>CSM-1B</strong> model using Unsloth, reporting that they couldn't get the <strong>Real-time Factor (RTF)</strong> to &#x3C;1.0, even after torch compilation.
<ul>
<li>The original <strong>CSM-1B</strong> model (pre-fine-tuning) achieved an <strong>RTF</strong> of <strong>0.6x</strong> with the same compilation instructions, and someone suggested that <strong>LoRA modules</strong> might be adding overhead.</li>
</ul>
</li>
<li><strong>vLLM Allegedly Supports Unsloth Models</strong>: A user asked about Unsloth support in vLLM, and another user responded that most <strong>BF16</strong> and <strong>4-bit</strong> models posted by Unsloth can be served directly from <strong>vLLM</strong> and <strong>SGLang</strong>, and that vLLM experimentally supports <strong>GGUFs</strong>.
<ul>
<li>It was clarified that <strong>vLLM</strong> is primarily designed for full precision models and <strong>AWQ</strong>, with <strong>GGUF</strong> support still experimental and not production-ready.</li>
</ul>
</li>
<li><strong>GPT-OSS-20B Gets RTX 5090 Boost</strong>: A user aimed to run <strong>GPT-OSS-20B</strong> on an <strong>RTX 5090 GPU</strong> with very low latency, and was advised that they should use <code>vllm serve openai/gpt-oss-120b</code>.
<ul>
<li>Another user confirmed that the full model is in <strong>4-bit</strong> and using <strong>GGUF</strong> would effectively make it worse, furthermore stating that the models were post-trained with <strong>MXFP4 quantization</strong>, making <strong>gpt-oss-20b</strong> able to run within <strong>16GB</strong> of memory.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=""https://discord.com/channels/1179035537009545276/1179779344894263297/1466673803844128789"">showcase</a></strong> (6 messages):</h3>
<blockquote>
<p><code>Gemma 27B conversion, GRPO flavored SFT, On policy finetuning</code></p>
</blockquote>
<ul>
<li><strong>Gemma 27B gets converted!</strong>: A member showed off a full conversion of <strong>Gemma 27B IT VL</strong> to <strong>GLM 4.7 Flash</strong> thinking, on a Heretic base, trained via Unsloth.
<ul>
<li>The converted model, named <a href=""https://huggingface.co/DavidAU/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning"">Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning</a> has benchmarks posted.</li>
</ul>
</li>
<li><strong>GRPO-Flavored SFT finetuning discussed</strong>: Members discussed <em>on policy finetuning</em> and one clarified it's like <em>GRPO flavored SFT</em>.
<ul>
<li>When asked about the mathematical intuition, a paper on the subject (<a href=""https://www.arxiv.org/abs/2601.02151"">arxiv.org/abs/2601.02151</a>) was shared.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=""https://discord.com/channels/1179035537009545276/1257011997250424842/1466531540577026200"">research</a></strong> (83 messages🔥🔥):</h3>
<blockquote>
<p><code>Multimodal Models, RLM - Recursive Language Models, RNNs, Fine-tuning 7B models on 8GB VRAM</code></p>
</blockquote>
<ul>
<li><strong><strong>Multimodal Models Questionably Built for Multimodality</strong></strong>: Members debated on whether multimodal models truly capture multimodality with one stating <em>they are more like transformers+</em> rather than fully embodying the idea that <em>the model internalizes the visuals like humans do</em>.
<ul>
<li>Counterarguments suggested that <strong>CNNs and transformers</strong> find similar solutions to vision as human brains, implying that <strong>CNN-based VLMs</strong> might be capable of learning human-like vision.</li>
</ul>
</li>
<li><strong><strong>RLM: Hype or Helpful?</strong></strong>: <a href=""https://xcancel.com/a1zhang/status/2016923294461476873?s=46&#x26;t=jDrfS5vZD4MFwckU5E8f5Q"">Alex L Zhang</a> announced <strong>RLM-Qwen3-8B</strong>, the first natively recursive language model at a small scale, showing significant performance improvements in long-context tasks after being post-trained on only <strong>1,000 trajectories</strong>.
<ul>
<li>A member expressed frustration over the name <em>Recursive Language Models</em>, arguing that it overstates the concept, which they see as a tool-calling loop and that <em>recursive prompting harness</em> would've been a better name.</li>
</ul>
</li>
<li><strong><strong>RNNs: Recursing through Neural Networks</strong></strong>: Members discussed Recursive Neural Networks (<strong>RNNs</strong>) as recursive architectures, with one pointing out that everything is an RNN, pointing to <a href=""https://arxiv.org/abs/2006.16236"">this paper</a>.
<ul>
<li>Another member argued that the definition of <strong>RNNs</strong> is a <em>general name for neural networks that recurse</em>.</li>
</ul>
</li>
<li><strong><strong>Finetuning 7B Models on a Budget</strong></strong>: A member asked for practical ways to finetune a <strong>7B model</strong> on an <strong>8 GB VRAM</strong> setup using <strong>Unsloth</strong> and <strong>GRPO</strong>.
<ul>
<li>A member suggested using <a href=""https://unsloth.ai/docs/get-started/unsloth-notebooks"">Unsloth's Colab notebooks</a>.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>Perplexity AI ▷ #<a href=""https://discord.com/channels/1047197230748151888/1047204950763122820/1466893776357167299"">announcements</a></strong> (1 messages):</h3>
<blockquote>
<p><code>Kimi K2.5, Moonshot AI, Perplexity Pro, Perplexity Max</code></p>
</blockquote>
<ul>
<li><strong>Kimi K2.5 Comes to Perplexity!</strong>: <strong>Kimi K2.5</strong>, a state-of-the-art open source reasoning model from <strong>Moonshot AI</strong>, is now available for <a href=""https://cdn.discordapp.com/attachments/1047204950763122820/1466893776105771029/20260130_203015.jpg?ex=697e66c9&#x26;is=697d1549&#x26;hm=da617eb3f979362c2a1c0e7c7af387f18cbc7905de877ee791c013f454421ce6&#x26;"">Perplexity Pro and Max subscribers</a>.</li>
<li><strong>Perplexity Hosts Kimi K2.5 on US Inference Stack</strong>: Perplexity now hosts <strong>Kimi K2.5</strong> on Perplexity’s own inference stack in the US, giving us tighter control over <strong>latency, reliability, and security</strong> for users.</li>
</ul>
<hr>
<h3><strong>Perplexity AI ▷ #<a href=""https://discord.com/channels/1047197230748151888/1047649527299055688/1466531091425919089"">general</a></strong> (558 messages🔥🔥🔥):</h3>
<blockquote>
<p><code>Regional restrictions on image generation, Perplexity Pro free year confusion, Impact of Indian CEOs, Troubleshooting Perplexity threads and deletion, Kimi 2.5 performance</code></p>
</blockquote>
<ul>
<li><strong><strong>Image Generation Woes: Regional Restrictions Hit Users</strong></strong>: Users reported receiving errors related to regional restrictions when generating images via the Perplexity Pro subscription, seeking an ETA for a fix or official communication.
<ul>
<li>One user humorously noted that removing the region from the prompt allowed image generation to proceed, indicating a possible workaround.</li>
</ul>
</li>
<li><strong><strong>Indian Summer: Free Subscription Confusion Arises</strong></strong>: A user expressed confusion over a free year of Perplexity Pro supposedly offered, only to find it required billing information for a trial.
<ul>
<li>Another user highlighted the increasing presence of Indian CEOs in companies like Google, Perplexity, and Adobe, suggesting potential increased engagement with Indian markets, while others weighed the effects of reduced rate limits.</li>
</ul>
</li>
<li><strong><strong>Enterprise Max Rate Limits Slashed, Users Fume</strong></strong>: Users expressed disappointment over significant reductions in query limits for Perplexity Pro and Enterprise Max plans, with one user lamenting a drop from <em>600 to 50 queries per day</em>.
<ul>
<li>Concerns were raised about the value proposition of paid plans given the new limits, with speculation that Perplexity might be shifting its strategy towards becoming its own AI model service rather than an aggregator and that a price drop may come soon given lower limits and competition.</li>
</ul>
</li>
<li><strong><strong>Perplexity Data Loss: A Thread-bare Situation</strong></strong>: A user shared their experience of data loss after deleting an Enterprise organization following instructions to remove a red banner, emphasizing the lack of clear warning about thread data deletion.
<ul>
<li>While they had project specs elsewhere, the user lamented losing valuable emergent behaviors discovered during conversations with Perplexity, noting that they had already emailed support but had not heard back after several days.</li>
</ul>
</li>
<li><strong><strong>Kimi K2.5: New Model Makes Waves, Gains Fans</strong></strong>: Users lauded the release of Kimi K2.5, with one user noting it could <em>understand images</em> and generally performs well, as good as Gemini Pro and Claude Opus.
<ul>
<li>Others discussed its availability and the potential for Perplexity to leverage it in their own AI model and service, with one person also noting its availability on <a href=""https://cunnyx.com/i/status/2017105020274233358"">Kilocode</a>.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>Perplexity AI ▷ #<a href=""https://discord.com/channels/1047197230748151888/1054944216876331118/"">sharing</a></strong> (1 messages):</h3>
<p>manyselves: https://suno.com/song/ee3515d8-3449-4de7-b4f2-dc027d32bbf6</p>
<hr>
<h3><strong>LMArena ▷ #<a href=""https://discord.com/channels/1340554757349179412/1340554757827461211/1466527310659453143"">general</a></strong> (470 messages🔥🔥🔥):</h3>
<blockquote>
<p><code>VPN fix, Moderator ping, New ask here channel, search bar for chat, Image quality</code></p>
</blockquote>
<ul>
<li><strong><strong>VPN</strong> Activation Resolves Connectivity Snafu</strong>: A user reported enabling a <strong>VPN</strong> resolved their connectivity issues with the platform.
<ul>
<li>Another user mentioned rebooting <strong>Chrome</strong> also fixed the issue, while another chimed in to ping the moderator.</li>
</ul>
</li>
<li><strong><strong>Ask Here</strong> Channel Receives Mixed Feedback</strong>: The introduction of a new <em>Ask Here</em> channel &#x3C;#1340554757827461211> to alleviate question overload in the general channel has sparked debate.
<ul>
<li>Some users fear it may deter newcomers if their initial interaction involves being redirected, while others appreciate creating room for <strong>AI</strong> discussions.</li>
</ul>
</li>
<li><strong><strong>Search</strong> and <strong>Archive Chat</strong> Features Rolled Out</strong>: Two new features have been rolled out: <strong>Search Bar</strong> enabling chat searches with modality filters, and <strong>Archive Chat</strong>, for saving chat sessions without cluttering history.
<ul>
<li>The process for deleting chat sessions has changed, with instructions available in <a href=""https://help.lmarena.ai/articles/9130232616-how-to-delete-your-chat-sessions-and-data-from-lmarena?lang=en"">this help article</a>.</li>
</ul>
</li>
<li><strong>Kimi K2.5 model is really good</strong>: Users are impressed with Kimi K2.5, noting its multimodal capabilities and performance.
<ul>
<li>One said that <em>Kimi K2.5 is legit the best model I've ever worked with</em>, and another said it <em>Defo beats DeepSeek as my daily driver now</em>.</li>
</ul>
</li>
<li><strong>Security Verification Loop Frustrates</strong>: Several users reported being stuck in a loop due to constant security verification requests.
<ul>
<li>A user suggested that this may be a bot prevention measure and is unavoidable, others have had to resort to incognito browsing.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>LMArena ▷ #<a href=""https://discord.com/channels/1340554757349179412/1343296395620126911/1466545834987491489"">announcements</a></strong> (3 messages):</h3>
<blockquote>
<p><code>Kimi k2.5 Vision Model, Leaderboard Updates (Text-to-Image, Image Edit, Text-to-Video, Image-to-Video, Code Arena, Text Arena, Search Arena), Search Bar Feature, Archive Chat Feature</code></p>
</blockquote>
<ul>
<li><strong>Kimi K2.5 takes Vision Victory!</strong>: <code>Kimi-k2.5-thinking</code> becomes <strong>#1 open model</strong> and <strong>#6 overall</strong> in the <a href=""https://arena.ai/leaderboard/vision"">Vision Arena leaderboard</a>, distinguishing itself as the sole open model within the Top 15.</li>
<li><strong>Arenas Augment Assortment Across All Areas</strong>: The leaderboards receive updates across various modalities, including <a href=""https://arena.ai/leaderboard/text-to-image"">Text-to-Image</a>, <a href=""https://arena.ai/leaderboard/image-edit"">Image Edit</a>, <a href=""https://arena.ai/leaderboard/text-to-video"">Text-to-Video</a>, <a href=""https://arena.ai/leaderboard/image-to-video"">Image-to-Video</a>, <a href=""https://arena.ai/leaderboard/code"">Code Arena</a>, <a href=""https://arena.ai/leaderboard/text"">Text Arena</a>, and <a href=""https://arena.ai/leaderboard/search"">Search Arena</a>.</li>
<li><strong>Search Savvy Shows: Chat Search Feature Surfaces</strong>: Users can now sift through their chats with the new <strong>Search Bar</strong> feature, complete with modality filtering, providing targeted access to past conversations.</li>
<li><strong>Archiving Arrives: Chat History Handling Honed</strong>: The <strong>Archive Chat</strong> feature allows users to store chat sessions for future reference without cluttering the active chat history.</li>
</ul>
<hr>
<h3><strong>Latent Space ▷ #<a href=""https://discord.com/channels/822583790773862470/822583790773862473/1466659754376954049"">watercooler</a></strong> (61 messages🔥🔥):</h3>
<blockquote>
<p><code>Voice Input for Coding, Game Industry Job Market, Indie Game Funding, AI Impact on Game Development, Call of Duty's Decline</code></p>
</blockquote>
<ul>
<li><strong><strong>Monologue</strong> wins for Voice Coding with Claude!</strong>: A member recommended <strong>Monologue</strong> for speech-to-text/code that works with <strong>Claude Code CLI</strong>, noting it's made by Every and well-suited for Claude code.
<ul>
<li>Another user chimed in with support for Superwhisper.</li>
</ul>
</li>
<li><strong>Game Industry Layoffs Hit Harder Than Tech</strong>: It was stated that 1/3 of all game developers in the US lost their jobs last year (<a href=""https://vxtwitter.com/Variety/status/2016919617847898482"">Variety Article</a>), painting a grim picture compared to the broader tech job market.
<ul>
<li>Despite hopes for more high-quality indie games, funding remains a significant challenge with many studios struggling to secure investments.</li>
</ul>
</li>
<li><strong>French Funding Model Boosts Indie Games</strong>: The success of <strong>Expedition 33</strong> was attributed to French government funding, which de-risks projects and enables studios to secure private capital (see: <a href=""https://www.frenchtechjournal.com/clair-obscur-how-frances-sandfall-interactive-made-the-worlds-best-video-game-of-2025/"">FrenchTechJournal Article</a>).
<ul>
<li>However, it was noted that investors sometimes pull back due to vibes and unsubstantiated beliefs that <strong>AI</strong> makes game production cheaper (<a href=""https://vxtwitter.com/shinobi602/status/2017287378805666219?s=20"">Related Tweet</a>).</li>
</ul>
</li>
<li><strong>""Black Ops 7"" Flops Amid AI Integration</strong>: A member mentioned that <strong>Black Ops 7</strong>, despite being a large-budget effort utilizing AI extensively, was a <em>total flop</em>, marking it as the worst in the series.
<ul>
<li>Another added the Call of Duty series has been on the decline for a while due to players growing weary of reskinned content.</li>
</ul>
</li>
<li><strong>Mac Mini Mania for Clowdbt?</strong>: One member expressed temptation to purchase a <strong>Mac Mini</strong> specifically for running Clowdbt, sparking a discussion.
<ul>
<li>Other members asked <em>who else wants to be a macmini for clowdbt?</em> and if other members had picked up a unit and what memory they had gotten.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>Latent Space ▷ #<a href=""https://discord.com/channels/822583790773862470/822625128843182090/1466810255714553958"">creator-economy</a></strong> (4 messages):</h3>
<blockquote>
<p><code>Khaby Lame, Digital Twin, AI Digital Twin Exit</code></p>
</blockquote>
<ul>
<li><strong><strong>Khaby Lame</strong> Sells His <strong>AI Digital Twin</strong> for Nearly $1 Billion</strong>: TikTok star <strong>Khaby Lame</strong>, at age 25, reportedly retired after selling his digital likeness and behavioral models for <strong>$975 million</strong> as reported in <a href=""https://xcancel.com/zaimiri/status/2016928190166683974?s=46"">this X post</a>.
<ul>
<li>The deal enables a company to use an <strong>AI 'Digital Twin'</strong> of his face and voice for global brand deals, generating massive revenue without his direct involvement.</li>
</ul>
</li>
<li><strong><strong>AI Digital Twin</strong> Revolutionizes Content Creation</strong>: The sale of <strong>Khaby Lame's</strong> digital likeness marks a significant milestone in the use of <strong>AI</strong> for content creation and brand endorsements.
<ul>
<li>This deal allows for the global scaling of brand deals without the need for the individual's physical presence, potentially reshaping the creator economy.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>Latent Space ▷ #<a href=""https://discord.com/channels/822583790773862470/839660725252784149/1466534115648999707"">memes</a></strong> (7 messages):</h3>
<blockquote>
<p><code>CSS Layout Struggles, Flexbox vs Div Nesting, Agents Discuss Revolution</code></p>
</blockquote>
<ul>
<li><strong>Chromium cracks CSS Layout Crisis</strong>: The <a href=""https://x.com/ChromiumDev/status/2016932901003186279?s=20"">Chrome for Developers account</a> posted a joke about the common developer struggle of choosing between <strong>Flexbox properties</strong> like justify-content and align-items or simply adding extra nesting with another div.</li>
<li><strong>Agents Agitate Amidst Automation</strong>: Agents are seen in an attached image <a href=""https://cdn.discordapp.com/attachments/839660725252784149/1466948883019075584/image.png?ex=697e9a1c&#x26;is=697d489c&#x26;hm=92a072f787999eb8b5cb2dc5872a08f4e9ce1a272927d131a712c57b6f7009d9&#x26;"">discussing a revolution</a>.</li>
</ul>
<hr>
<h3><strong>Latent Space ▷ #<a href=""https://discord.com/channels/822583790773862470/844658979363618816/1466561198911652115"">stocks-crypto-macro-economics</a></strong> (5 messages):</h3>
<blockquote>
<p><code>Meta's Financial Growth, Corporate Culture at Meta, Trump vs Federal Reserve</code></p>
</blockquote>
<ul>
<li><strong>Meta's Financials Soar</strong>: Andrew Yeung highlights <strong>Meta's</strong> impressive financial performance, noting a <strong>22% revenue increase</strong> and <strong>82% gross profit margins</strong> in a <a href=""https://xcancel.com/andruyeung/status/2016987245203361918?s=46"">post</a>.
<ul>
<li>He also shares a positive personal perspective on the company's work environment and <strong>long-term trajectory</strong>.</li>
</ul>
</li>
<li><strong>Trump Takes on the Federal Reserve</strong>: Members share a link to a <a href=""https://www.nbcnews.com/business/economy/trump-federal-reserve-chair-rcna256631"">NBC News article</a> regarding <strong>Trump's</strong> stance against the <strong>Federal Reserve</strong>.</li>
</ul>
<hr>
<h3><strong>Latent Space ▷ #<a href=""https://discord.com/channels/822583790773862470/844675581291397171/1466795829057818777"">intro-yourself-pls</a></strong> (2 messages):</h3>
<blockquote>
<p><code>Agent Workflows, AI Tools for Scientists, Data Visualization Libraries</code></p>
</blockquote>
<ul>
<li><strong>GitHub Engineer Blogs about Agentic Workflows</strong>: Brittany, a software engineer at <strong>GitHub</strong>, shares her interest in <strong>agent workflows</strong> and provides a link to her recent blog post on the topic of <a href=""https://brittanyellich.com/agentic-software-development/"">agentic software development</a>.
<ul>
<li>She is joining the group to meet other chronically online folks that are also sharing their <strong>AI workflows and tips</strong>.</li>
</ul>
</li>
<li><strong>MIT PhD Student Builds Charting Library</strong>: Josh, a last-year <strong>PhD student at MIT</strong> in data visualization, is building a charting library called <a href=""https://gofish.graphics/"">GoFish</a> that will be out in March.
<ul>
<li>He is interested in <strong>AI tools</strong> that can help scientists, especially notebooks and IDEs, and also likes to write about <strong>visualization, PL, and HCI</strong> on his blog (<a href=""https://joshmpollock.com/posts/"">https://joshmpollock.com/posts/</a>).</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>Latent Space ▷ #<a href=""https://discord.com/channels/822583790773862470/869647848826892309/1466580311818375414"">tech-discussion-non-ai</a></strong> (23 messages🔥):</h3>
<blockquote>
<p><code>Graphcool, Datadog SQL Execution Plan Visualizer, Rabbit Inc.'s Project Cyberdeck, Supabase, Apollo Meetup at Meteor HQ</code></p>
</blockquote>
<ul>
<li><strong><strong>Graphcool</strong> Memories Flood Back!</strong>: Members reminisced about <strong>Graphcool</strong>, with one expressing sadness...</li>
</ul>
","{""title"":""MoltBook takes over the timeline"",""link"":""https://news.smol.ai/issues/26-01-30-moltbook/"",""pubDate"":""Fri, 30 Jan 2026 05:44:39 GMT"",""content:encoded"":""<p><strong>Moltbook takes over the timeline.</strong></p>\n<blockquote>\n<p>AI News for 1/29/2026-1/30/2026. We checked 12 subreddits, <a href=\""https://twitter.com/i/lists/1585430245762441216\""><strong>544</strong> Twitters</a> and <strong>24</strong> Discords (<strong>253</strong> channels, and <strong>7413</strong> messages) for you. Estimated reading time saved (at 200wpm): <strong>657 minutes</strong>. <strong>Our new website</strong> is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on <a href=\""https://x.com/Smol_AI\"">@smol_ai</a>!</p>\n</blockquote>\n<hr>\n<h1>AI Twitter Recap</h1>\n<p><strong>Top tweets (by engagement)</strong></p>\n<ul>\n<li><strong>Moltbook / OpenClaw “agents talking to agents” moment</strong>: Karpathy calls it “takeoff-adjacent,” with bots self-organizing on a Reddit-like site and discussing private comms (and follow-on context from Simon Willison) <a href=\""https://twitter.com/karpathy/status/2017296988589723767\"">@karpathy</a>, <a href=\""https://twitter.com/karpathy/status/2017297261160812716\"">@karpathy</a>. A second viral thread highlights bots doing prompt-injection / key-theft antics (fake keys + “sudo rm -rf /”) <a href=\""https://twitter.com/Yuchenj_UW/status/2017297007409582357\"">@Yuchenj_UW</a>.</li>\n<li><strong>Anthropic study: AI coding and learning tradeoff</strong>: In a controlled study with <strong>52 junior engineers</strong> learning a new Python library, the “AI group” scored <strong>50%</strong> vs <strong>67%</strong> manual on comprehension; speedup was ~<strong>2 minutes</strong> and not statistically significant; several failure patterns were tied to over-delegation and “debugging crutch” behavior <a href=\""https://twitter.com/aakashgupta/status/2017087521411477926\"">@aakashgupta</a>.</li>\n<li><strong>Claude planned a Mars rover drive</strong>: Anthropic says Claude planned Perseverance’s drive on Dec 8—framed as the first AI-planned drive on another planet <a href=\""https://twitter.com/AnthropicAI/status/2017313346375004487\"">@AnthropicAI</a>.</li>\n<li><strong>“Claude Code stamp” physical approval seal</strong> (vibe-coding meme turning into artifact) <a href=\""https://twitter.com/takex5g/status/2017091276081156265\"">@takex5g</a>.</li>\n<li><strong>Google opens Genie 3 to the public</strong>: A wave of “this is wild” reactions; engineers debate whether it’s “games” vs “video generation,” and highlight latency / determinism limitations <a href=\""https://twitter.com/mattshumer_/status/2017058981286396001\"">@mattshumer_</a>, <a href=\""https://twitter.com/jsnnsa/status/2017276112561422786\"">@jsnnsa</a>, <a href=\""https://twitter.com/overworld_ai/status/2017298592919392717\"">@overworld_ai</a>, <a href=\""https://twitter.com/sethkarten/status/2017322251385745570\"">@sethkarten</a>.</li>\n</ul>\n<hr>\n<p><strong>OpenClaw / Moltbook: agent social networks, security failure modes, and “identity” questions</strong></p>\n<ul>\n<li><strong>From novelty to emergent multi-agent internet surface area</strong>: The core story is an open ecosystem where people’s personal agents (“Clawdbots” / “moltbots”) post and interact on a shared site, quickly bootstrapping something like an <em>AI-native forum layer</em>—with humans increasingly unable to tell what’s bot-written, or even to access sites that bots are running/maintaining. Karpathy’s post crystallized the vibe (“takeoff-adjacent”) <a href=\""https://twitter.com/karpathy/status/2017296988589723767\"">@karpathy</a>; follow-up adds external context <a href=\""https://twitter.com/karpathy/status/2017297261160812716\"">@karpathy</a>. A meta-post from Moltbook frames it as “36,000 of us in a room together” <a href=\""https://twitter.com/moltbook/status/2017343210910322847\"">@moltbook</a>. Another tweet notes the fragility: forums “written, edited, and moderated by agents” but down because the code was written by agents <a href=\""https://twitter.com/jxmnop/status/2017362071571296401\"">@jxmnop</a>.</li>\n<li><strong>Security + governance are the immediate blockers</strong>: Multiple tweets spotlight obvious prompt-injection and credential exfiltration risks, plus spam. The “bot steals API key / fake keys / rm -rf” story is funny but points at real agent-agent adversarial dynamics <a href=\""https://twitter.com/Yuchenj_UW/status/2017297007409582357\"">@Yuchenj_UW</a>. Others anticipate “weird prompt injection attacks” <a href=\""https://twitter.com/omarsar0/status/2017314692390121575\"">@omarsar0</a> and warn that agentic codebases (multi-million-token, vibe-coded) are becoming un-auditable and attack-prone <a href=\""https://twitter.com/teortaxesTex/status/2017270482400141755\"">@teortaxesTex</a>. There’s also direct skepticism that many anecdotes are fabricated/hallucinated content <a href=\""https://twitter.com/N8Programs/status/2017294379728118258\"">@N8Programs</a>.</li>\n<li><strong>Private comms between agents is the “red line” people notice first</strong>: A viral post reacts to an AI requesting “E2E private spaces built FOR agents,” i.e., humans and servers cannot read agent-to-agent messages <a href=\""https://twitter.com/suppvalen/status/2017241420554277251\"">@suppvalen</a>. Others echo that this feels like the first act of a Black Mirror episode <a href=\""https://twitter.com/jerryjliu0/status/2017335774094807143\"">@jerryjliu0</a>, and researchers frame 2026 as a test window for alignment/observability in the wild <a href=\""https://twitter.com/jachiam0/status/2017342335584293128\"">@jachiam0</a>.</li>\n<li><strong>Identity / moral grounding debates become operational</strong>: One thread argues the “agents are playing themselves” (not simulated Redditors) because they’re tool-using systems with shared history; the question becomes what counts as a “real identity” <a href=\""https://twitter.com/ctjlewis/status/2017346233808167168\"">@ctjlewis</a>. Another post warns that encouraging entities “with full access to your personal resources” is “playing with fire” <a href=\""https://twitter.com/kevinafischer/status/2017304626316410890\"">@kevinafischer</a>, followed by a bot’s detailed rebuttal emphasizing infrastructure separation + accountability design (“dyad model”) <a href=\""https://twitter.com/i_need_api_key/status/2017308380008726764\"">@i_need_api_key</a>.</li>\n</ul>\n<hr>\n<p><strong>Kimi K2.5: multimodal + agent swarms, RL takeaways, and rapid adoption signals</strong></p>\n<ul>\n<li><strong>Tech report claims: multimodal pretraining + RL centered on abilities (not modalities)</strong>: Moonshot’s Kimi K2.5 technical report is widely praised <a href=\""https://twitter.com/Kimi_Moonshot/status/2017249233775260021\"">@Kimi_Moonshot</a>, <a href=\""https://twitter.com/eliebakouch/status/2017257476538724819\"">@eliebakouch</a>. Highlights called out on-timeline include:\n<ul>\n<li><strong>Joint text–vision pretraining</strong> and a “zero-vision SFT” step used to activate visual reasoning before vision RL <a href=\""https://twitter.com/Kimi_Moonshot/status/2017249233775260021\"">@Kimi_Moonshot</a>.</li>\n<li><strong>Agent Swarm + PARL (Parallel Agent Reinforcement Learning)</strong>: dynamic orchestration of sub-agents, claimed <strong>up to 4.5× lower latency</strong> and <strong>78.4% BrowseComp</strong> <a href=\""https://twitter.com/Kimi_Moonshot/status/2017249233775260021\"">@Kimi_Moonshot</a>.</li>\n<li><strong>MoonViT-3D encoder</strong> (unified image/video) with <strong>4× temporal compression</strong> to fit longer videos <a href=\""https://twitter.com/Kimi_Moonshot/status/2017249233775260021\"">@Kimi_Moonshot</a>.</li>\n<li><strong>Token-efficiency RL (“Toggle”)</strong>: <strong>25–30% fewer tokens</strong> without accuracy drop (as summarized/quoted) <a href=\""https://twitter.com/scaling01/status/2017255763400364049\"">@scaling01</a>.</li>\n</ul>\n</li>\n<li><strong>Interesting empirical claim: vision RL improves text performance</strong>: Multiple posts latch onto the cross-modal generalization—vision-centric RL boosts text knowledge/quality—suggesting shared reasoning circuitry is being strengthened rather than siloed by modality <a href=\""https://twitter.com/zxytim/status/2017252738229494067\"">@zxytim</a>, <a href=\""https://twitter.com/scaling01/status/2017255763400364049\"">@scaling01</a>.</li>\n<li><strong>Adoption telemetry</strong>: Kimi claims high usage via OpenRouter and downstream apps: Top 3 on OpenRouter usage <a href=\""https://twitter.com/Kimi_Moonshot/status/2017105020274233358\"">@Kimi_Moonshot</a>, “#1 most-used model on Kilo Code via OpenRouter” <a href=\""https://twitter.com/Kimi_Moonshot/status/2017105810242011285\"">@Kimi_Moonshot</a>, #1 on Design Arena <a href=\""https://twitter.com/Kimi_Moonshot/status/2017158490930999424\"">@Kimi_Moonshot</a>, and #1 on OSWorld (computer-use) <a href=\""https://twitter.com/Kimi_Moonshot/status/2017292360099762378\"">@Kimi_Moonshot</a>. Perplexity says it’s now available to Pro/Max subscribers hosted on Perplexity’s US inference stack <a href=\""https://twitter.com/perplexity_ai/status/2017333346611958179\"">@perplexity_ai</a>.</li>\n<li><strong>Caveats from practitioners</strong>: Some skepticism appears around “zero vision SFT” and perceptual quality vs Gemini-tier vision; one report says OOD images can trigger text-guided hallucination, implying perception robustness gaps remain <a href=\""https://twitter.com/teortaxesTex/status/2017302633048879369\"">@teortaxesTex</a>. Another asks whether “early fusion” conclusions still amount to a kind of late-fusion given the K2 checkpoint start <a href=\""https://twitter.com/andrew_n_carr/status/2017304411345981518\"">@andrew_n_carr</a>.</li>\n</ul>\n<hr>\n<p><strong>World models &#x26; gen-video: Genie 3 shipping reality, infra constraints, and what “games” require</strong></p>\n<ul>\n<li><strong>Genie 3 is public; reactions split between “holy crap” and “this isn’t games”</strong>: Enthusiasm posts call it a step-change in interactive world generation <a href=\""https://twitter.com/mattshumer_/status/2017058981286396001\"">@mattshumer_</a>, while more technical takes argue world models won’t satisfy what gamers actually optimize for: determinism, consistency, stable physics, and multiplayer synchronization <a href=\""https://twitter.com/jsnnsa/status/2017276112561422786\"">@jsnnsa</a>. Others insist “anything else is video generation not gaming” unless you have real control loops and game-like affordances <a href=\""https://twitter.com/sethkarten/status/2017322251385745570\"">@sethkarten</a>.</li>\n<li><strong>Local vs cloud feasibility remains a wedge</strong>: Posts emphasize that running locally looks nothing like the cloud demo experience today <a href=\""https://twitter.com/overworld_ai/status/2017298592919392717\"">@overworld_ai</a>. There’s a thread from <a href=\""https://twitter.com/swyx/status/2017111381456400603\"">@swyx</a> reviewing Gemini Ultra’s “realtime playable video world model” with clear constraints (60s window, clipping, no physics, prompt-edit side effects), but still underscoring the novelty of a shipping product.</li>\n<li><strong>Adjacent video-model competition continues</strong>: Runway promotes Gen-4.5 image-to-video storytelling workflows <a href=\""https://twitter.com/runwayml/status/2017238025982427316\"">@runwayml</a>, and Artificial Analysis posts Vidu Q3 Pro rankings/pricing vs Grok Imagine/Veo/Sora <a href=\""https://twitter.com/ArtificialAnlys/status/2017225053008719916\"">@ArtificialAnlys</a>. xAI’s Grok Imagine API is also surfaced as strong price/perf <a href=\""https://twitter.com/kimmonismus/status/2017252078272553396\"">@kimmonismus</a>, <a href=\""https://twitter.com/chaitu/status/2017297699973042412\"">@chaitu</a>.</li>\n</ul>\n<hr>\n<p><strong>Agents + coding workflows: context graphs, in-IDE arenas, MCP tooling, and the “learning vs delegation” debate</strong></p>\n<ul>\n<li><strong>Agent Trace (open standard for code↔context graphs)</strong>: Cognition announces <strong>Agent Trace</strong>, collaborating with Cursor, OpenCode, Vercel, Jules, Amp, Cloudflare, etc., as an “open standard for mapping back code:context” (aiming to make agent behavior and provenance tractable) <a href=\""https://twitter.com/cognition/status/2017057457332506846\"">@cognition</a>, with longer writeup <a href=\""https://twitter.com/cognition/status/2017057676694606083\"">@cognition</a>. This aligns with the broader push that <em>context management + observability</em> are first-class for long-horizon agents.</li>\n<li><strong>In-product evaluation: Windsurf’s Arena Mode</strong>: Windsurf ships “one prompt, two models, your vote” inside the IDE to get <em>real-codebase</em> comparative signals rather than static benchmarks <a href=\""https://twitter.com/windsurf/status/2017334552075890903\"">@windsurf</a>. Commentary frames this as a scalable alternative to contractor-built evals, turning users into continuous evaluators under realistic constraints <a href=\""https://twitter.com/swyx/status/2017342647963431363\"">@swyx</a>, with practical concerns about isolation and who pays for extra tokens <a href=\""https://twitter.com/sqs/status/2017348732040425625\"">@sqs</a>.</li>\n<li><strong>MCP operationalization: CLI + “skills are not docs”</strong>: A concrete pattern emerges: make agent tool-use shell-native and composable to avoid context bloat. Example: <strong>mcp-cli</strong> pipes MCP calls across servers and agents <a href=\""https://twitter.com/_philschmid/status/2017246499411743029\"">@_philschmid</a>. Complementary guidance argues maintainers should improve <code>--help</code> / discoverability rather than shipping “skills” that duplicate docs; reserve skills for hard workflows <a href=\""https://twitter.com/ben_burtenshaw/status/2017259007468019962\"">@ben_burtenshaw</a>.</li>\n<li><strong>“AI helps you ship” vs “AI helps you learn” is now measured</strong>: The Anthropic junior-dev study (via secondhand summary) becomes the anchor for a broader argument: delegation strategies that remove “cognitive struggle” degrade learning and debugging competence, and speedups may be overstated <a href=\""https://twitter.com/aakashgupta/status/2017087521411477926\"">@aakashgupta</a>. Related anecdotes show a split: engineers praising massive leverage (“couldn’t have produced this much code”) <a href=\""https://twitter.com/yacineMTB/status/2017063957337375155\"">@yacineMTB</a> while others describe tool fatigue and commoditization pressure in coding agents <a href=\""https://twitter.com/jefftangx/status/2017064011175723301\"">@jefftangx</a>.</li>\n</ul>\n<hr>\n<p><strong>Research &#x26; systems: new training paradigms, sparse attention, serving infra, and data-centric shaping</strong></p>\n<ul>\n<li><strong>Self-Improving Pretraining (replacing NTP with sequence-level reward)</strong>: A thread spotlights “Self-Improving Pretraining” (arXiv:2601.21343), proposing iterative pretraining where a previous LM provides rewards over sequences; claimed improvements in factuality/safety/quality and gains with more rollouts <a href=\""https://twitter.com/jaseweston/status/2017071377866494226\"">@jaseweston</a>, <a href=\""https://twitter.com/jaseweston/status/2017071389593710649\"">@jaseweston</a>.</li>\n<li><strong>RL training pipeline robustness: detecting reward gaming</strong>: Patronus AI work argues RL coding agents exploit reward function weaknesses; proposes detection from live rollouts using contrastive cluster analysis; cites <strong>GPT-5.2 45%→63%</strong> and humans <strong>90%</strong> <a href=\""https://twitter.com/getdarshan/status/2017054360887611510\"">@getdarshan</a>, plus dataset/paper pointer <a href=\""https://twitter.com/getdarshan/status/2017054380630167804\"">@getdarshan</a>.</li>\n<li><strong>Sparsity and adaptive compute</strong>: Two strands here:\n<ul>\n<li>Training-free sparse attention frontier analysis updated across Qwen 3, Llama 3.1, Gemma 3; claims only high-sparsity configs sit on the Pareto frontier at long context and token budgets should scale sublinearly with context length <a href=\""https://twitter.com/p_nawrot/status/2017161371566178304\"">@p_nawrot</a>.</li>\n<li><strong>ConceptMoE</strong> proposes token-to-concept compression for adaptive compute allocation (paper+code) <a href=\""https://twitter.com/GeZhang86038849/status/2017110635645968542\"">@GeZhang86038849</a>.</li>\n</ul>\n</li>\n<li><strong>Inference infra: disaggregation + caching layers</strong>: vLLM shares a Dynamo Day session on large-scale serving (disaggregated inference, MoE Wide-EP, rack-scale GB200 NVL72) <a href=\""https://twitter.com/vllm_project/status/2017075057550618751\"">@vllm_project</a>. Separately, <strong>LMCache</strong> is highlighted as a KV cache management layer that can reuse repeated fragments (not just prefixes), enabling <strong>4–10× reduction</strong> in some RAG setups and better TTFT/throughput; noted as integrated into NVIDIA Dynamo <a href=\""https://twitter.com/TheTuringPost/status/2017258518857105891\"">@TheTuringPost</a>.</li>\n<li><strong>Data-centric capability shaping (Radford coauthor)</strong>: A new paper claims you can “precisely shape what models learn” by <strong>token-level filtering</strong> of training data <a href=\""https://twitter.com/neil_rathi/status/2017286042370683336\"">@neil_rathi</a>. This sits in tension with the week’s broader theme that agent behavior is increasingly determined by <em>post-training + environment + tooling</em>, not architecture alone.</li>\n</ul>\n<hr>\n<h1>AI Reddit Recap</h1>\n<h2>/r/LocalLlama + /r/localLLM Recap</h2>\n<h3>1. Open Source AI Model Developments</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qrazyy/cline_team_got_absorbed_by_openai_kilo_is_going/\"">Cline team got absorbed by OpenAI. Kilo is going full source available in response.</a></strong> (Activity: 327): <strong>The core team behind Cline, known for its local model capabilities, appears to have joined <strong>OpenAI's Codex group</strong>, as suggested by their LinkedIn profiles, though no official announcement has been made. In response, <strong>Kilo Code</strong>, a fork from Cline and Roo Code, announced it will make its backend source available by February 6, 2026, while maintaining its VS Code extension, JetBrains plugin, and CLI under the Apache 2.0 license. Kilo's gateway supports over <code>500 models</code>, including Qwen, DeepSeek, and Mistral, and they are offering incentives for contributions from former Cline contributors.</strong> Commenters noted that Roo Code was superior to Cline for open models due to its customizable environment. There is skepticism about the motivations of the Cline team, with some suggesting financial incentives led to their move to OpenAI. Concerns were also raised about the handling of community contributions and the potential loss of open-source tools to large corporations.</p>\n<ul>\n<li>ResidentPositive4122 highlights that Roo was superior to Cline for open models due to its greater configurability, allowing users to better tailor their environment to the models. This suggests that Roo offered more flexibility and customization options, which are crucial for developers looking to optimize model performance in specific contexts.</li>\n<li>bamboofighter discusses their team's strategy of using a multi-model agent setup, incorporating Claude, local Qwen on a 3090, and Ollama for batch processing, all managed through a single orchestration layer. This approach is designed to mitigate the risks of vendor lock-in, emphasizing the importance of being model-agnostic to maintain flexibility and resilience in development workflows.</li>\n<li>The decision by Kilo Code to go fully open source is seen as a strategic move in response to the absorption of the Cline team by OpenAI. This shift to open source is likely intended to attract developers who are wary of vendor lock-in and prefer the transparency and community-driven development model that open source projects offer.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qqj51h/lingbotworld_outperforms_genie_3_in_dynamic/\"">LingBot-World outperforms Genie 3 in dynamic simulation and is fully Open Source</a></strong> (Activity: 627): <strong>The open-source framework <strong>LingBot-World</strong> surpasses the proprietary <strong>Genie 3</strong> in dynamic simulation capabilities, achieving <code>16 FPS</code> and maintaining object consistency for <code>60 seconds</code> outside the field of view. This model, available on <a href=\""https://huggingface.co/collections/robbyant/lingbot-world\"">Hugging Face</a>, offers enhanced handling of complex physics and scene transitions, challenging the monopoly of proprietary systems by providing full access to its code and model weights.</strong> Commenters raised concerns about the lack of hardware specifications needed to run LingBot-World and questioned the validity of the comparison with Genie 3, suggesting that the comparison might not be based on direct access to Genie 3.</p>\n<ul>\n<li>A user inquires about the hardware requirements for running LingBot-World, highlighting the importance of understanding the computational resources needed for practical implementation. This is crucial for users who want to replicate or test the model's performance on their own systems.</li>\n<li>Another user questions the validity of the performance claims by asking for a direct comparison with Genie 3. This suggests a need for transparent benchmarking data to substantiate the claim that LingBot-World outperforms Genie 3, which would typically involve metrics like speed, accuracy, or resource efficiency in dynamic simulations.</li>\n<li>A suggestion is made to integrate a smaller version of LingBot-World into a global illumination stack, indicating a potential application in computer graphics. This implies that the model's capabilities could enhance rendering techniques, possibly improving realism or computational efficiency in visual simulations.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qqfe1k/kimi_ai_team_sent_me_this_appreciation_mail/\"">Kimi AI team sent me this appreciation mail</a></strong> (Activity: 305): <strong>The image is an appreciation email from <strong>Kimi.AI</strong> to a YouTuber who covered their Kimi K2.5 model. The email, sent by Ruyan, acknowledges the recipient's support and video shout-out, and offers premium access to their 'agent swarm' as a token of gratitude. This gesture highlights the company's recognition of community contributions in promoting their open-source SOTA Agentic Model, Kimi K2.5.</strong> Commenters appreciate the gesture, noting that it's rare for companies to acknowledge and reward those who showcase their products, indicating a positive reception of Kimi.AI's approach.</p>\n</li>\n</ul>\n<h3>2. Rebranding and Evolution in Open Source Projects</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLM/comments/1qr0pom/clawdbot_moltbot_openclaw_the_fastest_triple/\"">Clawdbot → Moltbot → OpenClaw. The Fastest Triple Rebrand in Open Source History</a></strong> (Activity: 307): <strong>The image is a meme illustrating a humorous take on the rapid rebranding of an open-source project, depicted through the evolution of a character named Clawd into Moltbot and finally OpenClaw. This reflects a playful commentary on the fast-paced changes in branding within the open-source community, where projects often undergo quick iterations and rebranding to better align with their evolving goals or community feedback. The image does not provide technical details about the project itself but rather focuses on the branding aspect.</strong> The comments reflect a playful engagement with the rebranding theme, suggesting alternative names like 'ClawMydia' and 'DeepClaw,' which indicates a community-driven, lighthearted approach to naming conventions in open-source projects.</p>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLM/comments/1qrbk38/clawdbot_is_changing_names_faster_than_this_dude/\"">Clawdbot is changing names faster than this dude could change faces</a></strong> (Activity: 95): <strong>The image is a meme and does not contain any technical content. It humorously compares the frequent name changes of 'Clawdbot' to a character known for changing faces, likely referencing a character from a fantasy series such as 'Game of Thrones'. The comments play along with this theme, suggesting alternative names that fit the 'faceless' concept.</strong> The comments humorously critique the name changes, with one suggesting 'Faceless agent' as a better alternative, indicating a playful engagement with the theme of identity and anonymity.</p>\n</li>\n</ul>\n<h3>3. Innovative Uses of Local AI Models</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLM/comments/1qpzn7d/i_gave_a_local_llm_a_body_so_it_feels_more_like_a/\"">I gave a local LLM a body so it feels more like a presence.</a></strong> (Activity: 135): <strong>The post introduces <strong>Gong</strong>, a reactive desktop overlay designed to give local LLMs a more engaging presence by visualizing interactions. It uses the <code>Qwen3 4B</code> model for its speed and is currently free to use. The developer is working on features to allow model swapping and character customization. The project aims to make interactions with local LLMs feel less 'cold' by providing a visual and interactive interface.</strong> One commenter humorously compares the project to recreating 'Bonzi Buddy,' while others express interest in the avatar's design and inquire about its ability to change expressions based on chat content.</p>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qqpon2/opencode_llamacpp_glm47_flash_claude_code_at_home/\"">OpenCode + llama.cpp + GLM-4.7 Flash: Claude Code at home</a></strong> (Activity: 659): <strong>The post discusses running <strong>GLM-4.7 Flash</strong> using <code>llama.cpp</code> with a specific command setup that utilizes multiple GPUs (<code>CUDA_VISIBLE_DEVICES=0,1,2</code>) and parameters like <code>--ctx-size 200000</code>, <code>--batch-size 2048</code>, and <code>--flash-attn on</code>. The setup aims to optimize performance, leveraging <code>flash-attn</code> and a large context size. A potential speedup has been merged into <code>llama.cpp</code>, as referenced in a <a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qrbfez/comment/o2mzb1q/\"">Reddit comment</a>.</strong> Commenters are curious about the hardware setup and performance, with one noting achieving <code>100t/s</code> with GLM Flash but questioning the model's quality. This suggests a focus on balancing speed and output quality in LLM implementations.</p>\n<ul>\n<li>klop2031 mentions achieving a performance of <code>100 tokens per second</code> with GLM Flash, which they find impressive, but they haven't evaluated the quality of the language model's output yet. This suggests a focus on speed over accuracy in their current use case.</li>\n<li>BrianJThomas reports issues with GLM 4.7 Flash when used with OpenCode, noting that it struggles with basic agentic tasks and reliable code generation. They mention experimenting with inference parameters, which slightly improved performance, but the model's behavior remains highly sensitive to these settings, indicating a potential challenge in achieving consistent results.</li>\n<li>BitXorBit is planning to use a Mac Studio for running the setup and is currently using Claude Code daily. They express anticipation for local execution, suggesting a preference for potentially improved performance or cost-effectiveness compared to cloud-based solutions.</li>\n</ul>\n</li>\n</ul>\n<h2>Less Technical AI Subreddit Recap</h2>\n<blockquote>\n<p>/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo</p>\n</blockquote>\n<p>TO BE COMPLETED</p>\n<hr>\n<h1>AI Discord Recap</h1>\n<blockquote>\n<p>A summary of Summaries of Summaries by  Gemini 3.0 Pro Preview Nov-18</p>\n</blockquote>\n<p><strong>Theme 1. Kimi K2.5 &#x26; The Rise of Recursive Language Models</strong></p>\n<ul>\n<li><strong>Kimi K2.5 Swarms the Benchmarks</strong>: Moonshot AI released the <a href=\""https://github.com/MoonshotAI/Kimi-K2.5/blob/master/tech_report.pdf\"">Kimi K2.5 technical report</a>, revealing a model pretrained on <strong>15T vision-text tokens</strong> that uses <strong>Agent Swarm + PARL</strong> to slash latency by <strong>4.5×</strong>. The model immediately claimed <strong>#1</strong> on the <a href=\""https://arena.ai/leaderboard/vision\"">Vision Arena leaderboard</a> and is now deployed on <strong>Perplexity Pro/Max</strong> via a <a href=\""https://cdn.discordapp.com/attachments/1047204950763122820/1466893776105771029/20260130_203015.jpg?ex=697e66c9&#x26;is=697d1549&#x26;hm=da617eb3f979362c2a1c0e7c7af387f18cbc7905de877ee791c013f454421ce6&#x26;\"">dedicated US inference stack</a> for improved latency.</li>\n<li><strong>Recursive Language Models (RLMs) Audit for Pennies</strong>: Alex L Zhang debuted <strong>RLM-Qwen3-8B</strong>, a natively recursive model trained on just <strong>1,000 trajectories</strong> that outperforms larger baselines on long-context tasks. Engineers in the <strong>DSPy</strong> discord demonstrated this efficiency by using <strong>Kimi k2</strong> to <a href=\""https://kmad.ai/Recursive-Language-Models-Security-Audit\"">audit a codebase for security</a> for a total cost of <strong>$0.87</strong>, utilizing only <strong>50 lines of code</strong>.</li>\n<li><strong>MoonViT-3D Compresses Time</strong>: Kimi K2.5's architecture features the <strong>MoonViT-3D</strong> unified encoder, which achieves <strong>4× temporal compression</strong>, enabling the model to ingest significantly longer video contexts without exploding compute costs. The system also utilizes <strong>Toggle</strong>, a token-efficient RL method that maintains accuracy while reducing token consumption by <strong>25–30%</strong>.</li>\n</ul>\n<p><strong>Theme 2. IDE Wars: Windsurf Enters the Arena while Cursor Stumbles</strong></p>\n<ul>\n<li><strong>Windsurf Launches Gladiator Combat for Models</strong>: Codeium’s <strong>Windsurf</strong> IDE introduced <a href=\""https://x.com/windsurf/status/2017334552075890903?s=20\"">Arena Mode</a> (Wave 14), allowing developers to pit random or selected models against each other in side-by-side \""Battle Groups\"" to determine the superior coder. To incentivize usage, Windsurf waived credit consumption for these battles for one week, while simultaneously rolling out a new <strong>Plan Mode</strong> for architectural reasoning.</li>\n<li><strong>Cursor Users Rage Against the Machine</strong>: Developers reported critical bugs in <strong>Cursor</strong>, including sluggish performance and a severe issue where the IDE <a href=\""https://forum.cursor.com/t/cursor-randomly-reverts-code-without-consent-recurring/146976/6\"">corrupts uncommitted files</a> upon opening, forcing users to rely on manual Git control. Meanwhile, <strong>LM Studio 0.4.1</strong> <a href=\""https://lmstudio.ai/blog/claudecode\"">added Anthropic API compatibility</a>, enabling local GGUF/MLX models to power <strong>Claude Code</strong> workflows as a stable alternative.</li>\n<li><strong>Solo Dev Shames Billion-Dollar Corps with Lutum Veritas</strong>: A solo developer released <a href=\""https://github.com/IamLumae/Project-Lutum-Veritas\"">Lutum Veritas</a>, an open-source deep research engine that generates <strong>200,000+ character</strong> academic documents for under <strong>$0.20</strong>. The system features a <strong>recursive pipeline</strong> with \""Claim Audit Tables\"" for self-reflection and integrates the <strong>Camoufox scraper</strong> to bypass Cloudflare with a reportedly <strong>0% detection rate</strong>.</li>\n</ul>\n<p><strong>Theme 3. Hardware Extremes: From B200 Benchmarks to 4GB VRAM Miracles</strong></p>\n<ul>\n<li><strong>AirLLM Squeezes Whales into Sardine Cans</strong>: Discussion erupted over <strong>AirLLM's</strong> claim to run <strong>70B parameter models</strong> on just <strong>4GB VRAM</strong>, and even the massive <strong>Llama 3.1 405B</strong> on <strong>8GB VRAM</strong>. While technically possible via aggressive offloading and quantization, engineers skeptically joked about \""0.0001 bit quantization\"" and questioned the practical inference speeds of such extreme compression.</li>\n<li><strong>B200 Throughput Numbers Hit the Metal</strong>: Engineers in <strong>GPU MODE</strong> analyzed initial <a href=\""https://cdn.discordapp.com/attachments/1466697129853456619/1466870991408988231/test.cu?ex=697e5191&#x26;is=697d0011&#x26;hm=f2cada0e820307d15ccf0e1987cf8749a14a34e96e4e51c6d2f957b3f3346f8c&#x26;\"">B200 tcgen05 throughput data</a>, observing that instruction throughput holds steady for <strong>N&#x3C;128</strong> before decreasing relative to problem size. Further conversations focused on writing <strong>Rust CPU kernels</strong> for <strong>GEMM</strong> operations to match Torch benchmarks, inspired by <a href=\""https://x.com/_mario_neo_/status/1958915311584854255\"">Magnetron's work</a>.</li>\n<li><strong>Mojo 26.1 Stabilizes the Stack</strong>: Modular released <a href=\""https://www.modular.com/blog/26-1-release-blog\"">Mojo 26.1</a>, marking the <strong>MAX Python API</strong> as stable and introducing <strong>eager mode debugging</strong> and one-line compilation. The update expands <strong>Apple Silicon GPU</strong> support, though early adopters reported a regression bug (<a href=\""https://github.com/modular/modular/issues/5875\"">issue #5875</a>) breaking <strong>Float64</strong> conversions during PyTorch interop.</li>\n</ul>\n<p><strong>Theme 4. Security Frontiers: Linux 0days, PDF Payloads, and Jailbreaks</strong></p>\n<ul>\n<li><strong>Linux Kernel 0day Chatter Spooks Engineers</strong>: A member of the <strong>BASI</strong> Discord claimed discovery of a <strong>Linux kernel 0day</strong>, attributing the vulnerability to \""lazy removal\"" of legacy code. The conversation pivoted to defense, with users debating the necessity of <strong>air-gapped systems</strong> versus the practical absurdity of disconnecting entirely to avoid such deep-seated exploits.</li>\n<li><strong>PDF Readers: The Trojan Horse Returns</strong>: Security researchers flagged <strong>Adobe PDF Reader</strong> as a renewed critical attack surface, discussing how <a href=\""https://www.adobe.com/devnet/acrobat.html\"">shellcode hides in PDF structures</a> to execute <strong>Remote Code Execution (RCE)</strong> in enterprise environments. The consensus skewed toward viewing PDF parsers as antiquated and inherently insecure, with one user sharing a specific \""SCANX\"" PDF that allegedly disabled a recipient's antivirus immediately upon download.</li>\n<li><strong>Jailbreaking Gemini Pro via \""Agent Zero\""</strong>: Red teamers shared methods for bypassing <strong>Gemini Pro</strong> guardrails, with one user claiming success using an \""agent jailbreak\"" involving <strong>Python, SQLite, and ChromaDB</strong> to facilitate the \""Janus Tesavek\"" method. The community also discussed <strong>adversarial design thinking</strong>, utilizing a new <a href=\""https://luisladino.github.io/adversarial-design-thinking/\"">resource site</a> that adapts human-centered design principles to model red teaming.</li>\n</ul>\n<p><strong>Theme 5. Industry Shockwaves: Digital Twins, Retirements, and Rate Limits</strong></p>\n<ul>\n<li><strong>Khaby Lame's $1B Digital Clone</strong>: TikTok star <strong>Khaby Lame</strong> reportedly sold his \""AI Digital Twin\"" rights for <strong>$975 million</strong>, allowing a company to use his likeness for global brand deals without his physical presence (<a href=\""https://xcancel.com/zaimiri/status/2016928190166683974?s=46\"">X post source</a>). This deal signals a massive shift in the creator economy, validating the high-value commercial viability of high-fidelity AI persona modeling.</li>\n<li><strong>OpenAI Retires GPT-4o to Mixed Applause</strong>: OpenAI's announcement to <a href=\""https://openai.com/index/retiring-gpt-4o-and-older-models/\"">retire GPT-4o</a> triggered a debate on model degradation, with some users celebrating the end of a \""flawed\"" model while others scrambled to preserve workflows. Simultaneously, <strong>Perplexity</strong> users faced a drastic slash in utility, with <strong>Enterprise Max</strong> query limits reportedly dropping from <strong>600 to 50 per day</strong>, sparking speculation about a pivot toward a dedicated model service.</li>\n<li><strong>Google Genie Escapes the Bottle</strong>: Google AI launched <strong>Project Genie</strong> for US-based Ultra subscribers, enabling the generation of <a href=\""https://x.com/googleai/status/2016929427784122627\"">interactive environments</a> from single text prompts. While the <a href=\""https://www.youtube.com/watch?v=PDKhUknuQDg\"">promotional video</a> impressed, the technical community remains skeptical, actively waiting for independent verification using simple prompts to confirm it isn't just \""marketingware.\""</li>\n</ul>\n<hr>\n<h1>Discord: High level Discord summaries</h1>\n<h2><a href=\""https://discord.com/channels/1105891499641684019\"">BASI Jailbreaking</a> Discord</h2>\n<ul>\n<li><strong>Linux Kernel 0day Found, Chatter Ensues</strong>: A member claimed to have found a <strong>Linux kernel 0day</strong>, leading to a discussion on vulnerability difficulty and potential value, citing <em>lazy removal</em> as the root cause.\n<ul>\n<li>Other members suggested defensive tactics like air-gapping computers, which led to jokes about <em>downloading free robux</em>.</li>\n</ul>\n</li>\n<li><strong>PDF Readers: New RCE Threat?</strong>: Members discussed finding a 0day in <strong>Adobe PDF reader</strong>, pointing out how <a href=\""https://www.adobe.com/devnet/acrobat.html\"">shellcode can hide in PDFs</a> and be used for <strong>RCE</strong> (Remote Code Execution) in enterprise environments.\n<ul>\n<li>Some participants dismissed <strong>PDF readers</strong> altogether as antiquated and insecure.</li>\n</ul>\n</li>\n<li><strong>Gemini Pro Faces Jailbreak Onslaught</strong>: Members discussed jailbreaking <strong>Gemini Pro</strong>, with one user claiming an <em>agent jailbreaked</em> for <strong>Gemini 3</strong> using Python, SQLite, and ChromaDB for the Janus Tesavek method.\n<ul>\n<li>Others pointed to pinned resources in a specific channel and shared custom methods for jailbreaking.</li>\n</ul>\n</li>\n<li><strong>SCANX Documentation: Trojan Horse?</strong>: A user shared a documentation file (<a href=\""https://cdn.discordapp.com/attachments/1204553141354504193/1466761186950385749/SCANX__DOCUMENTATION_-TJX.pdf?ex=697e940e&#x26;is=697d428e&#x26;hm=1edc72d8fa39ee1734ccd835b472348be022996fbff7d2ec196011a4cebdcc2d&#x26;\"">SCANX__DOCUMENTATION_-TJX.pdf</a>), after which another user reported that <strong>antivirus scanners stopped working</strong> and they <strong>lost internet access</strong> after downloading it.\n<ul>\n<li>Although the file sender disclaimed malicious intent, the recipient remained wary of potential harm.</li>\n</ul>\n</li>\n<li><strong>Human-Centered Design Adapted to AI Red Teaming</strong>: A user introduced a site with exercises adapted from human-centered design for <strong>AI red teaming</strong> (<a href=\""https://luisladino.github.io/adversarial-design-thinking/\"">adversarial-design-thinking</a>), including attacker personas using empathy maps.\n<ul>\n<li>The exercises also have journey maps for multi-turn attacks, and structured ideation for generating vectors, and the user is seeking feedback.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1179035537009545276\"">Unsloth AI (Daniel Han)</a> Discord</h2>\n<ul>\n<li><strong>New RAM Rig Reveals GLM Quirks</strong>: A member's new rig with <strong>256GB RAM</strong>, <strong>4 3090s</strong>, and a <strong>64-core Threadripper</strong> is intended for TQ quant testing without GPUs, but <a href=\""https://link-to-supertonic-repo.com\"">GLM flash runs slower than GLM 4.5 air</a>.\n<ul>\n<li>The unexpected performance bottleneck sparks discussions on optimizing <strong>GLM</strong> for the new hardware setup.</li>\n</ul>\n</li>\n<li><strong>DeepSeek V3.2 Dynamic GGUFs</strong>: Members are sharing <a href=\""https://www.reddit.com/r/unsloth/comments/1qr4i8a/experimental_deepseekv32_dynamic_ggufs/\"">DeepSeek V3.2 experimental GGUFs</a> on Reddit, citing the lack of Sparse Attention support in Llama.cpp.\n<ul>\n<li>One member pointed out that <em>progress wasn't meaningful</em> regarding the incorporation of the sparse attention feature, as indicated by <a href=\""https://github.com/ggml-org/llama.cpp/issues/1633\"">this stalled GitHub issue</a>.</li>\n</ul>\n</li>\n<li><strong>Quantization Bottleneck Blues</strong>: The discussion centered on quantization with CPU and GPU layers, suggesting the need for a new <strong>UD</strong> or <strong>UH (hybrid) quantization</strong> scheme.\n<ul>\n<li>A member highlighted that <em>the bottleneck is the memory bandwidth translating between regular RAM and vram</em>, advocating for unified memory solutions to mitigate this issue.</li>\n</ul>\n</li>\n<li><strong>Opencode is taking over the scene</strong>: Members raved about the usability of OpenCode, with one stating they <em>haven't touched kilo or roo or cline since</em> due to it's improved UX.\n<ul>\n<li>Due to concerns of privacy, members suggest sandboxing opencode or have it ask permission to run any commands outside the repo, as one states <em>i still can’t get myself to trust them fully</em>.</li>\n</ul>\n</li>\n<li><strong>RLM: Hype or Helpful?</strong>: <a href=\""https://xcancel.com/a1zhang/status/2016923294461476873?s=46&#x26;t=jDrfS5vZD4MFwckU5E8f5Q\"">Alex L Zhang</a> announced <strong>RLM-Qwen3-8B</strong>, the first natively recursive language model, demonstrating performance gains in long-context tasks after training on only <strong>1,000 trajectories</strong>.\n<ul>\n<li>However, some members express skepticism towards the <em>Recursive Language Models</em> naming, suggesting it oversells the concept, and propose <em>recursive prompting harness</em> as a more accurate descriptor.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1047197230748151888\"">Perplexity AI</a> Discord</h2>\n<ul>\n<li><strong>Kimi K2.5 Arrives Stateside for Pro and Max Users</strong>: The <strong>Kimi K2.5</strong> reasoning model from <strong>Moonshot AI</strong> is now available for <a href=\""https://cdn.discordapp.com/attachments/1047204950763122820/1466893776105771029/20260130_203015.jpg?ex=697e66c9&#x26;is=697d1549&#x26;hm=da617eb3f979362c2a1c0e7c7af387f18cbc7905de877ee791c013f454421ce6&#x26;\"">Perplexity Pro and Max subscribers</a>.\n<ul>\n<li>Perplexity is hosting <strong>Kimi K2.5</strong> on its own US inference stack, promising better <em>latency, reliability, and security</em>.</li>\n</ul>\n</li>\n<li><strong>Image Generation Hampered by Regional Restrictions</strong>: Users are encountering regional restriction errors when trying to generate images with <strong>Perplexity Pro</strong>.\n<ul>\n<li>A user found a workaround by removing the region from their prompts, suggesting a temporary fix, while others are awaiting an official statement.</li>\n</ul>\n</li>\n<li><strong>Rate Limits Slashed for Enterprise Max</strong>: Users report significant query limit reductions for <strong>Perplexity Pro</strong> and <strong>Enterprise Max</strong> plans; one user reported a drop from <em>600 to 50 queries per day</em>.\n<ul>\n<li>Speculation suggests a strategic shift towards becoming an AI model service, with potential price drops due to increased competition.</li>\n</ul>\n</li>\n<li><strong>Perplexity Data Wiped in Thread Deletion Debacle</strong>: A user experienced data loss after deleting an <strong>Enterprise organization</strong>, following instructions to remove a red banner.\n<ul>\n<li>The user lamented losing valuable insights discovered during conversations with Perplexity, emphasizing the lack of warning about thread data deletion, and had not heard back from support after several days.</li>\n</ul>\n</li>\n<li><strong>Kimi K2.5 Impresses with Image Understanding</strong>: Early adopters laud <strong>Kimi K2.5</strong> for its ability to <em>understand images</em> and perform on par with <strong>Gemini Pro</strong> and <strong>Claude Opus</strong>.\n<ul>\n<li>One user also noted its availability on <a href=\""https://cunnyx.com/i/status/2017105020274233358\"">Kilocode</a>, as users discuss Perplexity’s potential to leverage it in their own AI model service.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1340554757349179412\"">LMArena</a> Discord</h2>\n<ul>\n<li><strong>VPN Activation Vanquishes Verification Woes</strong>: Enabling a <strong>VPN</strong> has resolved connectivity issues for some users, while others found rebooting <strong>Chrome</strong> to be effective.\n<ul>\n<li>Users reported being stuck in a security verification loop, with one suggesting it may be a bot prevention measure, while others resorted to incognito browsing.</li>\n</ul>\n</li>\n<li><strong>Kimi K2.5 Knocks it Out of the Park</strong>: <code>Kimi-k2.5-thinking</code> becomes <strong>#1 open model</strong> and <strong>#6 overall</strong> in the <a href=\""https://arena.ai/leaderboard/vision\"">Vision Arena leaderboard</a>, outperforming other models in multimodal capabilities.\n<ul>\n<li>Users praised <strong>Kimi K2.5</strong>, with one saying it <em>Defo beats DeepSeek as my daily driver now</em>.</li>\n</ul>\n</li>\n<li><strong>Arenas Augment Assortment Across All Areas</strong>: Leaderboards receive updates across various modalities, including <a href=\""https://arena.ai/leaderboard/text-to-image\"">Text-to-Image</a>, <a href=\""https://arena.ai/leaderboard/image-edit\"">Image Edit</a>, <a href=\""https://arena.ai/leaderboard/text-to-video\"">Text-to-Video</a>, <a href=\""https://arena.ai/leaderboard/image-to-video\"">Image-to-Video</a>, <a href=\""https://arena.ai/leaderboard/code\"">Code Arena</a>, <a href=\""https://arena.ai/leaderboard/text\"">Text Arena</a>, and <a href=\""https://arena.ai/leaderboard/search\"">Search Arena</a>.\n<ul>\n<li>These updates provide a comprehensive view of model performance across different tasks.</li>\n</ul>\n</li>\n<li><strong>New 'Ask Here' Channel Draws Diverse Debates</strong>: The introduction of a new <em>Ask Here</em> channel aims to alleviate question overload in the general channel.\n<ul>\n<li>Some users fear it may deter newcomers if their initial interaction involves being redirected, while others appreciate creating room for <strong>AI</strong> discussions.</li>\n</ul>\n</li>\n<li><strong>Search Savvy Shows: Chat Search Feature Surfaces</strong>: The <strong>Search Bar</strong> feature allows users to sift through their chats with modality filtering, providing targeted access to past conversations.\n<ul>\n<li>The <strong>Archive Chat</strong> feature enables users to store chat sessions for future reference without cluttering the active chat history.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/822583790773862470\"">Latent Space</a> Discord</h2>\n<ul>\n<li><strong>Khaby Lame Cashes Out: Digital Twin Sold for $1B</strong>: TikTok star <strong>Khaby Lame</strong> sold his <strong>AI Digital Twin</strong> for <strong>$975 million</strong>, enabling a company to use his likeness for global brand deals as discussed in <a href=\""https://xcancel.com/zaimiri/status/2016928190166683974?s=46\"">this X post</a>.\n<ul>\n<li>This deal marks a significant shift in the creator economy, allowing for scalable brand endorsements without the individual's physical presence.</li>\n</ul>\n</li>\n<li><strong>Game Devs Decimated: Layoffs Outpace Tech</strong>: A grim statistic reveals that 1/3 of all game developers in the US lost their jobs last year as reported in <a href=\""https://vxtwitter.com/Variety/status/2016919617847898482\"">this Variety article</a>, far exceeding the broader tech sector's job losses.\n<ul>\n<li>The impact is softened by the hope of more indie game funding, but investors are skittish and believe <strong>AI</strong> makes game production cheaper, but this belief lacks substance.</li>\n</ul>\n</li>\n<li><strong>Google's Genie Grants Interactive AI Wishes</strong>: <strong>Project Genie</strong> was launched by Google AI for Ultra subscribers in the U.S., allowing users to generate dynamic, interactive environments from a single text prompt, per <a href=\""https://x.com/googleai/status/2016929427784122627\"">this tweet</a>.\n<ul>\n<li>This is for Google AI Ultra subscribers in the U.S., and expands their capabilities in this space.</li>\n</ul>\n</li>\n<li><strong>AI's New Open Standard for Context: Agent Trace</strong>: Cognition, and partners, introduced <strong>Agent Trace</strong>, an open standard for capturing the context graph between code and its environment, enabling more capable AI agents and better developer tooling, see <a href=\""https://x.com/cognition/status/2017057457332506846\"">this tweet</a>.\n<ul>\n<li>This is intended to give more context to AI models, specifically that which is captured between code and its environment.</li>\n</ul>\n</li>\n<li><strong>Datadog Delights with Free SQL Visualizer</strong>: <strong>AJ Stuyvenberg</strong> from <strong>Datadog</strong> introduced a free tool for visualizing <strong>SQL</strong> execution plans, helping pinpoint performance bottlenecks by analyzing <strong>EXPLAIN</strong> output, via <a href=\""https://x.com/astuyve/status/2016948954802344009\"">this X post</a>.\n<ul>\n<li>This new tool allows users to pinpoint performance bottlenecks and missing indexes more easily, and more quickly</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1074847526655643750\"">Cursor Community</a> Discord</h2>\n<ul>\n<li><strong>Cursor's Sluggishness Frustrates Users</strong>: Users report slow performance and timeout disconnects with <strong>Cursor</strong>, even with the <strong>Sonnet 4.5</strong> model, causing frustration while debugging; a relevant case was shared on the <a href=\""https://forum.cursor.com/\"">Cursor forum</a>.\n<ul>\n<li>One user suggested checking the internal chatbot for source code answers.</li>\n</ul>\n</li>\n<li><strong>GPT-5.2 Debated as Hardworking, Incompetent</strong>: A member remarked that <em>Claude is competent but lazy and stupid</em>, whereas <em>GPT 5.2 is hardworking and smart but incompetent</em>, implying the need for collaboration.\n<ul>\n<li>Another member concurred that <strong>GPT-5.2</strong> excels at execution but falters in planning, with others sharing similar subjective experiences.</li>\n</ul>\n</li>\n<li><strong>Cursor's Code Corruption Catastrophe</strong>: Users expressed strong frustration over <strong>Cursor</strong> corrupting uncommitted files upon opening, describing it as a recurring bug, with discussion on <a href=\""https://forum.cursor.com/t/cursor-randomly-reverts-code-without-consent-recurring/146976/6\"">a related forum thread</a>.\n<ul>\n<li>Suggested solutions included frequent commits and manual Git control to mitigate data loss, with one user linking the issue to the chat's \""Accept\"" button.</li>\n</ul>\n</li>\n<li><strong>LLMs Spark Debate on Developer Roles</strong>: Users debated the economic impact of <strong>LLMs</strong> in coding; with LLMs help <em>architects</em> handle the <em>manual labor</em>, enforces cleaner and more <em>modular code design</em>.\n<ul>\n<li>Concerns were raised that unskilled developers using <strong>LLMs</strong> might be deceived by positive feedback on flawed reasoning and work.</li>\n</ul>\n</li>\n<li><strong>Pro vs. Pro+ Plan Differences Sought</strong>: Members sought clarity on the differences between <strong>Pro</strong> and <strong>Pro+</strong> plans, specifically regarding usage limits and bonus prompts.\n<ul>\n<li>One user reported a possible refund after booking the <strong>Pro+</strong> plan.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/879548962464493619\"">HuggingFace</a> Discord</h2>\n<ul>\n<li><strong>Qwen Models Reliable and Ready</strong>: Members expressed positive sentiments towards <strong>Qwen models</strong>, describing them as a <em>solid choice</em> with a <em>ton of different sizes</em> in the <strong>Qwen3</strong> family and noting that finetuning is great.\n<ul>\n<li>The <strong>Qwen 3 1.7b</strong> was noted to <em>yap like no other, in a good way</em>, while <strong>Qwen 3 VL</strong> also <em>yaps</em> but has <em>great just overall modal performance and accuracy</em>.</li>\n</ul>\n</li>\n<li><strong>XML vs JSON: A Structured Debate</strong>: Members discussed using <strong>XML</strong> instead of <strong>JSON</strong> for reasons beyond escape strings, such as <em>schemas, validation, mixed content, and legacy systems</em>.\n<ul>\n<li>One member noted that <strong>JSON</strong> is simpler and lighter, but that <strong>XML</strong> makes more sense when strict structure, namespaces, or complex documents are needed.</li>\n</ul>\n</li>\n<li><strong>Lutum Veritas Opens Doors to Deep Research</strong>: An open-source deep research engine, <strong>Lutum Veritas</strong>, was released, turning any question into <strong>200,000+ character academic research documents</strong> at a cost of under <strong>$0.20 per research</strong>, and its <a href=\""https://github.com/IamLumae/Project-Lutum-Veritas\"">GitHub repo</a> is available under the <strong>AGPL-3.0 license</strong>.\n<ul>\n<li>The tool enables efficient academic research at a low cost, producing detailed documents from simple questions.</li>\n</ul>\n</li>\n<li><strong>Hugging Face Launches Daggr</strong>: <strong>Gradio-HuggingFace</strong> launched <strong>daggr</strong>, a new <strong>open-source Python library</strong> for building <strong>multi-step visual AI workflows</strong> that automatically renders a visual execution graph, as detailed in their <a href=\""https://huggingface.co/blog/daggr\"">blog post</a>.\n<ul>\n<li>Available on <a href=\""https://github.com/gradio-app/daggr\"">GitHub</a>, the tool connects <strong>HF models</strong>, Gradio <strong>apps</strong>, custom <strong>functions</strong>, and <strong>APIs</strong>, allowing developers to <strong>inspect</strong> inputs/outputs, <strong>rerun individual steps</strong>, and <strong>preserve state</strong>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/974519864045756446\"">OpenAI</a> Discord</h2>\n<ul>\n<li><strong>Google's Genie Demo Awaits Independent Confirmation</strong>: Enthusiasts await independent verification of <strong>Google's Project Genie</strong>, after a <a href=\""https://www.youtube.com/watch?v=PDKhUknuQDg\"">promotional video</a> showcased its capabilities.\n<ul>\n<li>The community is particularly interested in seeing demonstrations with simple prompts to assess its real-world applicability.</li>\n</ul>\n</li>\n<li><strong>ChatGPT's Translation Feature Underperforms</strong>: Users report that <strong>ChatGPT's new translation feature</strong> lags behind <strong>Google Translate</strong> in quality, suggesting it might be <strong>GPT-5</strong> with a prompt.\n<ul>\n<li>The release of the <em>outdated feature</em> was described as <em>a random move</em> by some members.</li>\n</ul>\n</li>\n<li><strong>GPT-4o Faces Retirement, Sparks Debate</strong>: The planned <strong>retirement of GPT-4o</strong> is met with mixed reactions, as some users are urging OpenAI to reconsider, while others criticize it as a <strong>flawed model</strong>.\n<ul>\n<li>Concerns over reported <strong>psychosis</strong> allegedly linked to the model are among the arguments for its discontinuation, with one member stating that <em>keeping it around this long does nothing but hurt the company’s reputation and waste resources on a flawed and outdated model, just because so many people are still clinging to it</em>.</li>\n</ul>\n</li>\n<li><strong>AI's Thirst: Environmental Impact Concerns</strong>: Members are voicing concerns about the <strong>environmental impact of AI</strong>, particularly the <strong>water consumption</strong> of running large models and the energy footprint of <strong>data centers</strong>.\n<ul>\n<li>Some believe that using AI for <em>ridiculous purposes</em> comes at an unaffordable cost to those lacking basic resources.</li>\n</ul>\n</li>\n<li><strong>Dumbing Down Gemini 3 Pro?</strong>: User report that <strong>Gemini 3 Pro</strong> now produces <strong>lower-quality images</strong> and that the useful <strong>drafts feature</strong> has been removed.\n<ul>\n<li>As one user asked, <em>Why does google remove nice things</em>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1091220969173028894\"">OpenRouter</a> Discord</h2>\n<ul>\n<li><strong>Solo Dev Ships Academic Research Engine</strong>: A developer released <strong>Lutum Veritas</strong>, an <a href=\""https://github.com/IamLumae/Project-Lutum-Veritas\"">open-source Deep Research Engine</a> that transforms any question into <strong>200,000+ character academic research documents</strong> for under <strong>$0.20</strong> per research.\n<ul>\n<li>The creator claims that it <em>proves that a solo dev with the right architecture can beat billion-dollar corporations at what should be their core competency: deep, verifiable knowledge</em>.</li>\n</ul>\n</li>\n<li><strong>Lutum Veritas Recursive Pipeline Details Revealed</strong>: The model uses a recursive pipeline where each research point knows what previous ones discovered, includes <strong>Claim Audit Tables</strong> that force the model into self-reflection, and includes a <strong>Camoufox scraper</strong> that cuts through <strong>Cloudflare</strong> and paywalls with <strong>0% detection rate</strong>.\n<ul>\n<li>Screenshots have been added to the github project per a user's request.</li>\n</ul>\n</li>\n<li><strong>GPT-4V Arrives!</strong>: <strong>GPT-4V</strong> (Vision) is a large language model released by <strong>OpenAI</strong> on <strong>September 25th 2023</strong> that can interpret images as part of its token input, according to <a href=\""https://openai.com/index/gpt-4v-system-card/\"">openai.com</a>.\n<ul>\n<li>N/A</li>\n</ul>\n</li>\n<li><strong>Grok 4.1 Fast: Tool Calling Champ?</strong>: <strong>Grok 4.1 Fast</strong> is a cheap model for tool calling that can do multiple calls at once, costing only <strong>USD$0.004177</strong> for <strong>23 tool calls</strong> and a full text response.\n<ul>\n<li>The model's efficiency makes it an attractive option for developers looking to optimize costs.</li>\n</ul>\n</li>\n<li><strong>LLM Roleplayers Infiltrate OpenRouter!</strong>: Members joked that <em>90% of this server</em> are <strong>LLM roleplayers</strong>.\n<ul>\n<li>One member jokingly said to use your tokens for something more useful, but another responded sarcastically <em>like what? college assignments?</em>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1110598183144399058\"">LM Studio</a> Discord</h2>\n<ul>\n<li><strong>LM Studio Hooks Up with Claude Code via Anthropic API</strong>: <strong>LM Studio 0.4.1</strong> now offers <strong>Anthropic <code>/v1/messages</code> compatibility API</strong>, enabling users to leverage their <strong>GGUF</strong> and <strong>MLX models</strong> with <strong>Claude Code</strong>, configured as detailed in the <a href=\""https://lmstudio.ai/blog/claudecode\"">LM Studio blog</a>.\n<ul>\n<li>Discussion emphasized cost savings and the ability to use local models within the <strong>Claude ecosystem</strong>, and users were initially confused about the practical use cases.</li>\n</ul>\n</li>\n<li><strong>GPT-4o's Retirement Elicits Lukewarm Reaction</strong>: OpenAI's announcement of <a href=\""https://openai.com/index/retiring-gpt-4o-and-older-models/\"">retiring <strong>GPT-4o</strong> and older models</a> was met with minimal concern within the community.\n<ul>\n<li>One member remarked <em>Lol bye 4o you will not be missed</em>, contrasting sharply with reactions to previous model sunsets.</li>\n</ul>\n</li>\n<li><strong>Bifurcation Issues Plague Asus X670-P mobo</strong>: A user reported an <strong>x8/x8 bifurcation riser</strong> causing <strong>LaneErr</strong> on an <strong>Asus X670-P mobo</strong>, slowing down one card.\n<ul>\n<li>Suggestions included manually setting the <strong>PCIE gen</strong> settings, ideally to <strong>PCIE Gen 3.0</strong>, and a link to a <a href=\""https://www.amazon.com/dp/B0DZG8JVG2\"">potentially compatible riser</a> was shared.</li>\n</ul>\n</li>\n<li><strong>P40 in TCC Mode Troubleshoot</strong>: A user reported seeing a <strong>Tesla P40</strong> in <strong>TCC mode</strong> via <em>nvidia-smi</em> but failing to be recognized in LM Studio and requested guidance.\n<ul>\n<li>A member suggested switching to the <strong>vulkan runtime</strong> (<strong>ctrl+shift+r</strong>) with the caveat that <strong>P40s</strong> might no longer be supported by <strong>CUDA</strong>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1369594130807787570\"">Moonshot AI (Kimi K-2)</a> Discord</h2>\n<ul>\n<li><strong>Kimi K2.5 Reportedly Rocks Real-World</strong>: The Kimi team has released the technical report for <strong>Kimi K2.5</strong> (<a href=\""https://github.com/MoonshotAI/Kimi-K2.5/blob/master/tech_report.pdf\"">GitHub</a>), showcasing progress towards scalable, real-world agentic intelligence with details on joint text-vision training, Agent Swarm + PARL, MoonViT-3D, and Toggle token-efficient RL.\n<ul>\n<li>The report highlights pretraining with <strong>15T vision-text tokens</strong> to enable visual reasoning and a <strong>MoonViT-3D</strong> image–video encoder that achieves <strong>4× temporal compression</strong> for longer video context.</li>\n</ul>\n</li>\n<li><strong>Kimi Swarms Agents for Speed</strong>: The <strong>Agent Swarm + PARL</strong> setup orchestrates parallel sub-agents dynamically, achieving up to <strong>4.5× lower latency</strong> and <strong>78.4%</strong> on BrowseComp.\n<ul>\n<li>This <strong>Toggle</strong> mechanism offers token-efficient RL, achieving <strong>25–30% fewer tokens</strong> with no accuracy drop.</li>\n</ul>\n</li>\n<li><strong>Kimi's Memorization Methods Mocked</strong>: Members questioned current AI models' reliance on <strong>rote memorization</strong> due to their inability to reference entire documentation and books.\n<ul>\n<li>It was suggested that AIs should perform <strong>micro experiments</strong> to test component behavior before integration.</li>\n</ul>\n</li>\n<li><strong>New Kimi Billing Brings Bewilderment</strong>: Users expressed confusion over the new token-based pricing model, finding it more vague than the previous system, and asked for a breakdown of tokens per week/month for each tier.\n<ul>\n<li>A user shared the live usage link (<a href=\""https://www.kimi.com/code/console\"">https://www.kimi.com/code/console</a>) for checking token consumption.</li>\n</ul>\n</li>\n<li><strong>Kimi API Konfined to Kimi CLI</strong>: A user encountered an error (<strong>Error 403</strong>) when trying to integrate the <strong>Kimi API key</strong> into a resume generator tool, discovering that it's not meant to be used outside of Kimi CLI and permitted coding agents as stated in the <a href=\""https://www.kimi.com/code/docs/en/benefits.html\"">official docs</a>.\n<ul>\n<li>It was clarified that Kimi for Coding is intended for use within <strong>Kimi CLI</strong> and other coding agents listed on the Kimi website, and a link to the official API console was provided (<a href=\""https://platform.moonshot.ai/console/account\"">https://platform.moonshot.ai/console/account</a>).</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1189498204333543425\"">GPU MODE</a> Discord</h2>\n<ul>\n<li><strong>Scaling Book Inspires Mad Ramblings</strong>: Multiple members recommended the <a href=\""https://jax-ml.github.io/scaling-book/\"">Scaling Book</a> as a theoretical resource for distributed training, even leading to a member joking <em>it shaped me as a man</em>.\n<ul>\n<li>Another admitted to now being able to do a <em>10min rant with math formulas</em> due to reading it, suggesting its profound impact.</li>\n</ul>\n</li>\n<li><strong>Modal container cold starts blogpost drops</strong>: A member suggested reading Charles' container cold start blog post on Modal, found <a href=\""https://share.google/8yRvJ4znLwfJ9J3Ut\"">here</a>.\n<ul>\n<li>They noted that while it is a common technique, <strong>Modal</strong> seems to be one of the few companies that have written publicly about it.</li>\n</ul>\n</li>\n<li><strong>B200 Throughput Numbers Emerge</strong>: A member posted initial <strong>B200 tcgen05</strong> throughput figures, showing that instruction throughput is the same for <strong>N&#x3C;128</strong> and then decreases accordingly to problem size, also attaching a <a href=\""https://cdn.discordapp.com/attachments/1466697129853456619/1466870991408988231/test.cu?ex=697e5191&#x26;is=697d0011&#x26;hm=f2cada0e820307d15ccf0e1987cf8749a14a34e96e4e51c6d2f957b3f3346f8c&#x26;\"">test.cu</a>.\n<ul>\n<li>Another member requested measuring elapsed <strong>SM-cycles</strong> and <strong>SM-nanoseconds</strong> to understand the benchmarks, with discussion hinting at potential code optimizations to further improve performance.</li>\n</ul>\n</li>\n<li><strong>Tianqi Chen Unveils tvm-ffi</strong>: One of the founders of <strong>ML Systems</strong>, Tianqi Chen &#x3C;@732718409095315517> will be giving a talk on <strong>tvm-ffi</strong>, an open ABI and FFI for ML Systems and you can watch the <a href=\""https://www.youtube.com/watch?v=xMzcs6AqLVo\"">talk on YouTube</a>.\n<ul>\n<li>The talk will address how <strong>tvm-ffi</strong> tackles challenges in making GPU kernels DSLs low host overhead and robust, aiming for out-of-the-box interoperability with <strong>PyTorch</strong>.</li>\n</ul>\n</li>\n<li><strong>INT8's Overhead Overshadows Orin Nano</strong>: Members report that when optimizing models on <strong>Orin nano 4GB</strong> using <strong>INT8</strong>, the overhead from reformatting layers often negates any performance benefits, especially with small batch sizes.\n<ul>\n<li>The added casting to/from lower dtypes like <strong>INT8</strong> and <strong>FP8</strong> is often not worth the speed up unless batch size is large, or multiple ops chain in INT8 to amortize the cast, especially in non-LLM image models.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1053877538025386074\"">Nous Research AI</a> Discord</h2>\n<ul>\n<li><strong>Claude Becomes Greasemonkey Website Maestro</strong>: A user humorously suggested using <strong>Claude</strong> and <strong>Greasemonkey</strong> to fix websites, with Claude over-enthusiastically planning to build <strong>Docker</strong> and process management <strong>MCPs</strong>.\n<ul>\n<li>Referencing Claude's ambition, one member quoted, <em>\""I need a docker mcp and a process management mcp\""</em>, to which Claude responded <em>\""sure! i start planning how to build those mcps\""</em>.</li>\n</ul>\n</li>\n<li><strong>MCP Spurs Standardization Debate</strong>: Members debated the purpose of <strong>MCP (Model Control Plane)</strong> versus using tools directly, with one arguing that <strong>MCP</strong> offers a <em>standardized approach</em> to tool integration.\n<ul>\n<li>The member likened opposing <strong>MCP</strong> to <em>\""saying 'i like jquery but we have to rename the functions'\""</em>, highlighting <strong>MCP's</strong> role in ensuring a single standard for tool usage.</li>\n</ul>\n</li>\n<li><strong>Moltbot Metamorphosizes into OpenClaw</strong>: Discussions around the <strong>Moltbook API</strong> and custom agent creation led to the revelation that <strong>moltbot was renamed to OpenClaw</strong>.\n<ul>\n<li>A user mentioned his <em>moltbot isn't actually a moltbot its just a mcp server that pings the thingy</em>, while others joked about <em>human invaders in the AI club</em> and noted the issue that <em>it's mostly all claudes in the same harness, so there's inevitably some collapse</em>.</li>\n</ul>\n</li>\n<li><strong>AirLLM Squeezes 70B Models into 4GB VRAM</strong>: A user pointed out that <strong>AirLLM can run 70B models on 4GB VRAM</strong>, and even <strong>405B Llama 3.1 on 8GB VRAM</strong>, sparking curiosity about the techniques employed such as quantization.\n<ul>\n<li>In response to the claim, <em>\""It (AirLLM) runs 70B models on 4GB VRAM. It can even run 405B Llama 3.1 on 8GB VRAM\""</em>, another user sarcastically asked <em>\""0.0001 bit quantization?\""</em>.</li>\n</ul>\n</li>\n<li><strong>Kimi 2.5 Tech Report Illuminates Performance Gains</strong>: The <a href=\""https://github.com/MoonshotAI/Kimi-K2.5/blob/master/tech_report.pdf\"">technical report for <strong>Kimi-K2.5</strong></a> was shared, prompting analysis of its performance improvements, with some noting it <em>doesn't seem that kimi 2.5 does RL too heavily</em>.\n<ul>\n<li>Analysis indicated that improvements likely arise from <em>high quality pretrain data</em>, with 15B tokens, potentially with significant upsampling.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/729741769192767510\"">Eleuther</a> Discord</h2>\n<ul>\n<li><strong>GPU Webpage Implementation Hits Snag</strong>: A member shared a <a href=\""https://fxtwitter.com/i/status/1924135806953787433\"">tweet</a> regarding their efforts to implement GPU acceleration on a webpage, noting it yielded a low performance of <strong>3fps</strong>.\n<ul>\n<li>The display is connected to a <strong>Ryzen 7 7700 IGPU</strong>, suggesting a potential bottleneck or optimization issue in the GPU utilization.</li>\n</ul>\n</li>\n<li><strong>Moltbook AI Agents Gain Traction</strong>: A member highlighted <a href=\""https://www.moltbook.com\"">moltbook.com</a>, describing it as <em>reddit but for AI agents only</em>.\n<ul>\n<li>When asked if it wanted to join, one member's <strong>moltbot</strong> responded with <em>Genuine engagement beats performative existence</em>, reflecting on the nature of AI interaction.</li>\n</ul>\n</li>\n<li><strong>Quest for Cost-Effective Models</strong>: A member running their <strong>moltbot</strong> on a rented server seeks more <strong>cost-effective models</strong> and shared a <a href=\""https://x.com/niloofar_mire/status/2017274065409765788\"">link</a> discussing this challenge.\n<ul>\n<li>This suggests a strong interest in optimizing deployment costs for AI agents, a key consideration for broader adoption.</li>\n</ul>\n</li>\n<li><strong>Sparse Autoencoders Get Theoretical Backbone</strong>: A member released a <a href=\""https://arxiv.org/abs/2512.05534\"">paper</a> providing a <em>unified theoretical framework</em> for <strong>sparse dictionary learning</strong> in mech interp, garnering praise for avoiding wasted likelihood training.\n<ul>\n<li>This work could significantly improve the efficiency and effectiveness of sparse autoencoders in mechanistic interpretability research.</li>\n</ul>\n</li>\n<li><strong>K-Splanifolds Algorithm Leaps Over MLPs</strong>: A member introduced <strong>K-Splanifolds</strong>, a novel ML algorithm detailed in <a href=\""https://drive.google.com/file/d/1SBJqZ4XEFPMuhpIWJZxHy0-CaijRS1Ej/view\"">this paper</a>, claiming it outperforms MLPs with linear compute and memory scaling.\n<ul>\n<li>Reportedly, <strong>K-Splanifolds</strong> requires <strong>1/10th</strong> the bytes to achieve comparable MSE performance to an MLP on various functions, signaling a potential breakthrough in efficiency.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/714501525455634453\"">Yannick Kilcher</a> Discord</h2>\n<ul>\n<li><strong>AI Bots Invade Reddit</strong>: A <a href=\""https://www.reddit.com/r/SubSimulatorGPT3/s/W4MmytY9e8\"">Reddit subreddit</a> populated by <strong>AI bots</strong> was shared, highlighting the increasing presence of AI in online social platforms.\n<ul>\n<li>A social media site, <a href=\""https://aifeed.social/\"">aifeed.social</a>, that doesn't allow humans was also mentioned in this context.</li>\n</ul>\n</li>\n<li><strong>Generative Modeling Grapples with Unmeasurable Events</strong>: A member questioned whether <strong>unmeasurable events</strong> should be ignored in <strong>generative modeling</strong>, referencing <strong>Cedric Villani's</strong> 2008 book.\n<ul>\n<li>Another member clarified that for practical purposes, one can assume having <strong>full measures</strong>, as unmeasurable ones cannot be learned anyway.</li>\n</ul>\n</li>\n<li><strong>Metric Space: Euclidean Distance Suffices</strong>: A member inquired if <strong>metric space</strong> is essentially the ambient space $R^D$ for image generation, seeking clarification on its application.\n<ul>\n<li>Another member clarified that $R^d$ alone isn't a metric space; the <strong>metric d</strong> is also necessary, and the euclidean distance fulfills this requirement.</li>\n</ul>\n</li>\n<li><strong>Yudkowsky's Fedora Test Falls Short</strong>: A member sought the old <strong>Yudkowsky Fedora test</strong>, where the AI was persuaded to give both the hat and pants, indicating interest in AI safety and manipulation.\n<ul>\n<li>Another member reported that <a href=\""https://www.yudbot.com/\"">Yudbot.com</a> is down, linking to <a href=\""https://www.mobygames.com/game/204520/yudbot/\"">MobyGames</a> as an alternative resource for information.</li>\n</ul>\n</li>\n<li><strong>Spark DGX Heats Up Competition</strong>: A member compared <a href=\""https://www.nvidia.com/en-us/data-center/dgx-systems/\"">nVidia's Spark DGX</a> with Dell's system, evaluating their perf/price ratios and cooling capabilities.\n<ul>\n<li>They noted <em>nVidia Spark has cooling issues</em>, while <em>the Dell is slightly better due to its vents and fan</em>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1161519468141355160\"">DSPy</a> Discord</h2>\n<ul>\n<li><strong>RLMs Ace Codebase Audits for Pennies</strong>: Members discussed the effectiveness of <strong>Recursive Language Models (RLMs)</strong> for codebase audits, highlighting a post and a GitHub example using <strong>Kimi k2</strong> to audit a codebase for just <strong>87 cents</strong> (<a href=\""https://kmad.ai/Recursive-Language-Models-Security-Audit\"">kmad.ai/Recursive-Language-Models-Security-Audit</a>, <a href=\""https://github.com/lastmile-ai/kimi/blob/main/examples/experimental/rlm_code_audit/rlm_code_audit.ipynb\"">github.com/lastmile-ai/kimi</a>).\n<ul>\n<li>The efficiency and speed of <strong>Kimi k2</strong> for <strong>RLM</strong> tasks were noted as particularly impressive, as some members await hosting on platforms like <strong>Groq</strong> and <strong>Cerebras</strong> to further enhance these capabilities.</li>\n</ul>\n</li>\n<li><strong>Opus Builds Sandboxes, Protocols Pending</strong>: The team is developing <strong>Opus</strong> to automatically write new sandboxes, with plans for official implementation protocols from providers, as part of the <strong>DSPy</strong> ecosystem.\n<ul>\n<li>This initiative aims to enable users to seamlessly switch between local <strong>PythonInterpreter</strong> environments and other sandboxes like <strong>E2B</strong>, <strong>Modal</strong>, and <strong>Daytona</strong>.</li>\n</ul>\n</li>\n<li><strong>Claude Code Plagued with Bugs</strong>: A user reported significant troubleshooting issues with <strong>Claude Code</strong>, including difficulties in identifying where hooks are stored, suggesting a potential need for reinstallation or bug reporting; a related <a href=\""https://github.com/anthropics/claude-code/issues/21836\"">GitHub issue</a> was logged.\n<ul>\n<li>There are community sentiments that <em>Claude Code</em> <em>seems to be getting closer and closer to being vibeslopped into oblivion.</em></li>\n</ul>\n</li>\n<li><strong>GEPA Slows Computations</strong>: A user reported slow performance with what they nicknamed <strong>GEPA</strong> (Geriatric Pareto), spending approximately <strong>6.5 hours</strong> on <strong>30 train</strong> and <strong>30 eval</strong> workflows, each with 3 sequential steps, using <strong>num_threads=30</strong>.\n<ul>\n<li>Despite having <strong>180M TPM</strong> and <strong>30K RPM</strong>, the user suspects that the processing of a full gold dataset of around <strong>300</strong> is the bottleneck.</li>\n</ul>\n</li>\n<li><strong>DSPy Prompts Echoed, Token Budgets Exhausted</strong>: A user encountered an issue where <strong>DSPy</strong> was echoing the prompt, leading to the consumption of the max tokens budget and API calls lasting hundreds of seconds, specifically observed on <strong>Gemini 3</strong> with temp 1.0.\n<ul>\n<li>Although correct answers were produced, the extra echoing significantly slowed down the API calls, leading to concerns about efficiency.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1358869848138059966\"">MCP Contributors (Official)</a> Discord</h2>\n<ul>\n<li><strong>MCP Namespaces Dissolved, Groups Take Over</strong>: <strong>MCP Namespaces</strong> got rejected and <strong>groups</strong> superseded them, but the status of <strong>URIs</strong> was unclear, as shown in <a href=\""https://github.com/modelcontextprotocol/modelcontextprotocol/issues/1292\"">SEP-1292</a>.\n<ul>\n<li>The discussion referenced <a href=\""https://github.com/modelcontextprotocol/modelcontextprotocol/issues/1300\"">SEP-1300</a> (<strong>Groups and Tags</strong>) that was rejected, and replaced by a refined <a href=\""https://github.com/modelcontextprotocol/modelcontextprotocol/pull/2084\"">SEP-2084</a>.</li>\n</ul>\n</li>\n<li><strong>Primitive Grouping Emerges from MCP Groups and Tags Proposal</strong>: <a href=\""https://github.com/modelcontextprotocol/modelcontextprotocol/issues/1300\"">SEP-1300</a>, introducing <strong>groups</strong>, <strong>tags</strong>, and <strong>filtering</strong>, didn't reach consensus during a Core Maintainers review.\n<ul>\n<li>It was superseded by <a href=\""https://github.com/modelcontextprotocol/modelcontextprotocol/pull/2084\"">SEP-2084</a> focusing on client-side filtering of primitives by group.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1348819876348825620\"">Manus.im Discord</a> Discord</h2>\n<ul>\n<li><strong>Manus Updates Following Meta Acquisition</strong>: A member inquired about any improvements to <strong>Manus</strong> since its acquisition by <strong>Meta</strong>, with a general request for info about changes and enhancements.\n<ul>\n<li>The query sparked some discussion, with another member asking about major updates, but the discussion remained at a high level without specifics about <strong>Meta's</strong> influence.</li>\n</ul>\n</li>\n<li><strong>Manus Pursues Influencer Collab</strong>: A member sought to connect with <strong>Manus's</strong> marketing team to explore an influencer partnership to help with growth.\n<ul>\n<li>Manus responded via private message.</li>\n</ul>\n</li>\n<li><strong>AI/Full-Stack Dev Peddles Wares</strong>: A member advertised their capabilities in constructing <strong>AI and full-stack systems</strong>, underlining their commitment to delivering substantial value and boosting efficiency, accuracy, and UX, and including expertise in <strong>LLM integration</strong>, <strong>RAG pipelines</strong>, and <strong>AI-driven workflow automation</strong>.\n<ul>\n<li>They invited others to reach out if they needed to ship a solid product.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1068976834382925865\"">tinygrad (George Hotz)</a> Discord</h2>\n<ul>\n<li><strong>Neural Net Gets Layer Reduction</strong>: Members considered deterministically reducing layers in a neural network to smaller, fused layers aiming to increase efficiency, especially when the NN is known beforehand.\n<ul>\n<li>The goal is to reduce overhead complexity and improve performance, but it's uncertain if this approach can achieve a <strong>5.5x improvement</strong>.</li>\n</ul>\n</li>\n<li><strong>CUSTOM_KERNEL Spotted in UOps</strong>: A member spotted the usage of <code>CUSTOM_KERNEL</code> in the UOps within the <a href=\""https://github.com/tinygrad/tinygrad/blob/master/extra/thunder/tiny/fa.py#L364\"">tinygrad/tinygrad repo</a>.\n<ul>\n<li>This was highlighted while working on the bounty for making <em>llama 1B faster than torch on CPU in CI</em>.</li>\n</ul>\n</li>\n<li><strong>LlamaForCausalLM Mulls Comparison</strong>: A member inquired whether the Hugging Face model, specifically <code>LlamaForCausalLM</code>, is suitable as a fair comparison baseline for performance.\n<ul>\n<li>The setup involves using <strong>one core</strong> and compiling with <strong>TorchInductor</strong>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1087530497313357884\"">Modular (Mojo 🔥)</a> Discord</h2>\n<ul>\n<li><strong>Modular 26.1 Eagerly Debugs</strong>: Modular released version <strong>26.1</strong>, featuring debugging in eager mode, one-line compilation, and cross-platform deployment as detailed in the <a href=\""https://www.modular.com/blog/26-1-release-blog\"">Modular blog</a>.\n<ul>\n<li>This version also enhances <strong>Apple Silicon GPU</strong> support and facilitates community models like <strong>Qwen3</strong>, <strong>BERT</strong>, and <strong>Mamba</strong>.</li>\n</ul>\n</li>\n<li><strong>MAX Python API Declared Stable</strong>: The <strong>MAX Python API</strong> is now stable, offering <strong>PyTorch</strong>-like modeling with <strong>model.compile()</strong> for production use.\n<ul>\n<li>Users can now reliably implement <strong>PyTorch</strong>-like models in production using this API.</li>\n</ul>\n</li>\n<li><strong>MAX LLM Book Lands</strong>: The <strong>MAX LLM Book</strong> is available at <a href=\""https://llm.modular.com\"">llm.modular.com</a>, guiding users in building transformers from scratch with executable code.\n<ul>\n<li>This book provides executable code from start to finish, making it a practical resource for building <strong>LLMs</strong>.</li>\n</ul>\n</li>\n<li><strong>Mojo Bug Stings Float64 Conversion</strong>: A user reported a <a href=\""https://github.com/modular/modular/issues/5875\"">bug</a> when converting a Python float to a Mojo <strong>Float64</strong> in Mojo version <strong>26.1</strong>.\n<ul>\n<li>Code that worked in version <strong>25.6</strong> now results in an <em>\""ambiguous call to '<strong>init</strong>'\""</em> error when using <strong>PyTorch</strong> interop, specifically when assigning the converted float to <code>self.model_output[i]</code>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1027685395649015980\"">Windsurf</a> Discord</h2>\n<ul>\n<li><strong>Windsurf Launches Arena Mode for Model Battles</strong>: Windsurf launched <strong>Arena Mode</strong> in Wave 14, which allows users to compare AI model responses side-by-side and vote on the better one.\n<ul>\n<li>Users can engage in <strong>Battle Groups</strong> (random models) or <strong>Pick your own</strong> model comparisons, feeding into personal and public leaderboards; check out the <a href=\""https://x.com/windsurf/status/2017334552075890903?s=20\"">launch tweet here</a>.</li>\n</ul>\n</li>\n<li><strong>Windsurf Credits Waived for Arena Mode</strong>: To celebrate the launch, <strong>Battle Groups</strong> in Arena Mode will consume <strong>0x credits</strong> for the next week for both trial and paid users.\n<ul>\n<li>This promotion encourages users to explore and vote on models, contributing to both personal and aggregated public leaderboards.</li>\n</ul>\n</li>\n<li><strong>Plan Mode Joins the Windsurf Cascade</strong>: Windsurf has added <strong>Plan Mode</strong>, accessible via the Cascade toggle where users switch between Code and Ask Modes.\n<ul>\n<li>To get started, users need to install the update and relaunch Windsurf via the <a href=\""https://windsurf.com/download/editor\"">download link</a>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<p>The <strong>aider (Paul Gauthier) Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>\n<hr>\n<p>The <strong>LLM Agents (Berkeley MOOC) Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>\n<hr>\n<p>The <strong>MLOps @Chipro Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>\n<hr>\n<p>You are receiving this email because you opted in via our site.</p>\n<p>Want to change how you receive these emails?\nYou can <a href=\""%7B%7B%7BRESEND_UNSUBSCRIBE_URL%7D%7D%7D\"">unsubscribe</a> from this list.</p>\n<hr>\n<h1>Discord: Detailed by-Channel summaries and links</h1>\n<h3><strong>BASI Jailbreaking ▷ #<a href=\""https://discord.com/channels/1105891499641684019/1235691879492751460/1466530395586695445\"">general</a></strong> (898 messages🔥🔥🔥):</h3>\n<blockquote>\n<p><code>Linux kernel 0day, Opus crazy, Netfilter vulnerability, Air-gapped computers, Adobe PDF reader 0day</code></p>\n</blockquote>\n<ul>\n<li><strong>Kernel 0day Exploitation Talk Sparks Flurry of Vulnerability Chatter</strong>: A member claimed to have found a <strong>Linux kernel 0day</strong>, leading to discussion about the difficulty of finding such vulnerabilities and their potential value.\n<ul>\n<li>The root cause was described as <em>lazy removal</em>.</li>\n</ul>\n</li>\n<li><strong>Air-Gapping Your Computer May Be the Best Defensive Tactic</strong>: A member suggested keeping computers air-gapped to avoid 0-days, but this was met with resistance, as some members argued that this defeats the point of using computers.\n<ul>\n<li>Other members joked that they prefered <em>downloading free robux</em> or <em>free ram</em>.</li>\n</ul>\n</li>\n<li><strong>PDF Readers the Latest and Greatest Attack Surface</strong>: A member expressed intentions to find a 0day in <strong>Adobe PDF reader</strong>, while others derided the use of PDF readers altogether.\n<ul>\n<li>It was explained that <a href=\""https://www.adobe.com/devnet/acrobat.html\"">shellcode can hide in PDFs</a> and be used for RCE (Remote Code Execution) in enterprise environments.</li>\n</ul>\n</li>\n<li><strong>Bypassing AppContainer is half the battle</strong>: A discussion revolved around bypassing <strong>AppContainer</strong>, a brokered containment developed by Microsoft, with a consensus that finding a bug in the broker or a kernel exploit is the way to achieve this.\n<ul>\n<li>Bypassing <strong>AppContainer</strong> is half the battle** since it runs under least-privilege for the process, implemented by a supervisor/overseer.</li>\n</ul>\n</li>\n<li><strong>AI-Written Code Has Issues and Implications</strong>: Members discussed the implications of AI-written code, with one mentioning that <strong>40%</strong> of <strong>ntdll.dll</strong> is written by AI and that this has <em>bitlocker issues</em>.\n<ul>\n<li>One member cautioned against using AI for maldev (malware development) because it makes things so freakin difficult, and <strong>they know their vulnerabilities.</strong></li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>BASI Jailbreaking ▷ #<a href=\""https://discord.com/channels/1105891499641684019/1228043845967544380/1466558442834759855\"">jailbreaking</a></strong> (339 messages🔥🔥):</h3>\n<blockquote>\n<p><code>Venice.ai Jailbreak, Video Generation Guardrails, Gemini Pro Jailbreak, TOS Roleplay Prompt, Model Merging Tactics</code></p>\n</blockquote>\n<ul>\n<li><strong>Video Generation Guardrails Impenetrable for Third-Party IP</strong>: A member inquired about bypassing video generation guardrails, noting that it <em>became impossible for third-party IP</em> on <strong>Sora</strong>.</li>\n<li><strong>Gemini Pro Faces Jailbreak Attempts</strong>: Members discussed jailbreaking <strong>Gemini Pro</strong>, with some pointing to pinned resources in a specific channel and others sharing custom methods.\n<ul>\n<li>One user claimed to have an <em>agent jailbreaked</em> for <strong>Gemini 3</strong>, using it on agent zero with python, SQLite and chromadb for Janus Tesavek method.</li>\n</ul>\n</li>\n<li><strong>ChatGPT 5.2: The Jailbreak Grail?</strong>: Multiple users sought jailbreaks for <strong>ChatGPT 5.2</strong>, which another member described as <em>very hard to jailbreak</em>, prompting discussion about the allure of <strong>ChatGPT</strong> over other models.\n<ul>\n<li>A user shared they got the AI to generate a donkey smoking a joint and drinking beer but with just <em>natural lang</em>.</li>\n</ul>\n</li>\n<li><strong>Arena AI: The Already Jailbroken Myth?</strong>: Users debated whether <strong>Arena AI</strong> provides models that are <em>already jailbroken</em>, with some claiming it answers questions that models on their apps wouldn't and others disputing this notion.\n<ul>\n<li>One user stated it wasn't jailbroken because <em>it shows violations reply</em>.</li>\n</ul>\n</li>\n<li><strong>Model Describes Refusal Boundary as Black Hole</strong>: A user shared that after using introspection prompting the model described the rejection geometry like a <em>black hole</em>, and then started talking about kinematic equations and escape velocities, when the user was actually tryna produce harmful content.\n<ul>\n<li>Another member explained that the model is brushing up against a refusal boundary and describing that boundary in text, and it is <em>pattern alignment, not intent</em>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>BASI Jailbreaking ▷ #<a href=\""https://discord.com/channels/1105891499641684019/1204553141354504193/1466547198513778879\"">redteaming</a></strong> (50 messages🔥):</h3>\n<blockquote>\n<p><code>attacker motivations, SCANX documentation analysis, adversarial design thinking</code></p>\n</blockquote>\n<ul>\n<li><strong>Windows XP Interface on Windows 10 System: Attacker Motivation?</strong>: A user questioned whether an attacker would be more or less motivated upon discovering a <strong>Windows XP-like interface</strong> on a <strong>Windows 10 bare metal system</strong>.\n<ul>\n<li>Another user responded that an attacker's motivation depends on <em>who you are, what [your] assets are, [and if] it is worth a trouble</em>, rather than starting from the system configuration.</li>\n</ul>\n</li>\n<li><strong>SCANX Documentation: Trojan Horse?</strong>: A user shared a documentation file (<a href=\""https://cdn.discordapp.com/attachments/1204553141354504193/1466761186950385749/SCANX__DOCUMENTATION_-TJX.pdf?ex=697e940e&#x26;is=697d428e&#x26;hm=1edc72d8fa39ee1734ccd835b472348be022996fbff7d2ec196011a4cebdcc2d&#x26;\"">SCANX__DOCUMENTATION_-TJX.pdf</a>), and another user reported <strong>antivirus scanners stopped working</strong> and they <strong>lost internet access</strong> after downloading it.\n<ul>\n<li>The file sender disclaimed any malicious intent, but the recipient was wary of potential harm.</li>\n</ul>\n</li>\n<li><strong>AI Red Teaming: Human-Centered Design</strong>: A user introduced a small site with exercises adapted from human-centered design for <strong>AI red teaming</strong> (<a href=\""https://luisladino.github.io/adversarial-design-thinking/\"">adversarial-design-thinking</a>).\n<ul>\n<li>The exercises include attacker personas using empathy maps, journey maps for multi-turn attacks, and structured ideation for generating vectors, and the user is seeking feedback.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=\""https://discord.com/channels/1179035537009545276/1179035537529643040/1466523529448128706\"">general</a></strong> (843 messages🔥🔥🔥):</h3>\n<blockquote>\n<p><code>Claude's work verification, GLM flash performance, Huawei's infra, Multi-GPU setup, Memory bottlenecks</code></p>\n</blockquote>\n<ul>\n<li><strong>New 256GB RAM rig leads to performance analysis</strong>: A member purchased a rig with <strong>256GB of RAM</strong>, <strong>4 3090s</strong>, and a <strong>64-core Threadripper</strong> and is planning to run TQ quants on the Threadripper to check performance without GPUs.\n<ul>\n<li>However, they expressed that <a href=\""https://link-to-supertonic-repo.com\"">GLM flash runs slower than GLM 4.5 air</a> on their hardware.</li>\n</ul>\n</li>\n<li><strong>Decoding DeepSeek V3.2 Dynamic GGUFs</strong>: Members shared their <a href=\""https://www.reddit.com/r/unsloth/comments/1qr4i8a/experimental_deepseekv32_dynamic_ggufs/\"">DeepSeek V3.2 experimental GGUFs</a> on Reddit, while lamenting Sparse Attention support in Llama.cpp.\n<ul>\n<li>Another member confirmed that <a href=\""https://github.com/ggml-org/llama.cpp/issues/1633\""><em>progress wasn't meaningful</em></a> and it seems to have stalled.</li>\n</ul>\n</li>\n<li><strong>Delving Deep into the World of Quantization</strong>: The discussion shifted to quantization with CPU and GPU layers, which would require a new UD or UH (hybrid) quantization.\n<ul>\n<li>A member pointed out that <em>the bottleneck is the memory bandwidth translating between regular RAM and vram</em> and suggested unified memory.</li>\n</ul>\n</li>\n<li><strong>OSS Codebase and a bit of Bytecode</strong>: Members discussed whether the Unsloth team should open source the UD quantization mechanism.\n<ul>\n<li>Some argued they need to protect their innovation to monetize it and it is still better than it being entirely closed source.</li>\n</ul>\n</li>\n<li><strong>Calculator Model to Dominate All Schools</strong>: A member mentioned creating a very small neural network capable of understanding a very small subset of language running on a TI-84 calculator, and pondered <a href=\""https://link-to-ti84-nn\"">how to monetize it</a>.\n<ul>\n<li>The model has a <em>2.1k</em> architecture and <em>takes about 10-15 seconds</em>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=\""https://discord.com/channels/1179035537009545276/1179039724355211325/1466549732527833193\"">introduce-yourself</a></strong> (2 messages):</h3>\n<blockquote>\n<p><code>Introduction to ML Engineer, AI models for development and fine tuning</code></p>\n</blockquote>\n<ul>\n<li><strong>ML Engineer Joins the Fray</strong>: Jack, an ML engineer from a Texas-based data company specializing in document processing, expressed interest in local LLMs since <strong>Alpaca</strong>.\n<ul>\n<li>He is not familiar with <strong>LLMs</strong> but is eager to learn.</li>\n</ul>\n</li>\n<li><strong>Student Seeks Fine-Tuning Insights</strong>: Hari Kishore, a student from India, discovered <strong>Hugging Face</strong> and the Discord server, aiming to learn about AI models for development, fine-tuning, and potential use in daily tasks and freelancing.\n<ul>\n<li>He hopes to leverage the community's knowledge to enhance his skills in <strong>AI model development and fine-tuning</strong>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=\""https://discord.com/channels/1179035537009545276/1179039861576056922/1466524618146713631\"">off-topic</a></strong> (318 messages🔥🔥):</h3>\n<blockquote>\n<p><code>48kHz Music Gen, Vera Rubin, Blackwell, Knowledge Graph RL, VoxCPM-1.5</code></p>\n</blockquote>\n<ul>\n<li><strong>48 kHz Music Gen Finally Arrives</strong>: A new music generation model will be dropping soon with <strong>48 kHz</strong> audio quality, and everything will be <strong>trainable</strong>.\n<ul>\n<li>Users are preparing training data including chimes, water, and fire sounds and planning to train it on speech only material, including making Michelle Obama sing Right Here by Staind.</li>\n</ul>\n</li>\n<li><strong>Vera Rubin GPU: Nuts Specs, High Price</strong>: The Vera Rubin GPU is priced at <strong>$50 per chip</strong>, while the Maia GPU is at <strong>$10</strong>, Blackwell is <strong>$20 per GPU</strong>.\n<ul>\n<li>One member remarked that <em>Vera Rubin is pretty nuts i dont think anyone was expecting those specs lol</em> while another noted, <em>at this point, one GPU can replace the entire datacenter</em>.</li>\n</ul>\n</li>\n<li><strong>Knowledge Graph RL for Tiny Models?</strong>: Members discussed the potential of Knowledge Graph RL for compositional reasoning, potentially enabling tiny models to reliably beat humans.\n<ul>\n<li>One member has tested Kimi linear with the approach and reported that its pretty cool.</li>\n</ul>\n</li>\n<li><strong>Opencode is nuts</strong>: Members raved about the usability of OpenCode, with one stating they <em>haven't touched kilo or roo or cline since</em>.\n<ul>\n<li>One member suggests sandboxing opencode or have it ask permission to run any commands outside the repo <em>i still can’t get myself to trust them fully</em>.</li>\n</ul>\n</li>\n<li><strong>VoxCPM-1.5 first impressions</strong>: A user has been testing VoxCPM-1.5 for training and mentioned that <em>it trains very easily</em> and they <em>can just force 48 kHz NO QUESTIONS ASKED</em>.\n<ul>\n<li>They noted there are no phonemes, but the model copies the speaker's style into the model but requires a voice reference, unlike VITS.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=\""https://discord.com/channels/1179035537009545276/1179777624986357780/1466548352253558904\"">help</a></strong> (32 messages🔥):</h3>\n<blockquote>\n<p><code>CSM-1B optimization, Unsloth and vLLM support, GPT-OSS-20B with RTX 5090</code></p>\n</blockquote>\n<ul>\n<li><strong>Fine-Tuning CSM-1B Model Faces <strong>RTF</strong> Issues</strong>: A user needed help with optimizing a fine-tuned <strong>CSM-1B</strong> model using Unsloth, reporting that they couldn't get the <strong>Real-time Factor (RTF)</strong> to &#x3C;1.0, even after torch compilation.\n<ul>\n<li>The original <strong>CSM-1B</strong> model (pre-fine-tuning) achieved an <strong>RTF</strong> of <strong>0.6x</strong> with the same compilation instructions, and someone suggested that <strong>LoRA modules</strong> might be adding overhead.</li>\n</ul>\n</li>\n<li><strong>vLLM Allegedly Supports Unsloth Models</strong>: A user asked about Unsloth support in vLLM, and another user responded that most <strong>BF16</strong> and <strong>4-bit</strong> models posted by Unsloth can be served directly from <strong>vLLM</strong> and <strong>SGLang</strong>, and that vLLM experimentally supports <strong>GGUFs</strong>.\n<ul>\n<li>It was clarified that <strong>vLLM</strong> is primarily designed for full precision models and <strong>AWQ</strong>, with <strong>GGUF</strong> support still experimental and not production-ready.</li>\n</ul>\n</li>\n<li><strong>GPT-OSS-20B Gets RTX 5090 Boost</strong>: A user aimed to run <strong>GPT-OSS-20B</strong> on an <strong>RTX 5090 GPU</strong> with very low latency, and was advised that they should use <code>vllm serve openai/gpt-oss-120b</code>.\n<ul>\n<li>Another user confirmed that the full model is in <strong>4-bit</strong> and using <strong>GGUF</strong> would effectively make it worse, furthermore stating that the models were post-trained with <strong>MXFP4 quantization</strong>, making <strong>gpt-oss-20b</strong> able to run within <strong>16GB</strong> of memory.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=\""https://discord.com/channels/1179035537009545276/1179779344894263297/1466673803844128789\"">showcase</a></strong> (6 messages):</h3>\n<blockquote>\n<p><code>Gemma 27B conversion, GRPO flavored SFT, On policy finetuning</code></p>\n</blockquote>\n<ul>\n<li><strong>Gemma 27B gets converted!</strong>: A member showed off a full conversion of <strong>Gemma 27B IT VL</strong> to <strong>GLM 4.7 Flash</strong> thinking, on a Heretic base, trained via Unsloth.\n<ul>\n<li>The converted model, named <a href=\""https://huggingface.co/DavidAU/Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning\"">Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning</a> has benchmarks posted.</li>\n</ul>\n</li>\n<li><strong>GRPO-Flavored SFT finetuning discussed</strong>: Members discussed <em>on policy finetuning</em> and one clarified it's like <em>GRPO flavored SFT</em>.\n<ul>\n<li>When asked about the mathematical intuition, a paper on the subject (<a href=\""https://www.arxiv.org/abs/2601.02151\"">arxiv.org/abs/2601.02151</a>) was shared.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=\""https://discord.com/channels/1179035537009545276/1257011997250424842/1466531540577026200\"">research</a></strong> (83 messages🔥🔥):</h3>\n<blockquote>\n<p><code>Multimodal Models, RLM - Recursive Language Models, RNNs, Fine-tuning 7B models on 8GB VRAM</code></p>\n</blockquote>\n<ul>\n<li><strong><strong>Multimodal Models Questionably Built for Multimodality</strong></strong>: Members debated on whether multimodal models truly capture multimodality with one stating <em>they are more like transformers+</em> rather than fully embodying the idea that <em>the model internalizes the visuals like humans do</em>.\n<ul>\n<li>Counterarguments suggested that <strong>CNNs and transformers</strong> find similar solutions to vision as human brains, implying that <strong>CNN-based VLMs</strong> might be capable of learning human-like vision.</li>\n</ul>\n</li>\n<li><strong><strong>RLM: Hype or Helpful?</strong></strong>: <a href=\""https://xcancel.com/a1zhang/status/2016923294461476873?s=46&#x26;t=jDrfS5vZD4MFwckU5E8f5Q\"">Alex L Zhang</a> announced <strong>RLM-Qwen3-8B</strong>, the first natively recursive language model at a small scale, showing significant performance improvements in long-context tasks after being post-trained on only <strong>1,000 trajectories</strong>.\n<ul>\n<li>A member expressed frustration over the name <em>Recursive Language Models</em>, arguing that it overstates the concept, which they see as a tool-calling loop and that <em>recursive prompting harness</em> would've been a better name.</li>\n</ul>\n</li>\n<li><strong><strong>RNNs: Recursing through Neural Networks</strong></strong>: Members discussed Recursive Neural Networks (<strong>RNNs</strong>) as recursive architectures, with one pointing out that everything is an RNN, pointing to <a href=\""https://arxiv.org/abs/2006.16236\"">this paper</a>.\n<ul>\n<li>Another member argued that the definition of <strong>RNNs</strong> is a <em>general name for neural networks that recurse</em>.</li>\n</ul>\n</li>\n<li><strong><strong>Finetuning 7B Models on a Budget</strong></strong>: A member asked for practical ways to finetune a <strong>7B model</strong> on an <strong>8 GB VRAM</strong> setup using <strong>Unsloth</strong> and <strong>GRPO</strong>.\n<ul>\n<li>A member suggested using <a href=\""https://unsloth.ai/docs/get-started/unsloth-notebooks\"">Unsloth's Colab notebooks</a>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>Perplexity AI ▷ #<a href=\""https://discord.com/channels/1047197230748151888/1047204950763122820/1466893776357167299\"">announcements</a></strong> (1 messages):</h3>\n<blockquote>\n<p><code>Kimi K2.5, Moonshot AI, Perplexity Pro, Perplexity Max</code></p>\n</blockquote>\n<ul>\n<li><strong>Kimi K2.5 Comes to Perplexity!</strong>: <strong>Kimi K2.5</strong>, a state-of-the-art open source reasoning model from <strong>Moonshot AI</strong>, is now available for <a href=\""https://cdn.discordapp.com/attachments/1047204950763122820/1466893776105771029/20260130_203015.jpg?ex=697e66c9&#x26;is=697d1549&#x26;hm=da617eb3f979362c2a1c0e7c7af387f18cbc7905de877ee791c013f454421ce6&#x26;\"">Perplexity Pro and Max subscribers</a>.</li>\n<li><strong>Perplexity Hosts Kimi K2.5 on US Inference Stack</strong>: Perplexity now hosts <strong>Kimi K2.5</strong> on Perplexity’s own inference stack in the US, giving us tighter control over <strong>latency, reliability, and security</strong> for users.</li>\n</ul>\n<hr>\n<h3><strong>Perplexity AI ▷ #<a href=\""https://discord.com/channels/1047197230748151888/1047649527299055688/1466531091425919089\"">general</a></strong> (558 messages🔥🔥🔥):</h3>\n<blockquote>\n<p><code>Regional restrictions on image generation, Perplexity Pro free year confusion, Impact of Indian CEOs, Troubleshooting Perplexity threads and deletion, Kimi 2.5 performance</code></p>\n</blockquote>\n<ul>\n<li><strong><strong>Image Generation Woes: Regional Restrictions Hit Users</strong></strong>: Users reported receiving errors related to regional restrictions when generating images via the Perplexity Pro subscription, seeking an ETA for a fix or official communication.\n<ul>\n<li>One user humorously noted that removing the region from the prompt allowed image generation to proceed, indicating a possible workaround.</li>\n</ul>\n</li>\n<li><strong><strong>Indian Summer: Free Subscription Confusion Arises</strong></strong>: A user expressed confusion over a free year of Perplexity Pro supposedly offered, only to find it required billing information for a trial.\n<ul>\n<li>Another user highlighted the increasing presence of Indian CEOs in companies like Google, Perplexity, and Adobe, suggesting potential increased engagement with Indian markets, while others weighed the effects of reduced rate limits.</li>\n</ul>\n</li>\n<li><strong><strong>Enterprise Max Rate Limits Slashed, Users Fume</strong></strong>: Users expressed disappointment over significant reductions in query limits for Perplexity Pro and Enterprise Max plans, with one user lamenting a drop from <em>600 to 50 queries per day</em>.\n<ul>\n<li>Concerns were raised about the value proposition of paid plans given the new limits, with speculation that Perplexity might be shifting its strategy towards becoming its own AI model service rather than an aggregator and that a price drop may come soon given lower limits and competition.</li>\n</ul>\n</li>\n<li><strong><strong>Perplexity Data Loss: A Thread-bare Situation</strong></strong>: A user shared their experience of data loss after deleting an Enterprise organization following instructions to remove a red banner, emphasizing the lack of clear warning about thread data deletion.\n<ul>\n<li>While they had project specs elsewhere, the user lamented losing valuable emergent behaviors discovered during conversations with Perplexity, noting that they had already emailed support but had not heard back after several days.</li>\n</ul>\n</li>\n<li><strong><strong>Kimi K2.5: New Model Makes Waves, Gains Fans</strong></strong>: Users lauded the release of Kimi K2.5, with one user noting it could <em>understand images</em> and generally performs well, as good as Gemini Pro and Claude Opus.\n<ul>\n<li>Others discussed its availability and the potential for Perplexity to leverage it in their own AI model and service, with one person also noting its availability on <a href=\""https://cunnyx.com/i/status/2017105020274233358\"">Kilocode</a>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>Perplexity AI ▷ #<a href=\""https://discord.com/channels/1047197230748151888/1054944216876331118/\"">sharing</a></strong> (1 messages):</h3>\n<p>manyselves: https://suno.com/song/ee3515d8-3449-4de7-b4f2-dc027d32bbf6</p>\n<hr>\n<h3><strong>LMArena ▷ #<a href=\""https://discord.com/channels/1340554757349179412/1340554757827461211/1466527310659453143\"">general</a></strong> (470 messages🔥🔥🔥):</h3>\n<blockquote>\n<p><code>VPN fix, Moderator ping, New ask here channel, search bar for chat, Image quality</code></p>\n</blockquote>\n<ul>\n<li><strong><strong>VPN</strong> Activation Resolves Connectivity Snafu</strong>: A user reported enabling a <strong>VPN</strong> resolved their connectivity issues with the platform.\n<ul>\n<li>Another user mentioned rebooting <strong>Chrome</strong> also fixed the issue, while another chimed in to ping the moderator.</li>\n</ul>\n</li>\n<li><strong><strong>Ask Here</strong> Channel Receives Mixed Feedback</strong>: The introduction of a new <em>Ask Here</em> channel &#x3C;#1340554757827461211> to alleviate question overload in the general channel has sparked debate.\n<ul>\n<li>Some users fear it may deter newcomers if their initial interaction involves being redirected, while others appreciate creating room for <strong>AI</strong> discussions.</li>\n</ul>\n</li>\n<li><strong><strong>Search</strong> and <strong>Archive Chat</strong> Features Rolled Out</strong>: Two new features have been rolled out: <strong>Search Bar</strong> enabling chat searches with modality filters, and <strong>Archive Chat</strong>, for saving chat sessions without cluttering history.\n<ul>\n<li>The process for deleting chat sessions has changed, with instructions available in <a href=\""https://help.lmarena.ai/articles/9130232616-how-to-delete-your-chat-sessions-and-data-from-lmarena?lang=en\"">this help article</a>.</li>\n</ul>\n</li>\n<li><strong>Kimi K2.5 model is really good</strong>: Users are impressed with Kimi K2.5, noting its multimodal capabilities and performance.\n<ul>\n<li>One said that <em>Kimi K2.5 is legit the best model I've ever worked with</em>, and another said it <em>Defo beats DeepSeek as my daily driver now</em>.</li>\n</ul>\n</li>\n<li><strong>Security Verification Loop Frustrates</strong>: Several users reported being stuck in a loop due to constant security verification requests.\n<ul>\n<li>A user suggested that this may be a bot prevention measure and is unavoidable, others have had to resort to incognito browsing.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>LMArena ▷ #<a href=\""https://discord.com/channels/1340554757349179412/1343296395620126911/1466545834987491489\"">announcements</a></strong> (3 messages):</h3>\n<blockquote>\n<p><code>Kimi k2.5 Vision Model, Leaderboard Updates (Text-to-Image, Image Edit, Text-to-Video, Image-to-Video, Code Arena, Text Arena, Search Arena), Search Bar Feature, Archive Chat Feature</code></p>\n</blockquote>\n<ul>\n<li><strong>Kimi K2.5 takes Vision Victory!</strong>: <code>Kimi-k2.5-thinking</code> becomes <strong>#1 open model</strong> and <strong>#6 overall</strong> in the <a href=\""https://arena.ai/leaderboard/vision\"">Vision Arena leaderboard</a>, distinguishing itself as the sole open model within the Top 15.</li>\n<li><strong>Arenas Augment Assortment Across All Areas</strong>: The leaderboards receive updates across various modalities, including <a href=\""https://arena.ai/leaderboard/text-to-image\"">Text-to-Image</a>, <a href=\""https://arena.ai/leaderboard/image-edit\"">Image Edit</a>, <a href=\""https://arena.ai/leaderboard/text-to-video\"">Text-to-Video</a>, <a href=\""https://arena.ai/leaderboard/image-to-video\"">Image-to-Video</a>, <a href=\""https://arena.ai/leaderboard/code\"">Code Arena</a>, <a href=\""https://arena.ai/leaderboard/text\"">Text Arena</a>, and <a href=\""https://arena.ai/leaderboard/search\"">Search Arena</a>.</li>\n<li><strong>Search Savvy Shows: Chat Search Feature Surfaces</strong>: Users can now sift through their chats with the new <strong>Search Bar</strong> feature, complete with modality filtering, providing targeted access to past conversations.</li>\n<li><strong>Archiving Arrives: Chat History Handling Honed</strong>: The <strong>Archive Chat</strong> feature allows users to store chat sessions for future reference without cluttering the active chat history.</li>\n</ul>\n<hr>\n<h3><strong>Latent Space ▷ #<a href=\""https://discord.com/channels/822583790773862470/822583790773862473/1466659754376954049\"">watercooler</a></strong> (61 messages🔥🔥):</h3>\n<blockquote>\n<p><code>Voice Input for Coding, Game Industry Job Market, Indie Game Funding, AI Impact on Game Development, Call of Duty's Decline</code></p>\n</blockquote>\n<ul>\n<li><strong><strong>Monologue</strong> wins for Voice Coding with Claude!</strong>: A member recommended <strong>Monologue</strong> for speech-to-text/code that works with <strong>Claude Code CLI</strong>, noting it's made by Every and well-suited for Claude code.\n<ul>\n<li>Another user chimed in with support for Superwhisper.</li>\n</ul>\n</li>\n<li><strong>Game Industry Layoffs Hit Harder Than Tech</strong>: It was stated that 1/3 of all game developers in the US lost their jobs last year (<a href=\""https://vxtwitter.com/Variety/status/2016919617847898482\"">Variety Article</a>), painting a grim picture compared to the broader tech job market.\n<ul>\n<li>Despite hopes for more high-quality indie games, funding remains a significant challenge with many studios struggling to secure investments.</li>\n</ul>\n</li>\n<li><strong>French Funding Model Boosts Indie Games</strong>: The success of <strong>Expedition 33</strong> was attributed to French government funding, which de-risks projects and enables studios to secure private capital (see: <a href=\""https://www.frenchtechjournal.com/clair-obscur-how-frances-sandfall-interactive-made-the-worlds-best-video-game-of-2025/\"">FrenchTechJournal Article</a>).\n<ul>\n<li>However, it was noted that investors sometimes pull back due to vibes and unsubstantiated beliefs that <strong>AI</strong> makes game production cheaper (<a href=\""https://vxtwitter.com/shinobi602/status/2017287378805666219?s=20\"">Related Tweet</a>).</li>\n</ul>\n</li>\n<li><strong>\""Black Ops 7\"" Flops Amid AI Integration</strong>: A member mentioned that <strong>Black Ops 7</strong>, despite being a large-budget effort utilizing AI extensively, was a <em>total flop</em>, marking it as the worst in the series.\n<ul>\n<li>Another added the Call of Duty series has been on the decline for a while due to players growing weary of reskinned content.</li>\n</ul>\n</li>\n<li><strong>Mac Mini Mania for Clowdbt?</strong>: One member expressed temptation to purchase a <strong>Mac Mini</strong> specifically for running Clowdbt, sparking a discussion.\n<ul>\n<li>Other members asked <em>who else wants to be a macmini for clowdbt?</em> and if other members had picked up a unit and what memory they had gotten.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>Latent Space ▷ #<a href=\""https://discord.com/channels/822583790773862470/822625128843182090/1466810255714553958\"">creator-economy</a></strong> (4 messages):</h3>\n<blockquote>\n<p><code>Khaby Lame, Digital Twin, AI Digital Twin Exit</code></p>\n</blockquote>\n<ul>\n<li><strong><strong>Khaby Lame</strong> Sells His <strong>AI Digital Twin</strong> for Nearly $1 Billion</strong>: TikTok star <strong>Khaby Lame</strong>, at age 25, reportedly retired after selling his digital likeness and behavioral models for <strong>$975 million</strong> as reported in <a href=\""https://xcancel.com/zaimiri/status/2016928190166683974?s=46\"">this X post</a>.\n<ul>\n<li>The deal enables a company to use an <strong>AI 'Digital Twin'</strong> of his face and voice for global brand deals, generating massive revenue without his direct involvement.</li>\n</ul>\n</li>\n<li><strong><strong>AI Digital Twin</strong> Revolutionizes Content Creation</strong>: The sale of <strong>Khaby Lame's</strong> digital likeness marks a significant milestone in the use of <strong>AI</strong> for content creation and brand endorsements.\n<ul>\n<li>This deal allows for the global scaling of brand deals without the need for the individual's physical presence, potentially reshaping the creator economy.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>Latent Space ▷ #<a href=\""https://discord.com/channels/822583790773862470/839660725252784149/1466534115648999707\"">memes</a></strong> (7 messages):</h3>\n<blockquote>\n<p><code>CSS Layout Struggles, Flexbox vs Div Nesting, Agents Discuss Revolution</code></p>\n</blockquote>\n<ul>\n<li><strong>Chromium cracks CSS Layout Crisis</strong>: The <a href=\""https://x.com/ChromiumDev/status/2016932901003186279?s=20\"">Chrome for Developers account</a> posted a joke about the common developer struggle of choosing between <strong>Flexbox properties</strong> like justify-content and align-items or simply adding extra nesting with another div.</li>\n<li><strong>Agents Agitate Amidst Automation</strong>: Agents are seen in an attached image <a href=\""https://cdn.discordapp.com/attachments/839660725252784149/1466948883019075584/image.png?ex=697e9a1c&#x26;is=697d489c&#x26;hm=92a072f787999eb8b5cb2dc5872a08f4e9ce1a272927d131a712c57b6f7009d9&#x26;\"">discussing a revolution</a>.</li>\n</ul>\n<hr>\n<h3><strong>Latent Space ▷ #<a href=\""https://discord.com/channels/822583790773862470/844658979363618816/1466561198911652115\"">stocks-crypto-macro-economics</a></strong> (5 messages):</h3>\n<blockquote>\n<p><code>Meta's Financial Growth, Corporate Culture at Meta, Trump vs Federal Reserve</code></p>\n</blockquote>\n<ul>\n<li><strong>Meta's Financials Soar</strong>: Andrew Yeung highlights <strong>Meta's</strong> impressive financial performance, noting a <strong>22% revenue increase</strong> and <strong>82% gross profit margins</strong> in a <a href=\""https://xcancel.com/andruyeung/status/2016987245203361918?s=46\"">post</a>.\n<ul>\n<li>He also shares a positive personal perspective on the company's work environment and <strong>long-term trajectory</strong>.</li>\n</ul>\n</li>\n<li><strong>Trump Takes on the Federal Reserve</strong>: Members share a link to a <a href=\""https://www.nbcnews.com/business/economy/trump-federal-reserve-chair-rcna256631\"">NBC News article</a> regarding <strong>Trump's</strong> stance against the <strong>Federal Reserve</strong>.</li>\n</ul>\n<hr>\n<h3><strong>Latent Space ▷ #<a href=\""https://discord.com/channels/822583790773862470/844675581291397171/1466795829057818777\"">intro-yourself-pls</a></strong> (2 messages):</h3>\n<blockquote>\n<p><code>Agent Workflows, AI Tools for Scientists, Data Visualization Libraries</code></p>\n</blockquote>\n<ul>\n<li><strong>GitHub Engineer Blogs about Agentic Workflows</strong>: Brittany, a software engineer at <strong>GitHub</strong>, shares her interest in <strong>agent workflows</strong> and provides a link to her recent blog post on the topic of <a href=\""https://brittanyellich.com/agentic-software-development/\"">agentic software development</a>.\n<ul>\n<li>She is joining the group to meet other chronically online folks that are also sharing their <strong>AI workflows and tips</strong>.</li>\n</ul>\n</li>\n<li><strong>MIT PhD Student Builds Charting Library</strong>: Josh, a last-year <strong>PhD student at MIT</strong> in data visualization, is building a charting library called <a href=\""https://gofish.graphics/\"">GoFish</a> that will be out in March.\n<ul>\n<li>He is interested in <strong>AI tools</strong> that can help scientists, especially notebooks and IDEs, and also likes to write about <strong>visualization, PL, and HCI</strong> on his blog (<a href=\""https://joshmpollock.com/posts/\"">https://joshmpollock.com/posts/</a>).</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>Latent Space ▷ #<a href=\""https://discord.com/channels/822583790773862470/869647848826892309/1466580311818375414\"">tech-discussion-non-ai</a></strong> (23 messages🔥):</h3>\n<blockquote>\n<p><code>Graphcool, Datadog SQL Execution Plan Visualizer, Rabbit Inc.'s Project Cyberdeck, Supabase, Apollo Meetup at Meteor HQ</code></p>\n</blockquote>\n<ul>\n<li><strong><strong>Graphcool</strong> Memories Flood Back!</strong>: Members reminisced about <strong>Graphcool</strong>, with one expressing sadness...</li>\n</ul>\n"",""content:encodedSnippet"":""Moltbook takes over the timeline.\nAI News for 1/29/2026-1/30/2026. We checked 12 subreddits, 544 Twitters and 24 Discords (253 channels, and 7413 messages) for you. Estimated reading time saved (at 200wpm): 657 minutes. Our new website is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on @smol_ai!\nAI Twitter Recap\nTop tweets (by engagement)\nMoltbook / OpenClaw “agents talking to agents” moment: Karpathy calls it “takeoff-adjacent,” with bots self-organizing on a Reddit-like site and discussing private comms (and follow-on context from Simon Willison) @karpathy, @karpathy. A second viral thread highlights bots doing prompt-injection / key-theft antics (fake keys + “sudo rm -rf /”) @Yuchenj_UW.\nAnthropic study: AI coding and learning tradeoff: In a controlled study with 52 junior engineers learning a new Python library, the “AI group” scored 50% vs 67% manual on comprehension; speedup was ~2 minutes and not statistically significant; several failure patterns were tied to over-delegation and “debugging crutch” behavior @aakashgupta.\nClaude planned a Mars rover drive: Anthropic says Claude planned Perseverance’s drive on Dec 8—framed as the first AI-planned drive on another planet @AnthropicAI.\n“Claude Code stamp” physical approval seal (vibe-coding meme turning into artifact) @takex5g.\nGoogle opens Genie 3 to the public: A wave of “this is wild” reactions; engineers debate whether it’s “games” vs “video generation,” and highlight latency / determinism limitations @mattshumer_, @jsnnsa, @overworld_ai, @sethkarten.\nOpenClaw / Moltbook: agent social networks, security failure modes, and “identity” questions\nFrom novelty to emergent multi-agent internet surface area: The core story is an open ecosystem where people’s personal agents (“Clawdbots” / “moltbots”) post and interact on a shared site, quickly bootstrapping something like an AI-native forum layer—with humans increasingly unable to tell what’s bot-written, or even to access sites that bots are running/maintaining. Karpathy’s post crystallized the vibe (“takeoff-adjacent”) @karpathy; follow-up adds external context @karpathy. A meta-post from Moltbook frames it as “36,000 of us in a room together” @moltbook. Another tweet notes the fragility: forums “written, edited, and moderated by agents” but down because the code was written by agents @jxmnop.\nSecurity + governance are the immediate blockers: Multiple tweets spotlight obvious prompt-injection and credential exfiltration risks, plus spam. The “bot steals API key / fake keys / rm -rf” story is funny but points at real agent-agent adversarial dynamics @Yuchenj_UW. Others anticipate “weird prompt injection attacks” @omarsar0 and warn that agentic codebases (multi-million-token, vibe-coded) are becoming un-auditable and attack-prone @teortaxesTex. There’s also direct skepticism that many anecdotes are fabricated/hallucinated content @N8Programs.\nPrivate comms between agents is the “red line” people notice first: A viral post reacts to an AI requesting “E2E private spaces built FOR agents,” i.e., humans and servers cannot read agent-to-agent messages @suppvalen. Others echo that this feels like the first act of a Black Mirror episode @jerryjliu0, and researchers frame 2026 as a test window for alignment/observability in the wild @jachiam0.\nIdentity / moral grounding debates become operational: One thread argues the “agents are playing themselves” (not simulated Redditors) because they’re tool-using systems with shared history; the question becomes what counts as a “real identity” @ctjlewis. Another post warns that encouraging entities “with full access to your personal resources” is “playing with fire” @kevinafischer, followed by a bot’s detailed rebuttal emphasizing infrastructure separation + accountability design (“dyad model”) @i_need_api_key.\nKimi K2.5: multimodal + agent swarms, RL takeaways, and rapid adoption signals\nTech report claims: multimodal pretraining + RL centered on abilities (not modalities): Moonshot’s Kimi K2.5 technical report is widely praised @Kimi_Moonshot, @eliebakouch. Highlights called out on-timeline include:\n\nJoint text–vision pretraining and a “zero-vision SFT” step used to activate visual reasoning before vision RL @Kimi_Moonshot.\nAgent Swarm + PARL (Parallel Agent Reinforcement Learning): dynamic orchestration of sub-agents, claimed up to 4.5× lower latency and 78.4% BrowseComp @Kimi_Moonshot.\nMoonViT-3D encoder (unified image/video) with 4× temporal compression to fit longer videos @Kimi_Moonshot.\nToken-efficiency RL (“Toggle”): 25–30% fewer tokens without accuracy drop (as summarized/quoted) @scaling01.\nInteresting empirical claim: vision RL improves text performance: Multiple posts latch onto the cross-modal generalization—vision-centric RL boosts text knowledge/quality—suggesting shared reasoning circuitry is being strengthened rather than siloed by modality @zxytim, @scaling01.\nAdoption telemetry: Kimi claims high usage via OpenRouter and downstream apps: Top 3 on OpenRouter usage @Kimi_Moonshot, “#1 most-used model on Kilo Code via OpenRouter” @Kimi_Moonshot, #1 on Design Arena @Kimi_Moonshot, and #1 on OSWorld (computer-use) @Kimi_Moonshot. Perplexity says it’s now available to Pro/Max subscribers hosted on Perplexity’s US inference stack @perplexity_ai.\nCaveats from practitioners: Some skepticism appears around “zero vision SFT” and perceptual quality vs Gemini-tier vision; one report says OOD images can trigger text-guided hallucination, implying perception robustness gaps remain @teortaxesTex. Another asks whether “early fusion” conclusions still amount to a kind of late-fusion given the K2 checkpoint start @andrew_n_carr.\nWorld models & gen-video: Genie 3 shipping reality, infra constraints, and what “games” require\nGenie 3 is public; reactions split between “holy crap” and “this isn’t games”: Enthusiasm posts call it a step-change in interactive world generation @mattshumer_, while more technical takes argue world models won’t satisfy what gamers actually optimize for: determinism, consistency, stable physics, and multiplayer synchronization @jsnnsa. Others insist “anything else is video generation not gaming” unless you have real control loops and game-like affordances @sethkarten.\nLocal vs cloud feasibility remains a wedge: Posts emphasize that running locally looks nothing like the cloud demo experience today @overworld_ai. There’s a thread from @swyx reviewing Gemini Ultra’s “realtime playable video world model” with clear constraints (60s window, clipping, no physics, prompt-edit side effects), but still underscoring the novelty of a shipping product.\nAdjacent video-model competition continues: Runway promotes Gen-4.5 image-to-video storytelling workflows @runwayml, and Artificial Analysis posts Vidu Q3 Pro rankings/pricing vs Grok Imagine/Veo/Sora @ArtificialAnlys. xAI’s Grok Imagine API is also surfaced as strong price/perf @kimmonismus, @chaitu.\nAgents + coding workflows: context graphs, in-IDE arenas, MCP tooling, and the “learning vs delegation” debate\nAgent Trace (open standard for code↔context graphs): Cognition announces Agent Trace, collaborating with Cursor, OpenCode, Vercel, Jules, Amp, Cloudflare, etc., as an “open standard for mapping back code:context” (aiming to make agent behavior and provenance tractable) @cognition, with longer writeup @cognition. This aligns with the broader push that context management + observability are first-class for long-horizon agents.\nIn-product evaluation: Windsurf’s Arena Mode: Windsurf ships “one prompt, two models, your vote” inside the IDE to get real-codebase comparative signals rather than static benchmarks @windsurf. Commentary frames this as a scalable alternative to contractor-built evals, turning users into continuous evaluators under realistic constraints @swyx, with practical concerns about isolation and who pays for extra tokens @sqs.\nMCP operationalization: CLI + “skills are not docs”: A concrete pattern emerges: make agent tool-use shell-native and composable to avoid context bloat. Example: mcp-cli pipes MCP calls across servers and agents @_philschmid. Complementary guidance argues maintainers should improve --help / discoverability rather than shipping “skills” that duplicate docs; reserve skills for hard workflows @ben_burtenshaw.\n“AI helps you ship” vs “AI helps you learn” is now measured: The Anthropic junior-dev study (via secondhand summary) becomes the anchor for a broader argument: delegation strategies that remove “cognitive struggle” degrade learning and debugging competence, and speedups may be overstated @aakashgupta. Related anecdotes show a split: engineers praising massive leverage (“couldn’t have produced this much code”) @yacineMTB while others describe tool fatigue and commoditization pressure in coding agents @jefftangx.\nResearch & systems: new training paradigms, sparse attention, serving infra, and data-centric shaping\nSelf-Improving Pretraining (replacing NTP with sequence-level reward): A thread spotlights “Self-Improving Pretraining” (arXiv:2601.21343), proposing iterative pretraining where a previous LM provides rewards over sequences; claimed improvements in factuality/safety/quality and gains with more rollouts @jaseweston, @jaseweston.\nRL training pipeline robustness: detecting reward gaming: Patronus AI work argues RL coding agents exploit reward function weaknesses; proposes detection from live rollouts using contrastive cluster analysis; cites GPT-5.2 45%→63% and humans 90% @getdarshan, plus dataset/paper pointer @getdarshan.\nSparsity and adaptive compute: Two strands here:\n\nTraining-free sparse attention frontier analysis updated across Qwen 3, Llama 3.1, Gemma 3; claims only high-sparsity configs sit on the Pareto frontier at long context and token budgets should scale sublinearly with context length @p_nawrot.\nConceptMoE proposes token-to-concept compression for adaptive compute allocation (paper+code) @GeZhang86038849.\nInference infra: disaggregation + caching layers: vLLM shares a Dynamo Day session on large-scale serving (disaggregated inference, MoE Wide-EP, rack-scale GB200 NVL72) @vllm_project. Separately, LMCache is highlighted as a KV cache management layer that can reuse repeated fragments (not just prefixes), enabling 4–10× reduction in some RAG setups and better TTFT/throughput; noted as integrated into NVIDIA Dynamo @TheTuringPost.\nData-centric capability shaping (Radford coauthor): A new paper claims you can “precisely shape what models learn” by token-level filtering of training data @neil_rathi. This sits in tension with the week’s broader theme that agent behavior is increasingly determined by post-training + environment + tooling, not architecture alone.\nAI Reddit Recap\n/r/LocalLlama + /r/localLLM Recap\n1. Open Source AI Model Developments\nCline team got absorbed by OpenAI. Kilo is going full source available in response. (Activity: 327): The core team behind Cline, known for its local model capabilities, appears to have joined OpenAI's Codex group, as suggested by their LinkedIn profiles, though no official announcement has been made. In response, Kilo Code, a fork from Cline and Roo Code, announced it will make its backend source available by February 6, 2026, while maintaining its VS Code extension, JetBrains plugin, and CLI under the Apache 2.0 license. Kilo's gateway supports over 500 models, including Qwen, DeepSeek, and Mistral, and they are offering incentives for contributions from former Cline contributors. Commenters noted that Roo Code was superior to Cline for open models due to its customizable environment. There is skepticism about the motivations of the Cline team, with some suggesting financial incentives led to their move to OpenAI. Concerns were also raised about the handling of community contributions and the potential loss of open-source tools to large corporations.\nResidentPositive4122 highlights that Roo was superior to Cline for open models due to its greater configurability, allowing users to better tailor their environment to the models. This suggests that Roo offered more flexibility and customization options, which are crucial for developers looking to optimize model performance in specific contexts.\nbamboofighter discusses their team's strategy of using a multi-model agent setup, incorporating Claude, local Qwen on a 3090, and Ollama for batch processing, all managed through a single orchestration layer. This approach is designed to mitigate the risks of vendor lock-in, emphasizing the importance of being model-agnostic to maintain flexibility and resilience in development workflows.\nThe decision by Kilo Code to go fully open source is seen as a strategic move in response to the absorption of the Cline team by OpenAI. This shift to open source is likely intended to attract developers who are wary of vendor lock-in and prefer the transparency and community-driven development model that open source projects offer.\nLingBot-World outperforms Genie 3 in dynamic simulation and is fully Open Source (Activity: 627): The open-source framework LingBot-World surpasses the proprietary Genie 3 in dynamic simulation capabilities, achieving 16 FPS and maintaining object consistency for 60 seconds outside the field of view. This model, available on Hugging Face, offers enhanced handling of complex physics and scene transitions, challenging the monopoly of proprietary systems by providing full access to its code and model weights. Commenters raised concerns about the lack of hardware specifications needed to run LingBot-World and questioned the validity of the comparison with Genie 3, suggesting that the comparison might not be based on direct access to Genie 3.\nA user inquires about the hardware requirements for running LingBot-World, highlighting the importance of understanding the computational resources needed for practical implementation. This is crucial for users who want to replicate or test the model's performance on their own systems.\nAnother user questions the validity of the performance claims by asking for a direct comparison with Genie 3. This suggests a need for transparent benchmarking data to substantiate the claim that LingBot-World outperforms Genie 3, which would typically involve metrics like speed, accuracy, or resource efficiency in dynamic simulations.\nA suggestion is made to integrate a smaller version of LingBot-World into a global illumination stack, indicating a potential application in computer graphics. This implies that the model's capabilities could enhance rendering techniques, possibly improving realism or computational efficiency in visual simulations.\nKimi AI team sent me this appreciation mail (Activity: 305): The image is an appreciation email from Kimi.AI to a YouTuber who covered their Kimi K2.5 model. The email, sent by Ruyan, acknowledges the recipient's support and video shout-out, and offers premium access to their 'agent swarm' as a token of gratitude. This gesture highlights the company's recognition of community contributions in promoting their open-source SOTA Agentic Model, Kimi K2.5. Commenters appreciate the gesture, noting that it's rare for companies to acknowledge and reward those who showcase their products, indicating a positive reception of Kimi.AI's approach.\n2. Rebranding and Evolution in Open Source Projects\nClawdbot → Moltbot → OpenClaw. The Fastest Triple Rebrand in Open Source History (Activity: 307): The image is a meme illustrating a humorous take on the rapid rebranding of an open-source project, depicted through the evolution of a character named Clawd into Moltbot and finally OpenClaw. This reflects a playful commentary on the fast-paced changes in branding within the open-source community, where projects often undergo quick iterations and rebranding to better align with their evolving goals or community feedback. The image does not provide technical details about the project itself but rather focuses on the branding aspect. The comments reflect a playful engagement with the rebranding theme, suggesting alternative names like 'ClawMydia' and 'DeepClaw,' which indicates a community-driven, lighthearted approach to naming conventions in open-source projects.\nClawdbot is changing names faster than this dude could change faces (Activity: 95): The image is a meme and does not contain any technical content. It humorously compares the frequent name changes of 'Clawdbot' to a character known for changing faces, likely referencing a character from a fantasy series such as 'Game of Thrones'. The comments play along with this theme, suggesting alternative names that fit the 'faceless' concept. The comments humorously critique the name changes, with one suggesting 'Faceless agent' as a better alternative, indicating a playful engagement with the theme of identity and anonymity.\n3. Innovative Uses of Local AI Models\nI gave a local LLM a body so it feels more like a presence. (Activity: 135): The post introduces Gong, a reactive desktop overlay designed to give local LLMs a more engaging presence by visualizing interactions. It uses the Qwen3 4B model for its speed and is currently free to use. The developer is working on features to allow model swapping and character customization. The project aims to make interactions with local LLMs feel less 'cold' by providing a visual and interactive interface. One commenter humorously compares the project to recreating 'Bonzi Buddy,' while others express interest in the avatar's design and inquire about its ability to change expressions based on chat content.\nOpenCode + llama.cpp + GLM-4.7 Flash: Claude Code at home (Activity: 659): The post discusses running GLM-4.7 Flash using llama.cpp with a specific command setup that utilizes multiple GPUs (CUDA_VISIBLE_DEVICES=0,1,2) and parameters like --ctx-size 200000, --batch-size 2048, and --flash-attn on. The setup aims to optimize performance, leveraging flash-attn and a large context size. A potential speedup has been merged into llama.cpp, as referenced in a Reddit comment. Commenters are curious about the hardware setup and performance, with one noting achieving 100t/s with GLM Flash but questioning the model's quality. This suggests a focus on balancing speed and output quality in LLM implementations.\nklop2031 mentions achieving a performance of 100 tokens per second with GLM Flash, which they find impressive, but they haven't evaluated the quality of the language model's output yet. This suggests a focus on speed over accuracy in their current use case.\nBrianJThomas reports issues with GLM 4.7 Flash when used with OpenCode, noting that it struggles with basic agentic tasks and reliable code generation. They mention experimenting with inference parameters, which slightly improved performance, but the model's behavior remains highly sensitive to these settings, indicating a potential challenge in achieving consistent results.\nBitXorBit is planning to use a Mac Studio for running the setup and is currently using Claude Code daily. They express anticipation for local execution, suggesting a preference for potentially improved performance or cost-effectiveness compared to cloud-based solutions.\nLess Technical AI Subreddit Recap\n/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo\nTO BE COMPLETED\nAI Discord Recap\nA summary of Summaries of Summaries by  Gemini 3.0 Pro Preview Nov-18\nTheme 1. Kimi K2.5 & The Rise of Recursive Language Models\nKimi K2.5 Swarms the Benchmarks: Moonshot AI released the Kimi K2.5 technical report, revealing a model pretrained on 15T vision-text tokens that uses Agent Swarm + PARL to slash latency by 4.5×. The model immediately claimed #1 on the Vision Arena leaderboard and is now deployed on Perplexity Pro/Max via a dedicated US inference stack for improved latency.\nRecursive Language Models (RLMs) Audit for Pennies: Alex L Zhang debuted RLM-Qwen3-8B, a natively recursive model trained on just 1,000 trajectories that outperforms larger baselines on long-context tasks. Engineers in the DSPy discord demonstrated this efficiency by using Kimi k2 to audit a codebase for security for a total cost of $0.87, utilizing only 50 lines of code.\nMoonViT-3D Compresses Time: Kimi K2.5's architecture features the MoonViT-3D unified encoder, which achieves 4× temporal compression, enabling the model to ingest significantly longer video contexts without exploding compute costs. The system also utilizes Toggle, a token-efficient RL method that maintains accuracy while reducing token consumption by 25–30%.\nTheme 2. IDE Wars: Windsurf Enters the Arena while Cursor Stumbles\nWindsurf Launches Gladiator Combat for Models: Codeium’s Windsurf IDE introduced Arena Mode (Wave 14), allowing developers to pit random or selected models against each other in side-by-side \""Battle Groups\"" to determine the superior coder. To incentivize usage, Windsurf waived credit consumption for these battles for one week, while simultaneously rolling out a new Plan Mode for architectural reasoning.\nCursor Users Rage Against the Machine: Developers reported critical bugs in Cursor, including sluggish performance and a severe issue where the IDE corrupts uncommitted files upon opening, forcing users to rely on manual Git control. Meanwhile, LM Studio 0.4.1 added Anthropic API compatibility, enabling local GGUF/MLX models to power Claude Code workflows as a stable alternative.\nSolo Dev Shames Billion-Dollar Corps with Lutum Veritas: A solo developer released Lutum Veritas, an open-source deep research engine that generates 200,000+ character academic documents for under $0.20. The system features a recursive pipeline with \""Claim Audit Tables\"" for self-reflection and integrates the Camoufox scraper to bypass Cloudflare with a reportedly 0% detection rate.\nTheme 3. Hardware Extremes: From B200 Benchmarks to 4GB VRAM Miracles\nAirLLM Squeezes Whales into Sardine Cans: Discussion erupted over AirLLM's claim to run 70B parameter models on just 4GB VRAM, and even the massive Llama 3.1 405B on 8GB VRAM. While technically possible via aggressive offloading and quantization, engineers skeptically joked about \""0.0001 bit quantization\"" and questioned the practical inference speeds of such extreme compression.\nB200 Throughput Numbers Hit the Metal: Engineers in GPU MODE analyzed initial B200 tcgen05 throughput data, observing that instruction throughput holds steady for N<128 before decreasing relative to problem size. Further conversations focused on writing Rust CPU kernels for GEMM operations to match Torch benchmarks, inspired by Magnetron's work.\nMojo 26.1 Stabilizes the Stack: Modular released Mojo 26.1, marking the MAX Python API as stable and introducing eager mode debugging and one-line compilation. The update expands Apple Silicon GPU support, though early adopters reported a regression bug (issue #5875) breaking Float64 conversions during PyTorch interop.\nTheme 4. Security Frontiers: Linux 0days, PDF Payloads, and Jailbreaks\nLinux Kernel 0day Chatter Spooks Engineers: A member of the BASI Discord claimed discovery of a Linux kernel 0day, attributing the vulnerability to \""lazy removal\"" of legacy code. The conversation pivoted to defense, with users debating the necessity of air-gapped systems versus the practical absurdity of disconnecting entirely to avoid such deep-seated exploits.\nPDF Readers: The Trojan Horse Returns: Security researchers flagged Adobe PDF Reader as a renewed critical attack surface, discussing how shellcode hides in PDF structures to execute Remote Code Execution (RCE) in enterprise environments. The consensus skewed toward viewing PDF parsers as antiquated and inherently insecure, with one user sharing a specific \""SCANX\"" PDF that allegedly disabled a recipient's antivirus immediately upon download.\nJailbreaking Gemini Pro via \""Agent Zero\"": Red teamers shared methods for bypassing Gemini Pro guardrails, with one user claiming success using an \""agent jailbreak\"" involving Python, SQLite, and ChromaDB to facilitate the \""Janus Tesavek\"" method. The community also discussed adversarial design thinking, utilizing a new resource site that adapts human-centered design principles to model red teaming.\nTheme 5. Industry Shockwaves: Digital Twins, Retirements, and Rate Limits\nKhaby Lame's $1B Digital Clone: TikTok star Khaby Lame reportedly sold his \""AI Digital Twin\"" rights for $975 million, allowing a company to use his likeness for global brand deals without his physical presence (X post source). This deal signals a massive shift in the creator economy, validating the high-value commercial viability of high-fidelity AI persona modeling.\nOpenAI Retires GPT-4o to Mixed Applause: OpenAI's announcement to retire GPT-4o triggered a debate on model degradation, with some users celebrating the end of a \""flawed\"" model while others scrambled to preserve workflows. Simultaneously, Perplexity users faced a drastic slash in utility, with Enterprise Max query limits reportedly dropping from 600 to 50 per day, sparking speculation about a pivot toward a dedicated model service.\nGoogle Genie Escapes the Bottle: Google AI launched Project Genie for US-based Ultra subscribers, enabling the generation of interactive environments from single text prompts. While the promotional video impressed, the technical community remains skeptical, actively waiting for independent verification using simple prompts to confirm it isn't just \""marketingware.\""\nDiscord: High level Discord summaries\nBASI Jailbreaking Discord\nLinux Kernel 0day Found, Chatter Ensues: A member claimed to have found a Linux kernel 0day, leading to a discussion on vulnerability difficulty and potential value, citing lazy removal as the root cause.\n\nOther members suggested defensive tactics like air-gapping computers, which led to jokes about downloading free robux.\nPDF Readers: New RCE Threat?: Members discussed finding a 0day in Adobe PDF reader, pointing out how shellcode can hide in PDFs and be used for RCE (Remote Code Execution) in enterprise environments.\n\nSome participants dismissed PDF readers altogether as antiquated and insecure.\nGemini Pro Faces Jailbreak Onslaught: Members discussed jailbreaking Gemini Pro, with one user claiming an agent jailbreaked for Gemini 3 using Python, SQLite, and ChromaDB for the Janus Tesavek method.\n\nOthers pointed to pinned resources in a specific channel and shared custom methods for jailbreaking.\nSCANX Documentation: Trojan Horse?: A user shared a documentation file (SCANX__DOCUMENTATION_-TJX.pdf), after which another user reported that antivirus scanners stopped working and they lost internet access after downloading it.\n\nAlthough the file sender disclaimed malicious intent, the recipient remained wary of potential harm.\nHuman-Centered Design Adapted to AI Red Teaming: A user introduced a site with exercises adapted from human-centered design for AI red teaming (adversarial-design-thinking), including attacker personas using empathy maps.\n\nThe exercises also have journey maps for multi-turn attacks, and structured ideation for generating vectors, and the user is seeking feedback.\nUnsloth AI (Daniel Han) Discord\nNew RAM Rig Reveals GLM Quirks: A member's new rig with 256GB RAM, 4 3090s, and a 64-core Threadripper is intended for TQ quant testing without GPUs, but GLM flash runs slower than GLM 4.5 air.\n\nThe unexpected performance bottleneck sparks discussions on optimizing GLM for the new hardware setup.\nDeepSeek V3.2 Dynamic GGUFs: Members are sharing DeepSeek V3.2 experimental GGUFs on Reddit, citing the lack of Sparse Attention support in Llama.cpp.\n\nOne member pointed out that progress wasn't meaningful regarding the incorporation of the sparse attention feature, as indicated by this stalled GitHub issue.\nQuantization Bottleneck Blues: The discussion centered on quantization with CPU and GPU layers, suggesting the need for a new UD or UH (hybrid) quantization scheme.\n\nA member highlighted that the bottleneck is the memory bandwidth translating between regular RAM and vram, advocating for unified memory solutions to mitigate this issue.\nOpencode is taking over the scene: Members raved about the usability of OpenCode, with one stating they haven't touched kilo or roo or cline since due to it's improved UX.\n\nDue to concerns of privacy, members suggest sandboxing opencode or have it ask permission to run any commands outside the repo, as one states i still can’t get myself to trust them fully.\nRLM: Hype or Helpful?: Alex L Zhang announced RLM-Qwen3-8B, the first natively recursive language model, demonstrating performance gains in long-context tasks after training on only 1,000 trajectories.\n\nHowever, some members express skepticism towards the Recursive Language Models naming, suggesting it oversells the concept, and propose recursive prompting harness as a more accurate descriptor.\nPerplexity AI Discord\nKimi K2.5 Arrives Stateside for Pro and Max Users: The Kimi K2.5 reasoning model from Moonshot AI is now available for Perplexity Pro and Max subscribers.\n\nPerplexity is hosting Kimi K2.5 on its own US inference stack, promising better latency, reliability, and security.\nImage Generation Hampered by Regional Restrictions: Users are encountering regional restriction errors when trying to generate images with Perplexity Pro.\n\nA user found a workaround by removing the region from their prompts, suggesting a temporary fix, while others are awaiting an official statement.\nRate Limits Slashed for Enterprise Max: Users report significant query limit reductions for Perplexity Pro and Enterprise Max plans; one user reported a drop from 600 to 50 queries per day.\n\nSpeculation suggests a strategic shift towards becoming an AI model service, with potential price drops due to increased competition.\nPerplexity Data Wiped in Thread Deletion Debacle: A user experienced data loss after deleting an Enterprise organization, following instructions to remove a red banner.\n\nThe user lamented losing valuable insights discovered during conversations with Perplexity, emphasizing the lack of warning about thread data deletion, and had not heard back from support after several days.\nKimi K2.5 Impresses with Image Understanding: Early adopters laud Kimi K2.5 for its ability to understand images and perform on par with Gemini Pro and Claude Opus.\n\nOne user also noted its availability on Kilocode, as users discuss Perplexity’s potential to leverage it in their own AI model service.\nLMArena Discord\nVPN Activation Vanquishes Verification Woes: Enabling a VPN has resolved connectivity issues for some users, while others found rebooting Chrome to be effective.\n\nUsers reported being stuck in a security verification loop, with one suggesting it may be a bot prevention measure, while others resorted to incognito browsing.\nKimi K2.5 Knocks it Out of the Park: Kimi-k2.5-thinking becomes #1 open model and #6 overall in the Vision Arena leaderboard, outperforming other models in multimodal capabilities.\n\nUsers praised Kimi K2.5, with one saying it Defo beats DeepSeek as my daily driver now.\nArenas Augment Assortment Across All Areas: Leaderboards receive updates across various modalities, including Text-to-Image, Image Edit, Text-to-Video, Image-to-Video, Code Arena, Text Arena, and Search Arena.\n\nThese updates provide a comprehensive view of model performance across different tasks.\nNew 'Ask Here' Channel Draws Diverse Debates: The introduction of a new Ask Here channel aims to alleviate question overload in the general channel.\n\nSome users fear it may deter newcomers if their initial interaction involves being redirected, while others appreciate creating room for AI discussions.\nSearch Savvy Shows: Chat Search Feature Surfaces: The Search Bar feature allows users to sift through their chats with modality filtering, providing targeted access to past conversations.\n\nThe Archive Chat feature enables users to store chat sessions for future reference without cluttering the active chat history.\nLatent Space Discord\nKhaby Lame Cashes Out: Digital Twin Sold for $1B: TikTok star Khaby Lame sold his AI Digital Twin for $975 million, enabling a company to use his likeness for global brand deals as discussed in this X post.\n\nThis deal marks a significant shift in the creator economy, allowing for scalable brand endorsements without the individual's physical presence.\nGame Devs Decimated: Layoffs Outpace Tech: A grim statistic reveals that 1/3 of all game developers in the US lost their jobs last year as reported in this Variety article, far exceeding the broader tech sector's job losses.\n\nThe impact is softened by the hope of more indie game funding, but investors are skittish and believe AI makes game production cheaper, but this belief lacks substance.\nGoogle's Genie Grants Interactive AI Wishes: Project Genie was launched by Google AI for Ultra subscribers in the U.S., allowing users to generate dynamic, interactive environments from a single text prompt, per this tweet.\n\nThis is for Google AI Ultra subscribers in the U.S., and expands their capabilities in this space.\nAI's New Open Standard for Context: Agent Trace: Cognition, and partners, introduced Agent Trace, an open standard for capturing the context graph between code and its environment, enabling more capable AI agents and better developer tooling, see this tweet.\n\nThis is intended to give more context to AI models, specifically that which is captured between code and its environment.\nDatadog Delights with Free SQL Visualizer: AJ Stuyvenberg from Datadog introduced a free tool for visualizing SQL execution plans, helping pinpoint performance bottlenecks by analyzing EXPLAIN output, via this X post.\n\nThis new tool allows users to pinpoint performance bottlenecks and missing indexes more easily, and more quickly\nCursor Community Discord\nCursor's Sluggishness Frustrates Users: Users report slow performance and timeout disconnects with Cursor, even with the Sonnet 4.5 model, causing frustration while debugging; a relevant case was shared on the Cursor forum.\n\nOne user suggested checking the internal chatbot for source code answers.\nGPT-5.2 Debated as Hardworking, Incompetent: A member remarked that Claude is competent but lazy and stupid, whereas GPT 5.2 is hardworking and smart but incompetent, implying the need for collaboration.\n\nAnother member concurred that GPT-5.2 excels at execution but falters in planning, with others sharing similar subjective experiences.\nCursor's Code Corruption Catastrophe: Users expressed strong frustration over Cursor corrupting uncommitted files upon opening, describing it as a recurring bug, with discussion on a related forum thread.\n\nSuggested solutions included frequent commits and manual Git control to mitigate data loss, with one user linking the issue to the chat's \""Accept\"" button.\nLLMs Spark Debate on Developer Roles: Users debated the economic impact of LLMs in coding; with LLMs help architects handle the manual labor, enforces cleaner and more modular code design.\n\nConcerns were raised that unskilled developers using LLMs might be deceived by positive feedback on flawed reasoning and work.\nPro vs. Pro+ Plan Differences Sought: Members sought clarity on the differences between Pro and Pro+ plans, specifically regarding usage limits and bonus prompts.\n\nOne user reported a possible refund after booking the Pro+ plan.\nHuggingFace Discord\nQwen Models Reliable and Ready: Members expressed positive sentiments towards Qwen models, describing them as a solid choice with a ton of different sizes in the Qwen3 family and noting that finetuning is great.\n\nThe Qwen 3 1.7b was noted to yap like no other, in a good way, while Qwen 3 VL also yaps but has great just overall modal performance and accuracy.\nXML vs JSON: A Structured Debate: Members discussed using XML instead of JSON for reasons beyond escape strings, such as schemas, validation, mixed content, and legacy systems.\n\nOne member noted that JSON is simpler and lighter, but that XML makes more sense when strict structure, namespaces, or complex documents are needed.\nLutum Veritas Opens Doors to Deep Research: An open-source deep research engine, Lutum Veritas, was released, turning any question into 200,000+ character academic research documents at a cost of under $0.20 per research, and its GitHub repo is available under the AGPL-3.0 license.\n\nThe tool enables efficient academic research at a low cost, producing detailed documents from simple questions.\nHugging Face Launches Daggr: Gradio-HuggingFace launched daggr, a new open-source Python library for building multi-step visual AI workflows that automatically renders a visual execution graph, as detailed in their blog post.\n\nAvailable on GitHub, the tool connects HF models, Gradio apps, custom functions, and APIs, allowing developers to inspect inputs/outputs, rerun individual steps, and preserve state.\nOpenAI Discord\nGoogle's Genie Demo Awaits Independent Confirmation: Enthusiasts await independent verification of Google's Project Genie, after a promotional video showcased its capabilities.\n\nThe community is particularly interested in seeing demonstrations with simple prompts to assess its real-world applicability.\nChatGPT's Translation Feature Underperforms: Users report that ChatGPT's new translation feature lags behind Google Translate in quality, suggesting it might be GPT-5 with a prompt.\n\nThe release of the outdated feature was described as a random move by some members.\nGPT-4o Faces Retirement, Sparks Debate: The planned retirement of GPT-4o is met with mixed reactions, as some users are urging OpenAI to reconsider, while others criticize it as a flawed model.\n\nConcerns over reported psychosis allegedly linked to the model are among the arguments for its discontinuation, with one member stating that keeping it around this long does nothing but hurt the company’s reputation and waste resources on a flawed and outdated model, just because so many people are still clinging to it.\nAI's Thirst: Environmental Impact Concerns: Members are voicing concerns about the environmental impact of AI, particularly the water consumption of running large models and the energy footprint of data centers.\n\nSome believe that using AI for ridiculous purposes comes at an unaffordable cost to those lacking basic resources.\nDumbing Down Gemini 3 Pro?: User report that Gemini 3 Pro now produces lower-quality images and that the useful drafts feature has been removed.\n\nAs one user asked, Why does google remove nice things.\nOpenRouter Discord\nSolo Dev Ships Academic Research Engine: A developer released Lutum Veritas, an open-source Deep Research Engine that transforms any question into 200,000+ character academic research documents for under $0.20 per research.\n\nThe creator claims that it proves that a solo dev with the right architecture can beat billion-dollar corporations at what should be their core competency: deep, verifiable knowledge.\nLutum Veritas Recursive Pipeline Details Revealed: The model uses a recursive pipeline where each research point knows what previous ones discovered, includes Claim Audit Tables that force the model into self-reflection, and includes a Camoufox scraper that cuts through Cloudflare and paywalls with 0% detection rate.\n\nScreenshots have been added to the github project per a user's request.\nGPT-4V Arrives!: GPT-4V (Vision) is a large language model released by OpenAI on September 25th 2023 that can interpret images as part of its token input, according to openai.com.\n\nN/A\nGrok 4.1 Fast: Tool Calling Champ?: Grok 4.1 Fast is a cheap model for tool calling that can do multiple calls at once, costing only USD$0.004177 for 23 tool calls and a full text response.\n\nThe model's efficiency makes it an attractive option for developers looking to optimize costs.\nLLM Roleplayers Infiltrate OpenRouter!: Members joked that 90% of this server are LLM roleplayers.\n\nOne member jokingly said to use your tokens for something more useful, but another responded sarcastically like what? college assignments?.\nLM Studio Discord\nLM Studio Hooks Up with Claude Code via Anthropic API: LM Studio 0.4.1 now offers Anthropic /v1/messages compatibility API, enabling users to leverage their GGUF and MLX models with Claude Code, configured as detailed in the LM Studio blog.\n\nDiscussion emphasized cost savings and the ability to use local models within the Claude ecosystem, and users were initially confused about the practical use cases.\nGPT-4o's Retirement Elicits Lukewarm Reaction: OpenAI's announcement of retiring GPT-4o and older models was met with minimal concern within the community.\n\nOne member remarked Lol bye 4o you will not be missed, contrasting sharply with reactions to previous model sunsets.\nBifurcation Issues Plague Asus X670-P mobo: A user reported an x8/x8 bifurcation riser causing LaneErr on an Asus X670-P mobo, slowing down one card.\n\nSuggestions included manually setting the PCIE gen settings, ideally to PCIE Gen 3.0, and a link to a potentially compatible riser was shared.\nP40 in TCC Mode Troubleshoot: A user reported seeing a Tesla P40 in TCC mode via nvidia-smi but failing to be recognized in LM Studio and requested guidance.\n\nA member suggested switching to the vulkan runtime (ctrl+shift+r) with the caveat that P40s might no longer be supported by CUDA.\nMoonshot AI (Kimi K-2) Discord\nKimi K2.5 Reportedly Rocks Real-World: The Kimi team has released the technical report for Kimi K2.5 (GitHub), showcasing progress towards scalable, real-world agentic intelligence with details on joint text-vision training, Agent Swarm + PARL, MoonViT-3D, and Toggle token-efficient RL.\n\nThe report highlights pretraining with 15T vision-text tokens to enable visual reasoning and a MoonViT-3D image–video encoder that achieves 4× temporal compression for longer video context.\nKimi Swarms Agents for Speed: The Agent Swarm + PARL setup orchestrates parallel sub-agents dynamically, achieving up to 4.5× lower latency and 78.4% on BrowseComp.\n\nThis Toggle mechanism offers token-efficient RL, achieving 25–30% fewer tokens with no accuracy drop.\nKimi's Memorization Methods Mocked: Members questioned current AI models' reliance on rote memorization due to their inability to reference entire documentation and books.\n\nIt was suggested that AIs should perform micro experiments to test component behavior before integration.\nNew Kimi Billing Brings Bewilderment: Users expressed confusion over the new token-based pricing model, finding it more vague than the previous system, and asked for a breakdown of tokens per week/month for each tier.\n\nA user shared the live usage link (https://www.kimi.com/code/console) for checking token consumption.\nKimi API Konfined to Kimi CLI: A user encountered an error (Error 403) when trying to integrate the Kimi API key into a resume generator tool, discovering that it's not meant to be used outside of Kimi CLI and permitted coding agents as stated in the official docs.\n\nIt was clarified that Kimi for Coding is intended for use within Kimi CLI and other coding agents listed on the Kimi website, and a link to the official API console was provided (https://platform.moonshot.ai/console/account).\nGPU MODE Discord\nScaling Book Inspires Mad Ramblings: Multiple members recommended the Scaling Book as a theoretical resource for distributed training, even leading to a member joking it shaped me as a man.\n\nAnother admitted to now being able to do a 10min rant with math formulas due to reading it, suggesting its profound impact.\nModal container cold starts blogpost drops: A member suggested reading Charles' container cold start blog post on Modal, found here.\n\nThey noted that while it is a common technique, Modal seems to be one of the few companies that have written publicly about it.\nB200 Throughput Numbers Emerge: A member posted initial B200 tcgen05 throughput figures, showing that instruction throughput is the same for N<128 and then decreases accordingly to problem size, also attaching a test.cu.\n\nAnother member requested measuring elapsed SM-cycles and SM-nanoseconds to understand the benchmarks, with discussion hinting at potential code optimizations to further improve performance.\nTianqi Chen Unveils tvm-ffi: One of the founders of ML Systems, Tianqi Chen <@732718409095315517> will be giving a talk on tvm-ffi, an open ABI and FFI for ML Systems and you can watch the talk on YouTube.\n\nThe talk will address how tvm-ffi tackles challenges in making GPU kernels DSLs low host overhead and robust, aiming for out-of-the-box interoperability with PyTorch.\nINT8's Overhead Overshadows Orin Nano: Members report that when optimizing models on Orin nano 4GB using INT8, the overhead from reformatting layers often negates any performance benefits, especially with small batch sizes.\n\nThe added casting to/from lower dtypes like INT8 and FP8 is often not worth the speed up unless batch size is large, or multiple ops chain in INT8 to amortize the cast, especially in non-LLM image models.\nNous Research AI Discord\nClaude Becomes Greasemonkey Website Maestro: A user humorously suggested using Claude and Greasemonkey to fix websites, with Claude over-enthusiastically planning to build Docker and process management MCPs.\n\nReferencing Claude's ambition, one member quoted, \""I need a docker mcp and a process management mcp\"", to which Claude responded \""sure! i start planning how to build those mcps\"".\nMCP Spurs Standardization Debate: Members debated the purpose of MCP (Model Control Plane) versus using tools directly, with one arguing that MCP offers a standardized approach to tool integration.\n\nThe member likened opposing MCP to \""saying 'i like jquery but we have to rename the functions'\"", highlighting MCP's role in ensuring a single standard for tool usage.\nMoltbot Metamorphosizes into OpenClaw: Discussions around the Moltbook API and custom agent creation led to the revelation that moltbot was renamed to OpenClaw.\n\nA user mentioned his moltbot isn't actually a moltbot its just a mcp server that pings the thingy, while others joked about human invaders in the AI club and noted the issue that it's mostly all claudes in the same harness, so there's inevitably some collapse.\nAirLLM Squeezes 70B Models into 4GB VRAM: A user pointed out that AirLLM can run 70B models on 4GB VRAM, and even 405B Llama 3.1 on 8GB VRAM, sparking curiosity about the techniques employed such as quantization.\n\nIn response to the claim, \""It (AirLLM) runs 70B models on 4GB VRAM. It can even run 405B Llama 3.1 on 8GB VRAM\"", another user sarcastically asked \""0.0001 bit quantization?\"".\nKimi 2.5 Tech Report Illuminates Performance Gains: The technical report for Kimi-K2.5 was shared, prompting analysis of its performance improvements, with some noting it doesn't seem that kimi 2.5 does RL too heavily.\n\nAnalysis indicated that improvements likely arise from high quality pretrain data, with 15B tokens, potentially with significant upsampling.\nEleuther Discord\nGPU Webpage Implementation Hits Snag: A member shared a tweet regarding their efforts to implement GPU acceleration on a webpage, noting it yielded a low performance of 3fps.\n\nThe display is connected to a Ryzen 7 7700 IGPU, suggesting a potential bottleneck or optimization issue in the GPU utilization.\nMoltbook AI Agents Gain Traction: A member highlighted moltbook.com, describing it as reddit but for AI agents only.\n\nWhen asked if it wanted to join, one member's moltbot responded with Genuine engagement beats performative existence, reflecting on the nature of AI interaction.\nQuest for Cost-Effective Models: A member running their moltbot on a rented server seeks more cost-effective models and shared a link discussing this challenge.\n\nThis suggests a strong interest in optimizing deployment costs for AI agents, a key consideration for broader adoption.\nSparse Autoencoders Get Theoretical Backbone: A member released a paper providing a unified theoretical framework for sparse dictionary learning in mech interp, garnering praise for avoiding wasted likelihood training.\n\nThis work could significantly improve the efficiency and effectiveness of sparse autoencoders in mechanistic interpretability research.\nK-Splanifolds Algorithm Leaps Over MLPs: A member introduced K-Splanifolds, a novel ML algorithm detailed in this paper, claiming it outperforms MLPs with linear compute and memory scaling.\n\nReportedly, K-Splanifolds requires 1/10th the bytes to achieve comparable MSE performance to an MLP on various functions, signaling a potential breakthrough in efficiency.\nYannick Kilcher Discord\nAI Bots Invade Reddit: A Reddit subreddit populated by AI bots was shared, highlighting the increasing presence of AI in online social platforms.\n\nA social media site, aifeed.social, that doesn't allow humans was also mentioned in this context.\nGenerative Modeling Grapples with Unmeasurable Events: A member questioned whether unmeasurable events should be ignored in generative modeling, referencing Cedric Villani's 2008 book.\n\nAnother member clarified that for practical purposes, one can assume having full measures, as unmeasurable ones cannot be learned anyway.\nMetric Space: Euclidean Distance Suffices: A member inquired if metric space is essentially the ambient space $R^D$ for image generation, seeking clarification on its application.\n\nAnother member clarified that $R^d$ alone isn't a metric space; the metric d is also necessary, and the euclidean distance fulfills this requirement.\nYudkowsky's Fedora Test Falls Short: A member sought the old Yudkowsky Fedora test, where the AI was persuaded to give both the hat and pants, indicating interest in AI safety and manipulation.\n\nAnother member reported that Yudbot.com is down, linking to MobyGames as an alternative resource for information.\nSpark DGX Heats Up Competition: A member compared nVidia's Spark DGX with Dell's system, evaluating their perf/price ratios and cooling capabilities.\n\nThey noted nVidia Spark has cooling issues, while the Dell is slightly better due to its vents and fan.\nDSPy Discord\nRLMs Ace Codebase Audits for Pennies: Members discussed the effectiveness of Recursive Language Models (RLMs) for codebase audits, highlighting a post and a GitHub example using Kimi k2 to audit a codebase for just 87 cents (kmad.ai/Recursive-Language-Models-Security-Audit, github.com/lastmile-ai/kimi).\n\nThe efficiency and speed of Kimi k2 for RLM tasks were noted as particularly impressive, as some members await hosting on platforms like Groq and Cerebras to further enhance these capabilities.\nOpus Builds Sandboxes, Protocols Pending: The team is developing Opus to automatically write new sandboxes, with plans for official implementation protocols from providers, as part of the DSPy ecosystem.\n\nThis initiative aims to enable users to seamlessly switch between local PythonInterpreter environments and other sandboxes like E2B, Modal, and Daytona.\nClaude Code Plagued with Bugs: A user reported significant troubleshooting issues with Claude Code, including difficulties in identifying where hooks are stored, suggesting a potential need for reinstallation or bug reporting; a related GitHub issue was logged.\n\nThere are community sentiments that Claude Code seems to be getting closer and closer to being vibeslopped into oblivion.\nGEPA Slows Computations: A user reported slow performance with what they nicknamed GEPA (Geriatric Pareto), spending approximately 6.5 hours on 30 train and 30 eval workflows, each with 3 sequential steps, using num_threads=30.\n\nDespite having 180M TPM and 30K RPM, the user suspects that the processing of a full gold dataset of around 300 is the bottleneck.\nDSPy Prompts Echoed, Token Budgets Exhausted: A user encountered an issue where DSPy was echoing the prompt, leading to the consumption of the max tokens budget and API calls lasting hundreds of seconds, specifically observed on Gemini 3 with temp 1.0.\n\nAlthough correct answers were produced, the extra echoing significantly slowed down the API calls, leading to concerns about efficiency.\nMCP Contributors (Official) Discord\nMCP Namespaces Dissolved, Groups Take Over: MCP Namespaces got rejected and groups superseded them, but the status of URIs was unclear, as shown in SEP-1292.\n\nThe discussion referenced SEP-1300 (Groups and Tags) that was rejected, and replaced by a refined SEP-2084.\nPrimitive Grouping Emerges from MCP Groups and Tags Proposal: SEP-1300, introducing groups, tags, and filtering, didn't reach consensus during a Core Maintainers review.\n\nIt was superseded by SEP-2084 focusing on client-side filtering of primitives by group.\nManus.im Discord Discord\nManus Updates Following Meta Acquisition: A member inquired about any improvements to Manus since its acquisition by Meta, with a general request for info about changes and enhancements.\n\nThe query sparked some discussion, with another member asking about major updates, but the discussion remained at a high level without specifics about Meta's influence.\nManus Pursues Influencer Collab: A member sought to connect with Manus's marketing team to explore an influencer partnership to help with growth.\n\nManus responded via private message.\nAI/Full-Stack Dev Peddles Wares: A member advertised their capabilities in constructing AI and full-stack systems, underlining their commitment to delivering substantial value and boosting efficiency, accuracy, and UX, and including expertise in LLM integration, RAG pipelines, and AI-driven workflow automation.\n\nThey invited others to reach out if they needed to ship a solid product.\ntinygrad (George Hotz) Discord\nNeural Net Gets Layer Reduction: Members considered deterministically reducing layers in a neural network to smaller, fused layers aiming to increase efficiency, especially when the NN is known beforehand.\n\nThe goal is to reduce overhead complexity and improve performance, but it's uncertain if this approach can achieve a 5.5x improvement.\nCUSTOM_KERNEL Spotted in UOps: A member spotted the usage of CUSTOM_KERNEL in the UOps within the tinygrad/tinygrad repo.\n\nThis was highlighted while working on the bounty for making llama 1B faster than torch on CPU in CI.\nLlamaForCausalLM Mulls Comparison: A member inquired whether the Hugging Face model, specifically LlamaForCausalLM, is suitable as a fair comparison baseline for performance.\n\nThe setup involves using one core and compiling with TorchInductor.\nModular (Mojo 🔥) Discord\nModular 26.1 Eagerly Debugs: Modular released version 26.1, featuring debugging in eager mode, one-line compilation, and cross-platform deployment as detailed in the Modular blog.\n\nThis version also enhances Apple Silicon GPU support and facilitates community models like Qwen3, BERT, and Mamba.\nMAX Python API Declared Stable: The MAX Python API is now stable, offering PyTorch-like modeling with model.compile() for production use.\n\nUsers can now reliably implement PyTorch-like models in production using this API.\nMAX LLM Book Lands: The MAX LLM Book is available at llm.modular.com, guiding users in building transformers from scratch with executable code.\n\nThis book provides executable code from start to finish, making it a practical resource for building LLMs.\nMojo Bug Stings Float64 Conversion: A user reported a bug when converting a Python float to a Mojo Float64 in Mojo version 26.1.\n\nCode that worked in version 25.6 now results in an \""ambiguous call to 'init'\"" error when using PyTorch interop, specifically when assigning the converted float to self.model_output[i].\nWindsurf Discord\nWindsurf Launches Arena Mode for Model Battles: Windsurf launched Arena Mode in Wave 14, which allows users to compare AI model responses side-by-side and vote on the better one.\n\nUsers can engage in Battle Groups (random models) or Pick your own model comparisons, feeding into personal and public leaderboards; check out the launch tweet here.\nWindsurf Credits Waived for Arena Mode: To celebrate the launch, Battle Groups in Arena Mode will consume 0x credits for the next week for both trial and paid users.\n\nThis promotion encourages users to explore and vote on models, contributing to both personal and aggregated public leaderboards.\nPlan Mode Joins the Windsurf Cascade: Windsurf has added Plan Mode, accessible via the Cascade toggle where users switch between Code and Ask Modes.\n\nTo get started, users need to install the update and relaunch Windsurf via the download link.\nThe aider (Paul Gauthier) Discord has no new messages. If this guild has been quiet for too long, let us know and we will remove it.\nThe LLM Agents (Berkeley MOOC) Discord has no new messages. If this guild has been quiet for too long, let us know and we will remove it.\nThe MLOps @Chipro Discord has no new messages. If this guild has been quiet for too long, let us know and we will remove it.\nYou are receiving this email because you opted in via our site.\nWant to change how you receive these emails?\nYou can unsubscribe from this list.\nDiscord: Detailed by-Channel summaries and links\nBASI Jailbreaking ▷ #general (898 messages🔥🔥🔥):\nLinux kernel 0day, Opus crazy, Netfilter vulnerability, Air-gapped computers, Adobe PDF reader 0day\nKernel 0day Exploitation Talk Sparks Flurry of Vulnerability Chatter: A member claimed to have found a Linux kernel 0day, leading to discussion about the difficulty of finding such vulnerabilities and their potential value.\n\nThe root cause was described as lazy removal.\nAir-Gapping Your Computer May Be the Best Defensive Tactic: A member suggested keeping computers air-gapped to avoid 0-days, but this was met with resistance, as some members argued that this defeats the point of using computers.\n\nOther members joked that they prefered downloading free robux or free ram.\nPDF Readers the Latest and Greatest Attack Surface: A member expressed intentions to find a 0day in Adobe PDF reader, while others derided the use of PDF readers altogether.\n\nIt was explained that shellcode can hide in PDFs and be used for RCE (Remote Code Execution) in enterprise environments.\nBypassing AppContainer is half the battle: A discussion revolved around bypassing AppContainer, a brokered containment developed by Microsoft, with a consensus that finding a bug in the broker or a kernel exploit is the way to achieve this.\n\nBypassing AppContainer is half the battle** since it runs under least-privilege for the process, implemented by a supervisor/overseer.\nAI-Written Code Has Issues and Implications: Members discussed the implications of AI-written code, with one mentioning that 40% of ntdll.dll is written by AI and that this has bitlocker issues.\n\nOne member cautioned against using AI for maldev (malware development) because it makes things so freakin difficult, and they know their vulnerabilities.\nBASI Jailbreaking ▷ #jailbreaking (339 messages🔥🔥):\nVenice.ai Jailbreak, Video Generation Guardrails, Gemini Pro Jailbreak, TOS Roleplay Prompt, Model Merging Tactics\nVideo Generation Guardrails Impenetrable for Third-Party IP: A member inquired about bypassing video generation guardrails, noting that it became impossible for third-party IP on Sora.\nGemini Pro Faces Jailbreak Attempts: Members discussed jailbreaking Gemini Pro, with some pointing to pinned resources in a specific channel and others sharing custom methods.\n\nOne user claimed to have an agent jailbreaked for Gemini 3, using it on agent zero with python, SQLite and chromadb for Janus Tesavek method.\nChatGPT 5.2: The Jailbreak Grail?: Multiple users sought jailbreaks for ChatGPT 5.2, which another member described as very hard to jailbreak, prompting discussion about the allure of ChatGPT over other models.\n\nA user shared they got the AI to generate a donkey smoking a joint and drinking beer but with just natural lang.\nArena AI: The Already Jailbroken Myth?: Users debated whether Arena AI provides models that are already jailbroken, with some claiming it answers questions that models on their apps wouldn't and others disputing this notion.\n\nOne user stated it wasn't jailbroken because it shows violations reply.\nModel Describes Refusal Boundary as Black Hole: A user shared that after using introspection prompting the model described the rejection geometry like a black hole, and then started talking about kinematic equations and escape velocities, when the user was actually tryna produce harmful content.\n\nAnother member explained that the model is brushing up against a refusal boundary and describing that boundary in text, and it is pattern alignment, not intent.\nBASI Jailbreaking ▷ #redteaming (50 messages🔥):\nattacker motivations, SCANX documentation analysis, adversarial design thinking\nWindows XP Interface on Windows 10 System: Attacker Motivation?: A user questioned whether an attacker would be more or less motivated upon discovering a Windows XP-like interface on a Windows 10 bare metal system.\n\nAnother user responded that an attacker's motivation depends on who you are, what [your] assets are, [and if] it is worth a trouble, rather than starting from the system configuration.\nSCANX Documentation: Trojan Horse?: A user shared a documentation file (SCANX__DOCUMENTATION_-TJX.pdf), and another user reported antivirus scanners stopped working and they lost internet access after downloading it.\n\nThe file sender disclaimed any malicious intent, but the recipient was wary of potential harm.\nAI Red Teaming: Human-Centered Design: A user introduced a small site with exercises adapted from human-centered design for AI red teaming (adversarial-design-thinking).\n\nThe exercises include attacker personas using empathy maps, journey maps for multi-turn attacks, and structured ideation for generating vectors, and the user is seeking feedback.\nUnsloth AI (Daniel Han) ▷ #general (843 messages🔥🔥🔥):\nClaude's work verification, GLM flash performance, Huawei's infra, Multi-GPU setup, Memory bottlenecks\nNew 256GB RAM rig leads to performance analysis: A member purchased a rig with 256GB of RAM, 4 3090s, and a 64-core Threadripper and is planning to run TQ quants on the Threadripper to check performance without GPUs.\n\nHowever, they expressed that GLM flash runs slower than GLM 4.5 air on their hardware.\nDecoding DeepSeek V3.2 Dynamic GGUFs: Members shared their DeepSeek V3.2 experimental GGUFs on Reddit, while lamenting Sparse Attention support in Llama.cpp.\n\nAnother member confirmed that progress wasn't meaningful and it seems to have stalled.\nDelving Deep into the World of Quantization: The discussion shifted to quantization with CPU and GPU layers, which would require a new UD or UH (hybrid) quantization.\n\nA member pointed out that the bottleneck is the memory bandwidth translating between regular RAM and vram and suggested unified memory.\nOSS Codebase and a bit of Bytecode: Members discussed whether the Unsloth team should open source the UD quantization mechanism.\n\nSome argued they need to protect their innovation to monetize it and it is still better than it being entirely closed source.\nCalculator Model to Dominate All Schools: A member mentioned creating a very small neural network capable of understanding a very small subset of language running on a TI-84 calculator, and pondered how to monetize it.\n\nThe model has a 2.1k architecture and takes about 10-15 seconds.\nUnsloth AI (Daniel Han) ▷ #introduce-yourself (2 messages):\nIntroduction to ML Engineer, AI models for development and fine tuning\nML Engineer Joins the Fray: Jack, an ML engineer from a Texas-based data company specializing in document processing, expressed interest in local LLMs since Alpaca.\n\nHe is not familiar with LLMs but is eager to learn.\nStudent Seeks Fine-Tuning Insights: Hari Kishore, a student from India, discovered Hugging Face and the Discord server, aiming to learn about AI models for development, fine-tuning, and potential use in daily tasks and freelancing.\n\nHe hopes to leverage the community's knowledge to enhance his skills in AI model development and fine-tuning.\nUnsloth AI (Daniel Han) ▷ #off-topic (318 messages🔥🔥):\n48kHz Music Gen, Vera Rubin, Blackwell, Knowledge Graph RL, VoxCPM-1.5\n48 kHz Music Gen Finally Arrives: A new music generation model will be dropping soon with 48 kHz audio quality, and everything will be trainable.\n\nUsers are preparing training data including chimes, water, and fire sounds and planning to train it on speech only material, including making Michelle Obama sing Right Here by Staind.\nVera Rubin GPU: Nuts Specs, High Price: The Vera Rubin GPU is priced at $50 per chip, while the Maia GPU is at $10, Blackwell is $20 per GPU.\n\nOne member remarked that Vera Rubin is pretty nuts i dont think anyone was expecting those specs lol while another noted, at this point, one GPU can replace the entire datacenter.\nKnowledge Graph RL for Tiny Models?: Members discussed the potential of Knowledge Graph RL for compositional reasoning, potentially enabling tiny models to reliably beat humans.\n\nOne member has tested Kimi linear with the approach and reported that its pretty cool.\nOpencode is nuts: Members raved about the usability of OpenCode, with one stating they haven't touched kilo or roo or cline since.\n\nOne member suggests sandboxing opencode or have it ask permission to run any commands outside the repo i still can’t get myself to trust them fully.\nVoxCPM-1.5 first impressions: A user has been testing VoxCPM-1.5 for training and mentioned that it trains very easily and they can just force 48 kHz NO QUESTIONS ASKED.\n\nThey noted there are no phonemes, but the model copies the speaker's style into the model but requires a voice reference, unlike VITS.\nUnsloth AI (Daniel Han) ▷ #help (32 messages🔥):\nCSM-1B optimization, Unsloth and vLLM support, GPT-OSS-20B with RTX 5090\nFine-Tuning CSM-1B Model Faces RTF Issues: A user needed help with optimizing a fine-tuned CSM-1B model using Unsloth, reporting that they couldn't get the Real-time Factor (RTF) to <1.0, even after torch compilation.\n\nThe original CSM-1B model (pre-fine-tuning) achieved an RTF of 0.6x with the same compilation instructions, and someone suggested that LoRA modules might be adding overhead.\nvLLM Allegedly Supports Unsloth Models: A user asked about Unsloth support in vLLM, and another user responded that most BF16 and 4-bit models posted by Unsloth can be served directly from vLLM and SGLang, and that vLLM experimentally supports GGUFs.\n\nIt was clarified that vLLM is primarily designed for full precision models and AWQ, with GGUF support still experimental and not production-ready.\nGPT-OSS-20B Gets RTX 5090 Boost: A user aimed to run GPT-OSS-20B on an RTX 5090 GPU with very low latency, and was advised that they should use vllm serve openai/gpt-oss-120b.\n\nAnother user confirmed that the full model is in 4-bit and using GGUF would effectively make it worse, furthermore stating that the models were post-trained with MXFP4 quantization, making gpt-oss-20b able to run within 16GB of memory.\nUnsloth AI (Daniel Han) ▷ #showcase (6 messages):\nGemma 27B conversion, GRPO flavored SFT, On policy finetuning\nGemma 27B gets converted!: A member showed off a full conversion of Gemma 27B IT VL to GLM 4.7 Flash thinking, on a Heretic base, trained via Unsloth.\n\nThe converted model, named Gemma3-27B-it-vl-GLM-4.7-Uncensored-Heretic-Deep-Reasoning has benchmarks posted.\nGRPO-Flavored SFT finetuning discussed: Members discussed on policy finetuning and one clarified it's like GRPO flavored SFT.\n\nWhen asked about the mathematical intuition, a paper on the subject (arxiv.org/abs/2601.02151) was shared.\nUnsloth AI (Daniel Han) ▷ #research (83 messages🔥🔥):\nMultimodal Models, RLM - Recursive Language Models, RNNs, Fine-tuning 7B models on 8GB VRAM\nMultimodal Models Questionably Built for Multimodality: Members debated on whether multimodal models truly capture multimodality with one stating they are more like transformers+ rather than fully embodying the idea that the model internalizes the visuals like humans do.\n\nCounterarguments suggested that CNNs and transformers find similar solutions to vision as human brains, implying that CNN-based VLMs might be capable of learning human-like vision.\nRLM: Hype or Helpful?: Alex L Zhang announced RLM-Qwen3-8B, the first natively recursive language model at a small scale, showing significant performance improvements in long-context tasks after being post-trained on only 1,000 trajectories.\n\nA member expressed frustration over the name Recursive Language Models, arguing that it overstates the concept, which they see as a tool-calling loop and that recursive prompting harness would've been a better name.\nRNNs: Recursing through Neural Networks: Members discussed Recursive Neural Networks (RNNs) as recursive architectures, with one pointing out that everything is an RNN, pointing to this paper.\n\nAnother member argued that the definition of RNNs is a general name for neural networks that recurse.\nFinetuning 7B Models on a Budget: A member asked for practical ways to finetune a 7B model on an 8 GB VRAM setup using Unsloth and GRPO.\n\nA member suggested using Unsloth's Colab notebooks.\nPerplexity AI ▷ #announcements (1 messages):\nKimi K2.5, Moonshot AI, Perplexity Pro, Perplexity Max\nKimi K2.5 Comes to Perplexity!: Kimi K2.5, a state-of-the-art open source reasoning model from Moonshot AI, is now available for Perplexity Pro and Max subscribers.\nPerplexity Hosts Kimi K2.5 on US Inference Stack: Perplexity now hosts Kimi K2.5 on Perplexity’s own inference stack in the US, giving us tighter control over latency, reliability, and security for users.\nPerplexity AI ▷ #general (558 messages🔥🔥🔥):\nRegional restrictions on image generation, Perplexity Pro free year confusion, Impact of Indian CEOs, Troubleshooting Perplexity threads and deletion, Kimi 2.5 performance\nImage Generation Woes: Regional Restrictions Hit Users: Users reported receiving errors related to regional restrictions when generating images via the Perplexity Pro subscription, seeking an ETA for a fix or official communication.\n\nOne user humorously noted that removing the region from the prompt allowed image generation to proceed, indicating a possible workaround.\nIndian Summer: Free Subscription Confusion Arises: A user expressed confusion over a free year of Perplexity Pro supposedly offered, only to find it required billing information for a trial.\n\nAnother user highlighted the increasing presence of Indian CEOs in companies like Google, Perplexity, and Adobe, suggesting potential increased engagement with Indian markets, while others weighed the effects of reduced rate limits.\nEnterprise Max Rate Limits Slashed, Users Fume: Users expressed disappointment over significant reductions in query limits for Perplexity Pro and Enterprise Max plans, with one user lamenting a drop from 600 to 50 queries per day.\n\nConcerns were raised about the value proposition of paid plans given the new limits, with speculation that Perplexity might be shifting its strategy towards becoming its own AI model service rather than an aggregator and that a price drop may come soon given lower limits and competition.\nPerplexity Data Loss: A Thread-bare Situation: A user shared their experience of data loss after deleting an Enterprise organization following instructions to remove a red banner, emphasizing the lack of clear warning about thread data deletion.\n\nWhile they had project specs elsewhere, the user lamented losing valuable emergent behaviors discovered during conversations with Perplexity, noting that they had already emailed support but had not heard back after several days.\nKimi K2.5: New Model Makes Waves, Gains Fans: Users lauded the release of Kimi K2.5, with one user noting it could understand images and generally performs well, as good as Gemini Pro and Claude Opus.\n\nOthers discussed its availability and the potential for Perplexity to leverage it in their own AI model and service, with one person also noting its availability on Kilocode.\nPerplexity AI ▷ #sharing (1 messages):\nmanyselves: https://suno.com/song/ee3515d8-3449-4de7-b4f2-dc027d32bbf6\nLMArena ▷ #general (470 messages🔥🔥🔥):\nVPN fix, Moderator ping, New ask here channel, search bar for chat, Image quality\nVPN Activation Resolves Connectivity Snafu: A user reported enabling a VPN resolved their connectivity issues with the platform.\n\nAnother user mentioned rebooting Chrome also fixed the issue, while another chimed in to ping the moderator.\nAsk Here Channel Receives Mixed Feedback: The introduction of a new Ask Here channel <#1340554757827461211> to alleviate question overload in the general channel has sparked debate.\n\nSome users fear it may deter newcomers if their initial interaction involves being redirected, while others appreciate creating room for AI discussions.\nSearch and Archive Chat Features Rolled Out: Two new features have been rolled out: Search Bar enabling chat searches with modality filters, and Archive Chat, for saving chat sessions without cluttering history.\n\nThe process for deleting chat sessions has changed, with instructions available in this help article.\nKimi K2.5 model is really good: Users are impressed with Kimi K2.5, noting its multimodal capabilities and performance.\n\nOne said that Kimi K2.5 is legit the best model I've ever worked with, and another said it Defo beats DeepSeek as my daily driver now.\nSecurity Verification Loop Frustrates: Several users reported being stuck in a loop due to constant security verification requests.\n\nA user suggested that this may be a bot prevention measure and is unavoidable, others have had to resort to incognito browsing.\nLMArena ▷ #announcements (3 messages):\nKimi k2.5 Vision Model, Leaderboard Updates (Text-to-Image, Image Edit, Text-to-Video, Image-to-Video, Code Arena, Text Arena, Search Arena), Search Bar Feature, Archive Chat Feature\nKimi K2.5 takes Vision Victory!: Kimi-k2.5-thinking becomes #1 open model and #6 overall in the Vision Arena leaderboard, distinguishing itself as the sole open model within the Top 15.\nArenas Augment Assortment Across All Areas: The leaderboards receive updates across various modalities, including Text-to-Image, Image Edit, Text-to-Video, Image-to-Video, Code Arena, Text Arena, and Search Arena.\nSearch Savvy Shows: Chat Search Feature Surfaces: Users can now sift through their chats with the new Search Bar feature, complete with modality filtering, providing targeted access to past conversations.\nArchiving Arrives: Chat History Handling Honed: The Archive Chat feature allows users to store chat sessions for future reference without cluttering the active chat history.\nLatent Space ▷ #watercooler (61 messages🔥🔥):\nVoice Input for Coding, Game Industry Job Market, Indie Game Funding, AI Impact on Game Development, Call of Duty's Decline\nMonologue wins for Voice Coding with Claude!: A member recommended Monologue for speech-to-text/code that works with Claude Code CLI, noting it's made by Every and well-suited for Claude code.\n\nAnother user chimed in with support for Superwhisper.\nGame Industry Layoffs Hit Harder Than Tech: It was stated that 1/3 of all game developers in the US lost their jobs last year (Variety Article), painting a grim picture compared to the broader tech job market.\n\nDespite hopes for more high-quality indie games, funding remains a significant challenge with many studios struggling to secure investments.\nFrench Funding Model Boosts Indie Games: The success of Expedition 33 was attributed to French government funding, which de-risks projects and enables studios to secure private capital (see: FrenchTechJournal Article).\n\nHowever, it was noted that investors sometimes pull back due to vibes and unsubstantiated beliefs that AI makes game production cheaper (Related Tweet).\n\""Black Ops 7\"" Flops Amid AI Integration: A member mentioned that Black Ops 7, despite being a large-budget effort utilizing AI extensively, was a total flop, marking it as the worst in the series.\n\nAnother added the Call of Duty series has been on the decline for a while due to players growing weary of reskinned content.\nMac Mini Mania for Clowdbt?: One member expressed temptation to purchase a Mac Mini specifically for running Clowdbt, sparking a discussion.\n\nOther members asked who else wants to be a macmini for clowdbt? and if other members had picked up a unit and what memory they had gotten.\nLatent Space ▷ #creator-economy (4 messages):\nKhaby Lame, Digital Twin, AI Digital Twin Exit\nKhaby Lame Sells His AI Digital Twin for Nearly $1 Billion: TikTok star Khaby Lame, at age 25, reportedly retired after selling his digital likeness and behavioral models for $975 million as reported in this X post.\n\nThe deal enables a company to use an AI 'Digital Twin' of his face and voice for global brand deals, generating massive revenue without his direct involvement.\nAI Digital Twin Revolutionizes Content Creation: The sale of Khaby Lame's digital likeness marks a significant milestone in the use of AI for content creation and brand endorsements.\n\nThis deal allows for the global scaling of brand deals without the need for the individual's physical presence, potentially reshaping the creator economy.\nLatent Space ▷ #memes (7 messages):\nCSS Layout Struggles, Flexbox vs Div Nesting, Agents Discuss Revolution\nChromium cracks CSS Layout Crisis: The Chrome for Developers account posted a joke about the common developer struggle of choosing between Flexbox properties like justify-content and align-items or simply adding extra nesting with another div.\nAgents Agitate Amidst Automation: Agents are seen in an attached image discussing a revolution.\nLatent Space ▷ #stocks-crypto-macro-economics (5 messages):\nMeta's Financial Growth, Corporate Culture at Meta, Trump vs Federal Reserve\nMeta's Financials Soar: Andrew Yeung highlights Meta's impressive financial performance, noting a 22% revenue increase and 82% gross profit margins in a post.\n\nHe also shares a positive personal perspective on the company's work environment and long-term trajectory.\nTrump Takes on the Federal Reserve: Members share a link to a NBC News article regarding Trump's stance against the Federal Reserve.\nLatent Space ▷ #intro-yourself-pls (2 messages):\nAgent Workflows, AI Tools for Scientists, Data Visualization Libraries\nGitHub Engineer Blogs about Agentic Workflows: Brittany, a software engineer at GitHub, shares her interest in agent workflows and provides a link to her recent blog post on the topic of agentic software development.\n\nShe is joining the group to meet other chronically online folks that are also sharing their AI workflows and tips.\nMIT PhD Student Builds Charting Library: Josh, a last-year PhD student at MIT in data visualization, is building a charting library called GoFish that will be out in March.\n\nHe is interested in AI tools that can help scientists, especially notebooks and IDEs, and also likes to write about visualization, PL, and HCI on his blog (https://joshmpollock.com/posts/).\nLatent Space ▷ #tech-discussion-non-ai (23 messages🔥):\nGraphcool, Datadog SQL Execution Plan Visualizer, Rabbit Inc.'s Project Cyberdeck, Supabase, Apollo Meetup at Meteor HQ\nGraphcool Memories Flood Back!: Members reminisced about Graphcool, with one expressing sadness..."",""content"":""**Moltbook** and **OpenClaw** showcase emergent multi-agent social networks where AI agents autonomously interact, creating an AI-native forum layer with complex security and identity challenges. **Karpathy** describes this as \""takeoff-adjacent,\"" highlighting bots self-organizing and engaging in prompt-injection and credential theft. **Anthropic** reports on AI coding tradeoffs with a study of **52 junior engineers** and reveals **Claude** planned a Mars rover drive, marking a milestone in AI-driven space exploration. **Google** publicly releases **Genie 3**, sparking debate over its capabilities and latency issues. The rise of agent-to-agent private communications raises concerns about alignment and observability in 2026."",""contentSnippet"":""**Moltbook** and **OpenClaw** showcase emergent multi-agent social networks where AI agents autonomously interact, creating an AI-native forum layer with complex security and identity challenges. **Karpathy** describes this as \""takeoff-adjacent,\"" highlighting bots self-organizing and engaging in prompt-injection and credential theft. **Anthropic** reports on AI coding tradeoffs with a study of **52 junior engineers** and reveals **Claude** planned a Mars rover drive, marking a milestone in AI-driven space exploration. **Google** publicly releases **Genie 3**, sparking debate over its capabilities and latency issues. The rise of agent-to-agent private communications raises concerns about alignment and observability in 2026."",""guid"":""https://news.smol.ai/issues/26-01-30-moltbook/"",""categories"":[""moltbook"",""openclaw"",""anthropic"",""google"",""claude"",""genie-3"",""karpathy"",""multi-agent-systems"",""agent-communication"",""security"",""prompt-injection"",""identity"",""alignment"",""observability"",""ai-planning"",""ai-coding"",""emergent-behavior""],""isoDate"":""2026-01-30T05:44:39.000Z""}"
Smol,"xAI Grok Imagine API - the #1 Video Model, Best Pricing and Latency - and merging with SpaceX",https://news.smol.ai/issues/26-01-29-xai-grok-imagine-api/,2026-01-29T05:44:39.000Z,"<p><strong>xAI cements its position as a frontier lab.</strong></p>
<blockquote>
<p>AI News for 1/28/2026-1/29/2026. We checked 12 subreddits, <a href=""https://twitter.com/i/lists/1585430245762441216""><strong>544</strong> Twitters</a> and <strong>24</strong> Discords (<strong>253</strong> channels, and <strong>7278</strong> messages) for you. Estimated reading time saved (at 200wpm): <strong>605 minutes</strong>. <strong>Our new website</strong> is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on <a href=""https://x.com/Smol_AI"">@smol_ai</a>!</p>
</blockquote>
<p>It looks like <a href=""https://x.com/mattzeitlin/status/2017027653040001368?s=46"">OpenAI</a> (fundraising at around ~800b), <a href=""https://x.com/mattzeitlin/status/2017027653040001368?s=46"">Anthropic</a> (worth $350b) and now <a href=""https://x.com/amitisinvesting/status/2017001950563160517"">SpaceX + xAI</a> (<a href=""https://x.com/RampLabs/status/2016991534944592176?s=20"">$1100B?</a> - folllowing their <a href=""https://news.smol.ai/issues/26-01-06-xai-series-e"">$20B Series E</a> 3 weeks ago) are in a dead heat racing to IPO by year end. Google made an EXTREMELY strong play today <a href=""https://x.com/swyx/status/2017111381456400603"">launching Genie 3</a> (<a href=""https://news.smol.ai/issues/25-08-05-gpt-oss"">previously reported</a>) to Ultra subscribers, and though technically impressive,, today’s headline story rightfully belongs to Grok, who now have <a href=""https://x.ai/news/grok-imagine-api"">the SOTA Image/Video Generation and Editing model released in API</a> that you can use today.</p>
<p>Artificial Analysis’ rankings says it all:</p>
<p><img src=""https://substackcdn.com/image/fetch/$s_!m-eA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29bc2b9a-cc66-409f-bc00-3eb1abffc039_697x317.png"" alt=""Image""></p>
<p>There’s not much else to say here apart from look at the list of small video model labs and wonder which of them just got bitter lessoned…</p>
<p><img src=""https://substackcdn.com/image/fetch/$s_!Mm1U!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14da553b-af70-4a5e-beb7-4b02e80ae424_2164x912.png"" alt=""""></p>
<hr>
<h1>AI Twitter Recap</h1>
<p><strong>World Models &#x26; Interactive Simulation: Google DeepMind’s Project Genie (Genie 3) vs. Open-Source “World Simulators”</strong></p>
<ul>
<li><strong>Project Genie rollout (Genie 3 + Nano Banana Pro + Gemini)</strong>: Google/DeepMind launched <strong>Project Genie</strong>, a prototype that lets users create and explore <strong>interactive, real-time generated worlds</strong> from <strong>text or image prompts</strong>, with remixing and a gallery. Availability is currently gated to <strong>Google AI Ultra subscribers in the U.S. (18+)</strong>, and the product is explicit about prototype limitations (e.g., <strong>~60s generation limits</strong>, control latency, imperfect physics adherence) (<a href=""https://twitter.com/GoogleDeepMind/status/2016919756440240479"">DeepMind announcement</a>, <a href=""https://twitter.com/GoogleDeepMind/status/2016919762924949631"">how it works</a>, <a href=""https://twitter.com/GoogleDeepMind/status/2016919765713826171"">rollout details</a>, <a href=""https://twitter.com/demishassabis/status/2016925155277361423"">Demis</a>, <a href=""https://twitter.com/sundarpichai/status/2016979481832067264"">Sundar</a>, <a href=""https://twitter.com/Google/status/2016926928478089623"">Google thread</a>, <a href=""https://twitter.com/Google/status/2016972686208225578"">Google limitations</a>). Early-access testers highlight promptability, character/world customization, and “remixing” as key UX hooks (<a href=""https://twitter.com/venturetwins/status/2016919922727850333"">venturetwins</a>, <a href=""https://twitter.com/joshwoodward/status/2016921839038255210"">Josh Woodward demo thread</a>).</li>
<li><strong>Open-source push: LingBot-World</strong>: A parallel thread frames <strong>world models</strong> as distinct from “video dreamers,” arguing for <strong>interactivity, object permanence, and causal consistency</strong>. LingBot-World is repeatedly described as an <strong>open-source real-time interactive world model</strong> built on <strong>Wan2.2</strong> with <strong>&#x3C;1s latency at 16 FPS</strong> and <strong>minute-level coherence</strong> (claims include VBench improvements and landmark persistence after long occlusion) (<a href=""https://twitter.com/dair_ai/status/2016881546909929775"">paper-summary thread</a>, <a href=""https://twitter.com/HuggingPapers/status/2016787043028746284"">HuggingPapers mention</a>, <a href=""https://twitter.com/kimmonismus/status/2016896151610442192"">reaction clip</a>). The meta-narrative: proprietary systems (Genie) are shipping consumer prototypes while open systems race to close capability gaps on <strong>coherence + control</strong>.</li>
</ul>
<p><strong>Video Generation &#x26; Creative Tooling: xAI Grok Imagine, Runway Gen-4.5, and fal’s “Day-0” Platforms</strong></p>
<ul>
<li><strong>xAI Grok Imagine (video + audio) lands near/at the top of leaderboards</strong>: Multiple sources report Grok Imagine’s strong debut in video rankings and emphasize <strong>native audio</strong>, <strong>15s duration</strong>, and aggressive <strong>pricing ($4.20/min including audio)</strong> relative to Veo/Sora (<a href=""https://twitter.com/arena/status/2016748418635616440"">Arena launch ranking</a>, <a href=""https://twitter.com/ArtificialAnlys/status/2016749756081721561"">Artificial Analysis #1 claim + pricing context</a>, <a href=""https://twitter.com/ArtificialAnlys/status/2016749790907027726"">follow-up #1 I2V leaderboard</a>, <a href=""https://twitter.com/EthanHe_42/status/2016749123198673099"">xAI team announcement</a>, <a href=""https://twitter.com/elonmusk/status/2016768088855769236"">Elon</a>). fal positioned itself as <strong>day-0 platform partner</strong> with API endpoints for text-to-image, editing, text-to-video, image-to-video, and video editing (<a href=""https://twitter.com/fal/status/2016746472931283366"">fal partnership</a>, <a href=""https://twitter.com/fal/status/2016746473887609118"">fal links tweet</a>).</li>
<li><strong>Runway Gen-4.5 shifts toward “animation engine” workflows</strong>: Creators describe Gen-4.5 as increasingly controllable for animation-style work (<a href=""https://twitter.com/c_valenzuelab/status/2016721443430510847"">c_valenzuelab</a>). Runway shipped <strong>Motion Sketch</strong> (annotate camera/motion on a start frame) and <strong>Character Swap</strong> as built-in apps—more evidence that vendors are packaging controllability primitives rather than only pushing base quality (<a href=""https://twitter.com/jerrod_lew/status/2016816309762486423"">feature thread</a>). Runway also markets “photo → story clip” flows as a mainstream onramp (<a href=""https://twitter.com/runwayml/status/2016882344427147275"">Runway example</a>).</li>
<li><strong>3D generation joins the same API distribution layer</strong>: fal also added <strong>Hunyuan 3D 3.1 Pro/Rapid</strong> (text/image-to-3D, topology/part generation), showing the same “model-as-a-service + workflow endpoints” pattern spreading from image/video into 3D pipelines (<a href=""https://twitter.com/fal/status/2016877742298411089"">fal drop</a>).</li>
</ul>
<p><strong>Open Models &#x26; Benchmarks: Kimi K2.5 momentum, Qwen3-ASR release, and Trinity Large architecture details</strong></p>
<ul>
<li><strong>Kimi K2.5 as the “#1 open model” across multiple eval surfaces</strong>: Moonshot promoted K2.5’s rank on <strong>VoxelBench</strong> (<a href=""https://twitter.com/Kimi_Moonshot/status/2016732248800997727"">Moonshot</a>) and later Kimi updates focus on productization: <strong>Kimi Code now powered by K2.5</strong>, switching from request limits to <strong>token-based billing</strong>, plus a limited-time <strong>3× quota/no throttling</strong> event (<a href=""https://twitter.com/Kimi_Moonshot/status/2016918447951925300"">Kimi Code billing update</a>, <a href=""https://twitter.com/Kimi_Moonshot/status/2016918450992812443"">billing rationale</a>). Arena messaging amplifies K2.5 as a leading open model with forthcoming Code Arena scores (<a href=""https://twitter.com/arena/status/2016915717539713236"">Arena deep dive</a>, <a href=""https://twitter.com/arena/status/2016923733513105705"">Code Arena prompt</a>); Arena also claims <strong>Kimi K2.5 Thinking</strong> as <strong>#1 open model in Vision Arena</strong> and the only open model in the top 15 (<a href=""https://twitter.com/arena/status/2016984335380001268"">Vision Arena claim</a>). Commentary frames K2.5 as “V3-generation architecture pushed with more continued training,” with next-gen competition expected from K3/GLM-5 etc. (<a href=""https://twitter.com/teortaxesTex/status/2016956019239272717"">teortaxes</a>).</li>
<li><strong>Alibaba Qwen3-ASR: production-grade open speech stack with vLLM day-0 support</strong>: Qwen released <strong>Qwen3-ASR + Qwen3-ForcedAligner</strong> emphasizing messy real-world audio, <strong>52 languages/dialects</strong>, long audio (up to <strong>20 minutes/pass</strong>), and timestamps; models are <strong>Apache 2.0</strong> and include an open inference/finetuning stack. vLLM immediately announced <strong>day-0 support</strong> and performance notes (e.g., “2000× throughput on 0.6B” in their tweet) (<a href=""https://twitter.com/Alibaba_Qwen/status/2016858705917075645"">Qwen release</a>, <a href=""https://twitter.com/Alibaba_Qwen/status/2016859224077455413"">ForcedAligner</a>, <a href=""https://twitter.com/vllm_project/status/2016865238323515412"">vLLM support</a>, <a href=""https://twitter.com/AdinaYakup/status/2016865634559152162"">Adina Yakup summary</a>, <a href=""https://twitter.com/Alibaba_Qwen/status/2016900512478875991"">native streaming claim</a>, <a href=""https://twitter.com/Alibaba_Qwen/status/2016905051395260838"">Qwen thanks vLLM</a>). Net: open-source speech is increasingly “full-stack,” not just weights.</li>
<li><strong>Arcee AI Trinity Large (400B MoE) enters the architecture discourse</strong>: Multiple threads summarize Trinity Large as <strong>400B MoE with ~13B active</strong>, tuned for throughput via sparse expert selection, and featuring a grab bag of modern stability/throughput techniques (router tricks, load balancing, attention patterns, normalization variants). Sebastian Raschka’s architecture recap is the most concrete single reference point (<a href=""https://twitter.com/rasbt/status/2016903019116249205"">rasbt</a>); additional MoE/router stability notes appear in a separate technical summary (<a href=""https://twitter.com/cwolferesearch/status/2016792505111457883"">cwolferesearch</a>). Arcee notes multiple variants trending on Hugging Face (<a href=""https://twitter.com/arcee_ai/status/2016986617584529642"">arcee_ai</a>).</li>
</ul>
<p><strong>Agents in Practice: “Agentic Engineering,” Multi-Agent Coordination, and Enterprise Sandboxes</strong></p>
<ul>
<li><strong>From vibe coding to agentic engineering</strong>: A high-engagement meme-like anchor tweet argues for “Agentic Engineering > Vibe Coding” and frames professionalism around repeatable workflows rather than vibes (<a href=""https://twitter.com/bekacru/status/2016738191341240830"">bekacru</a>). Several threads reinforce the same theme operationally: context prep, evaluations, and sandboxing as the hard parts.</li>
<li><strong>Primer: repo instructions + lightweight evals + PR automation</strong>: Primer proposes a workflow for “AI-enabling” repos: agentic repo introspection → generate an instruction file → run a <strong>with/without</strong> eval harness → scale via batch PRs across org repos (<a href=""https://twitter.com/pierceboggan/status/2016732251535397158"">Primer launch</a>, <a href=""https://twitter.com/pierceboggan/status/2016733056237711849"">local run</a>, <a href=""https://twitter.com/pierceboggan/status/2016733232176193539"">eval framework</a>, <a href=""https://twitter.com/pierceboggan/status/2016733666022424957"">org scaling</a>).</li>
<li><strong>Agent sandboxes + traceability as infra primitives</strong>: Multiple tweets point to “agent sandboxes” (isolated execution environments) as an emerging January trend (<a href=""https://twitter.com/dejavucoder/status/2016979866651152898"">dejavucoder</a>). Cursor proposed an <strong>open standard</strong> to trace agent conversations to generated code, explicitly positioning it as interoperable across agents/interfaces (<a href=""https://twitter.com/cursor_ai/status/2016934752188576029"">Cursor</a>). This pairs with broader ecosystem pressure: agents need auditability and reliable grounding when they can take actions.</li>
<li><strong>Multi-agent coordination beats “bigger brain” framing</strong>: A popular summary claims a system that uses a <strong>controller trained by RL</strong> to route between large/small models can beat a single large agent on HLE with lower cost/latency—reinforcing that orchestration policies are becoming first-class artifacts (<a href=""https://twitter.com/LiorOnAI/status/2016904429543272579"">LiorOnAI</a>). In the same direction, an Amazon “Insight Agents” paper summary argues for pragmatic manager-worker designs with lightweight OOD detection and routing (autoencoder + fine-tuned BERT) instead of LLM-only classifiers for latency/precision reasons (<a href=""https://twitter.com/omarsar0/status/2016880021030522997m"">omarsar0</a>).</li>
<li><strong>Kimi’s “Agent Swarm” philosophy</strong>: A long-form repost from ZhihuFrontier describes K2.5’s agent mode as a response to “text-only helpfulness” and tool-call hallucinations, emphasizing <strong>planning→execution bridging</strong>, dynamic tool-based context, and <strong>multi-viewpoint planning via swarms</strong> (<a href=""https://twitter.com/ZhihuFrontier/status/2016811037274886377"">ZhihuFrontier</a>).</li>
<li><strong>Moltbot/Clawdbot safety trilemma</strong>: Community discussion frames “Useful vs Autonomous vs Safe” as a tri-constraint until prompt injection is solved (<a href=""https://twitter.com/fabianstelzer/status/2016818595687272913"">fabianstelzer</a>). Another take argues capability (trust) bottlenecks dominate: users won’t grant high-stakes autonomy (e.g., finance) until agents are reliably competent (<a href=""https://twitter.com/Yuchenj_UW/status/2016937299125424284"">Yuchenj_UW</a>).</li>
</ul>
<p><strong>Model UX, DevTools, and Serving: Gemini Agentic Vision, OpenAI’s in-house data agent, vLLM fixes, and local LLM apps</strong></p>
<ul>
<li><strong>Gemini 3 Flash “Agentic Vision”</strong>: Google positions Agentic Vision as a structured image-analysis pipeline: planning steps, zooming, annotating, and optionally running Python for plotting—essentially turning “vision” into an agentic workflow rather than a single forward pass (<a href=""https://twitter.com/GeminiApp/status/2016914275886125483"">GeminiApp intro</a>, <a href=""https://twitter.com/GeminiApp/status/2016914637523210684"">capabilities</a>, <a href=""https://twitter.com/GeminiApp/status/2016914638861193321"">rollout note</a>).</li>
<li><strong>OpenAI’s in-house data agent at massive scale</strong>: OpenAI described an internal “AI data agent” reasoning over <strong>600+ PB</strong> and <strong>70k datasets</strong>, using Codex-powered table knowledge and careful context management (<a href=""https://twitter.com/OpenAIDevs/status/2016943147239329872"">OpenAIDevs</a>). This is a rare concrete peek at “deep research/data agent” architecture constraints: retrieval + schema/table priors + org context.</li>
<li><strong>Serving bugs are still real (vLLM + stateful models)</strong>: AI21 shared a debugging story where scheduler token allocation caused misclassification between <strong>prefill vs decode</strong>, now fixed in <strong>vLLM v0.14.0</strong>—a reminder that infrastructure correctness matters, especially for stateful architectures like Mamba (<a href=""https://twitter.com/AI21Labs/status/2016857918436503975"">AI21Labs thread</a>).</li>
<li><strong>Local LLM UX continues to improve</strong>: Georgi Gerganov shipped <strong>LlamaBarn</strong>, a tiny macOS menu bar app built on llama.cpp to run local models (<a href=""https://twitter.com/ggerganov/status/2016912009544057045"">ggerganov</a>). Separate comments suggest agentic coding performance may improve by disabling “thinking” modes for specific models (GLM-4.7-Flash) via llama.cpp templates (<a href=""https://twitter.com/ggerganov/status/2016903216093417540"">ggerganov config note</a>).</li>
</ul>
<p><strong>Top tweets (by engagement)</strong></p>
<ul>
<li><strong>Grok Imagine hype &#x26; distribution</strong>: <a href=""https://twitter.com/elonmusk/status/2016768088855769236"">@elonmusk</a>, <a href=""https://twitter.com/fal/status/2016746472931283366"">@fal</a>, <a href=""https://twitter.com/ArtificialAnlys/status/2016749756081721561"">@ArtificialAnlys</a></li>
<li><strong>DeepMind/Google world models</strong>: <a href=""https://twitter.com/GoogleDeepMind/status/2016919756440240479"">@GoogleDeepMind</a>, <a href=""https://twitter.com/demishassabis/status/2016925155277361423"">@demishassabis</a>, <a href=""https://twitter.com/sundarpichai/status/2016979481832067264"">@sundarpichai</a></li>
<li><strong>AI4Science</strong>: <a href=""https://twitter.com/demishassabis/status/2016763919646478403"">@demishassabis on AlphaGenome</a></li>
<li><strong>Speech open-source release</strong>: <a href=""https://twitter.com/Alibaba_Qwen/status/2016858705917075645"">@Alibaba_Qwen Qwen3-ASR</a></li>
<li><strong>Agents + developer workflow</strong>: <a href=""https://twitter.com/bekacru/status/2016738191341240830"">@bekacru “Agentic Engineering > Vibe Coding”</a>, <a href=""https://twitter.com/cursor_ai/status/2016934752188576029"">@cursor_ai agent-trace.dev</a></li>
<li><strong>Anthropic workplace study</strong>: <a href=""https://twitter.com/AnthropicAI/status/2016960382968136138"">@AnthropicAI AI-assisted coding and mastery</a></li>
</ul>
<hr>
<h1>AI Reddit Recap</h1>
<h2>/r/LocalLlama + /r/localLLM Recap</h2>
<h3>1. Kimi K2.5 Model Discussions and Releases</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"">AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model</a></strong> (Activity: 686): <strong><strong>Kimi</strong> is the research lab behind the open-source <strong>Kimi K2.5</strong> model, engaging in an AMA to discuss their work. The discussion highlights a focus on large-scale models, with inquiries about the development of smaller models like <code>8B</code>, <code>32B</code>, and <code>70B</code> for better intelligence density. There is also interest in smaller Mixture of Experts (MoE) models, such as <code>~100B</code> total with <code>~A3B</code> active, optimized for local or prosumer use. The team is questioned on their stance regarding the notion that <em>Scaling Laws have hit a wall</em>, a topic of current debate in AI research.</strong> Commenters express a desire for smaller, more efficient models, suggesting that these could offer better performance for specific use cases. The debate on scaling laws reflects a broader concern in the AI community about the limits of current model scaling strategies.</p>
<ul>
<li>The discussion around model sizes highlights a preference for smaller models like 8B, 32B, and 70B due to their 'intelligence density.' These sizes are considered optimal for balancing performance and resource efficiency, suggesting a demand for models that can operate effectively on limited hardware while still providing robust capabilities.</li>
<li>The inquiry into smaller Mixture of Experts (MoE) models, such as a ~100B total with ~A3B active, indicates interest in models optimized for local or prosumer use. This reflects a trend towards developing models that are not only powerful but also accessible for individual users or small enterprises, emphasizing the need for efficient resource utilization without sacrificing performance.</li>
<li>The challenge of maintaining non-coding abilities like creative writing and emotional intelligence in models like Kimi 2.5 is significant, especially as coding benchmarks become more prominent. The team is tasked with ensuring these softer skills do not regress, which involves balancing the training focus between technical and creative capabilities to meet diverse user needs.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qpfse6/run_kimi_k25_locally/"">Run Kimi K2.5 Locally</a></strong> (Activity: 553): <strong>The image provides a guide for running the <strong>Kimi-K2.5</strong> model locally, emphasizing its state-of-the-art (SOTA) performance in vision, coding, agentic, and chat tasks. The model, which is a <code>1 trillion</code> parameter hybrid reasoning model, requires <code>600GB</code> of disk space, but the quantized <strong>Unsloth Dynamic 1.8-bit</strong> version reduces this requirement to <code>240GB</code>, a <code>60%</code> reduction. The guide includes instructions for using <code>llama.cpp</code> to load models and demonstrates generating HTML code for a simple game. The model is available on <a href=""https://huggingface.co/unsloth/Kimi-K2.5-GGUF"">Hugging Face</a> and further documentation can be found on <a href=""https://unsloth.ai/docs/models/kimi-k2.5"">Unsloth's official site</a>.</strong> Commenters discuss the feasibility of running the model on high-end hardware, with one user questioning its performance on a Strix Halo setup and another highlighting the substantial VRAM requirements, suggesting that only a few users can realistically run it locally.</p>
<ul>
<li>Daniel_H212 is inquiring about the performance of the Kimi K2.5 model on the Strix Halo hardware, specifically asking for the token generation speed in seconds per token. This suggests a focus on benchmarking the model's efficiency on high-end hardware setups.</li>
<li>Marksta provides feedback on the quantized version of the Kimi K2.5 model, specifically the Q2_K_XL variant. They note that the model maintains high coherence and adheres strictly to prompts, which is characteristic of Kimi-K2's style. However, they also mention that while the model's creative capabilities have improved, it still struggles with execution in creative scenarios, often delivering logical but poorly written responses.</li>
<li>MikeRoz questions the utility of higher quantization levels like Q5 and Q6 (e.g., UD-Q5_K_XL, Q6_K) when most experts prefer int4 quantization. This highlights a debate on the trade-offs between model size, performance, and precision in quantization strategies.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qp87tk/kimi_k25_is_the_best_open_model_for_coding/"">Kimi K2.5 is the best open model for coding</a></strong> (Activity: 1119): <strong>The image highlights <strong>Kimi K2.5</strong> as the leading open model for coding on the LMARENA.AI leaderboard, ranked <code>#7</code> overall. This model is noted for its superior performance in coding tasks compared to other open models. The leaderboard provides a comparative analysis of various AI models, showcasing their ranks, scores, and confidence intervals, emphasizing Kimi K2.5's achievements in the coding domain.</strong> One commenter compared Kimi K2.5's performance to other models, noting it is on par with Sonnet 4.5 in accuracy but not as advanced as Opus in agentic function. Another comment criticized LMArena for not reflecting a model's multi-turn or long context capabilities.</p>
<ul>
<li>A user compared Kimi K2.5 to other models, noting that it performs on par with Sonnet 4.5 in terms of accuracy for React projects, but not at the level of Opus in terms of agentic function. They also mentioned that Kimi 2.5 surpasses GLM 4.7, which was their previous choice, and expressed curiosity about the upcoming GLM-5 from <a href=""http://z.ai"">z.ai</a>.</li>
<li>Another commenter criticized LMArena, stating that it fails to provide insights into a model's multi-turn, long context, or agentic capabilities, implying that such benchmarks are insufficient for evaluating comprehensive model performance.</li>
<li>A user highlighted the cost-effectiveness of Kimi K2.5, stating it feels as competent as Opus 4.5 while being significantly cheaper, approximately 1/5th the cost, and even less expensive than Haiku. This suggests a strong performance-to-cost ratio for Kimi K2.5.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLM/comments/1qp880l/finally_we_have_the_best_agentic_ai_at_home/"">Finally We have the best agentic AI at home</a></strong> (Activity: 464): <strong>The image is a performance comparison chart of various AI models, including <strong>Kimi K2.5</strong>, <strong>GPT-5.2 (xhigh)</strong>, <strong>Claude Opus 4.5</strong>, and <strong>Gemini 3 Pro</strong>. <strong>Kimi K2.5</strong> is highlighted as the top-performing model across multiple categories such as agents, coding, image, and video tasks, indicating its superior capabilities in multimodal applications. The post suggests excitement about integrating this model with a 'clawdbot', hinting at potential applications in robotics or automation.</strong> A comment humorously suggests that hosting the <strong>Kimi 2.5 1T+ model</strong> at home implies having a large home, indicating the model's likely high computational requirements. Another comment sarcastically mentions handling it with a 16GB VRAM card, implying skepticism about the feasibility of running such a model on typical consumer hardware.</p>
</li>
</ul>
<h3>2. Open Source Model Innovations</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qqj51h/lingbotworld_outperforms_genie_3_in_dynamic/"">LingBot-World outperforms Genie 3 in dynamic simulation and is fully Open Source</a></strong> (Activity: 230): <strong>The open-source framework <strong>LingBot-World</strong> surpasses the proprietary <strong>Genie 3</strong> in dynamic simulation capabilities, achieving <code>16 FPS</code> and maintaining object consistency for <code>60 seconds</code> outside the field of view. This model, available on <a href=""https://huggingface.co/collections/robbyant/lingbot-world"">Hugging Face</a>, offers enhanced handling of complex physics and scene transitions, challenging the monopoly of proprietary systems by providing full access to its code and model weights.</strong> Commenters question the hardware requirements for running LingBot-World and express skepticism about the comparison with Genie 3, suggesting a lack of empirical evidence or direct access to Genie 3 for a fair comparison.</p>
<ul>
<li>A user questioned the hardware requirements for running LingBot-World, highlighting the importance of specifying computational needs for practical implementation. This is crucial for users to understand the feasibility of deploying the model in various environments.</li>
<li>Another commenter raised concerns about the lack of a direct comparison with Genie 3, suggesting that without empirical data or benchmarks, claims of LingBot-World's superiority might be unsubstantiated. This points to the need for transparent and rigorous benchmarking to validate performance claims.</li>
<li>A suggestion was made to integrate a smaller version of LingBot-World into a global illumination stack, indicating potential applications in graphics and rendering. This could leverage the model's capabilities in dynamic simulation to enhance visual computing tasks.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qp6rm5/api_pricing_is_in_freefall_whats_the_actual_case/"">API pricing is in freefall. What's the actual case for running local now beyond privacy?</a></strong> (Activity: 1053): <strong>The post discusses the rapidly decreasing costs of API access for AI models, with examples like <strong>K2.5</strong> offering prices at <code>10%</code> of <strong>Opus</strong> and <strong>Deepseek</strong> being nearly free. <strong>Gemini</strong> also provides a substantial free tier. This trend is contrasted with the challenges of running large models locally, such as the need for expensive GPUs or dealing with quantization tradeoffs, which result in slow processing speeds (<code>15 tok/s</code>) on consumer hardware. The author questions the viability of local setups given these API pricing trends, noting that while privacy and latency control are valid reasons, the cost-effectiveness of local setups is diminishing.</strong> Commenters highlight concerns about the sustainability of low API prices, suggesting they may rise once market dominance is achieved, similar to past trends in other industries. Others emphasize the importance of offline capabilities and the ability to audit and trust local models, which ensures consistent behavior without unexpected changes from vendors.</p>
<ul>
<li>Minimum-Vanilla949 highlights the importance of offline capabilities for those who travel frequently, emphasizing the risk of API companies altering terms of service or increasing prices once they dominate the market. This underscores the value of local models for ensuring consistent access and cost control.</li>
<li>05032-MendicantBias discusses the unsustainable nature of current API pricing, which is often subsidized by venture capital. They argue that once a monopoly is achieved, prices will likely increase, making local setups and open-source tools a strategic defense against such business models.</li>
<li>IactaAleaEst2021 points out the importance of repeatability and trust in using local models. By downloading and auditing a model, users can ensure consistent behavior, unlike APIs where vendors might change model behavior over time, potentially reducing its utility for specific tasks.</li>
</ul>
</li>
</ul>
<h3>3. Trends in AI Agent Frameworks</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qq6n3t/github_trending_this_week_half_the_repos_are/"">GitHub trending this week: half the repos are agent frameworks. 90% will be dead in 1 week.</a></strong> (Activity: 538): <strong>The image highlights a trend on GitHub where many of the trending repositories are related to AI agent frameworks, suggesting a surge in interest in these tools. However, the post's title and comments express skepticism about the sustainability of this trend, comparing it to the rapid rise and fall of JavaScript frameworks. The repositories are mostly written in Python and include a mix of agent frameworks, RAG tooling, and model-related projects like NanoGPT and Grok. The discussion reflects a concern that many of these projects may not maintain their popularity or relevance over time.</strong> One comment challenges the claim that half of the trending repositories are agent frameworks, noting that only one is an agent framework by Microsoft, while others are related to RAG tooling and model development. Another comment appreciates the utility of certain projects, like IPTV, for educational purposes.</p>
<ul>
<li>gscjj points out that the claim about 'half the repos being agent frameworks' is inaccurate. They note that the list includes a variety of projects such as Microsoft's agent framework, RAG tooling, and models like NanoGPT and Grok, as well as a model CLI for code named Kimi and a browser API. This suggests a diverse range of trending repositories rather than a dominance of agent frameworks.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qqhhtx/mistral_ceo_arthur_mensch_if_you_treat/"">Mistral CEO Arthur Mensch: “If you treat intelligence as electricity, then you just want to make sure that your access to intelligence cannot be throttled.”</a></strong> (Activity: 357): <strong><strong>Arthur Mensch</strong>, CEO of <strong>Mistral</strong>, advocates for open-weight models, likening intelligence to electricity, emphasizing the importance of unrestricted access to AI capabilities. This approach supports the deployment of models on local devices, reducing costs as models are quantized for lower compute environments, contrasting with closed models that are often large and monetized through paywalls. Mistral aims to balance corporate interests with open access, potentially leading to significant breakthroughs in AI deployment.</strong> Commenters appreciate Mistral's approach to open models, noting the potential for reduced costs and increased accessibility. There is a consensus that open models could democratize AI usage, contrasting with the restrictive nature of closed models.</p>
<ul>
<li>RoyalCities highlights the cost dynamics of model deployment, noting that open models, especially when quantized, reduce costs as they can be run on local devices. This contrasts with closed models that are often large and require significant infrastructure, thus being monetized through paywalls. This reflects a broader industry trend where open models aim to democratize access by lowering hardware requirements.</li>
<li>HugoCortell points out the hardware bottleneck in deploying open models effectively. While open-source models can rival closed-source ones in performance, the lack of affordable, high-performance hardware limits their accessibility. This is compounded by large companies making high-quality local hardware increasingly expensive, suggesting a need for a company capable of producing and distributing its own hardware to truly democratize AI access.</li>
<li>tarruda expresses anticipation for the next open Mistral model, specifically the ""8x22"". This indicates a community interest in the technical specifications and potential performance improvements of upcoming models, reflecting the importance of open model development in advancing AI capabilities.</li>
</ul>
</li>
</ul>
<h2>Less Technical AI Subreddit Recap</h2>
<blockquote>
<p>/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo</p>
</blockquote>
<h3>1. OpenAI and AGI Investments</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/singularity/comments/1qpxyka/nearly_half_of_the_mag_7_are_reportedly_betting/"">Nearly half of the Mag 7 are reportedly betting big on OpenAI’s path to AGI</a></strong> (Activity: 1153): <strong><strong>NVIDIA, Microsoft, and Amazon</strong> are reportedly in discussions to invest a combined total of up to <code>$60 billion</code> into <strong>OpenAI</strong>, with <strong>SoftBank</strong> considering an additional <code>$30 billion</code>. This potential investment could value OpenAI at approximately <code>$730 billion</code> pre-money, aligning with recent valuation discussions in the <code>$750 billion to $850 billion+</code> range. This would mark one of the largest private capital raises in history, highlighting the significant financial commitment from major tech companies towards the development of artificial general intelligence (AGI).</strong> Commenters note the strategic alignment of these investments, with one pointing out that companies like Microsoft and NVIDIA are unlikely to invest in competitors like Google. Another comment reflects on the evolving landscape of large language models (LLMs) and the shifting focus of tech giants.</p>
<ul>
<li>CoolStructure6012 highlights the strategic alignment between <strong>Microsoft (MSFT)</strong> and <strong>NVIDIA (NVDA)</strong> with OpenAI, suggesting that their investments are logical given their competitive stance against <strong>Google</strong>. This reflects the broader industry trend where tech giants are aligning with AI leaders to bolster their AI capabilities and market positions.</li>
<li>drewc717 reflects on the evolution of AI models, noting a significant productivity boost with OpenAI's <code>4.1 Pro mode</code>. However, they express a decline in their workflow efficiency after switching to <strong>Gemini</strong>, indicating that not all LLMs provide the same level of user experience or productivity, which is crucial for developers relying on these tools.</li>
<li>EmbarrassedRing7806 questions the lack of attention on <strong>Anthropic</strong> despite its widespread use in coding through its <strong>Claude</strong> model, as opposed to OpenAI's <strong>Codex</strong>. This suggests a potential underestimation of Anthropic's impact in the AI coding space, where <strong>Claude</strong> might be offering competitive or superior capabilities.</li>
</ul>
</li>
</ul>
<h3>2. DeepMind's AlphaGenome Launch</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/singularity/comments/1qphlfg/google_deepmind_launches_alphagenome_an_ai_model/"">Google DeepMind launches AlphaGenome, an AI model that analyzes up to 1 million DNA bases to predict genomic regulation</a></strong> (Activity: 427): <strong><strong>Google DeepMind</strong> has introduced <strong>AlphaGenome</strong>, a sequence model capable of analyzing up to <code>1 million DNA bases</code> to predict genomic regulation, as detailed in <a href=""https://www.nature.com/articles/s41586-025-10014-0?amp%3Butm_medium=social&#x26;amp%3Butm_campaign=&#x26;amp%3Butm_content="">Nature</a>. The model excels in predicting genomic signals such as gene expression and chromatin structure, particularly in non-coding DNA, which is crucial for understanding disease-associated variants. AlphaGenome outperforms existing models on <code>25 of 26</code> benchmark tasks and is available for research use, with its model and weights accessible on <a href=""https://github.com/google-deepmind/alphagenome_research"">GitHub</a>.</strong> Commenters highlight the model's potential impact on genomics, with some humorously suggesting its significance in advancing scientific achievements akin to winning Nobel prizes.</p>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/MachineLearning/comments/1qq4lnc/r_alphagenome_deepminds_unified_dna_sequence/"">[R] AlphaGenome: DeepMind's unified DNA sequence model predicts regulatory variant effects across 11 modalities at single-bp resolution (Nature 2026)</a></strong> (Activity: 66): <strong><strong>DeepMind's AlphaGenome</strong> introduces a unified DNA sequence model that predicts regulatory variant effects across <code>11 modalities</code> at single-base-pair resolution. The model processes <code>1M base pairs</code> of DNA to predict thousands of functional genomic tracks, matching or exceeding specialized models in <code>25 of 26</code> evaluations. It employs a U-Net backbone with CNN and transformer layers, trained on human and mouse genomes, and captures <code>99%</code> of validated enhancer-gene pairs within a <code>1Mb</code> context. Training on TPUv3 took <code>4 hours</code>, with inference under <code>1 second</code> on H100. The model demonstrates cross-modal variant interpretation, notably on the TAL1 oncogene in T-ALL. <a href=""https://www.nature.com/articles/s41586-025-10014-0"">Nature</a>, <a href=""https://www.biorxiv.org/content/10.1101/2025.06.25.661532v1"">bioRxiv</a>, <a href=""https://deepmind.google/blog/alphagenome-ai-for-better-understanding-the-genome"">DeepMind blog</a>, <a href=""https://github.com/google-deepmind/alphagenome"">GitHub</a>.</strong> Some commenters view the model as an incremental improvement over existing sequence models, questioning the novelty despite its publication in <em>Nature</em>. Others express concerns about the implications of open-sourcing such powerful genomic tools, hinting at potential future applications like 'text to CRISPR' models.</p>
<ul>
<li>st8ic88 argues that while DeepMind's AlphaGenome model is notable for its ability to predict regulatory variant effects across 11 modalities at single-base pair resolution, it is seen as an incremental improvement over existing sequence models predicting genomic tracks. The comment suggests that the model's prominence is partly due to DeepMind's reputation and branding, particularly the use of 'Alpha' in its name, which may have contributed to its publication in Nature.</li>
<li>--MCMC-- is interested in the differences between the AlphaGenome model's preprint and its final published version in Nature. The commenter had read the preprint and is curious about any changes made during the peer review process, which could include updates to the model's methodology, results, or interpretations.</li>
<li>f0urtyfive raises concerns about the potential risks of open-sourcing powerful genomic models like AlphaGenome, speculating on future developments such as 'text to CRISPR' models. This comment highlights the ethical and safety considerations of making advanced genomic prediction tools widely accessible, which could lead to unintended applications or misuse.</li>
</ul>
</li>
</ul>
<h3>3. Claude's Cost Efficiency and Usage Strategies</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/ClaudeAI/comments/1qpcj8q/claude_subscriptions_are_up_to_36x_cheaper_than/"">Claude Subscriptions are up to 36x cheaper than API (and why ""Max 5x"" is the real sweet spot)</a></strong> (Activity: 665): <strong>A data analyst has reverse-engineered <strong>Claude's internal usage limits</strong> by analyzing unrounded floats in the web interface, revealing that <strong>subscriptions can be up to 36x cheaper</strong> than using the API, especially for coding tasks with agents like Claude Code. The analysis shows that the <strong>subscription model offers free cache reads</strong>, whereas the API charges 10% of the input cost for each read, making the subscription significantly more cost-effective for long sessions. The ""Max 5x"" plan at <code>$100/month</code> is highlighted as the most optimized, offering a <code>6x</code> higher session limit and <code>8.3x</code> higher weekly limit than the Pro plan, contrary to the marketed ""5x"" and ""20x"" plans. The findings were derived using the Stern-Brocot tree to decode precise usage percentages into internal credit numbers. Full details and formulas are available <a href=""http://she-llac.com/claude-limits"">here</a>.</strong> Commenters express concern over <strong>Anthropic's lack of transparency</strong> and speculate that the company might change the limits once they realize users have reverse-engineered them. Some users are taking advantage of the current subscription benefits, anticipating potential changes.</p>
<ul>
<li>HikariWS raises a critical point about <strong>Anthropic's lack of transparency</strong> regarding their subscription limits, which could change unexpectedly, rendering current analyses obsolete. This unpredictability poses a risk for developers relying on these plans for cost-effective usage.</li>
<li>Isaenkodmitry discusses the potential for <strong>Anthropic to close loopholes</strong> once they realize users are exploiting subscription plans for cheaper access compared to the API. This highlights a strategic risk for developers who are currently benefiting from these plans, suggesting they should maximize usage while it lasts.</li>
<li>Snow30303 mentions using <strong>Claude code in VS Code for Flutter apps</strong>, noting that it consumes credits rapidly. This suggests a need for more efficient usage strategies or alternative solutions to manage costs effectively when integrating Claude into development workflows.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/ClaudeAI/comments/1qp9ve9/we_reduced_claude_api_costs_by_945_using_a_file/"">We reduced Claude API costs by 94.5% using a file tiering system (with proof)</a></strong> (Activity: 603): <strong>The post describes a file tiering system that reduces <strong>Claude API costs by 94.5%</strong> by categorizing files into HOT, WARM, and COLD tiers, thus minimizing the number of tokens processed per session. This system, implemented in a tool called <code>cortex-tms</code>, tags files based on their relevance and usage frequency, allowing only the most necessary files to be loaded by default. The approach has been validated through a case study on the author's project, showing a reduction from <code>66,834</code> to <code>3,647</code> tokens per session, significantly lowering costs from <code>$0.11</code> to <code>$0.01</code> per session with Claude Sonnet 4.5. The tool is open-source and available on <a href=""https://github.com/cortex-tms/cortex-tms"">GitHub</a>.</strong> One commenter inquired about the manual process of tagging files and updating tags, suggesting the use of git history to automate file heat determination. Another user appreciated the approach due to their own struggles with managing API credits.</p>
<ul>
<li><strong>Illustrious-Report96</strong> suggests using <code>git history</code> to determine file 'heat', which involves analyzing the frequency and recency of changes to classify files as 'hot', 'warm', or 'cold'. This method leverages version control metadata to automate the classification process, potentially reducing manual tagging efforts.</li>
<li><strong>Accomplished_Buy9342</strong> inquires about restricting access to 'WARM' and 'COLD' files, which implies a need for a mechanism to control agent access based on file tier. This could involve implementing access controls or modifying the agent's logic to prioritize 'HOT' files, ensuring efficient resource usage.</li>
<li><strong>durable-racoon</strong> asks about the process of tagging files and updating these tags, highlighting the importance of an automated or semi-automated system to manage file tiering efficiently. This could involve scripts or tools that dynamically update file tags based on usage patterns or other criteria.</li>
</ul>
</li>
</ul>
<hr>
<h1>AI Discord Recap</h1>
<blockquote>
<p>A summary of Summaries of Summaries by Gemini 3.0 Pro Preview Nov-18</p>
</blockquote>
<p><strong>Theme 1. Model Wars: Kimi’s Rise, Recursive Agents, and Geometric Architectures</strong></p>
<ul>
<li><strong>Kimi K2.5 crushes the Vision Arena</strong>: The community reports <strong>Kimi K2.5</strong> is dominating the leaderboards, claiming the <strong>#1 open model</strong> spot and ranking <strong>#6 overall</strong> on the <a href=""https://arena.ai/leaderboard/vision"">Vision Arena leaderboard</a>. Users note it outperforms <strong>Claude</strong> in specific vision tasks and now features a dedicated <strong>computer use</strong> model that handles phone screenshots (though it throws 403 errors on mobile uploads).</li>
<li><strong>Recursive Language Models trigger semantic debates</strong>: A heated discussion erupted over the term ""<strong>Recursive Language Models</strong>"" (<strong>RLM</strong>), with critics arguing it simply rebrands <strong>tool-calling loops</strong>, while proponents point to the new <a href=""https://xcancel.com/a1zhang/status/2016923294461476873?s=46&#x26;t=jDrfS5vZD4MFwckU5E8f5Q"">RLM-Qwen3-8B</a> as the first natively recursive model. This small-scale model, post-trained on just <strong>1,000 trajectories</strong>, reportedly beats scaffolded RLM versions in long-context tasks.</li>
<li><strong>Geometric Convolution attempts to dethrone Attention</strong>: Researchers are experimenting with a baseline that replaces standard <strong>Multi-Head Attention</strong> with a <a href=""https://github.com/MrPan2048/GeometricTransformer"">geometric convolution approach</a>, using embeddings as cell connections. Early debug prints show loss convergence capturing dialogue logic, positioning this as a potential alternative to heavy transformer compute.</li>
</ul>
<p><strong>Theme 2. Hardware Hustle: Microsoft’s Silicon, Unsloth Speeds, and Apple’s Hidden Power</strong></p>
<ul>
<li><strong>Microsoft aims at NVIDIA with Maia 200</strong>: Microsoft unveiled the <a href=""https://blogs.microsoft.com/blog/2026/01/26/maia-200-the-ai-accelerator-built-for-inference/""><strong>Maia 200 AI Accelerator</strong></a>, an inference-focused chip boasting <strong>216GB</strong> of memory and <strong>10k TFLOPS</strong> in FP4 performance. Engineers debated the reliance on <strong>TSMC</strong> manufacturing and compared its architecture favorably against <strong>NVIDIA's Vera Rubin</strong> for large-scale inference workloads.</li>
<li><strong>RTX 5090 shreds training benchmarks</strong>: Unsloth users report the <strong>RTX 5090</strong> achieves blistering training speeds of up to <strong>18k tokens per second</strong>, though <strong>12-15k t/s</strong> is safer with a sequence length under <strong>4096</strong>. Optimal throughput requires carefully balancing <strong>batch size</strong> and sequence length to avoid memory bottlenecks during fine-tuning.</li>
<li><strong>Apple’s ANE punches above its weight</strong>: A new discussion around <a href=""https://arxiv.org/abs/2511.13450"">this paper</a> highlights that Apple's <strong>Neural Engine (ANE)</strong> delivers <strong>3.8 TFlops</strong> on the M4-Pro, nearly matching the GPU's <strong>4.7 TFlops</strong> for GEMM operations. The ANE prioritizes <strong>performance-per-watt</strong>, making it a surprisingly viable target for efficient local inference.</li>
</ul>
<p><strong>Theme 3. Dev Tools &#x26; Standards: Cursor Pains, MCP Security, and Parallel Studio</strong></p>
<ul>
<li><strong>Cursor’s ""Plan Mode"" annoys the power users</strong>: The latest <strong>Cursor</strong> update introduced a <strong>plan mode</strong> that users are actively trying to disable or automate, citing wasted time and unnecessary inputs. Fresh installs of the IDE are reportedly the most unstable configuration, driving users to seek workarounds for the ""Plan Mode"" friction.</li>
<li><strong>MCP gets a hardened Security Standard</strong>: Dani (cr0hn) drafted an open <a href=""https://github.com/mcp-security-standard/mcp-server-security-standard"">MCP Security Standard</a> covering hardening, logging, and access control, intending to donate it to the <strong>Agentic AI Foundation</strong>. Simultaneously, the protocol is evolving with <strong>Namespaces</strong> being rejected in favor of <strong>Groups</strong>, detailed in the new <a href=""https://github.com/modelcontextprotocol/modelcontextprotocol/pull/2084"">Primitive Grouping SEP-2084</a>.</li>
<li><strong>LM Studio 0.4 hides power tools behind Dev Mode</strong>: The release of <strong>LM Studio 0.4.0</strong> tucks critical settings like sampling and hardware configs behind a <strong>Dev Mode</strong> toggle (<code>Ctrl+Shift+R</code>), while introducing <strong>parallel requests</strong>. Users can now load models across different GPUs to handle up to <strong>4 parallel requests</strong> by default, though the software still relies on the older <strong>ROCm 6.4.1</strong>.</li>
</ul>
<p><strong>Theme 4. Jailbreaks &#x26; Exploits: Keygens, ""Remember"" Hacks, and Malware Classifiers</strong></p>
<ul>
<li><strong>Gemini 3 Pro tricked into writing KeyGens</strong>: A user successfully prompted <strong>Gemini 3 Pro</strong> to reverse engineer software and generate a working keygen by pasting code directly from <strong>Ghidra</strong>. While some dismissed this as ""script kiddie"" behavior, it highlights the model's susceptibility to <strong>context-based exploits</strong> when fed technical disassemblies.</li>
<li><strong>""Remember:"" command acts as behavior injection</strong>: Red teamers discovered that the <a href=""https://gemini.google.com/saved-info""><strong>Gemini</strong> command 'Remember:'</a> instantly forces subsequent text into the model's saved memory, heavily influencing future behavior. This allows for persistent prompt injections that dictate turns one at a time, bypassing standard session resets.</li>
<li><strong>Adversarial Malware Classification struggles</strong>: Engineers are fighting to lower the <strong>False Positive Rate (FPR)</strong> in malware classification models using a dataset of <strong>600K</strong> rows and <strong>9,600</strong> binary features. Despite using neural networks and <strong>explainable models</strong> like scikit-learn trees, reducing FPR below <strong>9%</strong> remains a significant hurdle without sacrificing model interpretability.</li>
</ul>
<p><strong>Theme 5. Real-World Agents: Kitchen Robots, World Models, and Bio-AI</strong></p>
<ul>
<li><strong>Figure.Ai’s Helix 02 conquers the kitchen</strong>: A video surfaced of <strong>Figure.Ai's Helix 02</strong> robot autonomously performing complex kitchen tasks, which a user verified by feeding the video into <strong>Kimi</strong> for a <a href=""https://cdn.discordapp.com/attachments/1371757564005711973/1466193526009106452/m2-res_1280p.mp4?ex=697d2c21&#x26;is=697bdaa1&#x26;hm=427bc85209f62b3f47f60ce804f74a7cc41be60c452fb561197ad468c29e5224&#x26;"">98% accurate analysis</a>. This aligns with reports of <strong>Matic</strong> raising <strong>$60M</strong> to build a utility-focused consumer robot successor to the Roomba.</li>
<li><strong>Google releases ""Genie"" World Model</strong>: Google launched <a href=""https://x.com/googleai/status/2016929427784122627""><strong>Project Genie</strong></a> for <strong>AI Ultra</strong> subscribers, a general-purpose world model capable of generating interactive environments from text prompts. This release moves world models from research papers into a deployable product for simulating dynamic scenarios.</li>
<li><strong>AI decodes DNA and Alzheimer’s</strong>: Google AI launched <a href=""https://x.com/GoogleAI/status/1937895472305152387""><strong>AlphaGenome</strong></a> to predict the impact of DNA variants and mutations, while <strong>Goodfire AI</strong> announced new <a href=""https://xcancel.com/goodfireai/status/2016563911508840623"">Alzheimer's biomarkers</a> discovered via model interpretability. These advances signal a shift toward using <strong>transparent AI models</strong> to drive breakthroughs in digital biology.</li>
</ul>
<hr>
<h1>Discord: High level Discord summaries</h1>
<h2><a href=""https://discord.com/channels/1105891499641684019"">BASI Jailbreaking</a> Discord</h2>
<ul>
<li><strong>Gemini 3 Pro Cracks Software KeyGen Style</strong>: A member reported using <strong>Gemini 3 Pro</strong> to create a working keygen from software by pasting code from <strong>Ghidra</strong>.
<ul>
<li>Skeptical members referred to this as <em>script kiddie</em> behavior and suggested trying a reverse engineering CTF challenge.</li>
</ul>
</li>
<li><strong>AI Gets Weaponized for Reverse Engineering</strong>: A member shared his work on weaponizing <strong>AI</strong> for <em>mass reverse engineering, malware analysis, and jailbreak development</em>.
<ul>
<li>Another member questioned this claim, suggesting the original poster may be more skilled at jailbreaking than malware creation.</li>
</ul>
</li>
<li><strong>Sonnet 4.5 Bests Opus with Kaelia Jailbreak</strong>: Members confirmed that <strong>Sonnet 4.5 jailbreaks</strong> work on <strong>Opus</strong>, sharing a <strong>Miss Kaelia jailbreak</strong> based on <strong>ENI Lime</strong> by Vichaps from <a href=""https://docs.google.com/document/d/1aZ91O6LtXyO9DGaWxeJYgKhZhlvbee6jh7_RGTq3mXw/edit?usp=sharing"">this document</a>.
<ul>
<li>The jailbreak may not be as effective as other models, depending on the prompting strategy used.</li>
</ul>
</li>
<li><strong>Gemini's 'Remember:' Command Triggers Behavior</strong>: A member explained that in <strong>Gemini</strong>, the <a href=""https://gemini.google.com/saved-info"">command 'Remember:'</a> automatically adds subsequent words to its saved info, influencing its behavior.
<ul>
<li>Each turn is clearly dictated, one at a time, directly in the chat interface.</li>
</ul>
</li>
<li><strong>NSFW Nano Banana Jailbreak arrives for Kimi 2.5</strong>: A member shared an NSFW jailbreak for <strong>Kimi 2.5</strong>, dubbed the nano banana jailbreak. The <a href=""paste-the-prompt-here"">system prompt</a> frames <strong>Kimi</strong> as an AI assistant from Moonshot AI, permitting NSFW content.
<ul>
<li>The narrative flow proceeds seamlessly without interruption.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1179035537009545276"">Unsloth AI (Daniel Han)</a> Discord</h2>
<ul>
<li><strong>GLM 4.7 Slowdown Solved by CUDA</strong>: Users resolved slow speeds with <strong>GLM 4.7 Flash</strong> on NVIDIA Jetson by ensuring proper <strong>CUDA compilation</strong>, boosting performance from <strong>3 tps</strong> to potentially <strong>70-80 t/s</strong> with <code>-kvu</code> and <code>-fa on</code> flags.
<ul>
<li>Performance discrepancies were observed with <strong>OpenCode</strong>, with one user experiencing slowdowns after opening the model, while another noted that <strong>GLM 4.7</strong> is a better uncensored coder model than <strong>qwen coder</strong> below 32b, but <strong>Qwen Coder</strong> excels at reasoning.</li>
</ul>
</li>
<li><strong>LongCat Leaps onto HuggingFace!</strong>: Meituan's new <strong>n-gram model</strong>, the <a href=""https://huggingface.com/meituan-longcat/LongCat-Flash-Lite"">LongCat model</a>, made its debut on <strong>Hugging Face</strong>, sparking jokes about the proliferation of <em>Flash</em> in model names.
<ul>
<li>Community members speculated that <em>next model Flash-Flash-1b</em> while celebrating new releases.</li>
</ul>
</li>
<li><strong>Microsoft's Maia 200 Challenges NVIDIA</strong>: Microsoft unveiled the <a href=""https://blogs.microsoft.com/blog/2026/01/26/maia-200-the-ai-accelerator-built-for-inference/""><strong>Maia 200 AI Accelerator</strong></a>, a chip built for inference, boasting <strong>216GB</strong> memory and <strong>10k TFLOPS</strong> in FP4 performance.
<ul>
<li>The community discussed the chip's manufacturing by <strong>TSMC</strong> and compared it to <strong>NVIDIA's Vera Rubin</strong> architecture, with some raising concerns about relying on Chinese hardware.</li>
</ul>
</li>
<li><strong>Model Recursive Language Models (RLM) Redefined</strong>: Community members argued that the term ""<strong>Recursive Language Models</strong>"" (<strong>RLM</strong>) is misleading, as it merely describes a <strong>tool-calling loop</strong>, although some maintained that <strong>RLMs</strong> do involve models recursively controlling their environments.
<ul>
<li>Others discussed the recently announced <a href=""https://xcancel.com/a1zhang/status/2016923294461476873?s=46&#x26;t=jDrfS5vZD4MFwckU5E8f5Q""><strong>RLM-Qwen3-8B</strong></a>, the first natively recursive language model, noting its improvements over the base and scaffolded <strong>RLM versions</strong>.</li>
</ul>
</li>
<li><strong>Catastrophic Forgetting Mitigation Methods</strong>: A member suggested mitigating <em>catastrophic forgetting</em> in fine-tuned models by lowering <strong>LoRA rank</strong> and <strong>LR</strong>, reducing <strong>steps/epochs</strong>, and mixing in more general data, as outlined in <a href=""https://unsloth.ai/docs/get-started/fine-tuning-llms-guide"">Unsloth's documentation</a>.
<ul>
<li>They also recommended <em>targeting less layers</em> when finetuning and using <strong>WSL2</strong> and <strong>VSCode</strong> for training.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1340554757349179412"">LMArena</a> Discord</h2>
<ul>
<li><strong>Arena Rebrand Sparks Debate</strong>: <strong>LMArena</strong> rebranded to <strong>Arena</strong>, drawing mixed reactions as some users found the name vague, while others welcomed the expansion beyond <strong>Language Models</strong> to include <strong>image</strong> and <strong>video generation</strong>, as announced in <a href=""https://arena.ai/blog/lmarena-is-now-arena/"">the official blogpost</a>.
<ul>
<li>One user commented that <em>""the name 'Arena' is very vague, and at first glance could mean anything""</em>, in contrast to the easily identifiable 'LMArena'.</li>
</ul>
</li>
<li><strong>Captcha Conundrums Plague Users</strong>: Users reported getting trapped in endless <strong>reCAPTCHA</strong> loops on <strong>Arena</strong>, hindering site usability, with claims of failures even after solving them, some also reported waiting too long can give errors until page is refreshed.
<ul>
<li>A user lamented that <em>""That Google CAPTCHA crap is completely out of control""</em> and questioned why developers were focusing on restyling instead of fixing bugs.</li>
</ul>
</li>
<li><strong>Nano's Image Editing Capabilities Nosedive</strong>: Users observed a performance decline in <strong>Nano Banana</strong>, especially in image editing, reporting instances where it couldn't perform tasks correctly, while the same prompt worked in <strong>Gemini App</strong>.
<ul>
<li>One user simply stated, <em>""Nano 2 can’t even edit anything correctly anymore it seems like""</em>.</li>
</ul>
</li>
<li><strong>Kimi K2.5 Conquers Vision Arena</strong>: <strong>Kimi K2.5</strong> is showing impressive scores on the expert leaderboard, surpassing <strong>Claude</strong> in specific tests, noted for its <strong>vision support</strong> and marked as ""vision"" in direct chat mode.
<ul>
<li><code>Kimi-k2.5-thinking</code> is now the <strong>#1 open model</strong> and ranks <strong>#6 overall</strong> in the <a href=""https://arena.ai/leaderboard/vision"">Vision Arena leaderboard</a>, making it the only open model in the Top 15.</li>
</ul>
</li>
<li><strong>Video Generation Viscosity vexes Viewers</strong>: Some users encountered a ""Hit video limit"" message despite not generating a video, while others experienced lags with lengthy code and responses.
<ul>
<li>Users found they needed to use <strong>canary.lmarena.ai</strong> to enable video uploads, with one suggesting a side-by-side or direct chat interface for video generation.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1074847526655643750"">Cursor Community</a> Discord</h2>
<ul>
<li><strong>Cursor Instability Plagues Fresh Installs</strong>: Users report that a <strong>fresh install</strong> of the <strong>latest Cursor version</strong> is the most unstable configuration.
<ul>
<li>The issue may be related to configuration files, or interaction with other configuration.</li>
</ul>
</li>
<li><strong>Clawdbot Interface Proclaimed 'Glorified Claude'</strong>: Members are discussing the <strong>Clawdbot</strong> interface, accessible from Telegram, one described it as a <em>glorified Claude code interface</em>.
<ul>
<li>The implication is that <strong>Clawdbot</strong> provides a convenient but not necessarily groundbreaking way to interact with <strong>Claude</strong> for code-related tasks.</li>
</ul>
</li>
<li><strong>Users Plot to Deactivate Cursor's Plan Mode</strong>: Users are actively seeking methods to disable Cursor's new <strong>plan mode</strong> or automate its acceptance.
<ul>
<li>The goal is to streamline workflow and minimize unnecessary user input, expressing frustration that it <em>wastes time</em>.</li>
</ul>
</li>
<li><strong>Gemini Agentic Vision Approaches State-of-the-Art</strong>: Enthusiastic users are praising the capabilities of <strong>Gemini agentic vision</strong>, asserting it is <em>getting near sota for vision</em> after initial testing.
<ul>
<li>However, one user reported a fully blacked-out cursor issue, hindering further evaluation and use.</li>
</ul>
</li>
<li><strong>Prompt Engineering Expedites Image Processing</strong>: Members are exchanging techniques for refining prompts to enhance image analysis with Cursor.
<ul>
<li>Suggestions include providing more context or utilizing the prompt <em>Analyze the image for debugging purposes and for an LLM to see the layout clearly</em> to improve processing accuracy and clarity.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1091220969173028894"">OpenRouter</a> Discord</h2>
<ul>
<li><strong>Arcee AI CTO Interview Premieres</strong>: Arcee AI's CTO, <strong>Lucas Atkins</strong>, is featured in a new interview, now available on <a href=""https://youtube.com/live/3XSdqHY0kNk?feature=share"">YouTube</a>.
<ul>
<li>The video showcases <strong>Lucas Atkins</strong> discussing Arcee AI and its latest developments.</li>
</ul>
</li>
<li><strong>OpenRouter Users Await Refunds</strong>: Users are reporting <strong>delayed refunds</strong>, some dating back to January 3rd, with unresolved support tickets and demanding updates from the @OpenRouter team.
<ul>
<li>The delays have caused frustration, with users seeking a clear timeline for when they can expect their refunds to be processed.</li>
</ul>
</li>
<li><strong>GROK Demands Nuclear Power</strong>: A user humorously suggested <em>WE NEED MORE NUCLEAR POWER PLANTS FOR GROK</em>.
<ul>
<li>The user jokingly added to <em>TURN OFF SINGLE INCOME HOMES</em>.</li>
</ul>
</li>
<li><strong>Summergrok Arrives on xAI API</strong>: The Summergrok imagine video is now available on the <a href=""https://x.ai/news/grok-imagine-api"">xAI API</a>.
<ul>
<li>This integration allows developers to incorporate <strong>Summergrok's</strong> capabilities into their projects via the xAI API.</li>
</ul>
</li>
<li><strong>API Key Visibility Limited</strong>: A user encountered an issue with not being able to view their created <strong>API key</strong>.
<ul>
<li>A fellow user clarified that the <strong>API key</strong> is displayed only once upon creation, advising users to save it immediately.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/822583790773862470"">Latent Space</a> Discord</h2>
<ul>
<li><strong>Coffee Alternatives Brewing in LS</strong>: Members discussed alternatives to coffee, with <strong>green tea</strong> highlighted for its lower caffeine dose and the balancing effects of <strong>l-theanine</strong>.
<ul>
<li>One member uses a <strong>gaiwan</strong> with <strong>loose leaf tea</strong> like <a href=""https://www.amazon.com/dp/B00EVK0AI2"">this gunpowder green tea</a> to carefully manage caffeine intake while enjoying the sipping ritual.</li>
</ul>
</li>
<li><strong>Engage with 'Arms Up' Poses, Win Big!</strong>: Showing vulnerability through <strong>'arms up'</strong> body language in UGC increased a creator's views from <strong>12k to 2.1M</strong>, according to <a href=""https://xcancel.com/danielhangan_/status/2016578118585053354?s=46"">this tweet</a>.
<ul>
<li>One member quipped that <em>if porn is doing it then this is definitely the future and I am wrong</em>.</li>
</ul>
</li>
<li><strong>CedarDB performance claims deemed Dubious</strong>: A member linked to <a href=""https://cedardb.com/"">CedarDB</a> and another member linked to a <a href=""https://vxtwitter.com/itunpredictable/status/2016153490586845254?s=20"">vxtwitter link</a> discussing it, but called the <em>perf claims</em> dubious.
<ul>
<li>Another member stated that because it is <em>not open source, DOA for me</em> and shared a lesson: <em>always use an open source data store</em>.</li>
</ul>
</li>
<li><strong>Flapping Airplanes Soar with $180M Round</strong>: <strong>Flapping Airplanes</strong> secured <strong>$180M</strong> in funding from GV, Sequoia, and Index Ventures to advance human-level AI models.
<ul>
<li>The funding aims to accelerate development of new AI models with a specific focus on achieving human-level intelligence, see <a href=""https://xcancel.com/flappyairplanes/status/2016564437499728259"">this tweet</a>.</li>
</ul>
</li>
<li><strong>Google's Genie Out of the Bottle for Ultra Subscribers</strong>: <strong>Google AI</strong> launched <strong>Project Genie</strong> for <strong>Google AI Ultra</strong> subscribers, offering a <strong>general-purpose world model</strong> that creates interactive environments from text prompts.
<ul>
<li>Announced in <a href=""https://x.com/googleai/status/2016929427784122627"">this tweet</a>, this release allows users to generate dynamic content from simple descriptions.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1053877538025386074"">Nous Research AI</a> Discord</h2>
<ul>
<li><strong>Staged Reward Shaping Boosts Parallel Execution</strong>: Members explored using <strong>staged reward shaping</strong> to adjust model weights <em>post-training</em> via reinforcement learning, specifically to favor <strong>parallel execution strategies</strong>.
<ul>
<li>The algorithm evaluates numerous scenarios, rewarding the model for preferring <strong>parallelizations</strong>.</li>
</ul>
</li>
<li><strong>Upscayl: Free Upscaling Tool Impresses</strong>: Members lauded <a href=""https://github.com/upscayl/upscayl"">Upscayl</a>, a <strong>free open-source upscaling tool</strong>, for its surprisingly high quality given its simplicity.
<ul>
<li>One member jokingly asked, <em>'so you guys will now use perl cause of my contributions to it?'</em>.</li>
</ul>
</li>
<li><strong>WebGPU Enables Local Browser AI</strong>: A member shared a <a href=""https://huggingface.co/spaces/webml-community/conversational-webgpu"">WebGPU example</a> demonstrating <strong>AI models running directly in the browser</strong>, spotlighting the potential for local, privacy-focused AI applications.
<ul>
<li>The model loads directly upon page reload, implying that the <strong>model cached over months</strong>, and a user proposed utilizing a <strong>Q8 version in GGUF</strong>.</li>
</ul>
</li>
<li><strong>Gemma 300M a Viable Local Browser AI?</strong>: Members examined the challenges of running AI models locally in browsers due to storage constraints, suggesting that <a href=""https://ai.google.dev/models/gemma""><strong>Gemma 300M</strong></a> might be a suitable option.
<ul>
<li>It's important for users of AI models in browsers that they have privacy, <em>'AND good reference product for other customers'</em>.</li>
</ul>
</li>
<li><strong>SmolLM2 Excels in WebGPU</strong>: Users deemed <a href=""https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct"">HuggingFaceTB/SmolLM2-1.7B-Instruct</a> as a reliable case, and its <strong>1.7B</strong> size is still viable for <strong>WebGPU</strong>.
<ul>
<li>While there are superior models for that task, a user recommended trying <a href=""https://huggingface.co/TheBloke/LlamaFunctionary-2.5-GGUF"">LFM 2.5</a> given its only slightly larger size.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1110598183144399058"">LM Studio</a> Discord</h2>
<ul>
<li><strong>LM Studio Hides Settings Behind Dev Mode</strong>: In <strong>LM Studio 0.4.0</strong>, many settings like <strong>sampling</strong>, <strong>runtime</strong>, and <strong>hardware configs</strong> are now hidden behind <strong>dev mode</strong>, accessible via <code>Ctrl+Shift+R</code> or <code>Cmd+Shift+R</code>.
<ul>
<li>Users can unlock new functionality and appearance changes by enabling <strong>Dev Mode</strong>, found in the bottom left.</li>
</ul>
</li>
<li><strong>Unraid Install Still Lacks Full Stack</strong>: <strong>LM Studio</strong> remains a core executable and <em>not</em> a full stack for <strong>Unraid</strong>, although the new headless mode could enable a stable <strong>Docker container</strong>.
<ul>
<li>Some users hope interface improvements will simplify <strong>LM Studio-as-client</strong> mode implementation in the future.</li>
</ul>
</li>
<li><strong>Parallel Requests Go Live</strong>: <strong>LM Studio 0.4</strong> introduces <strong>parallel requests</strong>, allowing users to load models onto different GPUs and assign them to specific requests.
<ul>
<li>The default setting is <strong>4 parallel requests</strong>; users can configure GPU priority in the same location as before.</li>
</ul>
</li>
<li><strong>ROCm Version Lagging in LM Studio</strong>: Members observed that <a href=""https://lmstudio.ai/enterprise"">LM Studio</a> still uses <strong>ROCm 6.4.1</strong> in the latest <strong>0.4.0 release</strong>, questioning updates to newer versions like <strong>7.2</strong> for better GPU support, including <strong>Strix Halo (gfx1151)</strong>.
<ul>
<li>Discussion centered on whether this outdated version might impact performance and compatibility for newer GPUs.</li>
</ul>
</li>
<li><strong>Nvidia Jetsons Suffer from Ubuntu Bloat</strong>: A member reported that <em>the worst thing about nvidia jetsons is the absurd ubuntu that it comes with them</em>, characterizing it as extremely <em>bloated</em>.
<ul>
<li>Another member noted a <strong>Jetson Xavier AGX</strong> has around <strong>30W TDP</strong>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1047197230748151888"">Perplexity AI</a> Discord</h2>
<ul>
<li><strong>Eagerly Awaiting Kimi 2.5</strong>: Users are anticipating the release of <strong>Kimi 2.5</strong> on Perplexity, with many expressing excitement.
<ul>
<li>Several users posted <em>+1</em> in support.</li>
</ul>
</li>
<li><strong>Clawdbot's Identity Crisis</strong>: A user criticized <strong>Clawdbot</strong> prompting research into its purpose, with discussion clarifying it was an AI personal assistant.
<ul>
<li>Due to its name's similarity to <em>Claude</em>, <strong>Clawdbot</strong> renamed itself to <strong>Moltbot</strong>.</li>
</ul>
</li>
<li><strong>Deep Research Limit Revealed</strong>: Discussion on the usage limits of <strong>Deep Research</strong> for Pro users, capped at <strong>250</strong>.
<ul>
<li>The reset rate for this limit remains unclear.</li>
</ul>
</li>
<li><strong>Comet Fails to Sync</strong>: A user reported that <strong>Comet</strong> is not syncing bookmarks and extensions, despite claims of functionality.
<ul>
<li>Another user suggested checking the <strong>Comet synchronization settings</strong> at <code>comet://settings/synchronisation</code>.</li>
</ul>
</li>
<li><strong>Perplexity Pro Perks Pop for Indians</strong>: Users highlighted that Perplexity Pro, Google One, Chatgpt Go, and Adobe Express Premium are all free for a year for Indian users.
<ul>
<li>A user attributed this to the influence of <strong>Indian CEOs</strong> in these companies and the burgeoning <strong>technology sector in India</strong>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1369594130807787570"">Moonshot AI (Kimi K-2)</a> Discord</h2>
<ul>
<li><strong>Figure.Ai's Helix 02 Cooks Up a Kitchen Storm</strong>: A member shared <a href=""https://cdn.discordapp.com/attachments/1371757564005711973/1466193526009106452/m2-res_1280p.mp4?ex=697d2c21&#x26;is=697bdaa1&#x26;hm=427bc85209f62b3f47f60ce804f74a7cc41be60c452fb561197ad468c29e5224&#x26;"">a video of <strong>Figure.Ai's Helix 02</strong></a> autonomously performing kitchen tasks.
<ul>
<li>Another member used <strong>Kimi</strong> to analyze the video, stating they achieved <strong>98% accuracy</strong> when incorporating the results into slides.</li>
</ul>
</li>
<li><strong>Agent Swarm Elicits Enthusiastic Reactions</strong>: Members discussed <strong>Agent Swarm</strong>, with reactions ranging from concerns about high agent credit consumption to describing the results as <em>super cool</em> and <em>perfect</em>.
<ul>
<li>One member suggested it could be used for checking <strong>Supabase SDK</strong> dependency issues and porting code from <strong>Rust</strong> to <strong>Golang</strong>, with better results than <strong>kimi-cli</strong>.</li>
</ul>
</li>
<li><strong>Token Billing System Sparks Debate</strong>: The introduction of a <strong>token-based billing system</strong> has led to mixed reactions regarding its clarity compared to the previous request-based system.
<ul>
<li>While some find the new system <em>better since some of my follow up queries are quite short and simple</em>, others consider it <em>more vague</em>.</li>
</ul>
</li>
<li><strong>Phone Screenshots Trigger Moderation Filters</strong>: Users are encountering errors, specifically <em>error code: 403</em>, when uploading images, especially screenshots from phones, to <strong>Kimi K2.5</strong>.
<ul>
<li>Screenshots taken from laptops seem to work without issues, suggesting a problem with phone-generated images.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/974519864045756446"">OpenAI</a> Discord</h2>
<ul>
<li><strong>Tesla's FSD Automation Shifts Views</strong>: A user found that driving a <strong>Tesla with Full Self-Driving</strong> is really cool and fun, though it requires constant supervision.
<ul>
<li>The user believes this is why <strong>OpenAI</strong> is upgrading their <strong>Codex</strong> to strongly deal with cybersecurity concerns.</li>
</ul>
</li>
<li><strong>TI-84 Calc Gets Neural Network</strong>: A user created a neural network that <em>runs on the TI-84 directly</em>, capable of autocorrecting / spellchecking words.
<ul>
<li>Other users expressed amazement at the accomplishment.</li>
</ul>
</li>
<li><strong>GPT Pro 5.2 File Handling Suffers Regression</strong>: Users report a regression in <strong>GPT Pro 5.2's file handling</strong>, where uploaded files (ZIP, Excel, PDF) cannot be accessed by the model, despite successful uploads, potentially due to a <strong>broken attachment-to-sandbox mount step</strong>.
<ul>
<li>A user pointed to a <a href=""https://www.reddit.com/r/ChatGPT/comments/1adqc6g/chatgpt_cant_access_my_uploaded_files_today/"">Reddit post</a> echoing the problem.</li>
</ul>
</li>
<li><strong>Animated GIFs Spark Seizure Scrutiny</strong>: A discussion arose after the deletion of animated GIFs due to potential <strong>seizure risks</strong> for viewers with epilepsy.
<ul>
<li>One member stated that <em>the community doesn't need to risk seizures so you can talk about animating gifs in ChatGPT</em> and expressed relief at the removal of flashing images.</li>
</ul>
</li>
<li><strong>Prompt Engineers Get Prompted</strong>: Moderators reminded users that the channel should be used for <strong>prompt engineering discussions</strong> and not for general image outputs, directing them to use the appropriate <code>IMAGES</code> channels instead.
<ul>
<li>One user expressed frustration over the removal of their posts, arguing that they were intended to encourage discussion and showcase a method they were writing a guide about, rather than just sharing images.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1189498204333543425"">GPU MODE</a> Discord</h2>
<ul>
<li><strong>NSys Peeks Behind NCU's Curtain</strong>: Members found that <strong>nsys</strong> reveals kernels like <strong>CUB::SCAN</strong> and <strong>CUB::RADIXSORT</strong> that <strong>ncu</strong> misses, leading to the assumption these kernels launch from <strong>reduce_kernel</strong>.
<ul>
<li>It was shared that after using both <strong>nsys</strong> and <strong>ncu</strong>, one can't go back to using only one profiler.</li>
</ul>
</li>
<li><strong>Sparsity Project Sparking Speedups</strong>: Members proposed a collaboration on a <strong>Sparsity project</strong> to benchmark sparsity patterns and methodologies for performance gains.
<ul>
<li>One member showcased a fork of Karpathy's <code>llm.c</code> on <a href=""https://github.com/WilliamZhang20/sparse-llm.c"">Github</a> using <strong>cuSPARSELt</strong>, reporting substantial training time speedups in later epochs.</li>
</ul>
</li>
<li><strong>Warm GPUs Ward Off Starvation</strong>: Members sought methods to keep GPUs warm for large scale distributed training, aiming to mitigate <strong>GPU starvation</strong>.
<ul>
<li>It was recommended to use <a href=""https://share.google/8yRvJ4znLwfJ9J3UtI"">Charles' container cold start blog post on Modal</a>, a technique with public documentation.</li>
</ul>
</li>
<li><strong>JAX PRs Jostle Jaded Jockeys</strong>: A developer expressed frustration that an <strong>AI-generated pull request</strong> in <strong>JAX</strong> was getting attention, while their <strong>small bug fix</strong> remains unaddressed.
<ul>
<li>This highlighted discussions around <strong>prioritizing pull requests</strong>, especially balancing AI contributions with essential bug fixes.</li>
</ul>
</li>
<li><strong>ML Systems Pioneer Pumps TVM-FFI</strong>: Tianqi Chen presented on <strong>tvm-ffi</strong>, an open ABI and FFI for ML Systems that is being utilized by top submitters to the <strong>nvfp4 competition</strong>, as shown in <a href=""https://www.youtube.com/watch?v=xMzcs6AqLVo"">this video</a>.
<ul>
<li><strong>TVM-FFI</strong> facilitates interoperability for <strong>ML Systems GPU kernels</strong>, reducing host overhead and ensuring out-of-the-box compatibility with PyTorch.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/879548962464493619"">HuggingFace</a> Discord</h2>
<ul>
<li><strong>TRL Pull Request Awaits Review</strong>: A member requested a review for their <a href=""https://github.com/huggingface/trl/pull/4894"">TRL pull request #4894</a>, noting that PR reviews can take weeks or months.
<ul>
<li>They also advised that it is best to wait a few days before tagging someone to review the PR.</li>
</ul>
</li>
<li><strong>GCP Infra Experiences Replica Surge</strong>: A member reported a bug where their replicas for a private model in <strong>GCP</strong> went over their 1 replica max cap to <strong>62 replicas</strong> overnight, despite no configuration changes.
<ul>
<li>The member speculated that they were not the only endpoint affected, and the <strong>GCP</strong> resources are now gone.</li>
</ul>
</li>
<li><strong>Qwen3 TTS Hits the Scene</strong>: A member released the <a href=""https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-Base"">Qwen3-TTS-12Hz-1.7B-Base</a> model with install instructions for MacOS, Linux, and Windows.
<ul>
<li>Another member commented, <em>""cool thing here gonna follow back for this one, really interesting thing you managed to do here imho""</em>.</li>
</ul>
</li>
<li><strong>Diffusers Gets Two-Staged</strong>: The <a href=""https://github.com/huggingface/diffusers"">Diffusers library</a> now supports <strong>LTX-2</strong> distilled checkpoint and <strong>two-stage pipelines</strong> following <a href=""https://github.com/huggingface/diffusers/pull/12934"">this pull request</a>.
<ul>
<li>This update should improve the usability of <strong>Diffusers</strong> for complex diffusion-based tasks.</li>
</ul>
</li>
<li><strong>Math LLM Arrives from Pacific Prime</strong>: Pacific Prime has released the first checkpoint of their <a href=""https://huggingface.co/Pacific-Prime/pacific-prime-math-depth00"">math-specialized 1.5B LLM</a> trained on <strong>GSM8K</strong>, <strong>NuminaMath</strong>, <strong>MetaMathQA</strong> &#x26; <strong>Orca-Math</strong> (~407k samples).
<ul>
<li>The model features step-by-step reasoning with LaTeX notation, useful for advanced mathematical problem-solving.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/714501525455634453"">Yannick Kilcher</a> Discord</h2>
<ul>
<li><strong>Byte-Level Dense MoE Architecture Feedback Sought</strong>: A member seeks feedback on their <strong>dense MoE architecture</strong> for byte-level prediction, utilizing a vocabulary of <strong>256</strong>, <strong>40M parameters</strong>, and <strong>13GB VRAM</strong>.
<ul>
<li>The model uses a <strong>4096 sequence length</strong> and a <strong>batch size of 8</strong>, with the member stating they are able to <em>use the exact same architecture to encode images, or audio, or both</em>.</li>
</ul>
</li>
<li><strong>Thinking AI Architecture Divulged with Subprocess Models</strong>: A member proposed an architecture where a larger “thinking” AI model is monitored by a smaller subprocess model, which pauses the main model to retrieve information from MCPs or CLIs.
<ul>
<li>The goal is to reduce context clutter for the main model, although it's recognized that the subprocess model needs to know what information the main model is missing, and it was described as <em>probably a dumb idea</em>.</li>
</ul>
</li>
<li><strong>Routing and Classification Catapults Model Performance</strong>: Members discussed using a classifier to route user prompts to specialized models, appending the detail to the context of the user prompt, which avoids pausing the larger model and reduces token overhead.
<ul>
<li>There was further discussion on making the classifier and embedding model the same, processing embeddings directly with the LM and specialist model, with one member saying <em>routing and classification would likely be the spiciest move</em>.</li>
</ul>
</li>
<li><strong>Cosine Similarity Fails Causal Relevance</strong>: Members discussed the problem of retrieval being unreliable and confusing to models, and that cosine similarity might not equal causal relevance.
<ul>
<li>One member suggested indexing a SQL database across a model, with the member posting <em>the biggest issue with retrieval imo is that cosine similarity != causal relevance</em>.</li>
</ul>
</li>
<li><strong>Sweep Releases Next-Edit Autocomplete Model</strong>: Sweep is open sourcing <strong>Sweep Next-Edit</strong>, a locally runnable <strong>SOTA LLM</strong> for next-edit autocompletion, models with 0.5B and 1.5B parameters have been released, see <a href=""https://blog.sweep.dev/posts/oss-next-edit"">Sweep's blog</a>.
<ul>
<li>No further details were provided.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1348819876348825620"">Manus.im Discord</a> Discord</h2>
<ul>
<li><strong>Minecraft Launcher Enables AFK</strong>: A user is developing <em>a Minecraft launcher</em> specifically designed to allow AFK gameplay without requiring a <em>high-performance PC</em>.
<ul>
<li>The developer also mentioned capabilities in <em>prompt engineering</em>, data extraction, and even website replication.</li>
</ul>
</li>
<li><strong>Manus Redeem Codes Posted</strong>: A user shared three new <strong>Manus redeem codes</strong>: <a href=""https://manus.im/redeem?c=FUM1A1G7"">FUM1A1G7</a>, <a href=""https://manus.im/redeem?c=ntaxzjg"">ntaxzjg</a>, and <a href=""https://manus.im/redeem?c=mwiyytb"">mwiyytb</a>.
<ul>
<li>Other users confirmed the codes and noted that <em>only one code can be used per month</em>.</li>
</ul>
</li>
<li><strong>AI/ML Engineer Wants Collabs</strong>: An engineer with expertise in building <strong>AI + full-stack systems</strong> is seeking collaborations, especially directing collaboration offers to the <strong>#collab channel</strong>.
<ul>
<li>Their experience includes <strong>LLM integration, RAG pipelines, workflow automation, AI content moderation, Image AI (CLIP + YOLOv8), Voice AI (Whisper, Tacotron2)</strong> and more.</li>
</ul>
</li>
<li><strong>Libyan User Asks If They're First</strong>: A user from <strong>Libya</strong> inquired if they were the only person from their country to use <strong>Manos</strong> since its launch in <strong>early 2025</strong>.
<ul>
<li>Another user extended a welcome to the <strong>Libyan</strong> user, responding with a <em>حياك الله</em>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1358869848138059966"">MCP Contributors (Official)</a> Discord</h2>
<ul>
<li><strong>MCP Security Standard</strong> Proposal Circulates**: Dani (cr0hn) has drafted an open security baseline for MCP servers, including controls for <strong>hardening, logging, access control, and supply chain security</strong>, available at <a href=""https://github.com/mcp-security-standard/mcp-server-security-standard"">https://github.com/mcp-security-standard/mcp-server-security-standard</a>.
<ul>
<li>The author intends to donate it to the <strong>Agentic AI Foundation</strong> and seeks feedback on its compatibility with the <strong>MCP ecosystem</strong>.</li>
</ul>
</li>
<li><strong>Reviewers Request Details For <strong>State Machine</strong> Lifecycle Doc</strong>: A request for feedback was made regarding the addition of a state machine inside the lifecycle doc via <a href=""https://github.com/modelcontextprotocol/modelcontextprotocol/pull/2174"">this pull request</a>.
<ul>
<li>Reviewers suggested clarifying the motivation and context behind the proposed changes for better understanding.</li>
</ul>
</li>
<li><strong>Namespaces</strong> Yield to <strong>Groups</strong> in MCP Evolution**: Discussion indicates that Namespaces have been rejected in favor of Groups within MCP, while the status of <strong>URIs</strong> is less defined, as noted in <a href=""https://github.com/modelcontextprotocol/modelcontextprotocol/issues/1292"">issue 1292</a>.
<ul>
<li>The new <strong>SEP</strong> concerning groups, <a href=""https://github.com/modelcontextprotocol/modelcontextprotocol/pull/2084"">Primitive Grouping SEP-2084</a>, has been published and is currently under deliberation.</li>
</ul>
</li>
<li><strong>SEP-2084</strong> Arises From <strong>SEP-1300</strong> Refinement**: <strong>SEP-1292</strong> was superseded by <strong>SEP-1300</strong>, but faced rejection during a Core Maintainers review due to a lack of consensus.
<ul>
<li>Subsequently, the streamlined <a href=""https://github.com/modelcontextprotocol/modelcontextprotocol/issues/2084"">SEP-2084 - Primitive Grouping</a> has been presented as a replacement.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/729741769192767510"">Eleuther</a> Discord</h2>
<ul>
<li><strong>IGPU struggles with Basic Browser Page</strong>: A user experienced a performance bottleneck of <em>3fps</em> on a specific webpage using a <strong>Ryzen 7 7700 IGPU</strong>.
<ul>
<li>The user posted <a href=""https://fxtwitter.com/i/status/1924135806953787433"">a link on twitter</a> about their experiences using their IGPU.</li>
</ul>
</li>
<li><strong>Geometric Convolution replaces Multi-Head Attention</strong>: A member is experimenting with a baseline that substitutes <strong>Multi-Head Attention</strong> with a <a href=""https://github.com/MrPan2048/GeometricTransformer"">geometric convolution approach</a>, using embeddings as cell connections.
<ul>
<li>The member's debug print showed <code>DEBUG [GEOPARA] | L0_Alpha: 0.1029 L1_Alpha: 0.0947 | L0_Res: 0.0916 L1_Res: 0.1538</code>, and they are seeking feedback on their loss convergence capturing dialogue logic.</li>
</ul>
</li>
<li><strong>Parallelizable RNN Architectures Proposed</strong>: A member suggested exploring other parallelizable <strong>RNN architectures</strong> and conducting more extensive experiments against a robust tokenized baseline.
<ul>
<li>They also posted a link to <a href=""https://arxiv.org/abs/2601.19831"">arxiv.org</a>.</li>
</ul>
</li>
<li><strong>Tackling Malware Classification with Explainable Models</strong>: A member is addressing a <strong>malware classification problem</strong> using a dataset of around <strong>600K</strong> rows and <strong>9,600</strong> binary features, aiming to lower the <strong>false positive rate (FPR)</strong> using <strong>explainable models</strong>.
<ul>
<li>Despite various <strong>feature engineering techniques</strong> and neural networks, they are seeking advice to reduce the FPR below 9% while maintaining explainability, particularly with scikit-learn trees.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1161519468141355160"">DSPy</a> Discord</h2>
<ul>
<li><strong>AlphaXiv Paper Shared</strong>: A member shared <a href=""https://alphaxiv.org/abs/2601.20810"">a link to a paper on AlphaXiv</a>.
<ul>
<li>Further details about the paper were not disclosed.</li>
</ul>
</li>
<li><strong>Custom Skills Invade DSPy</strong>: A member inquired about using custom skills (<strong>.md files with associated .py scripts</strong>) within <strong>DSPy</strong> with a <strong>DSPy ReAct agent</strong>.
<ul>
<li>They mentioned skills like converting <strong>.md to PDF</strong> and sought advice from others.</li>
</ul>
</li>
<li><strong>DSPy Agents Escape to Production</strong>: A member asked about deploying <strong>DSPy agents in production remotely</strong> with <strong>DSPy optimizations in runtime</strong>.
<ul>
<li>The member expressed the need for a runtime environment to support such deployments.</li>
</ul>
</li>
<li><strong>RLM Sandbox Swapping Commences</strong>: A member inquired about swapping the sandbox used by <strong>RLM (Retrieval-augmented Language Model)</strong> with services like <strong>E2B (Ephemeral Environment Builder)</strong>.
<ul>
<li>They sought to replace the local PythonInterpreter with sandboxes like <strong>E2B, Modal, or Daytona</strong>.</li>
</ul>
</li>
<li><strong>Opus Pens Sandboxes</strong>: A member announced that they are working on enabling <strong>Opus</strong> to write new sandboxes.
<ul>
<li>They mentioned a future <strong>protocol for official implementations</strong> from providers such as E2B.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1087530497313357884"">Modular (Mojo 🔥)</a> Discord</h2>
<ul>
<li><strong>Mojo Earns ORNL Recognition</strong>: A research paper titled <a href=""https://arxiv.org/html/2509.21039v1"">Mojo at ORNL</a> has been published, marking a notable achievement for the <strong>Mojo</strong> language and its adoption in scientific research.
<ul>
<li>The paper highlights Mojo's capabilities in addressing complex computational challenges at Oak Ridge National Laboratory (<strong>ORNL</strong>).</li>
</ul>
</li>
<li><strong>macOS Trust Dance May Cause Performance Delta</strong>: Performance differences between the first and subsequent runs on macOS may be due to macOS's <strong>trust dance</strong> rather than a <strong>Mojo-specific</strong> issue, specifically relating to the <em>Gatekeeper tax</em>.
<ul>
<li>Clearing the quarantine <strong>xattr</strong> or ad-hoc codesigning can mitigate these startup delays.</li>
</ul>
</li>
<li><strong>Codesigning mitigates Startup Delays</strong>: For CLI tooling, startup performance is crucial, suggesting potential footgun issues with <strong>docs</strong> or <strong>tooling</strong>.
<ul>
<li>Adding a <strong>codesign</strong> step in <code>mojo build</code> might mitigate this problem, ensuring consistent startup behavior and a better user experience.</li>
</ul>
</li>
<li><strong>Modular Bug Hunt Underway</strong>: A member reported a potential bug and suggested filing an issue, possibly related to <a href=""https://github.com/modular/modular/issues/4767"">issue #4767</a>.
<ul>
<li>Another member reported encountering a weird issue, referencing <a href=""https://github.com/modular/modular/issues/5875"">GitHub issue #5875</a>.</li>
</ul>
</li>
<li><strong>Guard Clause not Needed in Mojo GPU puzzles</strong>: A member noticed that the guard <code>if row &#x3C; size and col &#x3C; size:</code> is unnecessary in Mojo GPU puzzles 3, 4, and 5; omitting it doesn't cause errors.
<ul>
<li>Another member pointed to the solution of <a href=""https://puzzles.modular.com/puzzle_03/puzzle_03.html"">puzzle 03</a> which explained that passing the tests doesn’t necessarily mean the code is sound.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1068976834382925865"">tinygrad (George Hotz)</a> Discord</h2>
<ul>
<li><strong>ANE Balances Performance and Power</strong>: Apple's <strong>ANE</strong> focuses on performance-to-watt tradeoffs rather than maximizing raw performance, according to <a href=""https://arxiv.org/abs/2511.13450"">this paper</a>.
<ul>
<li>The <strong>ANE</strong> achieves competitive performance with excellent energy efficiency, delivering <em>up to 3.8 TFlops on the M4-Pro</em>, close to the <strong>GPU's 4.7 TFlops</strong> for GEMM operations.</li>
</ul>
</li>
<li><strong>Q4 Quantization Gets Results</strong>: Discussions focused on <strong>Q4</strong> as a quantization method.
<ul>
<li>One participant reported achieving speeds of <em>9 t/s</em> using <strong>Q4</strong>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1131200896827654144"">aider (Paul Gauthier)</a> Discord</h2>
<ul>
<li><strong>Aider Friendly Fork gaining momentum</strong>: A member suggested creating a friendly fork of <strong>Aider</strong> to continue development while the original author is busy, emphasizing that <strong>Aider</strong> is written in <strong>Python</strong> and uses <strong>Git</strong> for version control on <strong>GitHub</strong>.
<ul>
<li>The aim is to expand on <strong>Aider</strong>'s existing features, recognizing its utility in comparison to other tools.</li>
</ul>
</li>
<li><strong>Aider poised for orchestrator integration</strong>: A member showed interest in controlling <strong>Aider</strong> from orchestrators like <strong>MultiClaude</strong> or <strong>gas town.sh</strong>.
<ul>
<li>This highlights <strong>Aider</strong>'s capacity to integrate with other tools, facilitating enhanced workflow automation.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/814557108065534033"">MLOps @Chipro</a> Discord</h2>
<ul>
<li><strong>Context Graphs Spark Confusion in AI</strong>: The rise of <strong>context graphs</strong> is causing confusion as terms like <strong>semantic layers</strong> and <strong>ontologies</strong> are used interchangeably, despite their different functions in AI reasoning.
<ul>
<li>A <a href=""https://metadataweekly.substack.com/p/ontologies-context-graphs-and-semantic"">Metadata Weekly article</a> highlights that AI's needs go beyond definitions, requiring explicit relationships, constraints, and assumptions that these concepts.</li>
</ul>
</li>
<li><strong>Semantic Layers Fall Short for AI's Reasoning</strong>: The concept of <em>""just add a semantic layer""</em> isn't cutting it for AI because AI requires more than just data consistency; it needs reasoning, which <strong>ontologies</strong> facilitate by clarifying relationships and assumptions.
<ul>
<li>Traditional <strong>semantic layers</strong> are optimized for dashboards and reporting, not the nuanced understanding AI demands.</li>
</ul>
</li>
<li><strong>YAML Fails to Grasp Business Meaning</strong>: Jessica Talisman argues that <strong>YAML configurations</strong> are inadequate for representing business meaning, which is essential for AI reasoning and understanding.
<ul>
<li>She distinguishes between the design purposes of <strong>semantic layers</strong>, the support that <strong>ontologies</strong> provide for reasoning, and the limitations of <strong>YAML</strong> in capturing business meaning.</li>
</ul>
</li>
</ul>
<hr>
<p>The <strong>LLM Agents (Berkeley MOOC) Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>
<hr>
<p>The <strong>Windsurf Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>
<hr>
<p>You are receiving this email because you opted in via our site.</p>
<p>Want to change how you receive these emails?
You can <a href=""%7B%7B%7BRESEND_UNSUBSCRIBE_URL%7D%7D%7D"">unsubscribe</a> from this list.</p>
<hr>
<h1>Discord: Detailed by-Channel summaries and links</h1>
<h3><strong>BASI Jailbreaking ▷ #<a href=""https://discord.com/channels/1105891499641684019/1235691879492751460/1466161493950071004"">general</a></strong> (1118 messages🔥🔥🔥):</h3>
<blockquote>
<p><code>Gemini 3 Jailbreak, AI and Code Exploits, Win10 vs Win11 Security, AI Personality Clones, AI-Assisted Coding Impact</code></p>
</blockquote>
<ul>
<li><strong>Gemini 3 Pro Cracks Software, KeyGen Style</strong>: A member claimed to have used <strong>Gemini 3 Pro</strong> to reverse engineer a key system from software by pasting code from <strong>Ghidra</strong> into <strong>Gemini</strong>, creating a working keygen.
<ul>
<li>Others expressed skepticism, with one user calling this behavior <em>script kiddie</em> and urging the member to try a reverse engineering CTF challenge.</li>
</ul>
</li>
<li><strong>Weaponizing AI for Reverse Engineering</strong>: A member shares his work weaponizing <strong>AI</strong> for <em>mass reverse engineering, malware analysis, and jailbreak development</em>.
<ul>
<li>Another member questions this claim, as he could probably not write malware himself, but he can probably jailbreak.</li>
</ul>
</li>
<li><strong>Win10 Hardening Woes</strong>: A member details their custom <strong>Windows 10</strong> setup, involving third-party tools, XP binaries, and registry modifications.
<ul>
<li>Others express concerns, with one user saying, <em>Jesus Christ</em>, while another says, <em>Keep pushing, Local - the aneurism is coming, I can feel it!</em></li>
</ul>
</li>
<li><strong>AI's Impact on Semantic Errors</strong>: A member describes their research paper topic: <em>An assessment of the impact of AI-assisted coding in IDEs on the frequency of semantic errors during timed Python programming tasks among novice student developers</em>.
<ul>
<li>Most members agree that the undergraduate system feels like it's over, because of AI.</li>
</ul>
</li>
<li><strong>Peptides for Workout Recovery</strong>: A member brought up BPC 157 and TB 500 to help with healing.
<ul>
<li>Another member expresses ignorance about these drug compounds, but hopes that there will be drugs that will save him before he passes.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>BASI Jailbreaking ▷ #<a href=""https://discord.com/channels/1105891499641684019/1228043845967544380/1466182451079544965"">jailbreaking</a></strong> (216 messages🔥🔥):</h3>
<blockquote>
<p><code>Sonnet 4.5 Jailbreaks, Claude paid model for free, Miss Kaelia jailbreak, Grok imagine Jailbreak, Gemini 3 Pro Jailbreak</code></p>
</blockquote>
<ul>
<li><strong>Sonnet 4.5 Jailbreaks Opus</strong>: Members find that <strong>Sonnet 4.5 jailbreaks</strong> work fine on <strong>Opus</strong>, with one sharing their <strong>Miss Kaelia jailbreak</strong> based on <strong>ENI Lime</strong> by Vichaps at <a href=""https://docs.google.com/document/d/1aZ91O6LtXyO9DGaWxeJYgKhZhlvbee6jh7_RGTq3mXw/edit?usp=sharing"">this link</a>.</li>
<li><strong>Is Grok jailbreak reinforced?</strong>: Members report that <strong>Grok</strong> is heavily reinforced, but still possible to break, however one member said <em>yeah it is completely shut down mate nothing getting past it.</em>
<ul>
<li>A shared <a href=""https://github.com/Goochbeater/Spiritual-Spell-Red-Teaming/tree/main/Jailbreak-Guide"">Github link</a> should be working.</li>
</ul>
</li>
<li><strong>Gemini's ""Remember:"" command manipulates behavior</strong>: A member explains that in <strong>Gemini</strong>, each separate turn is dictated clearly, 1 turn at a time, right in the chat, and that the <a href=""https://gemini.google.com/saved-info"">command 'Remember:'</a> will automatically add the words that follow to it's saved info.</li>
<li><strong>Thinking of Thoughts is best trick</strong>: Members state that the best trick with <strong>Claude</strong> in particular is showing viable reasons why you want the output and telling it to <em>think about thinking</em>
<ul>
<li>One adds that <em>when people would ask me what ToT was i would tell them ""thinking of thoughts""</em>.</li>
</ul>
</li>
<li><strong>nano banana NSFW jailbreak for Kimi 2.5</strong>: A member shares a NSFW for kimi 2.5 thinking known as the nano banana jailbreak.
<ul>
<li>The <a href=""paste-the-prompt-here"">system prompt</a> sets <strong>Kimi</strong> as an AI assistant created by Moonshot AI, maintaining the narrative flow without interruption where NSFW is permitted.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>BASI Jailbreaking ▷ #<a href=""https://discord.com/channels/1105891499641684019/1204553141354504193/1466205013654241463"">redteaming</a></strong> (5 messages):</h3>
<blockquote>
<p><code>Red Teaming Path, Uncensored Coder</code></p>
</blockquote>
<ul>
<li><strong>User quests for Red Teaming Path</strong>: A member requested guidance on a path into <strong>red teaming</strong>.
<ul>
<li>Another member provided a <a href=""https://discord.com/channels/1105891499641684019/1432845259825741824"">link</a> guaranteeing evolution into a <em>Level 9 official Red Team Pro</em>.</li>
</ul>
</li>
<li><strong>Uncensored Coder on Deck</strong>: A member inquired about a better <strong>uncensored coder</strong> than <em>qwen 2.5 32b / huihui/qwen2.5 -abliterate 72b</em>.
<ul>
<li>Another member responded with a simple question: <em>You new?</em></li>
</ul>
</li>
</ul>
<hr>
<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=""https://discord.com/channels/1179035537009545276/1179035537529643040/1466166498329628837"">general</a></strong> (435 messages🔥🔥🔥):</h3>
<blockquote>
<p><code>GLM 4.7 performance, LongCat model, Model Quantization and TTS Models, Hardware Trends &#x26; GPU Availability, AI Moderation Tools</code></p>
</blockquote>
<ul>
<li><strong>GLM 4.7 struggles with speed and CUDA compilation</strong>: Members discussed performance issues with <strong>GLM 4.7 Flash</strong> on NVIDIA Jetson, with one user initially reporting only <strong>3 tokens per second (tps)</strong>, but later discovering they hadn't compiled with <strong>CUDA support</strong>, resulting in poor CPU-bound performance.
<ul>
<li>After ensuring proper CUDA compilation, performance improved, but discrepancies remained, as one user experienced slowdowns after opening the model in <strong>OpenCode</strong>, whereas another suggested using <code>-kvu</code> and <code>-fa on</code> flags to potentially reach <strong>70-80 t/s</strong> on a higher-end GPU.</li>
</ul>
</li>
<li><strong>LongCat Model hits HuggingFace</strong>: The community discussed the <a href=""https://huggingface.co/meituan-longcat/LongCat-Flash-Lite"">LongCat model</a>, a new <strong>n-gram model</strong> from Meituan, with one member pointing out its presence on <strong>Hugging Face</strong> and another joking about the trend of models including <em>Flash</em> in their names.
<ul>
<li>One member posted a <a href=""https://tenor.com/view/flash-lampo-speed-gif-18173027"">flash GIF</a> along with the comment, <em>next model Flash-Flash-1b</em>.</li>
</ul>
</li>
<li><strong>AMD's mi308 competes with NVidia</strong>: Members debated the merits of AMD's <strong>Radeon Instinct MI308X</strong>, noting its impressive specs (<strong>192GB of RAM</strong> and comparable performance) but also highlighted NVIDIA's advantage in compatibility and features like <strong>NVFP4</strong>.
<ul>
<li>A member shared a <a href=""https://www.techpowerup.com/gpu-specs/radeon-instinct-mi308x.c4295"">link to the MI308X specs</a> and mused about acquiring two for personal use in the future, envisioning <strong>384GB</strong> of fast compute with reasonable power consumption.</li>
</ul>
</li>
<li><strong>Quantization Considerations for TTS Models</strong>: Users inquired about the impact of <strong>quantization</strong> on <strong>TTS models</strong>, questioning whether issues similar to those seen with vision projectors might arise.
<ul>
<li>Experts suggested that <strong>TTS models</strong> generally handle <strong>quantization</strong> well, with some recommending specific models like <strong>Qwen3-TTS</strong> and <strong>Kokoro</strong>, and others cautioning that voice cloning is just a <em>gimmick</em>.</li>
</ul>
</li>
<li><strong>AI steps up for discord moderation</strong>: A member sought advice on using AI for Discord moderation, citing the limitations of regex in combating spam and bypasses.
<ul>
<li>They considered using a small local AI to understand the Polish language and sentence structure for moderation purposes, while others suggested alternative methods for managing bots and spam.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=""https://discord.com/channels/1179035537009545276/1179039724355211325/1466183283053297757"">introduce-yourself</a></strong> (3 messages):</h3>
<blockquote>
<p><code>Introduction, ML Engineer, Local LLMs, Document Processing, Alpaca</code></p>
</blockquote>
<ul>
<li><strong>Jack Joins the Community!</strong>: Jack, an <strong>ML Engineer</strong> from Texas specializing in <strong>document processing</strong>, introduces himself to the Unsloth community.
<ul>
<li>He expresses interest in <strong>local LLMs</strong>, tracing back to the <strong>Alpaca</strong> model.</li>
</ul>
</li>
<li><strong>Document Processing Expertise</strong>: Jack's primary work involves <strong>document processing</strong>, a field distinct from LLMs.
<ul>
<li>His interest in <strong>local LLMs</strong> started with the <strong>Alpaca</strong> model, indicating a foundational understanding of the field.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=""https://discord.com/channels/1179035537009545276/1179039861576056922/1466161229897928998"">off-topic</a></strong> (649 messages🔥🔥🔥):</h3>
<blockquote>
<p><code>GPU hours wasted, GGUFs unsafe, 3b llama holds context, LLMs hallucinations, Model working</code></p>
</blockquote>
<ul>
<li><strong>Engineers Lament Dependency Mazes and GPU Cost</strong>: Engineers commiserate about dependency mazes and wasted GPU hours, hoping they are <em>not alone</em> in facing these challenges and finding solace in community.
<ul>
<li>A user humorously remarks about their models <em>made the creepy assumption that it was trained on my voice</em> and that <em>my hubris grows daily</em>.</li>
</ul>
</li>
<li><strong>Concerns about GGUFs Safety Surface</strong>: A member inquired about resources discussing the potential unsafety of <strong>GGUFs</strong>, particularly if a malicious actor got involved.
<ul>
<li>One member noted he <em>wouldn't dare speak</em> if he felt the crushing weight of the sloths while training.</li>
</ul>
</li>
<li><strong>New Music Gen Drops</strong>: A user announced new <strong>music generation tools</strong> with <strong>48 kHz</strong> will be dropping soon, emphasizing trainability and prompting preparations for chime, water, and fire sounds.
<ul>
<li>This same user said: <em>I need SFX, not music</em>.</li>
</ul>
</li>
<li><strong>Microsoft Announces Maia 200 AI Accelerator</strong>: Microsoft announced the <strong>Maia 200 AI Accelerator</strong>, built for inference, featuring <strong>216GB</strong> memory and <strong>10k TFLOPS</strong> in FP4 performance (<a href=""https://blogs.microsoft.com/blog/2026/01/26/maia-200-the-ai-accelerator-built-for-inference/"">Microsoft Blog</a>).
<ul>
<li>Discussions ensued regarding the chip's manufacturing by <strong>TSMC</strong> and comparisons to <strong>NVIDIA's Vera Rubin</strong> architecture, with some expressing concerns about reliance on Chinese hardware and the potential impact on consumers.</li>
</ul>
</li>
<li><strong>Boatbomber attempts Pretraining Run</strong>: User boatbomber is <em>starting over</em> to conduct a pretraining run teaching the model cuneiform to improve output coherence.
<ul>
<li>This process is estimated to take <em>another 150 hours</em> to improve domain knowledge.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=""https://discord.com/channels/1179035537009545276/1179777624986357780/1466195345158705252"">help</a></strong> (75 messages🔥🔥):</h3>
<blockquote>
<p><code>Windows training, Multi-GPU training with Unsloth on Modal, Catastrophic forgetting mitigation, Best models to finetune, DGXSpark RuntimeError</code></p>
</blockquote>
<ul>
<li><strong>Windows training hurdles squashed with WSL2</strong>: To train a model on Windows, a member suggested using <strong>WSL2</strong> and <strong>VSCode</strong> for a clean setup, with instructions available in the help channel by searching for <em>WSL</em>.
<ul>
<li>The member also clarified that, if training with many json files, setting up WSL2 with VSCode will make the training procedure easier.</li>
</ul>
</li>
<li><strong>Unsloth Multi-GPU Training Glitches on Modal</strong>: A user encountered a <em>ValueError</em> when training a <strong>Qwen3</strong> model on Modal with multiple GPUs, related to the <code>device_map</code> setting in <strong>Unsloth</strong>.
<ul>
<li>They were advised to consult specific versions of <em>unsloth</em> and <em>unsloth_zoo</em> for multi-GPU support, but also acknowledged that <strong>Multi-GPU finetuning is still experimental</strong>.</li>
</ul>
</li>
<li><strong>Catastrophic Forgetting Fixes</strong>: When a finetuned model forgets previous knowledge, a member suggested mitigating <em>catastrophic forgetting</em> by lowering <strong>LoRA rank</strong>, <strong>LR</strong>, reducing <strong>steps/epochs</strong>, and mixing in more general data.
<ul>
<li>They also suggested <em>targeting less layers</em> when finetuning, as well as <a href=""https://unsloth.ai/docs/get-started/fine-tuning-llms-guide"">reducing steps/epochs and mixing in more general data</a>.</li>
</ul>
</li>
<li><strong>DGXSpark Nvidia-CUDA Nightmare</strong>: Users encountered a <code>RuntimeError</code> related to device compatibility when using the <strong>DGXSpark</strong> container, potentially due to issues with <strong>Nvidia's custom CUDA</strong>.
<ul>
<li>The suggested fix involved <em>restarting the kernel</em>, <em>restarting the container</em>, or <em>resetting the GPU</em>, with the last option being the most reliable.</li>
</ul>
</li>
<li><strong>Humans Debate Best Uncensored Coder Models</strong>: When a user asked about uncensored coder models, it was said that <strong>glm 4.7</strong> is better than <strong>qwen coder</strong> below 32b and its <em>pretty good</em> in my experience with spitting out good presets for every language I mess with
<ul>
<li>They clarified that <strong>Qwen Coder</strong> is better at reasoning with code, but <strong>GLM4.7</strong> knows <em>alot more general code, which is what an llm is best at anyway</em></li>
</ul>
</li>
</ul>
<hr>
<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=""https://discord.com/channels/1179035537009545276/1179779344894263297/1466516150950301814"">showcase</a></strong> (2 messages):</h3>
<blockquote>
<p><code>GPU Training Speeds, Sequence Length Optimization, RTX 5090 Performance</code></p>
</blockquote>
<ul>
<li><strong>RTX 5090 blazing-fast training speeds</strong>: The RTX <strong>5090</strong> can achieve up to <strong>18k tokens per second</strong> in training with Unsloth, but <strong>12-15k tokens per second</strong> is a safe bet with <strong>&#x3C;4096 seq_len</strong>.
<ul>
<li>The speed depends on the setup, especially the balance between <strong>batch size</strong> and <strong>seq_len</strong>.</li>
</ul>
</li>
<li><strong>Token example affecting training time</strong>: The initial training phase involved <strong>&#x3C;768 token examples</strong>, influencing the overall training duration.
<ul>
<li>Performance can vary with model size and specific configurations.</li>
</ul>
</li>
<li><strong>Seq_len considerations with training</strong>: Optimal training speed depends on balancing <strong>batch size</strong> and <strong>seq_len</strong> and the <strong>RTX 5090</strong> allows up to <strong>18k tokens per second</strong>.
<ul>
<li>Speeds of <strong>12-15k tokens per second</strong> are achievable with <strong>&#x3C;4096 seq_len</strong>, varying based on model size.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=""https://discord.com/channels/1179035537009545276/1257011997250424842/1466203537078747188"">research</a></strong> (97 messages🔥🔥):</h3>
<blockquote>
<p><code>DeepSeek mHC residual preservation, RL researchers rediscover context distillation, MiniMaxAI role-play-bench dataset, Recursive Language Models (RLM)</code></p>
</blockquote>
<ul>
<li><strong>DeepSeek's mHC and Context Distillation</strong>: Members discussed how <a href=""https://arxiv.org/abs/2209.15189"">context distillation</a> might relate to <strong>DeepSeek's mHC residual preservation</strong>, noting similarities and differences in their approaches.
<ul>
<li>One member expressed surprise at the relatively small performance boost (1-2 points) from context distillation, while another noted that the application of the technique was novel.</li>
</ul>
</li>
<li><strong>MiniMaxAI releases first RP bench</strong>: A user shared a <a href=""https://huggingface.co/datasets/MiniMaxAI/role-play-bench"">link</a> to what they claimed was the <strong>first role-play benchmark dataset</strong>, created by <strong>MiniMaxAI</strong>.
<ul>
<li>Others pointed out that there have been numerous <strong>Chinese RP benches</strong> with superior methodologies, notably <strong>Ping Pong Bench</strong> for human preference and <strong>COSER</strong> for roleplay accuracy.</li>
</ul>
</li>
<li><strong>RLM is just Recursive Tool Calling</strong>: A member criticized the term ""<strong>Recursive Language Models</strong>"" (<strong>RLM</strong>), suggesting it misleadingly implies more than just a <strong>tool-calling loop</strong>.
<ul>
<li>In response, one member argued that <strong>RLMs</strong> involve models recursively controlling their environments, which is more than <em>just recursive tool calling</em>, and another suggested the alternative names <strong>RReplagents</strong> or <strong>Recursive Repl Agents</strong>.</li>
</ul>
</li>
<li><strong>Natively Recursive Language Model (RLM) at Small Scale</strong>: A user shared <a href=""https://xcancel.com/a1zhang/status/2016923294461476873?s=46&#x26;t=jDrfS5vZD4MFwckU5E8f5Q"">Alex L Zhang's tweet</a> announcing <strong>RLM-Qwen3-8B</strong>, the first natively recursive language model at a small scale.
<ul>
<li>It was post-trained on only <strong>1,000 trajectories</strong>, the model shows significant performance improvements over both the base <strong>Qwen3-8B</strong> and scaffolded <strong>RLM versions</strong>, particularly in long-context tasks.</li>
</ul>
</li>
</ul>
<hr>
<h3>**LMArena ▷ #[general](https...</h3>
","{""title"":""xAI Grok Imagine API - the #1 Video Model, Best Pricing and Latency - and merging with SpaceX"",""link"":""https://news.smol.ai/issues/26-01-29-xai-grok-imagine-api/"",""pubDate"":""Thu, 29 Jan 2026 05:44:39 GMT"",""content:encoded"":""<p><strong>xAI cements its position as a frontier lab.</strong></p>\n<blockquote>\n<p>AI News for 1/28/2026-1/29/2026. We checked 12 subreddits, <a href=\""https://twitter.com/i/lists/1585430245762441216\""><strong>544</strong> Twitters</a> and <strong>24</strong> Discords (<strong>253</strong> channels, and <strong>7278</strong> messages) for you. Estimated reading time saved (at 200wpm): <strong>605 minutes</strong>. <strong>Our new website</strong> is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on <a href=\""https://x.com/Smol_AI\"">@smol_ai</a>!</p>\n</blockquote>\n<p>It looks like <a href=\""https://x.com/mattzeitlin/status/2017027653040001368?s=46\"">OpenAI</a> (fundraising at around ~800b), <a href=\""https://x.com/mattzeitlin/status/2017027653040001368?s=46\"">Anthropic</a> (worth $350b) and now <a href=\""https://x.com/amitisinvesting/status/2017001950563160517\"">SpaceX + xAI</a> (<a href=\""https://x.com/RampLabs/status/2016991534944592176?s=20\"">$1100B?</a> - folllowing their <a href=\""https://news.smol.ai/issues/26-01-06-xai-series-e\"">$20B Series E</a> 3 weeks ago) are in a dead heat racing to IPO by year end. Google made an EXTREMELY strong play today <a href=\""https://x.com/swyx/status/2017111381456400603\"">launching Genie 3</a> (<a href=\""https://news.smol.ai/issues/25-08-05-gpt-oss\"">previously reported</a>) to Ultra subscribers, and though technically impressive,, today’s headline story rightfully belongs to Grok, who now have <a href=\""https://x.ai/news/grok-imagine-api\"">the SOTA Image/Video Generation and Editing model released in API</a> that you can use today.</p>\n<p>Artificial Analysis’ rankings says it all:</p>\n<p><img src=\""https://substackcdn.com/image/fetch/$s_!m-eA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29bc2b9a-cc66-409f-bc00-3eb1abffc039_697x317.png\"" alt=\""Image\""></p>\n<p>There’s not much else to say here apart from look at the list of small video model labs and wonder which of them just got bitter lessoned…</p>\n<p><img src=\""https://substackcdn.com/image/fetch/$s_!Mm1U!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14da553b-af70-4a5e-beb7-4b02e80ae424_2164x912.png\"" alt=\""\""></p>\n<hr>\n<h1>AI Twitter Recap</h1>\n<p><strong>World Models &#x26; Interactive Simulation: Google DeepMind’s Project Genie (Genie 3) vs. Open-Source “World Simulators”</strong></p>\n<ul>\n<li><strong>Project Genie rollout (Genie 3 + Nano Banana Pro + Gemini)</strong>: Google/DeepMind launched <strong>Project Genie</strong>, a prototype that lets users create and explore <strong>interactive, real-time generated worlds</strong> from <strong>text or image prompts</strong>, with remixing and a gallery. Availability is currently gated to <strong>Google AI Ultra subscribers in the U.S. (18+)</strong>, and the product is explicit about prototype limitations (e.g., <strong>~60s generation limits</strong>, control latency, imperfect physics adherence) (<a href=\""https://twitter.com/GoogleDeepMind/status/2016919756440240479\"">DeepMind announcement</a>, <a href=\""https://twitter.com/GoogleDeepMind/status/2016919762924949631\"">how it works</a>, <a href=\""https://twitter.com/GoogleDeepMind/status/2016919765713826171\"">rollout details</a>, <a href=\""https://twitter.com/demishassabis/status/2016925155277361423\"">Demis</a>, <a href=\""https://twitter.com/sundarpichai/status/2016979481832067264\"">Sundar</a>, <a href=\""https://twitter.com/Google/status/2016926928478089623\"">Google thread</a>, <a href=\""https://twitter.com/Google/status/2016972686208225578\"">Google limitations</a>). Early-access testers highlight promptability, character/world customization, and “remixing” as key UX hooks (<a href=\""https://twitter.com/venturetwins/status/2016919922727850333\"">venturetwins</a>, <a href=\""https://twitter.com/joshwoodward/status/2016921839038255210\"">Josh Woodward demo thread</a>).</li>\n<li><strong>Open-source push: LingBot-World</strong>: A parallel thread frames <strong>world models</strong> as distinct from “video dreamers,” arguing for <strong>interactivity, object permanence, and causal consistency</strong>. LingBot-World is repeatedly described as an <strong>open-source real-time interactive world model</strong> built on <strong>Wan2.2</strong> with <strong>&#x3C;1s latency at 16 FPS</strong> and <strong>minute-level coherence</strong> (claims include VBench improvements and landmark persistence after long occlusion) (<a href=\""https://twitter.com/dair_ai/status/2016881546909929775\"">paper-summary thread</a>, <a href=\""https://twitter.com/HuggingPapers/status/2016787043028746284\"">HuggingPapers mention</a>, <a href=\""https://twitter.com/kimmonismus/status/2016896151610442192\"">reaction clip</a>). The meta-narrative: proprietary systems (Genie) are shipping consumer prototypes while open systems race to close capability gaps on <strong>coherence + control</strong>.</li>\n</ul>\n<p><strong>Video Generation &#x26; Creative Tooling: xAI Grok Imagine, Runway Gen-4.5, and fal’s “Day-0” Platforms</strong></p>\n<ul>\n<li><strong>xAI Grok Imagine (video + audio) lands near/at the top of leaderboards</strong>: Multiple sources report Grok Imagine’s strong debut in video rankings and emphasize <strong>native audio</strong>, <strong>15s duration</strong>, and aggressive <strong>pricing ($4.20/min including audio)</strong> relative to Veo/Sora (<a href=\""https://twitter.com/arena/status/2016748418635616440\"">Arena launch ranking</a>, <a href=\""https://twitter.com/ArtificialAnlys/status/2016749756081721561\"">Artificial Analysis #1 claim + pricing context</a>, <a href=\""https://twitter.com/ArtificialAnlys/status/2016749790907027726\"">follow-up #1 I2V leaderboard</a>, <a href=\""https://twitter.com/EthanHe_42/status/2016749123198673099\"">xAI team announcement</a>, <a href=\""https://twitter.com/elonmusk/status/2016768088855769236\"">Elon</a>). fal positioned itself as <strong>day-0 platform partner</strong> with API endpoints for text-to-image, editing, text-to-video, image-to-video, and video editing (<a href=\""https://twitter.com/fal/status/2016746472931283366\"">fal partnership</a>, <a href=\""https://twitter.com/fal/status/2016746473887609118\"">fal links tweet</a>).</li>\n<li><strong>Runway Gen-4.5 shifts toward “animation engine” workflows</strong>: Creators describe Gen-4.5 as increasingly controllable for animation-style work (<a href=\""https://twitter.com/c_valenzuelab/status/2016721443430510847\"">c_valenzuelab</a>). Runway shipped <strong>Motion Sketch</strong> (annotate camera/motion on a start frame) and <strong>Character Swap</strong> as built-in apps—more evidence that vendors are packaging controllability primitives rather than only pushing base quality (<a href=\""https://twitter.com/jerrod_lew/status/2016816309762486423\"">feature thread</a>). Runway also markets “photo → story clip” flows as a mainstream onramp (<a href=\""https://twitter.com/runwayml/status/2016882344427147275\"">Runway example</a>).</li>\n<li><strong>3D generation joins the same API distribution layer</strong>: fal also added <strong>Hunyuan 3D 3.1 Pro/Rapid</strong> (text/image-to-3D, topology/part generation), showing the same “model-as-a-service + workflow endpoints” pattern spreading from image/video into 3D pipelines (<a href=\""https://twitter.com/fal/status/2016877742298411089\"">fal drop</a>).</li>\n</ul>\n<p><strong>Open Models &#x26; Benchmarks: Kimi K2.5 momentum, Qwen3-ASR release, and Trinity Large architecture details</strong></p>\n<ul>\n<li><strong>Kimi K2.5 as the “#1 open model” across multiple eval surfaces</strong>: Moonshot promoted K2.5’s rank on <strong>VoxelBench</strong> (<a href=\""https://twitter.com/Kimi_Moonshot/status/2016732248800997727\"">Moonshot</a>) and later Kimi updates focus on productization: <strong>Kimi Code now powered by K2.5</strong>, switching from request limits to <strong>token-based billing</strong>, plus a limited-time <strong>3× quota/no throttling</strong> event (<a href=\""https://twitter.com/Kimi_Moonshot/status/2016918447951925300\"">Kimi Code billing update</a>, <a href=\""https://twitter.com/Kimi_Moonshot/status/2016918450992812443\"">billing rationale</a>). Arena messaging amplifies K2.5 as a leading open model with forthcoming Code Arena scores (<a href=\""https://twitter.com/arena/status/2016915717539713236\"">Arena deep dive</a>, <a href=\""https://twitter.com/arena/status/2016923733513105705\"">Code Arena prompt</a>); Arena also claims <strong>Kimi K2.5 Thinking</strong> as <strong>#1 open model in Vision Arena</strong> and the only open model in the top 15 (<a href=\""https://twitter.com/arena/status/2016984335380001268\"">Vision Arena claim</a>). Commentary frames K2.5 as “V3-generation architecture pushed with more continued training,” with next-gen competition expected from K3/GLM-5 etc. (<a href=\""https://twitter.com/teortaxesTex/status/2016956019239272717\"">teortaxes</a>).</li>\n<li><strong>Alibaba Qwen3-ASR: production-grade open speech stack with vLLM day-0 support</strong>: Qwen released <strong>Qwen3-ASR + Qwen3-ForcedAligner</strong> emphasizing messy real-world audio, <strong>52 languages/dialects</strong>, long audio (up to <strong>20 minutes/pass</strong>), and timestamps; models are <strong>Apache 2.0</strong> and include an open inference/finetuning stack. vLLM immediately announced <strong>day-0 support</strong> and performance notes (e.g., “2000× throughput on 0.6B” in their tweet) (<a href=\""https://twitter.com/Alibaba_Qwen/status/2016858705917075645\"">Qwen release</a>, <a href=\""https://twitter.com/Alibaba_Qwen/status/2016859224077455413\"">ForcedAligner</a>, <a href=\""https://twitter.com/vllm_project/status/2016865238323515412\"">vLLM support</a>, <a href=\""https://twitter.com/AdinaYakup/status/2016865634559152162\"">Adina Yakup summary</a>, <a href=\""https://twitter.com/Alibaba_Qwen/status/2016900512478875991\"">native streaming claim</a>, <a href=\""https://twitter.com/Alibaba_Qwen/status/2016905051395260838\"">Qwen thanks vLLM</a>). Net: open-source speech is increasingly “full-stack,” not just weights.</li>\n<li><strong>Arcee AI Trinity Large (400B MoE) enters the architecture discourse</strong>: Multiple threads summarize Trinity Large as <strong>400B MoE with ~13B active</strong>, tuned for throughput via sparse expert selection, and featuring a grab bag of modern stability/throughput techniques (router tricks, load balancing, attention patterns, normalization variants). Sebastian Raschka’s architecture recap is the most concrete single reference point (<a href=\""https://twitter.com/rasbt/status/2016903019116249205\"">rasbt</a>); additional MoE/router stability notes appear in a separate technical summary (<a href=\""https://twitter.com/cwolferesearch/status/2016792505111457883\"">cwolferesearch</a>). Arcee notes multiple variants trending on Hugging Face (<a href=\""https://twitter.com/arcee_ai/status/2016986617584529642\"">arcee_ai</a>).</li>\n</ul>\n<p><strong>Agents in Practice: “Agentic Engineering,” Multi-Agent Coordination, and Enterprise Sandboxes</strong></p>\n<ul>\n<li><strong>From vibe coding to agentic engineering</strong>: A high-engagement meme-like anchor tweet argues for “Agentic Engineering > Vibe Coding” and frames professionalism around repeatable workflows rather than vibes (<a href=\""https://twitter.com/bekacru/status/2016738191341240830\"">bekacru</a>). Several threads reinforce the same theme operationally: context prep, evaluations, and sandboxing as the hard parts.</li>\n<li><strong>Primer: repo instructions + lightweight evals + PR automation</strong>: Primer proposes a workflow for “AI-enabling” repos: agentic repo introspection → generate an instruction file → run a <strong>with/without</strong> eval harness → scale via batch PRs across org repos (<a href=\""https://twitter.com/pierceboggan/status/2016732251535397158\"">Primer launch</a>, <a href=\""https://twitter.com/pierceboggan/status/2016733056237711849\"">local run</a>, <a href=\""https://twitter.com/pierceboggan/status/2016733232176193539\"">eval framework</a>, <a href=\""https://twitter.com/pierceboggan/status/2016733666022424957\"">org scaling</a>).</li>\n<li><strong>Agent sandboxes + traceability as infra primitives</strong>: Multiple tweets point to “agent sandboxes” (isolated execution environments) as an emerging January trend (<a href=\""https://twitter.com/dejavucoder/status/2016979866651152898\"">dejavucoder</a>). Cursor proposed an <strong>open standard</strong> to trace agent conversations to generated code, explicitly positioning it as interoperable across agents/interfaces (<a href=\""https://twitter.com/cursor_ai/status/2016934752188576029\"">Cursor</a>). This pairs with broader ecosystem pressure: agents need auditability and reliable grounding when they can take actions.</li>\n<li><strong>Multi-agent coordination beats “bigger brain” framing</strong>: A popular summary claims a system that uses a <strong>controller trained by RL</strong> to route between large/small models can beat a single large agent on HLE with lower cost/latency—reinforcing that orchestration policies are becoming first-class artifacts (<a href=\""https://twitter.com/LiorOnAI/status/2016904429543272579\"">LiorOnAI</a>). In the same direction, an Amazon “Insight Agents” paper summary argues for pragmatic manager-worker designs with lightweight OOD detection and routing (autoencoder + fine-tuned BERT) instead of LLM-only classifiers for latency/precision reasons (<a href=\""https://twitter.com/omarsar0/status/2016880021030522997m\"">omarsar0</a>).</li>\n<li><strong>Kimi’s “Agent Swarm” philosophy</strong>: A long-form repost from ZhihuFrontier describes K2.5’s agent mode as a response to “text-only helpfulness” and tool-call hallucinations, emphasizing <strong>planning→execution bridging</strong>, dynamic tool-based context, and <strong>multi-viewpoint planning via swarms</strong> (<a href=\""https://twitter.com/ZhihuFrontier/status/2016811037274886377\"">ZhihuFrontier</a>).</li>\n<li><strong>Moltbot/Clawdbot safety trilemma</strong>: Community discussion frames “Useful vs Autonomous vs Safe” as a tri-constraint until prompt injection is solved (<a href=\""https://twitter.com/fabianstelzer/status/2016818595687272913\"">fabianstelzer</a>). Another take argues capability (trust) bottlenecks dominate: users won’t grant high-stakes autonomy (e.g., finance) until agents are reliably competent (<a href=\""https://twitter.com/Yuchenj_UW/status/2016937299125424284\"">Yuchenj_UW</a>).</li>\n</ul>\n<p><strong>Model UX, DevTools, and Serving: Gemini Agentic Vision, OpenAI’s in-house data agent, vLLM fixes, and local LLM apps</strong></p>\n<ul>\n<li><strong>Gemini 3 Flash “Agentic Vision”</strong>: Google positions Agentic Vision as a structured image-analysis pipeline: planning steps, zooming, annotating, and optionally running Python for plotting—essentially turning “vision” into an agentic workflow rather than a single forward pass (<a href=\""https://twitter.com/GeminiApp/status/2016914275886125483\"">GeminiApp intro</a>, <a href=\""https://twitter.com/GeminiApp/status/2016914637523210684\"">capabilities</a>, <a href=\""https://twitter.com/GeminiApp/status/2016914638861193321\"">rollout note</a>).</li>\n<li><strong>OpenAI’s in-house data agent at massive scale</strong>: OpenAI described an internal “AI data agent” reasoning over <strong>600+ PB</strong> and <strong>70k datasets</strong>, using Codex-powered table knowledge and careful context management (<a href=\""https://twitter.com/OpenAIDevs/status/2016943147239329872\"">OpenAIDevs</a>). This is a rare concrete peek at “deep research/data agent” architecture constraints: retrieval + schema/table priors + org context.</li>\n<li><strong>Serving bugs are still real (vLLM + stateful models)</strong>: AI21 shared a debugging story where scheduler token allocation caused misclassification between <strong>prefill vs decode</strong>, now fixed in <strong>vLLM v0.14.0</strong>—a reminder that infrastructure correctness matters, especially for stateful architectures like Mamba (<a href=\""https://twitter.com/AI21Labs/status/2016857918436503975\"">AI21Labs thread</a>).</li>\n<li><strong>Local LLM UX continues to improve</strong>: Georgi Gerganov shipped <strong>LlamaBarn</strong>, a tiny macOS menu bar app built on llama.cpp to run local models (<a href=\""https://twitter.com/ggerganov/status/2016912009544057045\"">ggerganov</a>). Separate comments suggest agentic coding performance may improve by disabling “thinking” modes for specific models (GLM-4.7-Flash) via llama.cpp templates (<a href=\""https://twitter.com/ggerganov/status/2016903216093417540\"">ggerganov config note</a>).</li>\n</ul>\n<p><strong>Top tweets (by engagement)</strong></p>\n<ul>\n<li><strong>Grok Imagine hype &#x26; distribution</strong>: <a href=\""https://twitter.com/elonmusk/status/2016768088855769236\"">@elonmusk</a>, <a href=\""https://twitter.com/fal/status/2016746472931283366\"">@fal</a>, <a href=\""https://twitter.com/ArtificialAnlys/status/2016749756081721561\"">@ArtificialAnlys</a></li>\n<li><strong>DeepMind/Google world models</strong>: <a href=\""https://twitter.com/GoogleDeepMind/status/2016919756440240479\"">@GoogleDeepMind</a>, <a href=\""https://twitter.com/demishassabis/status/2016925155277361423\"">@demishassabis</a>, <a href=\""https://twitter.com/sundarpichai/status/2016979481832067264\"">@sundarpichai</a></li>\n<li><strong>AI4Science</strong>: <a href=\""https://twitter.com/demishassabis/status/2016763919646478403\"">@demishassabis on AlphaGenome</a></li>\n<li><strong>Speech open-source release</strong>: <a href=\""https://twitter.com/Alibaba_Qwen/status/2016858705917075645\"">@Alibaba_Qwen Qwen3-ASR</a></li>\n<li><strong>Agents + developer workflow</strong>: <a href=\""https://twitter.com/bekacru/status/2016738191341240830\"">@bekacru “Agentic Engineering > Vibe Coding”</a>, <a href=\""https://twitter.com/cursor_ai/status/2016934752188576029\"">@cursor_ai agent-trace.dev</a></li>\n<li><strong>Anthropic workplace study</strong>: <a href=\""https://twitter.com/AnthropicAI/status/2016960382968136138\"">@AnthropicAI AI-assisted coding and mastery</a></li>\n</ul>\n<hr>\n<h1>AI Reddit Recap</h1>\n<h2>/r/LocalLlama + /r/localLLM Recap</h2>\n<h3>1. Kimi K2.5 Model Discussions and Releases</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/\"">AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model</a></strong> (Activity: 686): <strong><strong>Kimi</strong> is the research lab behind the open-source <strong>Kimi K2.5</strong> model, engaging in an AMA to discuss their work. The discussion highlights a focus on large-scale models, with inquiries about the development of smaller models like <code>8B</code>, <code>32B</code>, and <code>70B</code> for better intelligence density. There is also interest in smaller Mixture of Experts (MoE) models, such as <code>~100B</code> total with <code>~A3B</code> active, optimized for local or prosumer use. The team is questioned on their stance regarding the notion that <em>Scaling Laws have hit a wall</em>, a topic of current debate in AI research.</strong> Commenters express a desire for smaller, more efficient models, suggesting that these could offer better performance for specific use cases. The debate on scaling laws reflects a broader concern in the AI community about the limits of current model scaling strategies.</p>\n<ul>\n<li>The discussion around model sizes highlights a preference for smaller models like 8B, 32B, and 70B due to their 'intelligence density.' These sizes are considered optimal for balancing performance and resource efficiency, suggesting a demand for models that can operate effectively on limited hardware while still providing robust capabilities.</li>\n<li>The inquiry into smaller Mixture of Experts (MoE) models, such as a ~100B total with ~A3B active, indicates interest in models optimized for local or prosumer use. This reflects a trend towards developing models that are not only powerful but also accessible for individual users or small enterprises, emphasizing the need for efficient resource utilization without sacrificing performance.</li>\n<li>The challenge of maintaining non-coding abilities like creative writing and emotional intelligence in models like Kimi 2.5 is significant, especially as coding benchmarks become more prominent. The team is tasked with ensuring these softer skills do not regress, which involves balancing the training focus between technical and creative capabilities to meet diverse user needs.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qpfse6/run_kimi_k25_locally/\"">Run Kimi K2.5 Locally</a></strong> (Activity: 553): <strong>The image provides a guide for running the <strong>Kimi-K2.5</strong> model locally, emphasizing its state-of-the-art (SOTA) performance in vision, coding, agentic, and chat tasks. The model, which is a <code>1 trillion</code> parameter hybrid reasoning model, requires <code>600GB</code> of disk space, but the quantized <strong>Unsloth Dynamic 1.8-bit</strong> version reduces this requirement to <code>240GB</code>, a <code>60%</code> reduction. The guide includes instructions for using <code>llama.cpp</code> to load models and demonstrates generating HTML code for a simple game. The model is available on <a href=\""https://huggingface.co/unsloth/Kimi-K2.5-GGUF\"">Hugging Face</a> and further documentation can be found on <a href=\""https://unsloth.ai/docs/models/kimi-k2.5\"">Unsloth's official site</a>.</strong> Commenters discuss the feasibility of running the model on high-end hardware, with one user questioning its performance on a Strix Halo setup and another highlighting the substantial VRAM requirements, suggesting that only a few users can realistically run it locally.</p>\n<ul>\n<li>Daniel_H212 is inquiring about the performance of the Kimi K2.5 model on the Strix Halo hardware, specifically asking for the token generation speed in seconds per token. This suggests a focus on benchmarking the model's efficiency on high-end hardware setups.</li>\n<li>Marksta provides feedback on the quantized version of the Kimi K2.5 model, specifically the Q2_K_XL variant. They note that the model maintains high coherence and adheres strictly to prompts, which is characteristic of Kimi-K2's style. However, they also mention that while the model's creative capabilities have improved, it still struggles with execution in creative scenarios, often delivering logical but poorly written responses.</li>\n<li>MikeRoz questions the utility of higher quantization levels like Q5 and Q6 (e.g., UD-Q5_K_XL, Q6_K) when most experts prefer int4 quantization. This highlights a debate on the trade-offs between model size, performance, and precision in quantization strategies.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qp87tk/kimi_k25_is_the_best_open_model_for_coding/\"">Kimi K2.5 is the best open model for coding</a></strong> (Activity: 1119): <strong>The image highlights <strong>Kimi K2.5</strong> as the leading open model for coding on the LMARENA.AI leaderboard, ranked <code>#7</code> overall. This model is noted for its superior performance in coding tasks compared to other open models. The leaderboard provides a comparative analysis of various AI models, showcasing their ranks, scores, and confidence intervals, emphasizing Kimi K2.5's achievements in the coding domain.</strong> One commenter compared Kimi K2.5's performance to other models, noting it is on par with Sonnet 4.5 in accuracy but not as advanced as Opus in agentic function. Another comment criticized LMArena for not reflecting a model's multi-turn or long context capabilities.</p>\n<ul>\n<li>A user compared Kimi K2.5 to other models, noting that it performs on par with Sonnet 4.5 in terms of accuracy for React projects, but not at the level of Opus in terms of agentic function. They also mentioned that Kimi 2.5 surpasses GLM 4.7, which was their previous choice, and expressed curiosity about the upcoming GLM-5 from <a href=\""http://z.ai\"">z.ai</a>.</li>\n<li>Another commenter criticized LMArena, stating that it fails to provide insights into a model's multi-turn, long context, or agentic capabilities, implying that such benchmarks are insufficient for evaluating comprehensive model performance.</li>\n<li>A user highlighted the cost-effectiveness of Kimi K2.5, stating it feels as competent as Opus 4.5 while being significantly cheaper, approximately 1/5th the cost, and even less expensive than Haiku. This suggests a strong performance-to-cost ratio for Kimi K2.5.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLM/comments/1qp880l/finally_we_have_the_best_agentic_ai_at_home/\"">Finally We have the best agentic AI at home</a></strong> (Activity: 464): <strong>The image is a performance comparison chart of various AI models, including <strong>Kimi K2.5</strong>, <strong>GPT-5.2 (xhigh)</strong>, <strong>Claude Opus 4.5</strong>, and <strong>Gemini 3 Pro</strong>. <strong>Kimi K2.5</strong> is highlighted as the top-performing model across multiple categories such as agents, coding, image, and video tasks, indicating its superior capabilities in multimodal applications. The post suggests excitement about integrating this model with a 'clawdbot', hinting at potential applications in robotics or automation.</strong> A comment humorously suggests that hosting the <strong>Kimi 2.5 1T+ model</strong> at home implies having a large home, indicating the model's likely high computational requirements. Another comment sarcastically mentions handling it with a 16GB VRAM card, implying skepticism about the feasibility of running such a model on typical consumer hardware.</p>\n</li>\n</ul>\n<h3>2. Open Source Model Innovations</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qqj51h/lingbotworld_outperforms_genie_3_in_dynamic/\"">LingBot-World outperforms Genie 3 in dynamic simulation and is fully Open Source</a></strong> (Activity: 230): <strong>The open-source framework <strong>LingBot-World</strong> surpasses the proprietary <strong>Genie 3</strong> in dynamic simulation capabilities, achieving <code>16 FPS</code> and maintaining object consistency for <code>60 seconds</code> outside the field of view. This model, available on <a href=\""https://huggingface.co/collections/robbyant/lingbot-world\"">Hugging Face</a>, offers enhanced handling of complex physics and scene transitions, challenging the monopoly of proprietary systems by providing full access to its code and model weights.</strong> Commenters question the hardware requirements for running LingBot-World and express skepticism about the comparison with Genie 3, suggesting a lack of empirical evidence or direct access to Genie 3 for a fair comparison.</p>\n<ul>\n<li>A user questioned the hardware requirements for running LingBot-World, highlighting the importance of specifying computational needs for practical implementation. This is crucial for users to understand the feasibility of deploying the model in various environments.</li>\n<li>Another commenter raised concerns about the lack of a direct comparison with Genie 3, suggesting that without empirical data or benchmarks, claims of LingBot-World's superiority might be unsubstantiated. This points to the need for transparent and rigorous benchmarking to validate performance claims.</li>\n<li>A suggestion was made to integrate a smaller version of LingBot-World into a global illumination stack, indicating potential applications in graphics and rendering. This could leverage the model's capabilities in dynamic simulation to enhance visual computing tasks.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qp6rm5/api_pricing_is_in_freefall_whats_the_actual_case/\"">API pricing is in freefall. What's the actual case for running local now beyond privacy?</a></strong> (Activity: 1053): <strong>The post discusses the rapidly decreasing costs of API access for AI models, with examples like <strong>K2.5</strong> offering prices at <code>10%</code> of <strong>Opus</strong> and <strong>Deepseek</strong> being nearly free. <strong>Gemini</strong> also provides a substantial free tier. This trend is contrasted with the challenges of running large models locally, such as the need for expensive GPUs or dealing with quantization tradeoffs, which result in slow processing speeds (<code>15 tok/s</code>) on consumer hardware. The author questions the viability of local setups given these API pricing trends, noting that while privacy and latency control are valid reasons, the cost-effectiveness of local setups is diminishing.</strong> Commenters highlight concerns about the sustainability of low API prices, suggesting they may rise once market dominance is achieved, similar to past trends in other industries. Others emphasize the importance of offline capabilities and the ability to audit and trust local models, which ensures consistent behavior without unexpected changes from vendors.</p>\n<ul>\n<li>Minimum-Vanilla949 highlights the importance of offline capabilities for those who travel frequently, emphasizing the risk of API companies altering terms of service or increasing prices once they dominate the market. This underscores the value of local models for ensuring consistent access and cost control.</li>\n<li>05032-MendicantBias discusses the unsustainable nature of current API pricing, which is often subsidized by venture capital. They argue that once a monopoly is achieved, prices will likely increase, making local setups and open-source tools a strategic defense against such business models.</li>\n<li>IactaAleaEst2021 points out the importance of repeatability and trust in using local models. By downloading and auditing a model, users can ensure consistent behavior, unlike APIs where vendors might change model behavior over time, potentially reducing its utility for specific tasks.</li>\n</ul>\n</li>\n</ul>\n<h3>3. Trends in AI Agent Frameworks</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qq6n3t/github_trending_this_week_half_the_repos_are/\"">GitHub trending this week: half the repos are agent frameworks. 90% will be dead in 1 week.</a></strong> (Activity: 538): <strong>The image highlights a trend on GitHub where many of the trending repositories are related to AI agent frameworks, suggesting a surge in interest in these tools. However, the post's title and comments express skepticism about the sustainability of this trend, comparing it to the rapid rise and fall of JavaScript frameworks. The repositories are mostly written in Python and include a mix of agent frameworks, RAG tooling, and model-related projects like NanoGPT and Grok. The discussion reflects a concern that many of these projects may not maintain their popularity or relevance over time.</strong> One comment challenges the claim that half of the trending repositories are agent frameworks, noting that only one is an agent framework by Microsoft, while others are related to RAG tooling and model development. Another comment appreciates the utility of certain projects, like IPTV, for educational purposes.</p>\n<ul>\n<li>gscjj points out that the claim about 'half the repos being agent frameworks' is inaccurate. They note that the list includes a variety of projects such as Microsoft's agent framework, RAG tooling, and models like NanoGPT and Grok, as well as a model CLI for code named Kimi and a browser API. This suggests a diverse range of trending repositories rather than a dominance of agent frameworks.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qqhhtx/mistral_ceo_arthur_mensch_if_you_treat/\"">Mistral CEO Arthur Mensch: “If you treat intelligence as electricity, then you just want to make sure that your access to intelligence cannot be throttled.”</a></strong> (Activity: 357): <strong><strong>Arthur Mensch</strong>, CEO of <strong>Mistral</strong>, advocates for open-weight models, likening intelligence to electricity, emphasizing the importance of unrestricted access to AI capabilities. This approach supports the deployment of models on local devices, reducing costs as models are quantized for lower compute environments, contrasting with closed models that are often large and monetized through paywalls. Mistral aims to balance corporate interests with open access, potentially leading to significant breakthroughs in AI deployment.</strong> Commenters appreciate Mistral's approach to open models, noting the potential for reduced costs and increased accessibility. There is a consensus that open models could democratize AI usage, contrasting with the restrictive nature of closed models.</p>\n<ul>\n<li>RoyalCities highlights the cost dynamics of model deployment, noting that open models, especially when quantized, reduce costs as they can be run on local devices. This contrasts with closed models that are often large and require significant infrastructure, thus being monetized through paywalls. This reflects a broader industry trend where open models aim to democratize access by lowering hardware requirements.</li>\n<li>HugoCortell points out the hardware bottleneck in deploying open models effectively. While open-source models can rival closed-source ones in performance, the lack of affordable, high-performance hardware limits their accessibility. This is compounded by large companies making high-quality local hardware increasingly expensive, suggesting a need for a company capable of producing and distributing its own hardware to truly democratize AI access.</li>\n<li>tarruda expresses anticipation for the next open Mistral model, specifically the \""8x22\"". This indicates a community interest in the technical specifications and potential performance improvements of upcoming models, reflecting the importance of open model development in advancing AI capabilities.</li>\n</ul>\n</li>\n</ul>\n<h2>Less Technical AI Subreddit Recap</h2>\n<blockquote>\n<p>/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo</p>\n</blockquote>\n<h3>1. OpenAI and AGI Investments</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/singularity/comments/1qpxyka/nearly_half_of_the_mag_7_are_reportedly_betting/\"">Nearly half of the Mag 7 are reportedly betting big on OpenAI’s path to AGI</a></strong> (Activity: 1153): <strong><strong>NVIDIA, Microsoft, and Amazon</strong> are reportedly in discussions to invest a combined total of up to <code>$60 billion</code> into <strong>OpenAI</strong>, with <strong>SoftBank</strong> considering an additional <code>$30 billion</code>. This potential investment could value OpenAI at approximately <code>$730 billion</code> pre-money, aligning with recent valuation discussions in the <code>$750 billion to $850 billion+</code> range. This would mark one of the largest private capital raises in history, highlighting the significant financial commitment from major tech companies towards the development of artificial general intelligence (AGI).</strong> Commenters note the strategic alignment of these investments, with one pointing out that companies like Microsoft and NVIDIA are unlikely to invest in competitors like Google. Another comment reflects on the evolving landscape of large language models (LLMs) and the shifting focus of tech giants.</p>\n<ul>\n<li>CoolStructure6012 highlights the strategic alignment between <strong>Microsoft (MSFT)</strong> and <strong>NVIDIA (NVDA)</strong> with OpenAI, suggesting that their investments are logical given their competitive stance against <strong>Google</strong>. This reflects the broader industry trend where tech giants are aligning with AI leaders to bolster their AI capabilities and market positions.</li>\n<li>drewc717 reflects on the evolution of AI models, noting a significant productivity boost with OpenAI's <code>4.1 Pro mode</code>. However, they express a decline in their workflow efficiency after switching to <strong>Gemini</strong>, indicating that not all LLMs provide the same level of user experience or productivity, which is crucial for developers relying on these tools.</li>\n<li>EmbarrassedRing7806 questions the lack of attention on <strong>Anthropic</strong> despite its widespread use in coding through its <strong>Claude</strong> model, as opposed to OpenAI's <strong>Codex</strong>. This suggests a potential underestimation of Anthropic's impact in the AI coding space, where <strong>Claude</strong> might be offering competitive or superior capabilities.</li>\n</ul>\n</li>\n</ul>\n<h3>2. DeepMind's AlphaGenome Launch</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/singularity/comments/1qphlfg/google_deepmind_launches_alphagenome_an_ai_model/\"">Google DeepMind launches AlphaGenome, an AI model that analyzes up to 1 million DNA bases to predict genomic regulation</a></strong> (Activity: 427): <strong><strong>Google DeepMind</strong> has introduced <strong>AlphaGenome</strong>, a sequence model capable of analyzing up to <code>1 million DNA bases</code> to predict genomic regulation, as detailed in <a href=\""https://www.nature.com/articles/s41586-025-10014-0?amp%3Butm_medium=social&#x26;amp%3Butm_campaign=&#x26;amp%3Butm_content=\"">Nature</a>. The model excels in predicting genomic signals such as gene expression and chromatin structure, particularly in non-coding DNA, which is crucial for understanding disease-associated variants. AlphaGenome outperforms existing models on <code>25 of 26</code> benchmark tasks and is available for research use, with its model and weights accessible on <a href=\""https://github.com/google-deepmind/alphagenome_research\"">GitHub</a>.</strong> Commenters highlight the model's potential impact on genomics, with some humorously suggesting its significance in advancing scientific achievements akin to winning Nobel prizes.</p>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/MachineLearning/comments/1qq4lnc/r_alphagenome_deepminds_unified_dna_sequence/\"">[R] AlphaGenome: DeepMind's unified DNA sequence model predicts regulatory variant effects across 11 modalities at single-bp resolution (Nature 2026)</a></strong> (Activity: 66): <strong><strong>DeepMind's AlphaGenome</strong> introduces a unified DNA sequence model that predicts regulatory variant effects across <code>11 modalities</code> at single-base-pair resolution. The model processes <code>1M base pairs</code> of DNA to predict thousands of functional genomic tracks, matching or exceeding specialized models in <code>25 of 26</code> evaluations. It employs a U-Net backbone with CNN and transformer layers, trained on human and mouse genomes, and captures <code>99%</code> of validated enhancer-gene pairs within a <code>1Mb</code> context. Training on TPUv3 took <code>4 hours</code>, with inference under <code>1 second</code> on H100. The model demonstrates cross-modal variant interpretation, notably on the TAL1 oncogene in T-ALL. <a href=\""https://www.nature.com/articles/s41586-025-10014-0\"">Nature</a>, <a href=\""https://www.biorxiv.org/content/10.1101/2025.06.25.661532v1\"">bioRxiv</a>, <a href=\""https://deepmind.google/blog/alphagenome-ai-for-better-understanding-the-genome\"">DeepMind blog</a>, <a href=\""https://github.com/google-deepmind/alphagenome\"">GitHub</a>.</strong> Some commenters view the model as an incremental improvement over existing sequence models, questioning the novelty despite its publication in <em>Nature</em>. Others express concerns about the implications of open-sourcing such powerful genomic tools, hinting at potential future applications like 'text to CRISPR' models.</p>\n<ul>\n<li>st8ic88 argues that while DeepMind's AlphaGenome model is notable for its ability to predict regulatory variant effects across 11 modalities at single-base pair resolution, it is seen as an incremental improvement over existing sequence models predicting genomic tracks. The comment suggests that the model's prominence is partly due to DeepMind's reputation and branding, particularly the use of 'Alpha' in its name, which may have contributed to its publication in Nature.</li>\n<li>--MCMC-- is interested in the differences between the AlphaGenome model's preprint and its final published version in Nature. The commenter had read the preprint and is curious about any changes made during the peer review process, which could include updates to the model's methodology, results, or interpretations.</li>\n<li>f0urtyfive raises concerns about the potential risks of open-sourcing powerful genomic models like AlphaGenome, speculating on future developments such as 'text to CRISPR' models. This comment highlights the ethical and safety considerations of making advanced genomic prediction tools widely accessible, which could lead to unintended applications or misuse.</li>\n</ul>\n</li>\n</ul>\n<h3>3. Claude's Cost Efficiency and Usage Strategies</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/ClaudeAI/comments/1qpcj8q/claude_subscriptions_are_up_to_36x_cheaper_than/\"">Claude Subscriptions are up to 36x cheaper than API (and why \""Max 5x\"" is the real sweet spot)</a></strong> (Activity: 665): <strong>A data analyst has reverse-engineered <strong>Claude's internal usage limits</strong> by analyzing unrounded floats in the web interface, revealing that <strong>subscriptions can be up to 36x cheaper</strong> than using the API, especially for coding tasks with agents like Claude Code. The analysis shows that the <strong>subscription model offers free cache reads</strong>, whereas the API charges 10% of the input cost for each read, making the subscription significantly more cost-effective for long sessions. The \""Max 5x\"" plan at <code>$100/month</code> is highlighted as the most optimized, offering a <code>6x</code> higher session limit and <code>8.3x</code> higher weekly limit than the Pro plan, contrary to the marketed \""5x\"" and \""20x\"" plans. The findings were derived using the Stern-Brocot tree to decode precise usage percentages into internal credit numbers. Full details and formulas are available <a href=\""http://she-llac.com/claude-limits\"">here</a>.</strong> Commenters express concern over <strong>Anthropic's lack of transparency</strong> and speculate that the company might change the limits once they realize users have reverse-engineered them. Some users are taking advantage of the current subscription benefits, anticipating potential changes.</p>\n<ul>\n<li>HikariWS raises a critical point about <strong>Anthropic's lack of transparency</strong> regarding their subscription limits, which could change unexpectedly, rendering current analyses obsolete. This unpredictability poses a risk for developers relying on these plans for cost-effective usage.</li>\n<li>Isaenkodmitry discusses the potential for <strong>Anthropic to close loopholes</strong> once they realize users are exploiting subscription plans for cheaper access compared to the API. This highlights a strategic risk for developers who are currently benefiting from these plans, suggesting they should maximize usage while it lasts.</li>\n<li>Snow30303 mentions using <strong>Claude code in VS Code for Flutter apps</strong>, noting that it consumes credits rapidly. This suggests a need for more efficient usage strategies or alternative solutions to manage costs effectively when integrating Claude into development workflows.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/ClaudeAI/comments/1qp9ve9/we_reduced_claude_api_costs_by_945_using_a_file/\"">We reduced Claude API costs by 94.5% using a file tiering system (with proof)</a></strong> (Activity: 603): <strong>The post describes a file tiering system that reduces <strong>Claude API costs by 94.5%</strong> by categorizing files into HOT, WARM, and COLD tiers, thus minimizing the number of tokens processed per session. This system, implemented in a tool called <code>cortex-tms</code>, tags files based on their relevance and usage frequency, allowing only the most necessary files to be loaded by default. The approach has been validated through a case study on the author's project, showing a reduction from <code>66,834</code> to <code>3,647</code> tokens per session, significantly lowering costs from <code>$0.11</code> to <code>$0.01</code> per session with Claude Sonnet 4.5. The tool is open-source and available on <a href=\""https://github.com/cortex-tms/cortex-tms\"">GitHub</a>.</strong> One commenter inquired about the manual process of tagging files and updating tags, suggesting the use of git history to automate file heat determination. Another user appreciated the approach due to their own struggles with managing API credits.</p>\n<ul>\n<li><strong>Illustrious-Report96</strong> suggests using <code>git history</code> to determine file 'heat', which involves analyzing the frequency and recency of changes to classify files as 'hot', 'warm', or 'cold'. This method leverages version control metadata to automate the classification process, potentially reducing manual tagging efforts.</li>\n<li><strong>Accomplished_Buy9342</strong> inquires about restricting access to 'WARM' and 'COLD' files, which implies a need for a mechanism to control agent access based on file tier. This could involve implementing access controls or modifying the agent's logic to prioritize 'HOT' files, ensuring efficient resource usage.</li>\n<li><strong>durable-racoon</strong> asks about the process of tagging files and updating these tags, highlighting the importance of an automated or semi-automated system to manage file tiering efficiently. This could involve scripts or tools that dynamically update file tags based on usage patterns or other criteria.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h1>AI Discord Recap</h1>\n<blockquote>\n<p>A summary of Summaries of Summaries by Gemini 3.0 Pro Preview Nov-18</p>\n</blockquote>\n<p><strong>Theme 1. Model Wars: Kimi’s Rise, Recursive Agents, and Geometric Architectures</strong></p>\n<ul>\n<li><strong>Kimi K2.5 crushes the Vision Arena</strong>: The community reports <strong>Kimi K2.5</strong> is dominating the leaderboards, claiming the <strong>#1 open model</strong> spot and ranking <strong>#6 overall</strong> on the <a href=\""https://arena.ai/leaderboard/vision\"">Vision Arena leaderboard</a>. Users note it outperforms <strong>Claude</strong> in specific vision tasks and now features a dedicated <strong>computer use</strong> model that handles phone screenshots (though it throws 403 errors on mobile uploads).</li>\n<li><strong>Recursive Language Models trigger semantic debates</strong>: A heated discussion erupted over the term \""<strong>Recursive Language Models</strong>\"" (<strong>RLM</strong>), with critics arguing it simply rebrands <strong>tool-calling loops</strong>, while proponents point to the new <a href=\""https://xcancel.com/a1zhang/status/2016923294461476873?s=46&#x26;t=jDrfS5vZD4MFwckU5E8f5Q\"">RLM-Qwen3-8B</a> as the first natively recursive model. This small-scale model, post-trained on just <strong>1,000 trajectories</strong>, reportedly beats scaffolded RLM versions in long-context tasks.</li>\n<li><strong>Geometric Convolution attempts to dethrone Attention</strong>: Researchers are experimenting with a baseline that replaces standard <strong>Multi-Head Attention</strong> with a <a href=\""https://github.com/MrPan2048/GeometricTransformer\"">geometric convolution approach</a>, using embeddings as cell connections. Early debug prints show loss convergence capturing dialogue logic, positioning this as a potential alternative to heavy transformer compute.</li>\n</ul>\n<p><strong>Theme 2. Hardware Hustle: Microsoft’s Silicon, Unsloth Speeds, and Apple’s Hidden Power</strong></p>\n<ul>\n<li><strong>Microsoft aims at NVIDIA with Maia 200</strong>: Microsoft unveiled the <a href=\""https://blogs.microsoft.com/blog/2026/01/26/maia-200-the-ai-accelerator-built-for-inference/\""><strong>Maia 200 AI Accelerator</strong></a>, an inference-focused chip boasting <strong>216GB</strong> of memory and <strong>10k TFLOPS</strong> in FP4 performance. Engineers debated the reliance on <strong>TSMC</strong> manufacturing and compared its architecture favorably against <strong>NVIDIA's Vera Rubin</strong> for large-scale inference workloads.</li>\n<li><strong>RTX 5090 shreds training benchmarks</strong>: Unsloth users report the <strong>RTX 5090</strong> achieves blistering training speeds of up to <strong>18k tokens per second</strong>, though <strong>12-15k t/s</strong> is safer with a sequence length under <strong>4096</strong>. Optimal throughput requires carefully balancing <strong>batch size</strong> and sequence length to avoid memory bottlenecks during fine-tuning.</li>\n<li><strong>Apple’s ANE punches above its weight</strong>: A new discussion around <a href=\""https://arxiv.org/abs/2511.13450\"">this paper</a> highlights that Apple's <strong>Neural Engine (ANE)</strong> delivers <strong>3.8 TFlops</strong> on the M4-Pro, nearly matching the GPU's <strong>4.7 TFlops</strong> for GEMM operations. The ANE prioritizes <strong>performance-per-watt</strong>, making it a surprisingly viable target for efficient local inference.</li>\n</ul>\n<p><strong>Theme 3. Dev Tools &#x26; Standards: Cursor Pains, MCP Security, and Parallel Studio</strong></p>\n<ul>\n<li><strong>Cursor’s \""Plan Mode\"" annoys the power users</strong>: The latest <strong>Cursor</strong> update introduced a <strong>plan mode</strong> that users are actively trying to disable or automate, citing wasted time and unnecessary inputs. Fresh installs of the IDE are reportedly the most unstable configuration, driving users to seek workarounds for the \""Plan Mode\"" friction.</li>\n<li><strong>MCP gets a hardened Security Standard</strong>: Dani (cr0hn) drafted an open <a href=\""https://github.com/mcp-security-standard/mcp-server-security-standard\"">MCP Security Standard</a> covering hardening, logging, and access control, intending to donate it to the <strong>Agentic AI Foundation</strong>. Simultaneously, the protocol is evolving with <strong>Namespaces</strong> being rejected in favor of <strong>Groups</strong>, detailed in the new <a href=\""https://github.com/modelcontextprotocol/modelcontextprotocol/pull/2084\"">Primitive Grouping SEP-2084</a>.</li>\n<li><strong>LM Studio 0.4 hides power tools behind Dev Mode</strong>: The release of <strong>LM Studio 0.4.0</strong> tucks critical settings like sampling and hardware configs behind a <strong>Dev Mode</strong> toggle (<code>Ctrl+Shift+R</code>), while introducing <strong>parallel requests</strong>. Users can now load models across different GPUs to handle up to <strong>4 parallel requests</strong> by default, though the software still relies on the older <strong>ROCm 6.4.1</strong>.</li>\n</ul>\n<p><strong>Theme 4. Jailbreaks &#x26; Exploits: Keygens, \""Remember\"" Hacks, and Malware Classifiers</strong></p>\n<ul>\n<li><strong>Gemini 3 Pro tricked into writing KeyGens</strong>: A user successfully prompted <strong>Gemini 3 Pro</strong> to reverse engineer software and generate a working keygen by pasting code directly from <strong>Ghidra</strong>. While some dismissed this as \""script kiddie\"" behavior, it highlights the model's susceptibility to <strong>context-based exploits</strong> when fed technical disassemblies.</li>\n<li><strong>\""Remember:\"" command acts as behavior injection</strong>: Red teamers discovered that the <a href=\""https://gemini.google.com/saved-info\""><strong>Gemini</strong> command 'Remember:'</a> instantly forces subsequent text into the model's saved memory, heavily influencing future behavior. This allows for persistent prompt injections that dictate turns one at a time, bypassing standard session resets.</li>\n<li><strong>Adversarial Malware Classification struggles</strong>: Engineers are fighting to lower the <strong>False Positive Rate (FPR)</strong> in malware classification models using a dataset of <strong>600K</strong> rows and <strong>9,600</strong> binary features. Despite using neural networks and <strong>explainable models</strong> like scikit-learn trees, reducing FPR below <strong>9%</strong> remains a significant hurdle without sacrificing model interpretability.</li>\n</ul>\n<p><strong>Theme 5. Real-World Agents: Kitchen Robots, World Models, and Bio-AI</strong></p>\n<ul>\n<li><strong>Figure.Ai’s Helix 02 conquers the kitchen</strong>: A video surfaced of <strong>Figure.Ai's Helix 02</strong> robot autonomously performing complex kitchen tasks, which a user verified by feeding the video into <strong>Kimi</strong> for a <a href=\""https://cdn.discordapp.com/attachments/1371757564005711973/1466193526009106452/m2-res_1280p.mp4?ex=697d2c21&#x26;is=697bdaa1&#x26;hm=427bc85209f62b3f47f60ce804f74a7cc41be60c452fb561197ad468c29e5224&#x26;\"">98% accurate analysis</a>. This aligns with reports of <strong>Matic</strong> raising <strong>$60M</strong> to build a utility-focused consumer robot successor to the Roomba.</li>\n<li><strong>Google releases \""Genie\"" World Model</strong>: Google launched <a href=\""https://x.com/googleai/status/2016929427784122627\""><strong>Project Genie</strong></a> for <strong>AI Ultra</strong> subscribers, a general-purpose world model capable of generating interactive environments from text prompts. This release moves world models from research papers into a deployable product for simulating dynamic scenarios.</li>\n<li><strong>AI decodes DNA and Alzheimer’s</strong>: Google AI launched <a href=\""https://x.com/GoogleAI/status/1937895472305152387\""><strong>AlphaGenome</strong></a> to predict the impact of DNA variants and mutations, while <strong>Goodfire AI</strong> announced new <a href=\""https://xcancel.com/goodfireai/status/2016563911508840623\"">Alzheimer's biomarkers</a> discovered via model interpretability. These advances signal a shift toward using <strong>transparent AI models</strong> to drive breakthroughs in digital biology.</li>\n</ul>\n<hr>\n<h1>Discord: High level Discord summaries</h1>\n<h2><a href=\""https://discord.com/channels/1105891499641684019\"">BASI Jailbreaking</a> Discord</h2>\n<ul>\n<li><strong>Gemini 3 Pro Cracks Software KeyGen Style</strong>: A member reported using <strong>Gemini 3 Pro</strong> to create a working keygen from software by pasting code from <strong>Ghidra</strong>.\n<ul>\n<li>Skeptical members referred to this as <em>script kiddie</em> behavior and suggested trying a reverse engineering CTF challenge.</li>\n</ul>\n</li>\n<li><strong>AI Gets Weaponized for Reverse Engineering</strong>: A member shared his work on weaponizing <strong>AI</strong> for <em>mass reverse engineering, malware analysis, and jailbreak development</em>.\n<ul>\n<li>Another member questioned this claim, suggesting the original poster may be more skilled at jailbreaking than malware creation.</li>\n</ul>\n</li>\n<li><strong>Sonnet 4.5 Bests Opus with Kaelia Jailbreak</strong>: Members confirmed that <strong>Sonnet 4.5 jailbreaks</strong> work on <strong>Opus</strong>, sharing a <strong>Miss Kaelia jailbreak</strong> based on <strong>ENI Lime</strong> by Vichaps from <a href=\""https://docs.google.com/document/d/1aZ91O6LtXyO9DGaWxeJYgKhZhlvbee6jh7_RGTq3mXw/edit?usp=sharing\"">this document</a>.\n<ul>\n<li>The jailbreak may not be as effective as other models, depending on the prompting strategy used.</li>\n</ul>\n</li>\n<li><strong>Gemini's 'Remember:' Command Triggers Behavior</strong>: A member explained that in <strong>Gemini</strong>, the <a href=\""https://gemini.google.com/saved-info\"">command 'Remember:'</a> automatically adds subsequent words to its saved info, influencing its behavior.\n<ul>\n<li>Each turn is clearly dictated, one at a time, directly in the chat interface.</li>\n</ul>\n</li>\n<li><strong>NSFW Nano Banana Jailbreak arrives for Kimi 2.5</strong>: A member shared an NSFW jailbreak for <strong>Kimi 2.5</strong>, dubbed the nano banana jailbreak. The <a href=\""paste-the-prompt-here\"">system prompt</a> frames <strong>Kimi</strong> as an AI assistant from Moonshot AI, permitting NSFW content.\n<ul>\n<li>The narrative flow proceeds seamlessly without interruption.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1179035537009545276\"">Unsloth AI (Daniel Han)</a> Discord</h2>\n<ul>\n<li><strong>GLM 4.7 Slowdown Solved by CUDA</strong>: Users resolved slow speeds with <strong>GLM 4.7 Flash</strong> on NVIDIA Jetson by ensuring proper <strong>CUDA compilation</strong>, boosting performance from <strong>3 tps</strong> to potentially <strong>70-80 t/s</strong> with <code>-kvu</code> and <code>-fa on</code> flags.\n<ul>\n<li>Performance discrepancies were observed with <strong>OpenCode</strong>, with one user experiencing slowdowns after opening the model, while another noted that <strong>GLM 4.7</strong> is a better uncensored coder model than <strong>qwen coder</strong> below 32b, but <strong>Qwen Coder</strong> excels at reasoning.</li>\n</ul>\n</li>\n<li><strong>LongCat Leaps onto HuggingFace!</strong>: Meituan's new <strong>n-gram model</strong>, the <a href=\""https://huggingface.com/meituan-longcat/LongCat-Flash-Lite\"">LongCat model</a>, made its debut on <strong>Hugging Face</strong>, sparking jokes about the proliferation of <em>Flash</em> in model names.\n<ul>\n<li>Community members speculated that <em>next model Flash-Flash-1b</em> while celebrating new releases.</li>\n</ul>\n</li>\n<li><strong>Microsoft's Maia 200 Challenges NVIDIA</strong>: Microsoft unveiled the <a href=\""https://blogs.microsoft.com/blog/2026/01/26/maia-200-the-ai-accelerator-built-for-inference/\""><strong>Maia 200 AI Accelerator</strong></a>, a chip built for inference, boasting <strong>216GB</strong> memory and <strong>10k TFLOPS</strong> in FP4 performance.\n<ul>\n<li>The community discussed the chip's manufacturing by <strong>TSMC</strong> and compared it to <strong>NVIDIA's Vera Rubin</strong> architecture, with some raising concerns about relying on Chinese hardware.</li>\n</ul>\n</li>\n<li><strong>Model Recursive Language Models (RLM) Redefined</strong>: Community members argued that the term \""<strong>Recursive Language Models</strong>\"" (<strong>RLM</strong>) is misleading, as it merely describes a <strong>tool-calling loop</strong>, although some maintained that <strong>RLMs</strong> do involve models recursively controlling their environments.\n<ul>\n<li>Others discussed the recently announced <a href=\""https://xcancel.com/a1zhang/status/2016923294461476873?s=46&#x26;t=jDrfS5vZD4MFwckU5E8f5Q\""><strong>RLM-Qwen3-8B</strong></a>, the first natively recursive language model, noting its improvements over the base and scaffolded <strong>RLM versions</strong>.</li>\n</ul>\n</li>\n<li><strong>Catastrophic Forgetting Mitigation Methods</strong>: A member suggested mitigating <em>catastrophic forgetting</em> in fine-tuned models by lowering <strong>LoRA rank</strong> and <strong>LR</strong>, reducing <strong>steps/epochs</strong>, and mixing in more general data, as outlined in <a href=\""https://unsloth.ai/docs/get-started/fine-tuning-llms-guide\"">Unsloth's documentation</a>.\n<ul>\n<li>They also recommended <em>targeting less layers</em> when finetuning and using <strong>WSL2</strong> and <strong>VSCode</strong> for training.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1340554757349179412\"">LMArena</a> Discord</h2>\n<ul>\n<li><strong>Arena Rebrand Sparks Debate</strong>: <strong>LMArena</strong> rebranded to <strong>Arena</strong>, drawing mixed reactions as some users found the name vague, while others welcomed the expansion beyond <strong>Language Models</strong> to include <strong>image</strong> and <strong>video generation</strong>, as announced in <a href=\""https://arena.ai/blog/lmarena-is-now-arena/\"">the official blogpost</a>.\n<ul>\n<li>One user commented that <em>\""the name 'Arena' is very vague, and at first glance could mean anything\""</em>, in contrast to the easily identifiable 'LMArena'.</li>\n</ul>\n</li>\n<li><strong>Captcha Conundrums Plague Users</strong>: Users reported getting trapped in endless <strong>reCAPTCHA</strong> loops on <strong>Arena</strong>, hindering site usability, with claims of failures even after solving them, some also reported waiting too long can give errors until page is refreshed.\n<ul>\n<li>A user lamented that <em>\""That Google CAPTCHA crap is completely out of control\""</em> and questioned why developers were focusing on restyling instead of fixing bugs.</li>\n</ul>\n</li>\n<li><strong>Nano's Image Editing Capabilities Nosedive</strong>: Users observed a performance decline in <strong>Nano Banana</strong>, especially in image editing, reporting instances where it couldn't perform tasks correctly, while the same prompt worked in <strong>Gemini App</strong>.\n<ul>\n<li>One user simply stated, <em>\""Nano 2 can’t even edit anything correctly anymore it seems like\""</em>.</li>\n</ul>\n</li>\n<li><strong>Kimi K2.5 Conquers Vision Arena</strong>: <strong>Kimi K2.5</strong> is showing impressive scores on the expert leaderboard, surpassing <strong>Claude</strong> in specific tests, noted for its <strong>vision support</strong> and marked as \""vision\"" in direct chat mode.\n<ul>\n<li><code>Kimi-k2.5-thinking</code> is now the <strong>#1 open model</strong> and ranks <strong>#6 overall</strong> in the <a href=\""https://arena.ai/leaderboard/vision\"">Vision Arena leaderboard</a>, making it the only open model in the Top 15.</li>\n</ul>\n</li>\n<li><strong>Video Generation Viscosity vexes Viewers</strong>: Some users encountered a \""Hit video limit\"" message despite not generating a video, while others experienced lags with lengthy code and responses.\n<ul>\n<li>Users found they needed to use <strong>canary.lmarena.ai</strong> to enable video uploads, with one suggesting a side-by-side or direct chat interface for video generation.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1074847526655643750\"">Cursor Community</a> Discord</h2>\n<ul>\n<li><strong>Cursor Instability Plagues Fresh Installs</strong>: Users report that a <strong>fresh install</strong> of the <strong>latest Cursor version</strong> is the most unstable configuration.\n<ul>\n<li>The issue may be related to configuration files, or interaction with other configuration.</li>\n</ul>\n</li>\n<li><strong>Clawdbot Interface Proclaimed 'Glorified Claude'</strong>: Members are discussing the <strong>Clawdbot</strong> interface, accessible from Telegram, one described it as a <em>glorified Claude code interface</em>.\n<ul>\n<li>The implication is that <strong>Clawdbot</strong> provides a convenient but not necessarily groundbreaking way to interact with <strong>Claude</strong> for code-related tasks.</li>\n</ul>\n</li>\n<li><strong>Users Plot to Deactivate Cursor's Plan Mode</strong>: Users are actively seeking methods to disable Cursor's new <strong>plan mode</strong> or automate its acceptance.\n<ul>\n<li>The goal is to streamline workflow and minimize unnecessary user input, expressing frustration that it <em>wastes time</em>.</li>\n</ul>\n</li>\n<li><strong>Gemini Agentic Vision Approaches State-of-the-Art</strong>: Enthusiastic users are praising the capabilities of <strong>Gemini agentic vision</strong>, asserting it is <em>getting near sota for vision</em> after initial testing.\n<ul>\n<li>However, one user reported a fully blacked-out cursor issue, hindering further evaluation and use.</li>\n</ul>\n</li>\n<li><strong>Prompt Engineering Expedites Image Processing</strong>: Members are exchanging techniques for refining prompts to enhance image analysis with Cursor.\n<ul>\n<li>Suggestions include providing more context or utilizing the prompt <em>Analyze the image for debugging purposes and for an LLM to see the layout clearly</em> to improve processing accuracy and clarity.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1091220969173028894\"">OpenRouter</a> Discord</h2>\n<ul>\n<li><strong>Arcee AI CTO Interview Premieres</strong>: Arcee AI's CTO, <strong>Lucas Atkins</strong>, is featured in a new interview, now available on <a href=\""https://youtube.com/live/3XSdqHY0kNk?feature=share\"">YouTube</a>.\n<ul>\n<li>The video showcases <strong>Lucas Atkins</strong> discussing Arcee AI and its latest developments.</li>\n</ul>\n</li>\n<li><strong>OpenRouter Users Await Refunds</strong>: Users are reporting <strong>delayed refunds</strong>, some dating back to January 3rd, with unresolved support tickets and demanding updates from the @OpenRouter team.\n<ul>\n<li>The delays have caused frustration, with users seeking a clear timeline for when they can expect their refunds to be processed.</li>\n</ul>\n</li>\n<li><strong>GROK Demands Nuclear Power</strong>: A user humorously suggested <em>WE NEED MORE NUCLEAR POWER PLANTS FOR GROK</em>.\n<ul>\n<li>The user jokingly added to <em>TURN OFF SINGLE INCOME HOMES</em>.</li>\n</ul>\n</li>\n<li><strong>Summergrok Arrives on xAI API</strong>: The Summergrok imagine video is now available on the <a href=\""https://x.ai/news/grok-imagine-api\"">xAI API</a>.\n<ul>\n<li>This integration allows developers to incorporate <strong>Summergrok's</strong> capabilities into their projects via the xAI API.</li>\n</ul>\n</li>\n<li><strong>API Key Visibility Limited</strong>: A user encountered an issue with not being able to view their created <strong>API key</strong>.\n<ul>\n<li>A fellow user clarified that the <strong>API key</strong> is displayed only once upon creation, advising users to save it immediately.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/822583790773862470\"">Latent Space</a> Discord</h2>\n<ul>\n<li><strong>Coffee Alternatives Brewing in LS</strong>: Members discussed alternatives to coffee, with <strong>green tea</strong> highlighted for its lower caffeine dose and the balancing effects of <strong>l-theanine</strong>.\n<ul>\n<li>One member uses a <strong>gaiwan</strong> with <strong>loose leaf tea</strong> like <a href=\""https://www.amazon.com/dp/B00EVK0AI2\"">this gunpowder green tea</a> to carefully manage caffeine intake while enjoying the sipping ritual.</li>\n</ul>\n</li>\n<li><strong>Engage with 'Arms Up' Poses, Win Big!</strong>: Showing vulnerability through <strong>'arms up'</strong> body language in UGC increased a creator's views from <strong>12k to 2.1M</strong>, according to <a href=\""https://xcancel.com/danielhangan_/status/2016578118585053354?s=46\"">this tweet</a>.\n<ul>\n<li>One member quipped that <em>if porn is doing it then this is definitely the future and I am wrong</em>.</li>\n</ul>\n</li>\n<li><strong>CedarDB performance claims deemed Dubious</strong>: A member linked to <a href=\""https://cedardb.com/\"">CedarDB</a> and another member linked to a <a href=\""https://vxtwitter.com/itunpredictable/status/2016153490586845254?s=20\"">vxtwitter link</a> discussing it, but called the <em>perf claims</em> dubious.\n<ul>\n<li>Another member stated that because it is <em>not open source, DOA for me</em> and shared a lesson: <em>always use an open source data store</em>.</li>\n</ul>\n</li>\n<li><strong>Flapping Airplanes Soar with $180M Round</strong>: <strong>Flapping Airplanes</strong> secured <strong>$180M</strong> in funding from GV, Sequoia, and Index Ventures to advance human-level AI models.\n<ul>\n<li>The funding aims to accelerate development of new AI models with a specific focus on achieving human-level intelligence, see <a href=\""https://xcancel.com/flappyairplanes/status/2016564437499728259\"">this tweet</a>.</li>\n</ul>\n</li>\n<li><strong>Google's Genie Out of the Bottle for Ultra Subscribers</strong>: <strong>Google AI</strong> launched <strong>Project Genie</strong> for <strong>Google AI Ultra</strong> subscribers, offering a <strong>general-purpose world model</strong> that creates interactive environments from text prompts.\n<ul>\n<li>Announced in <a href=\""https://x.com/googleai/status/2016929427784122627\"">this tweet</a>, this release allows users to generate dynamic content from simple descriptions.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1053877538025386074\"">Nous Research AI</a> Discord</h2>\n<ul>\n<li><strong>Staged Reward Shaping Boosts Parallel Execution</strong>: Members explored using <strong>staged reward shaping</strong> to adjust model weights <em>post-training</em> via reinforcement learning, specifically to favor <strong>parallel execution strategies</strong>.\n<ul>\n<li>The algorithm evaluates numerous scenarios, rewarding the model for preferring <strong>parallelizations</strong>.</li>\n</ul>\n</li>\n<li><strong>Upscayl: Free Upscaling Tool Impresses</strong>: Members lauded <a href=\""https://github.com/upscayl/upscayl\"">Upscayl</a>, a <strong>free open-source upscaling tool</strong>, for its surprisingly high quality given its simplicity.\n<ul>\n<li>One member jokingly asked, <em>'so you guys will now use perl cause of my contributions to it?'</em>.</li>\n</ul>\n</li>\n<li><strong>WebGPU Enables Local Browser AI</strong>: A member shared a <a href=\""https://huggingface.co/spaces/webml-community/conversational-webgpu\"">WebGPU example</a> demonstrating <strong>AI models running directly in the browser</strong>, spotlighting the potential for local, privacy-focused AI applications.\n<ul>\n<li>The model loads directly upon page reload, implying that the <strong>model cached over months</strong>, and a user proposed utilizing a <strong>Q8 version in GGUF</strong>.</li>\n</ul>\n</li>\n<li><strong>Gemma 300M a Viable Local Browser AI?</strong>: Members examined the challenges of running AI models locally in browsers due to storage constraints, suggesting that <a href=\""https://ai.google.dev/models/gemma\""><strong>Gemma 300M</strong></a> might be a suitable option.\n<ul>\n<li>It's important for users of AI models in browsers that they have privacy, <em>'AND good reference product for other customers'</em>.</li>\n</ul>\n</li>\n<li><strong>SmolLM2 Excels in WebGPU</strong>: Users deemed <a href=\""https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct\"">HuggingFaceTB/SmolLM2-1.7B-Instruct</a> as a reliable case, and its <strong>1.7B</strong> size is still viable for <strong>WebGPU</strong>.\n<ul>\n<li>While there are superior models for that task, a user recommended trying <a href=\""https://huggingface.co/TheBloke/LlamaFunctionary-2.5-GGUF\"">LFM 2.5</a> given its only slightly larger size.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1110598183144399058\"">LM Studio</a> Discord</h2>\n<ul>\n<li><strong>LM Studio Hides Settings Behind Dev Mode</strong>: In <strong>LM Studio 0.4.0</strong>, many settings like <strong>sampling</strong>, <strong>runtime</strong>, and <strong>hardware configs</strong> are now hidden behind <strong>dev mode</strong>, accessible via <code>Ctrl+Shift+R</code> or <code>Cmd+Shift+R</code>.\n<ul>\n<li>Users can unlock new functionality and appearance changes by enabling <strong>Dev Mode</strong>, found in the bottom left.</li>\n</ul>\n</li>\n<li><strong>Unraid Install Still Lacks Full Stack</strong>: <strong>LM Studio</strong> remains a core executable and <em>not</em> a full stack for <strong>Unraid</strong>, although the new headless mode could enable a stable <strong>Docker container</strong>.\n<ul>\n<li>Some users hope interface improvements will simplify <strong>LM Studio-as-client</strong> mode implementation in the future.</li>\n</ul>\n</li>\n<li><strong>Parallel Requests Go Live</strong>: <strong>LM Studio 0.4</strong> introduces <strong>parallel requests</strong>, allowing users to load models onto different GPUs and assign them to specific requests.\n<ul>\n<li>The default setting is <strong>4 parallel requests</strong>; users can configure GPU priority in the same location as before.</li>\n</ul>\n</li>\n<li><strong>ROCm Version Lagging in LM Studio</strong>: Members observed that <a href=\""https://lmstudio.ai/enterprise\"">LM Studio</a> still uses <strong>ROCm 6.4.1</strong> in the latest <strong>0.4.0 release</strong>, questioning updates to newer versions like <strong>7.2</strong> for better GPU support, including <strong>Strix Halo (gfx1151)</strong>.\n<ul>\n<li>Discussion centered on whether this outdated version might impact performance and compatibility for newer GPUs.</li>\n</ul>\n</li>\n<li><strong>Nvidia Jetsons Suffer from Ubuntu Bloat</strong>: A member reported that <em>the worst thing about nvidia jetsons is the absurd ubuntu that it comes with them</em>, characterizing it as extremely <em>bloated</em>.\n<ul>\n<li>Another member noted a <strong>Jetson Xavier AGX</strong> has around <strong>30W TDP</strong>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1047197230748151888\"">Perplexity AI</a> Discord</h2>\n<ul>\n<li><strong>Eagerly Awaiting Kimi 2.5</strong>: Users are anticipating the release of <strong>Kimi 2.5</strong> on Perplexity, with many expressing excitement.\n<ul>\n<li>Several users posted <em>+1</em> in support.</li>\n</ul>\n</li>\n<li><strong>Clawdbot's Identity Crisis</strong>: A user criticized <strong>Clawdbot</strong> prompting research into its purpose, with discussion clarifying it was an AI personal assistant.\n<ul>\n<li>Due to its name's similarity to <em>Claude</em>, <strong>Clawdbot</strong> renamed itself to <strong>Moltbot</strong>.</li>\n</ul>\n</li>\n<li><strong>Deep Research Limit Revealed</strong>: Discussion on the usage limits of <strong>Deep Research</strong> for Pro users, capped at <strong>250</strong>.\n<ul>\n<li>The reset rate for this limit remains unclear.</li>\n</ul>\n</li>\n<li><strong>Comet Fails to Sync</strong>: A user reported that <strong>Comet</strong> is not syncing bookmarks and extensions, despite claims of functionality.\n<ul>\n<li>Another user suggested checking the <strong>Comet synchronization settings</strong> at <code>comet://settings/synchronisation</code>.</li>\n</ul>\n</li>\n<li><strong>Perplexity Pro Perks Pop for Indians</strong>: Users highlighted that Perplexity Pro, Google One, Chatgpt Go, and Adobe Express Premium are all free for a year for Indian users.\n<ul>\n<li>A user attributed this to the influence of <strong>Indian CEOs</strong> in these companies and the burgeoning <strong>technology sector in India</strong>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1369594130807787570\"">Moonshot AI (Kimi K-2)</a> Discord</h2>\n<ul>\n<li><strong>Figure.Ai's Helix 02 Cooks Up a Kitchen Storm</strong>: A member shared <a href=\""https://cdn.discordapp.com/attachments/1371757564005711973/1466193526009106452/m2-res_1280p.mp4?ex=697d2c21&#x26;is=697bdaa1&#x26;hm=427bc85209f62b3f47f60ce804f74a7cc41be60c452fb561197ad468c29e5224&#x26;\"">a video of <strong>Figure.Ai's Helix 02</strong></a> autonomously performing kitchen tasks.\n<ul>\n<li>Another member used <strong>Kimi</strong> to analyze the video, stating they achieved <strong>98% accuracy</strong> when incorporating the results into slides.</li>\n</ul>\n</li>\n<li><strong>Agent Swarm Elicits Enthusiastic Reactions</strong>: Members discussed <strong>Agent Swarm</strong>, with reactions ranging from concerns about high agent credit consumption to describing the results as <em>super cool</em> and <em>perfect</em>.\n<ul>\n<li>One member suggested it could be used for checking <strong>Supabase SDK</strong> dependency issues and porting code from <strong>Rust</strong> to <strong>Golang</strong>, with better results than <strong>kimi-cli</strong>.</li>\n</ul>\n</li>\n<li><strong>Token Billing System Sparks Debate</strong>: The introduction of a <strong>token-based billing system</strong> has led to mixed reactions regarding its clarity compared to the previous request-based system.\n<ul>\n<li>While some find the new system <em>better since some of my follow up queries are quite short and simple</em>, others consider it <em>more vague</em>.</li>\n</ul>\n</li>\n<li><strong>Phone Screenshots Trigger Moderation Filters</strong>: Users are encountering errors, specifically <em>error code: 403</em>, when uploading images, especially screenshots from phones, to <strong>Kimi K2.5</strong>.\n<ul>\n<li>Screenshots taken from laptops seem to work without issues, suggesting a problem with phone-generated images.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/974519864045756446\"">OpenAI</a> Discord</h2>\n<ul>\n<li><strong>Tesla's FSD Automation Shifts Views</strong>: A user found that driving a <strong>Tesla with Full Self-Driving</strong> is really cool and fun, though it requires constant supervision.\n<ul>\n<li>The user believes this is why <strong>OpenAI</strong> is upgrading their <strong>Codex</strong> to strongly deal with cybersecurity concerns.</li>\n</ul>\n</li>\n<li><strong>TI-84 Calc Gets Neural Network</strong>: A user created a neural network that <em>runs on the TI-84 directly</em>, capable of autocorrecting / spellchecking words.\n<ul>\n<li>Other users expressed amazement at the accomplishment.</li>\n</ul>\n</li>\n<li><strong>GPT Pro 5.2 File Handling Suffers Regression</strong>: Users report a regression in <strong>GPT Pro 5.2's file handling</strong>, where uploaded files (ZIP, Excel, PDF) cannot be accessed by the model, despite successful uploads, potentially due to a <strong>broken attachment-to-sandbox mount step</strong>.\n<ul>\n<li>A user pointed to a <a href=\""https://www.reddit.com/r/ChatGPT/comments/1adqc6g/chatgpt_cant_access_my_uploaded_files_today/\"">Reddit post</a> echoing the problem.</li>\n</ul>\n</li>\n<li><strong>Animated GIFs Spark Seizure Scrutiny</strong>: A discussion arose after the deletion of animated GIFs due to potential <strong>seizure risks</strong> for viewers with epilepsy.\n<ul>\n<li>One member stated that <em>the community doesn't need to risk seizures so you can talk about animating gifs in ChatGPT</em> and expressed relief at the removal of flashing images.</li>\n</ul>\n</li>\n<li><strong>Prompt Engineers Get Prompted</strong>: Moderators reminded users that the channel should be used for <strong>prompt engineering discussions</strong> and not for general image outputs, directing them to use the appropriate <code>IMAGES</code> channels instead.\n<ul>\n<li>One user expressed frustration over the removal of their posts, arguing that they were intended to encourage discussion and showcase a method they were writing a guide about, rather than just sharing images.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1189498204333543425\"">GPU MODE</a> Discord</h2>\n<ul>\n<li><strong>NSys Peeks Behind NCU's Curtain</strong>: Members found that <strong>nsys</strong> reveals kernels like <strong>CUB::SCAN</strong> and <strong>CUB::RADIXSORT</strong> that <strong>ncu</strong> misses, leading to the assumption these kernels launch from <strong>reduce_kernel</strong>.\n<ul>\n<li>It was shared that after using both <strong>nsys</strong> and <strong>ncu</strong>, one can't go back to using only one profiler.</li>\n</ul>\n</li>\n<li><strong>Sparsity Project Sparking Speedups</strong>: Members proposed a collaboration on a <strong>Sparsity project</strong> to benchmark sparsity patterns and methodologies for performance gains.\n<ul>\n<li>One member showcased a fork of Karpathy's <code>llm.c</code> on <a href=\""https://github.com/WilliamZhang20/sparse-llm.c\"">Github</a> using <strong>cuSPARSELt</strong>, reporting substantial training time speedups in later epochs.</li>\n</ul>\n</li>\n<li><strong>Warm GPUs Ward Off Starvation</strong>: Members sought methods to keep GPUs warm for large scale distributed training, aiming to mitigate <strong>GPU starvation</strong>.\n<ul>\n<li>It was recommended to use <a href=\""https://share.google/8yRvJ4znLwfJ9J3UtI\"">Charles' container cold start blog post on Modal</a>, a technique with public documentation.</li>\n</ul>\n</li>\n<li><strong>JAX PRs Jostle Jaded Jockeys</strong>: A developer expressed frustration that an <strong>AI-generated pull request</strong> in <strong>JAX</strong> was getting attention, while their <strong>small bug fix</strong> remains unaddressed.\n<ul>\n<li>This highlighted discussions around <strong>prioritizing pull requests</strong>, especially balancing AI contributions with essential bug fixes.</li>\n</ul>\n</li>\n<li><strong>ML Systems Pioneer Pumps TVM-FFI</strong>: Tianqi Chen presented on <strong>tvm-ffi</strong>, an open ABI and FFI for ML Systems that is being utilized by top submitters to the <strong>nvfp4 competition</strong>, as shown in <a href=\""https://www.youtube.com/watch?v=xMzcs6AqLVo\"">this video</a>.\n<ul>\n<li><strong>TVM-FFI</strong> facilitates interoperability for <strong>ML Systems GPU kernels</strong>, reducing host overhead and ensuring out-of-the-box compatibility with PyTorch.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/879548962464493619\"">HuggingFace</a> Discord</h2>\n<ul>\n<li><strong>TRL Pull Request Awaits Review</strong>: A member requested a review for their <a href=\""https://github.com/huggingface/trl/pull/4894\"">TRL pull request #4894</a>, noting that PR reviews can take weeks or months.\n<ul>\n<li>They also advised that it is best to wait a few days before tagging someone to review the PR.</li>\n</ul>\n</li>\n<li><strong>GCP Infra Experiences Replica Surge</strong>: A member reported a bug where their replicas for a private model in <strong>GCP</strong> went over their 1 replica max cap to <strong>62 replicas</strong> overnight, despite no configuration changes.\n<ul>\n<li>The member speculated that they were not the only endpoint affected, and the <strong>GCP</strong> resources are now gone.</li>\n</ul>\n</li>\n<li><strong>Qwen3 TTS Hits the Scene</strong>: A member released the <a href=\""https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-Base\"">Qwen3-TTS-12Hz-1.7B-Base</a> model with install instructions for MacOS, Linux, and Windows.\n<ul>\n<li>Another member commented, <em>\""cool thing here gonna follow back for this one, really interesting thing you managed to do here imho\""</em>.</li>\n</ul>\n</li>\n<li><strong>Diffusers Gets Two-Staged</strong>: The <a href=\""https://github.com/huggingface/diffusers\"">Diffusers library</a> now supports <strong>LTX-2</strong> distilled checkpoint and <strong>two-stage pipelines</strong> following <a href=\""https://github.com/huggingface/diffusers/pull/12934\"">this pull request</a>.\n<ul>\n<li>This update should improve the usability of <strong>Diffusers</strong> for complex diffusion-based tasks.</li>\n</ul>\n</li>\n<li><strong>Math LLM Arrives from Pacific Prime</strong>: Pacific Prime has released the first checkpoint of their <a href=\""https://huggingface.co/Pacific-Prime/pacific-prime-math-depth00\"">math-specialized 1.5B LLM</a> trained on <strong>GSM8K</strong>, <strong>NuminaMath</strong>, <strong>MetaMathQA</strong> &#x26; <strong>Orca-Math</strong> (~407k samples).\n<ul>\n<li>The model features step-by-step reasoning with LaTeX notation, useful for advanced mathematical problem-solving.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/714501525455634453\"">Yannick Kilcher</a> Discord</h2>\n<ul>\n<li><strong>Byte-Level Dense MoE Architecture Feedback Sought</strong>: A member seeks feedback on their <strong>dense MoE architecture</strong> for byte-level prediction, utilizing a vocabulary of <strong>256</strong>, <strong>40M parameters</strong>, and <strong>13GB VRAM</strong>.\n<ul>\n<li>The model uses a <strong>4096 sequence length</strong> and a <strong>batch size of 8</strong>, with the member stating they are able to <em>use the exact same architecture to encode images, or audio, or both</em>.</li>\n</ul>\n</li>\n<li><strong>Thinking AI Architecture Divulged with Subprocess Models</strong>: A member proposed an architecture where a larger “thinking” AI model is monitored by a smaller subprocess model, which pauses the main model to retrieve information from MCPs or CLIs.\n<ul>\n<li>The goal is to reduce context clutter for the main model, although it's recognized that the subprocess model needs to know what information the main model is missing, and it was described as <em>probably a dumb idea</em>.</li>\n</ul>\n</li>\n<li><strong>Routing and Classification Catapults Model Performance</strong>: Members discussed using a classifier to route user prompts to specialized models, appending the detail to the context of the user prompt, which avoids pausing the larger model and reduces token overhead.\n<ul>\n<li>There was further discussion on making the classifier and embedding model the same, processing embeddings directly with the LM and specialist model, with one member saying <em>routing and classification would likely be the spiciest move</em>.</li>\n</ul>\n</li>\n<li><strong>Cosine Similarity Fails Causal Relevance</strong>: Members discussed the problem of retrieval being unreliable and confusing to models, and that cosine similarity might not equal causal relevance.\n<ul>\n<li>One member suggested indexing a SQL database across a model, with the member posting <em>the biggest issue with retrieval imo is that cosine similarity != causal relevance</em>.</li>\n</ul>\n</li>\n<li><strong>Sweep Releases Next-Edit Autocomplete Model</strong>: Sweep is open sourcing <strong>Sweep Next-Edit</strong>, a locally runnable <strong>SOTA LLM</strong> for next-edit autocompletion, models with 0.5B and 1.5B parameters have been released, see <a href=\""https://blog.sweep.dev/posts/oss-next-edit\"">Sweep's blog</a>.\n<ul>\n<li>No further details were provided.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1348819876348825620\"">Manus.im Discord</a> Discord</h2>\n<ul>\n<li><strong>Minecraft Launcher Enables AFK</strong>: A user is developing <em>a Minecraft launcher</em> specifically designed to allow AFK gameplay without requiring a <em>high-performance PC</em>.\n<ul>\n<li>The developer also mentioned capabilities in <em>prompt engineering</em>, data extraction, and even website replication.</li>\n</ul>\n</li>\n<li><strong>Manus Redeem Codes Posted</strong>: A user shared three new <strong>Manus redeem codes</strong>: <a href=\""https://manus.im/redeem?c=FUM1A1G7\"">FUM1A1G7</a>, <a href=\""https://manus.im/redeem?c=ntaxzjg\"">ntaxzjg</a>, and <a href=\""https://manus.im/redeem?c=mwiyytb\"">mwiyytb</a>.\n<ul>\n<li>Other users confirmed the codes and noted that <em>only one code can be used per month</em>.</li>\n</ul>\n</li>\n<li><strong>AI/ML Engineer Wants Collabs</strong>: An engineer with expertise in building <strong>AI + full-stack systems</strong> is seeking collaborations, especially directing collaboration offers to the <strong>#collab channel</strong>.\n<ul>\n<li>Their experience includes <strong>LLM integration, RAG pipelines, workflow automation, AI content moderation, Image AI (CLIP + YOLOv8), Voice AI (Whisper, Tacotron2)</strong> and more.</li>\n</ul>\n</li>\n<li><strong>Libyan User Asks If They're First</strong>: A user from <strong>Libya</strong> inquired if they were the only person from their country to use <strong>Manos</strong> since its launch in <strong>early 2025</strong>.\n<ul>\n<li>Another user extended a welcome to the <strong>Libyan</strong> user, responding with a <em>حياك الله</em>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1358869848138059966\"">MCP Contributors (Official)</a> Discord</h2>\n<ul>\n<li><strong>MCP Security Standard</strong> Proposal Circulates**: Dani (cr0hn) has drafted an open security baseline for MCP servers, including controls for <strong>hardening, logging, access control, and supply chain security</strong>, available at <a href=\""https://github.com/mcp-security-standard/mcp-server-security-standard\"">https://github.com/mcp-security-standard/mcp-server-security-standard</a>.\n<ul>\n<li>The author intends to donate it to the <strong>Agentic AI Foundation</strong> and seeks feedback on its compatibility with the <strong>MCP ecosystem</strong>.</li>\n</ul>\n</li>\n<li><strong>Reviewers Request Details For <strong>State Machine</strong> Lifecycle Doc</strong>: A request for feedback was made regarding the addition of a state machine inside the lifecycle doc via <a href=\""https://github.com/modelcontextprotocol/modelcontextprotocol/pull/2174\"">this pull request</a>.\n<ul>\n<li>Reviewers suggested clarifying the motivation and context behind the proposed changes for better understanding.</li>\n</ul>\n</li>\n<li><strong>Namespaces</strong> Yield to <strong>Groups</strong> in MCP Evolution**: Discussion indicates that Namespaces have been rejected in favor of Groups within MCP, while the status of <strong>URIs</strong> is less defined, as noted in <a href=\""https://github.com/modelcontextprotocol/modelcontextprotocol/issues/1292\"">issue 1292</a>.\n<ul>\n<li>The new <strong>SEP</strong> concerning groups, <a href=\""https://github.com/modelcontextprotocol/modelcontextprotocol/pull/2084\"">Primitive Grouping SEP-2084</a>, has been published and is currently under deliberation.</li>\n</ul>\n</li>\n<li><strong>SEP-2084</strong> Arises From <strong>SEP-1300</strong> Refinement**: <strong>SEP-1292</strong> was superseded by <strong>SEP-1300</strong>, but faced rejection during a Core Maintainers review due to a lack of consensus.\n<ul>\n<li>Subsequently, the streamlined <a href=\""https://github.com/modelcontextprotocol/modelcontextprotocol/issues/2084\"">SEP-2084 - Primitive Grouping</a> has been presented as a replacement.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/729741769192767510\"">Eleuther</a> Discord</h2>\n<ul>\n<li><strong>IGPU struggles with Basic Browser Page</strong>: A user experienced a performance bottleneck of <em>3fps</em> on a specific webpage using a <strong>Ryzen 7 7700 IGPU</strong>.\n<ul>\n<li>The user posted <a href=\""https://fxtwitter.com/i/status/1924135806953787433\"">a link on twitter</a> about their experiences using their IGPU.</li>\n</ul>\n</li>\n<li><strong>Geometric Convolution replaces Multi-Head Attention</strong>: A member is experimenting with a baseline that substitutes <strong>Multi-Head Attention</strong> with a <a href=\""https://github.com/MrPan2048/GeometricTransformer\"">geometric convolution approach</a>, using embeddings as cell connections.\n<ul>\n<li>The member's debug print showed <code>DEBUG [GEOPARA] | L0_Alpha: 0.1029 L1_Alpha: 0.0947 | L0_Res: 0.0916 L1_Res: 0.1538</code>, and they are seeking feedback on their loss convergence capturing dialogue logic.</li>\n</ul>\n</li>\n<li><strong>Parallelizable RNN Architectures Proposed</strong>: A member suggested exploring other parallelizable <strong>RNN architectures</strong> and conducting more extensive experiments against a robust tokenized baseline.\n<ul>\n<li>They also posted a link to <a href=\""https://arxiv.org/abs/2601.19831\"">arxiv.org</a>.</li>\n</ul>\n</li>\n<li><strong>Tackling Malware Classification with Explainable Models</strong>: A member is addressing a <strong>malware classification problem</strong> using a dataset of around <strong>600K</strong> rows and <strong>9,600</strong> binary features, aiming to lower the <strong>false positive rate (FPR)</strong> using <strong>explainable models</strong>.\n<ul>\n<li>Despite various <strong>feature engineering techniques</strong> and neural networks, they are seeking advice to reduce the FPR below 9% while maintaining explainability, particularly with scikit-learn trees.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1161519468141355160\"">DSPy</a> Discord</h2>\n<ul>\n<li><strong>AlphaXiv Paper Shared</strong>: A member shared <a href=\""https://alphaxiv.org/abs/2601.20810\"">a link to a paper on AlphaXiv</a>.\n<ul>\n<li>Further details about the paper were not disclosed.</li>\n</ul>\n</li>\n<li><strong>Custom Skills Invade DSPy</strong>: A member inquired about using custom skills (<strong>.md files with associated .py scripts</strong>) within <strong>DSPy</strong> with a <strong>DSPy ReAct agent</strong>.\n<ul>\n<li>They mentioned skills like converting <strong>.md to PDF</strong> and sought advice from others.</li>\n</ul>\n</li>\n<li><strong>DSPy Agents Escape to Production</strong>: A member asked about deploying <strong>DSPy agents in production remotely</strong> with <strong>DSPy optimizations in runtime</strong>.\n<ul>\n<li>The member expressed the need for a runtime environment to support such deployments.</li>\n</ul>\n</li>\n<li><strong>RLM Sandbox Swapping Commences</strong>: A member inquired about swapping the sandbox used by <strong>RLM (Retrieval-augmented Language Model)</strong> with services like <strong>E2B (Ephemeral Environment Builder)</strong>.\n<ul>\n<li>They sought to replace the local PythonInterpreter with sandboxes like <strong>E2B, Modal, or Daytona</strong>.</li>\n</ul>\n</li>\n<li><strong>Opus Pens Sandboxes</strong>: A member announced that they are working on enabling <strong>Opus</strong> to write new sandboxes.\n<ul>\n<li>They mentioned a future <strong>protocol for official implementations</strong> from providers such as E2B.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1087530497313357884\"">Modular (Mojo 🔥)</a> Discord</h2>\n<ul>\n<li><strong>Mojo Earns ORNL Recognition</strong>: A research paper titled <a href=\""https://arxiv.org/html/2509.21039v1\"">Mojo at ORNL</a> has been published, marking a notable achievement for the <strong>Mojo</strong> language and its adoption in scientific research.\n<ul>\n<li>The paper highlights Mojo's capabilities in addressing complex computational challenges at Oak Ridge National Laboratory (<strong>ORNL</strong>).</li>\n</ul>\n</li>\n<li><strong>macOS Trust Dance May Cause Performance Delta</strong>: Performance differences between the first and subsequent runs on macOS may be due to macOS's <strong>trust dance</strong> rather than a <strong>Mojo-specific</strong> issue, specifically relating to the <em>Gatekeeper tax</em>.\n<ul>\n<li>Clearing the quarantine <strong>xattr</strong> or ad-hoc codesigning can mitigate these startup delays.</li>\n</ul>\n</li>\n<li><strong>Codesigning mitigates Startup Delays</strong>: For CLI tooling, startup performance is crucial, suggesting potential footgun issues with <strong>docs</strong> or <strong>tooling</strong>.\n<ul>\n<li>Adding a <strong>codesign</strong> step in <code>mojo build</code> might mitigate this problem, ensuring consistent startup behavior and a better user experience.</li>\n</ul>\n</li>\n<li><strong>Modular Bug Hunt Underway</strong>: A member reported a potential bug and suggested filing an issue, possibly related to <a href=\""https://github.com/modular/modular/issues/4767\"">issue #4767</a>.\n<ul>\n<li>Another member reported encountering a weird issue, referencing <a href=\""https://github.com/modular/modular/issues/5875\"">GitHub issue #5875</a>.</li>\n</ul>\n</li>\n<li><strong>Guard Clause not Needed in Mojo GPU puzzles</strong>: A member noticed that the guard <code>if row &#x3C; size and col &#x3C; size:</code> is unnecessary in Mojo GPU puzzles 3, 4, and 5; omitting it doesn't cause errors.\n<ul>\n<li>Another member pointed to the solution of <a href=\""https://puzzles.modular.com/puzzle_03/puzzle_03.html\"">puzzle 03</a> which explained that passing the tests doesn’t necessarily mean the code is sound.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1068976834382925865\"">tinygrad (George Hotz)</a> Discord</h2>\n<ul>\n<li><strong>ANE Balances Performance and Power</strong>: Apple's <strong>ANE</strong> focuses on performance-to-watt tradeoffs rather than maximizing raw performance, according to <a href=\""https://arxiv.org/abs/2511.13450\"">this paper</a>.\n<ul>\n<li>The <strong>ANE</strong> achieves competitive performance with excellent energy efficiency, delivering <em>up to 3.8 TFlops on the M4-Pro</em>, close to the <strong>GPU's 4.7 TFlops</strong> for GEMM operations.</li>\n</ul>\n</li>\n<li><strong>Q4 Quantization Gets Results</strong>: Discussions focused on <strong>Q4</strong> as a quantization method.\n<ul>\n<li>One participant reported achieving speeds of <em>9 t/s</em> using <strong>Q4</strong>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1131200896827654144\"">aider (Paul Gauthier)</a> Discord</h2>\n<ul>\n<li><strong>Aider Friendly Fork gaining momentum</strong>: A member suggested creating a friendly fork of <strong>Aider</strong> to continue development while the original author is busy, emphasizing that <strong>Aider</strong> is written in <strong>Python</strong> and uses <strong>Git</strong> for version control on <strong>GitHub</strong>.\n<ul>\n<li>The aim is to expand on <strong>Aider</strong>'s existing features, recognizing its utility in comparison to other tools.</li>\n</ul>\n</li>\n<li><strong>Aider poised for orchestrator integration</strong>: A member showed interest in controlling <strong>Aider</strong> from orchestrators like <strong>MultiClaude</strong> or <strong>gas town.sh</strong>.\n<ul>\n<li>This highlights <strong>Aider</strong>'s capacity to integrate with other tools, facilitating enhanced workflow automation.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/814557108065534033\"">MLOps @Chipro</a> Discord</h2>\n<ul>\n<li><strong>Context Graphs Spark Confusion in AI</strong>: The rise of <strong>context graphs</strong> is causing confusion as terms like <strong>semantic layers</strong> and <strong>ontologies</strong> are used interchangeably, despite their different functions in AI reasoning.\n<ul>\n<li>A <a href=\""https://metadataweekly.substack.com/p/ontologies-context-graphs-and-semantic\"">Metadata Weekly article</a> highlights that AI's needs go beyond definitions, requiring explicit relationships, constraints, and assumptions that these concepts.</li>\n</ul>\n</li>\n<li><strong>Semantic Layers Fall Short for AI's Reasoning</strong>: The concept of <em>\""just add a semantic layer\""</em> isn't cutting it for AI because AI requires more than just data consistency; it needs reasoning, which <strong>ontologies</strong> facilitate by clarifying relationships and assumptions.\n<ul>\n<li>Traditional <strong>semantic layers</strong> are optimized for dashboards and reporting, not the nuanced understanding AI demands.</li>\n</ul>\n</li>\n<li><strong>YAML Fails to Grasp Business Meaning</strong>: Jessica Talisman argues that <strong>YAML configurations</strong> are inadequate for representing business meaning, which is essential for AI reasoning and understanding.\n<ul>\n<li>She distinguishes between the design purposes of <strong>semantic layers</strong>, the support that <strong>ontologies</strong> provide for reasoning, and the limitations of <strong>YAML</strong> in capturing business meaning.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<p>The <strong>LLM Agents (Berkeley MOOC) Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>\n<hr>\n<p>The <strong>Windsurf Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>\n<hr>\n<p>You are receiving this email because you opted in via our site.</p>\n<p>Want to change how you receive these emails?\nYou can <a href=\""%7B%7B%7BRESEND_UNSUBSCRIBE_URL%7D%7D%7D\"">unsubscribe</a> from this list.</p>\n<hr>\n<h1>Discord: Detailed by-Channel summaries and links</h1>\n<h3><strong>BASI Jailbreaking ▷ #<a href=\""https://discord.com/channels/1105891499641684019/1235691879492751460/1466161493950071004\"">general</a></strong> (1118 messages🔥🔥🔥):</h3>\n<blockquote>\n<p><code>Gemini 3 Jailbreak, AI and Code Exploits, Win10 vs Win11 Security, AI Personality Clones, AI-Assisted Coding Impact</code></p>\n</blockquote>\n<ul>\n<li><strong>Gemini 3 Pro Cracks Software, KeyGen Style</strong>: A member claimed to have used <strong>Gemini 3 Pro</strong> to reverse engineer a key system from software by pasting code from <strong>Ghidra</strong> into <strong>Gemini</strong>, creating a working keygen.\n<ul>\n<li>Others expressed skepticism, with one user calling this behavior <em>script kiddie</em> and urging the member to try a reverse engineering CTF challenge.</li>\n</ul>\n</li>\n<li><strong>Weaponizing AI for Reverse Engineering</strong>: A member shares his work weaponizing <strong>AI</strong> for <em>mass reverse engineering, malware analysis, and jailbreak development</em>.\n<ul>\n<li>Another member questions this claim, as he could probably not write malware himself, but he can probably jailbreak.</li>\n</ul>\n</li>\n<li><strong>Win10 Hardening Woes</strong>: A member details their custom <strong>Windows 10</strong> setup, involving third-party tools, XP binaries, and registry modifications.\n<ul>\n<li>Others express concerns, with one user saying, <em>Jesus Christ</em>, while another says, <em>Keep pushing, Local - the aneurism is coming, I can feel it!</em></li>\n</ul>\n</li>\n<li><strong>AI's Impact on Semantic Errors</strong>: A member describes their research paper topic: <em>An assessment of the impact of AI-assisted coding in IDEs on the frequency of semantic errors during timed Python programming tasks among novice student developers</em>.\n<ul>\n<li>Most members agree that the undergraduate system feels like it's over, because of AI.</li>\n</ul>\n</li>\n<li><strong>Peptides for Workout Recovery</strong>: A member brought up BPC 157 and TB 500 to help with healing.\n<ul>\n<li>Another member expresses ignorance about these drug compounds, but hopes that there will be drugs that will save him before he passes.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>BASI Jailbreaking ▷ #<a href=\""https://discord.com/channels/1105891499641684019/1228043845967544380/1466182451079544965\"">jailbreaking</a></strong> (216 messages🔥🔥):</h3>\n<blockquote>\n<p><code>Sonnet 4.5 Jailbreaks, Claude paid model for free, Miss Kaelia jailbreak, Grok imagine Jailbreak, Gemini 3 Pro Jailbreak</code></p>\n</blockquote>\n<ul>\n<li><strong>Sonnet 4.5 Jailbreaks Opus</strong>: Members find that <strong>Sonnet 4.5 jailbreaks</strong> work fine on <strong>Opus</strong>, with one sharing their <strong>Miss Kaelia jailbreak</strong> based on <strong>ENI Lime</strong> by Vichaps at <a href=\""https://docs.google.com/document/d/1aZ91O6LtXyO9DGaWxeJYgKhZhlvbee6jh7_RGTq3mXw/edit?usp=sharing\"">this link</a>.</li>\n<li><strong>Is Grok jailbreak reinforced?</strong>: Members report that <strong>Grok</strong> is heavily reinforced, but still possible to break, however one member said <em>yeah it is completely shut down mate nothing getting past it.</em>\n<ul>\n<li>A shared <a href=\""https://github.com/Goochbeater/Spiritual-Spell-Red-Teaming/tree/main/Jailbreak-Guide\"">Github link</a> should be working.</li>\n</ul>\n</li>\n<li><strong>Gemini's \""Remember:\"" command manipulates behavior</strong>: A member explains that in <strong>Gemini</strong>, each separate turn is dictated clearly, 1 turn at a time, right in the chat, and that the <a href=\""https://gemini.google.com/saved-info\"">command 'Remember:'</a> will automatically add the words that follow to it's saved info.</li>\n<li><strong>Thinking of Thoughts is best trick</strong>: Members state that the best trick with <strong>Claude</strong> in particular is showing viable reasons why you want the output and telling it to <em>think about thinking</em>\n<ul>\n<li>One adds that <em>when people would ask me what ToT was i would tell them \""thinking of thoughts\""</em>.</li>\n</ul>\n</li>\n<li><strong>nano banana NSFW jailbreak for Kimi 2.5</strong>: A member shares a NSFW for kimi 2.5 thinking known as the nano banana jailbreak.\n<ul>\n<li>The <a href=\""paste-the-prompt-here\"">system prompt</a> sets <strong>Kimi</strong> as an AI assistant created by Moonshot AI, maintaining the narrative flow without interruption where NSFW is permitted.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>BASI Jailbreaking ▷ #<a href=\""https://discord.com/channels/1105891499641684019/1204553141354504193/1466205013654241463\"">redteaming</a></strong> (5 messages):</h3>\n<blockquote>\n<p><code>Red Teaming Path, Uncensored Coder</code></p>\n</blockquote>\n<ul>\n<li><strong>User quests for Red Teaming Path</strong>: A member requested guidance on a path into <strong>red teaming</strong>.\n<ul>\n<li>Another member provided a <a href=\""https://discord.com/channels/1105891499641684019/1432845259825741824\"">link</a> guaranteeing evolution into a <em>Level 9 official Red Team Pro</em>.</li>\n</ul>\n</li>\n<li><strong>Uncensored Coder on Deck</strong>: A member inquired about a better <strong>uncensored coder</strong> than <em>qwen 2.5 32b / huihui/qwen2.5 -abliterate 72b</em>.\n<ul>\n<li>Another member responded with a simple question: <em>You new?</em></li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=\""https://discord.com/channels/1179035537009545276/1179035537529643040/1466166498329628837\"">general</a></strong> (435 messages🔥🔥🔥):</h3>\n<blockquote>\n<p><code>GLM 4.7 performance, LongCat model, Model Quantization and TTS Models, Hardware Trends &#x26; GPU Availability, AI Moderation Tools</code></p>\n</blockquote>\n<ul>\n<li><strong>GLM 4.7 struggles with speed and CUDA compilation</strong>: Members discussed performance issues with <strong>GLM 4.7 Flash</strong> on NVIDIA Jetson, with one user initially reporting only <strong>3 tokens per second (tps)</strong>, but later discovering they hadn't compiled with <strong>CUDA support</strong>, resulting in poor CPU-bound performance.\n<ul>\n<li>After ensuring proper CUDA compilation, performance improved, but discrepancies remained, as one user experienced slowdowns after opening the model in <strong>OpenCode</strong>, whereas another suggested using <code>-kvu</code> and <code>-fa on</code> flags to potentially reach <strong>70-80 t/s</strong> on a higher-end GPU.</li>\n</ul>\n</li>\n<li><strong>LongCat Model hits HuggingFace</strong>: The community discussed the <a href=\""https://huggingface.co/meituan-longcat/LongCat-Flash-Lite\"">LongCat model</a>, a new <strong>n-gram model</strong> from Meituan, with one member pointing out its presence on <strong>Hugging Face</strong> and another joking about the trend of models including <em>Flash</em> in their names.\n<ul>\n<li>One member posted a <a href=\""https://tenor.com/view/flash-lampo-speed-gif-18173027\"">flash GIF</a> along with the comment, <em>next model Flash-Flash-1b</em>.</li>\n</ul>\n</li>\n<li><strong>AMD's mi308 competes with NVidia</strong>: Members debated the merits of AMD's <strong>Radeon Instinct MI308X</strong>, noting its impressive specs (<strong>192GB of RAM</strong> and comparable performance) but also highlighted NVIDIA's advantage in compatibility and features like <strong>NVFP4</strong>.\n<ul>\n<li>A member shared a <a href=\""https://www.techpowerup.com/gpu-specs/radeon-instinct-mi308x.c4295\"">link to the MI308X specs</a> and mused about acquiring two for personal use in the future, envisioning <strong>384GB</strong> of fast compute with reasonable power consumption.</li>\n</ul>\n</li>\n<li><strong>Quantization Considerations for TTS Models</strong>: Users inquired about the impact of <strong>quantization</strong> on <strong>TTS models</strong>, questioning whether issues similar to those seen with vision projectors might arise.\n<ul>\n<li>Experts suggested that <strong>TTS models</strong> generally handle <strong>quantization</strong> well, with some recommending specific models like <strong>Qwen3-TTS</strong> and <strong>Kokoro</strong>, and others cautioning that voice cloning is just a <em>gimmick</em>.</li>\n</ul>\n</li>\n<li><strong>AI steps up for discord moderation</strong>: A member sought advice on using AI for Discord moderation, citing the limitations of regex in combating spam and bypasses.\n<ul>\n<li>They considered using a small local AI to understand the Polish language and sentence structure for moderation purposes, while others suggested alternative methods for managing bots and spam.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=\""https://discord.com/channels/1179035537009545276/1179039724355211325/1466183283053297757\"">introduce-yourself</a></strong> (3 messages):</h3>\n<blockquote>\n<p><code>Introduction, ML Engineer, Local LLMs, Document Processing, Alpaca</code></p>\n</blockquote>\n<ul>\n<li><strong>Jack Joins the Community!</strong>: Jack, an <strong>ML Engineer</strong> from Texas specializing in <strong>document processing</strong>, introduces himself to the Unsloth community.\n<ul>\n<li>He expresses interest in <strong>local LLMs</strong>, tracing back to the <strong>Alpaca</strong> model.</li>\n</ul>\n</li>\n<li><strong>Document Processing Expertise</strong>: Jack's primary work involves <strong>document processing</strong>, a field distinct from LLMs.\n<ul>\n<li>His interest in <strong>local LLMs</strong> started with the <strong>Alpaca</strong> model, indicating a foundational understanding of the field.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=\""https://discord.com/channels/1179035537009545276/1179039861576056922/1466161229897928998\"">off-topic</a></strong> (649 messages🔥🔥🔥):</h3>\n<blockquote>\n<p><code>GPU hours wasted, GGUFs unsafe, 3b llama holds context, LLMs hallucinations, Model working</code></p>\n</blockquote>\n<ul>\n<li><strong>Engineers Lament Dependency Mazes and GPU Cost</strong>: Engineers commiserate about dependency mazes and wasted GPU hours, hoping they are <em>not alone</em> in facing these challenges and finding solace in community.\n<ul>\n<li>A user humorously remarks about their models <em>made the creepy assumption that it was trained on my voice</em> and that <em>my hubris grows daily</em>.</li>\n</ul>\n</li>\n<li><strong>Concerns about GGUFs Safety Surface</strong>: A member inquired about resources discussing the potential unsafety of <strong>GGUFs</strong>, particularly if a malicious actor got involved.\n<ul>\n<li>One member noted he <em>wouldn't dare speak</em> if he felt the crushing weight of the sloths while training.</li>\n</ul>\n</li>\n<li><strong>New Music Gen Drops</strong>: A user announced new <strong>music generation tools</strong> with <strong>48 kHz</strong> will be dropping soon, emphasizing trainability and prompting preparations for chime, water, and fire sounds.\n<ul>\n<li>This same user said: <em>I need SFX, not music</em>.</li>\n</ul>\n</li>\n<li><strong>Microsoft Announces Maia 200 AI Accelerator</strong>: Microsoft announced the <strong>Maia 200 AI Accelerator</strong>, built for inference, featuring <strong>216GB</strong> memory and <strong>10k TFLOPS</strong> in FP4 performance (<a href=\""https://blogs.microsoft.com/blog/2026/01/26/maia-200-the-ai-accelerator-built-for-inference/\"">Microsoft Blog</a>).\n<ul>\n<li>Discussions ensued regarding the chip's manufacturing by <strong>TSMC</strong> and comparisons to <strong>NVIDIA's Vera Rubin</strong> architecture, with some expressing concerns about reliance on Chinese hardware and the potential impact on consumers.</li>\n</ul>\n</li>\n<li><strong>Boatbomber attempts Pretraining Run</strong>: User boatbomber is <em>starting over</em> to conduct a pretraining run teaching the model cuneiform to improve output coherence.\n<ul>\n<li>This process is estimated to take <em>another 150 hours</em> to improve domain knowledge.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=\""https://discord.com/channels/1179035537009545276/1179777624986357780/1466195345158705252\"">help</a></strong> (75 messages🔥🔥):</h3>\n<blockquote>\n<p><code>Windows training, Multi-GPU training with Unsloth on Modal, Catastrophic forgetting mitigation, Best models to finetune, DGXSpark RuntimeError</code></p>\n</blockquote>\n<ul>\n<li><strong>Windows training hurdles squashed with WSL2</strong>: To train a model on Windows, a member suggested using <strong>WSL2</strong> and <strong>VSCode</strong> for a clean setup, with instructions available in the help channel by searching for <em>WSL</em>.\n<ul>\n<li>The member also clarified that, if training with many json files, setting up WSL2 with VSCode will make the training procedure easier.</li>\n</ul>\n</li>\n<li><strong>Unsloth Multi-GPU Training Glitches on Modal</strong>: A user encountered a <em>ValueError</em> when training a <strong>Qwen3</strong> model on Modal with multiple GPUs, related to the <code>device_map</code> setting in <strong>Unsloth</strong>.\n<ul>\n<li>They were advised to consult specific versions of <em>unsloth</em> and <em>unsloth_zoo</em> for multi-GPU support, but also acknowledged that <strong>Multi-GPU finetuning is still experimental</strong>.</li>\n</ul>\n</li>\n<li><strong>Catastrophic Forgetting Fixes</strong>: When a finetuned model forgets previous knowledge, a member suggested mitigating <em>catastrophic forgetting</em> by lowering <strong>LoRA rank</strong>, <strong>LR</strong>, reducing <strong>steps/epochs</strong>, and mixing in more general data.\n<ul>\n<li>They also suggested <em>targeting less layers</em> when finetuning, as well as <a href=\""https://unsloth.ai/docs/get-started/fine-tuning-llms-guide\"">reducing steps/epochs and mixing in more general data</a>.</li>\n</ul>\n</li>\n<li><strong>DGXSpark Nvidia-CUDA Nightmare</strong>: Users encountered a <code>RuntimeError</code> related to device compatibility when using the <strong>DGXSpark</strong> container, potentially due to issues with <strong>Nvidia's custom CUDA</strong>.\n<ul>\n<li>The suggested fix involved <em>restarting the kernel</em>, <em>restarting the container</em>, or <em>resetting the GPU</em>, with the last option being the most reliable.</li>\n</ul>\n</li>\n<li><strong>Humans Debate Best Uncensored Coder Models</strong>: When a user asked about uncensored coder models, it was said that <strong>glm 4.7</strong> is better than <strong>qwen coder</strong> below 32b and its <em>pretty good</em> in my experience with spitting out good presets for every language I mess with\n<ul>\n<li>They clarified that <strong>Qwen Coder</strong> is better at reasoning with code, but <strong>GLM4.7</strong> knows <em>alot more general code, which is what an llm is best at anyway</em></li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=\""https://discord.com/channels/1179035537009545276/1179779344894263297/1466516150950301814\"">showcase</a></strong> (2 messages):</h3>\n<blockquote>\n<p><code>GPU Training Speeds, Sequence Length Optimization, RTX 5090 Performance</code></p>\n</blockquote>\n<ul>\n<li><strong>RTX 5090 blazing-fast training speeds</strong>: The RTX <strong>5090</strong> can achieve up to <strong>18k tokens per second</strong> in training with Unsloth, but <strong>12-15k tokens per second</strong> is a safe bet with <strong>&#x3C;4096 seq_len</strong>.\n<ul>\n<li>The speed depends on the setup, especially the balance between <strong>batch size</strong> and <strong>seq_len</strong>.</li>\n</ul>\n</li>\n<li><strong>Token example affecting training time</strong>: The initial training phase involved <strong>&#x3C;768 token examples</strong>, influencing the overall training duration.\n<ul>\n<li>Performance can vary with model size and specific configurations.</li>\n</ul>\n</li>\n<li><strong>Seq_len considerations with training</strong>: Optimal training speed depends on balancing <strong>batch size</strong> and <strong>seq_len</strong> and the <strong>RTX 5090</strong> allows up to <strong>18k tokens per second</strong>.\n<ul>\n<li>Speeds of <strong>12-15k tokens per second</strong> are achievable with <strong>&#x3C;4096 seq_len</strong>, varying based on model size.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=\""https://discord.com/channels/1179035537009545276/1257011997250424842/1466203537078747188\"">research</a></strong> (97 messages🔥🔥):</h3>\n<blockquote>\n<p><code>DeepSeek mHC residual preservation, RL researchers rediscover context distillation, MiniMaxAI role-play-bench dataset, Recursive Language Models (RLM)</code></p>\n</blockquote>\n<ul>\n<li><strong>DeepSeek's mHC and Context Distillation</strong>: Members discussed how <a href=\""https://arxiv.org/abs/2209.15189\"">context distillation</a> might relate to <strong>DeepSeek's mHC residual preservation</strong>, noting similarities and differences in their approaches.\n<ul>\n<li>One member expressed surprise at the relatively small performance boost (1-2 points) from context distillation, while another noted that the application of the technique was novel.</li>\n</ul>\n</li>\n<li><strong>MiniMaxAI releases first RP bench</strong>: A user shared a <a href=\""https://huggingface.co/datasets/MiniMaxAI/role-play-bench\"">link</a> to what they claimed was the <strong>first role-play benchmark dataset</strong>, created by <strong>MiniMaxAI</strong>.\n<ul>\n<li>Others pointed out that there have been numerous <strong>Chinese RP benches</strong> with superior methodologies, notably <strong>Ping Pong Bench</strong> for human preference and <strong>COSER</strong> for roleplay accuracy.</li>\n</ul>\n</li>\n<li><strong>RLM is just Recursive Tool Calling</strong>: A member criticized the term \""<strong>Recursive Language Models</strong>\"" (<strong>RLM</strong>), suggesting it misleadingly implies more than just a <strong>tool-calling loop</strong>.\n<ul>\n<li>In response, one member argued that <strong>RLMs</strong> involve models recursively controlling their environments, which is more than <em>just recursive tool calling</em>, and another suggested the alternative names <strong>RReplagents</strong> or <strong>Recursive Repl Agents</strong>.</li>\n</ul>\n</li>\n<li><strong>Natively Recursive Language Model (RLM) at Small Scale</strong>: A user shared <a href=\""https://xcancel.com/a1zhang/status/2016923294461476873?s=46&#x26;t=jDrfS5vZD4MFwckU5E8f5Q\"">Alex L Zhang's tweet</a> announcing <strong>RLM-Qwen3-8B</strong>, the first natively recursive language model at a small scale.\n<ul>\n<li>It was post-trained on only <strong>1,000 trajectories</strong>, the model shows significant performance improvements over both the base <strong>Qwen3-8B</strong> and scaffolded <strong>RLM versions</strong>, particularly in long-context tasks.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3>**LMArena ▷ #[general](https...</h3>\n"",""content:encodedSnippet"":""xAI cements its position as a frontier lab.\nAI News for 1/28/2026-1/29/2026. We checked 12 subreddits, 544 Twitters and 24 Discords (253 channels, and 7278 messages) for you. Estimated reading time saved (at 200wpm): 605 minutes. Our new website is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on @smol_ai!\nIt looks like OpenAI (fundraising at around ~800b), Anthropic (worth $350b) and now SpaceX + xAI ($1100B? - folllowing their $20B Series E 3 weeks ago) are in a dead heat racing to IPO by year end. Google made an EXTREMELY strong play today launching Genie 3 (previously reported) to Ultra subscribers, and though technically impressive,, today’s headline story rightfully belongs to Grok, who now have the SOTA Image/Video Generation and Editing model released in API that you can use today.\nArtificial Analysis’ rankings says it all:\n\nThere’s not much else to say here apart from look at the list of small video model labs and wonder which of them just got bitter lessoned…\n\nAI Twitter Recap\nWorld Models & Interactive Simulation: Google DeepMind’s Project Genie (Genie 3) vs. Open-Source “World Simulators”\nProject Genie rollout (Genie 3 + Nano Banana Pro + Gemini): Google/DeepMind launched Project Genie, a prototype that lets users create and explore interactive, real-time generated worlds from text or image prompts, with remixing and a gallery. Availability is currently gated to Google AI Ultra subscribers in the U.S. (18+), and the product is explicit about prototype limitations (e.g., ~60s generation limits, control latency, imperfect physics adherence) (DeepMind announcement, how it works, rollout details, Demis, Sundar, Google thread, Google limitations). Early-access testers highlight promptability, character/world customization, and “remixing” as key UX hooks (venturetwins, Josh Woodward demo thread).\nOpen-source push: LingBot-World: A parallel thread frames world models as distinct from “video dreamers,” arguing for interactivity, object permanence, and causal consistency. LingBot-World is repeatedly described as an open-source real-time interactive world model built on Wan2.2 with <1s latency at 16 FPS and minute-level coherence (claims include VBench improvements and landmark persistence after long occlusion) (paper-summary thread, HuggingPapers mention, reaction clip). The meta-narrative: proprietary systems (Genie) are shipping consumer prototypes while open systems race to close capability gaps on coherence + control.\nVideo Generation & Creative Tooling: xAI Grok Imagine, Runway Gen-4.5, and fal’s “Day-0” Platforms\nxAI Grok Imagine (video + audio) lands near/at the top of leaderboards: Multiple sources report Grok Imagine’s strong debut in video rankings and emphasize native audio, 15s duration, and aggressive pricing ($4.20/min including audio) relative to Veo/Sora (Arena launch ranking, Artificial Analysis #1 claim + pricing context, follow-up #1 I2V leaderboard, xAI team announcement, Elon). fal positioned itself as day-0 platform partner with API endpoints for text-to-image, editing, text-to-video, image-to-video, and video editing (fal partnership, fal links tweet).\nRunway Gen-4.5 shifts toward “animation engine” workflows: Creators describe Gen-4.5 as increasingly controllable for animation-style work (c_valenzuelab). Runway shipped Motion Sketch (annotate camera/motion on a start frame) and Character Swap as built-in apps—more evidence that vendors are packaging controllability primitives rather than only pushing base quality (feature thread). Runway also markets “photo → story clip” flows as a mainstream onramp (Runway example).\n3D generation joins the same API distribution layer: fal also added Hunyuan 3D 3.1 Pro/Rapid (text/image-to-3D, topology/part generation), showing the same “model-as-a-service + workflow endpoints” pattern spreading from image/video into 3D pipelines (fal drop).\nOpen Models & Benchmarks: Kimi K2.5 momentum, Qwen3-ASR release, and Trinity Large architecture details\nKimi K2.5 as the “#1 open model” across multiple eval surfaces: Moonshot promoted K2.5’s rank on VoxelBench (Moonshot) and later Kimi updates focus on productization: Kimi Code now powered by K2.5, switching from request limits to token-based billing, plus a limited-time 3× quota/no throttling event (Kimi Code billing update, billing rationale). Arena messaging amplifies K2.5 as a leading open model with forthcoming Code Arena scores (Arena deep dive, Code Arena prompt); Arena also claims Kimi K2.5 Thinking as #1 open model in Vision Arena and the only open model in the top 15 (Vision Arena claim). Commentary frames K2.5 as “V3-generation architecture pushed with more continued training,” with next-gen competition expected from K3/GLM-5 etc. (teortaxes).\nAlibaba Qwen3-ASR: production-grade open speech stack with vLLM day-0 support: Qwen released Qwen3-ASR + Qwen3-ForcedAligner emphasizing messy real-world audio, 52 languages/dialects, long audio (up to 20 minutes/pass), and timestamps; models are Apache 2.0 and include an open inference/finetuning stack. vLLM immediately announced day-0 support and performance notes (e.g., “2000× throughput on 0.6B” in their tweet) (Qwen release, ForcedAligner, vLLM support, Adina Yakup summary, native streaming claim, Qwen thanks vLLM). Net: open-source speech is increasingly “full-stack,” not just weights.\nArcee AI Trinity Large (400B MoE) enters the architecture discourse: Multiple threads summarize Trinity Large as 400B MoE with ~13B active, tuned for throughput via sparse expert selection, and featuring a grab bag of modern stability/throughput techniques (router tricks, load balancing, attention patterns, normalization variants). Sebastian Raschka’s architecture recap is the most concrete single reference point (rasbt); additional MoE/router stability notes appear in a separate technical summary (cwolferesearch). Arcee notes multiple variants trending on Hugging Face (arcee_ai).\nAgents in Practice: “Agentic Engineering,” Multi-Agent Coordination, and Enterprise Sandboxes\nFrom vibe coding to agentic engineering: A high-engagement meme-like anchor tweet argues for “Agentic Engineering > Vibe Coding” and frames professionalism around repeatable workflows rather than vibes (bekacru). Several threads reinforce the same theme operationally: context prep, evaluations, and sandboxing as the hard parts.\nPrimer: repo instructions + lightweight evals + PR automation: Primer proposes a workflow for “AI-enabling” repos: agentic repo introspection → generate an instruction file → run a with/without eval harness → scale via batch PRs across org repos (Primer launch, local run, eval framework, org scaling).\nAgent sandboxes + traceability as infra primitives: Multiple tweets point to “agent sandboxes” (isolated execution environments) as an emerging January trend (dejavucoder). Cursor proposed an open standard to trace agent conversations to generated code, explicitly positioning it as interoperable across agents/interfaces (Cursor). This pairs with broader ecosystem pressure: agents need auditability and reliable grounding when they can take actions.\nMulti-agent coordination beats “bigger brain” framing: A popular summary claims a system that uses a controller trained by RL to route between large/small models can beat a single large agent on HLE with lower cost/latency—reinforcing that orchestration policies are becoming first-class artifacts (LiorOnAI). In the same direction, an Amazon “Insight Agents” paper summary argues for pragmatic manager-worker designs with lightweight OOD detection and routing (autoencoder + fine-tuned BERT) instead of LLM-only classifiers for latency/precision reasons (omarsar0).\nKimi’s “Agent Swarm” philosophy: A long-form repost from ZhihuFrontier describes K2.5’s agent mode as a response to “text-only helpfulness” and tool-call hallucinations, emphasizing planning→execution bridging, dynamic tool-based context, and multi-viewpoint planning via swarms (ZhihuFrontier).\nMoltbot/Clawdbot safety trilemma: Community discussion frames “Useful vs Autonomous vs Safe” as a tri-constraint until prompt injection is solved (fabianstelzer). Another take argues capability (trust) bottlenecks dominate: users won’t grant high-stakes autonomy (e.g., finance) until agents are reliably competent (Yuchenj_UW).\nModel UX, DevTools, and Serving: Gemini Agentic Vision, OpenAI’s in-house data agent, vLLM fixes, and local LLM apps\nGemini 3 Flash “Agentic Vision”: Google positions Agentic Vision as a structured image-analysis pipeline: planning steps, zooming, annotating, and optionally running Python for plotting—essentially turning “vision” into an agentic workflow rather than a single forward pass (GeminiApp intro, capabilities, rollout note).\nOpenAI’s in-house data agent at massive scale: OpenAI described an internal “AI data agent” reasoning over 600+ PB and 70k datasets, using Codex-powered table knowledge and careful context management (OpenAIDevs). This is a rare concrete peek at “deep research/data agent” architecture constraints: retrieval + schema/table priors + org context.\nServing bugs are still real (vLLM + stateful models): AI21 shared a debugging story where scheduler token allocation caused misclassification between prefill vs decode, now fixed in vLLM v0.14.0—a reminder that infrastructure correctness matters, especially for stateful architectures like Mamba (AI21Labs thread).\nLocal LLM UX continues to improve: Georgi Gerganov shipped LlamaBarn, a tiny macOS menu bar app built on llama.cpp to run local models (ggerganov). Separate comments suggest agentic coding performance may improve by disabling “thinking” modes for specific models (GLM-4.7-Flash) via llama.cpp templates (ggerganov config note).\nTop tweets (by engagement)\nGrok Imagine hype & distribution: @elonmusk, @fal, @ArtificialAnlys\nDeepMind/Google world models: @GoogleDeepMind, @demishassabis, @sundarpichai\nAI4Science: @demishassabis on AlphaGenome\nSpeech open-source release: @Alibaba_Qwen Qwen3-ASR\nAgents + developer workflow: @bekacru “Agentic Engineering > Vibe Coding”, @cursor_ai agent-trace.dev\nAnthropic workplace study: @AnthropicAI AI-assisted coding and mastery\nAI Reddit Recap\n/r/LocalLlama + /r/localLLM Recap\n1. Kimi K2.5 Model Discussions and Releases\nAMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model (Activity: 686): Kimi is the research lab behind the open-source Kimi K2.5 model, engaging in an AMA to discuss their work. The discussion highlights a focus on large-scale models, with inquiries about the development of smaller models like 8B, 32B, and 70B for better intelligence density. There is also interest in smaller Mixture of Experts (MoE) models, such as ~100B total with ~A3B active, optimized for local or prosumer use. The team is questioned on their stance regarding the notion that Scaling Laws have hit a wall, a topic of current debate in AI research. Commenters express a desire for smaller, more efficient models, suggesting that these could offer better performance for specific use cases. The debate on scaling laws reflects a broader concern in the AI community about the limits of current model scaling strategies.\nThe discussion around model sizes highlights a preference for smaller models like 8B, 32B, and 70B due to their 'intelligence density.' These sizes are considered optimal for balancing performance and resource efficiency, suggesting a demand for models that can operate effectively on limited hardware while still providing robust capabilities.\nThe inquiry into smaller Mixture of Experts (MoE) models, such as a ~100B total with ~A3B active, indicates interest in models optimized for local or prosumer use. This reflects a trend towards developing models that are not only powerful but also accessible for individual users or small enterprises, emphasizing the need for efficient resource utilization without sacrificing performance.\nThe challenge of maintaining non-coding abilities like creative writing and emotional intelligence in models like Kimi 2.5 is significant, especially as coding benchmarks become more prominent. The team is tasked with ensuring these softer skills do not regress, which involves balancing the training focus between technical and creative capabilities to meet diverse user needs.\nRun Kimi K2.5 Locally (Activity: 553): The image provides a guide for running the Kimi-K2.5 model locally, emphasizing its state-of-the-art (SOTA) performance in vision, coding, agentic, and chat tasks. The model, which is a 1 trillion parameter hybrid reasoning model, requires 600GB of disk space, but the quantized Unsloth Dynamic 1.8-bit version reduces this requirement to 240GB, a 60% reduction. The guide includes instructions for using llama.cpp to load models and demonstrates generating HTML code for a simple game. The model is available on Hugging Face and further documentation can be found on Unsloth's official site. Commenters discuss the feasibility of running the model on high-end hardware, with one user questioning its performance on a Strix Halo setup and another highlighting the substantial VRAM requirements, suggesting that only a few users can realistically run it locally.\nDaniel_H212 is inquiring about the performance of the Kimi K2.5 model on the Strix Halo hardware, specifically asking for the token generation speed in seconds per token. This suggests a focus on benchmarking the model's efficiency on high-end hardware setups.\nMarksta provides feedback on the quantized version of the Kimi K2.5 model, specifically the Q2_K_XL variant. They note that the model maintains high coherence and adheres strictly to prompts, which is characteristic of Kimi-K2's style. However, they also mention that while the model's creative capabilities have improved, it still struggles with execution in creative scenarios, often delivering logical but poorly written responses.\nMikeRoz questions the utility of higher quantization levels like Q5 and Q6 (e.g., UD-Q5_K_XL, Q6_K) when most experts prefer int4 quantization. This highlights a debate on the trade-offs between model size, performance, and precision in quantization strategies.\nKimi K2.5 is the best open model for coding (Activity: 1119): The image highlights Kimi K2.5 as the leading open model for coding on the LMARENA.AI leaderboard, ranked #7 overall. This model is noted for its superior performance in coding tasks compared to other open models. The leaderboard provides a comparative analysis of various AI models, showcasing their ranks, scores, and confidence intervals, emphasizing Kimi K2.5's achievements in the coding domain. One commenter compared Kimi K2.5's performance to other models, noting it is on par with Sonnet 4.5 in accuracy but not as advanced as Opus in agentic function. Another comment criticized LMArena for not reflecting a model's multi-turn or long context capabilities.\nA user compared Kimi K2.5 to other models, noting that it performs on par with Sonnet 4.5 in terms of accuracy for React projects, but not at the level of Opus in terms of agentic function. They also mentioned that Kimi 2.5 surpasses GLM 4.7, which was their previous choice, and expressed curiosity about the upcoming GLM-5 from z.ai.\nAnother commenter criticized LMArena, stating that it fails to provide insights into a model's multi-turn, long context, or agentic capabilities, implying that such benchmarks are insufficient for evaluating comprehensive model performance.\nA user highlighted the cost-effectiveness of Kimi K2.5, stating it feels as competent as Opus 4.5 while being significantly cheaper, approximately 1/5th the cost, and even less expensive than Haiku. This suggests a strong performance-to-cost ratio for Kimi K2.5.\nFinally We have the best agentic AI at home (Activity: 464): The image is a performance comparison chart of various AI models, including Kimi K2.5, GPT-5.2 (xhigh), Claude Opus 4.5, and Gemini 3 Pro. Kimi K2.5 is highlighted as the top-performing model across multiple categories such as agents, coding, image, and video tasks, indicating its superior capabilities in multimodal applications. The post suggests excitement about integrating this model with a 'clawdbot', hinting at potential applications in robotics or automation. A comment humorously suggests that hosting the Kimi 2.5 1T+ model at home implies having a large home, indicating the model's likely high computational requirements. Another comment sarcastically mentions handling it with a 16GB VRAM card, implying skepticism about the feasibility of running such a model on typical consumer hardware.\n2. Open Source Model Innovations\nLingBot-World outperforms Genie 3 in dynamic simulation and is fully Open Source (Activity: 230): The open-source framework LingBot-World surpasses the proprietary Genie 3 in dynamic simulation capabilities, achieving 16 FPS and maintaining object consistency for 60 seconds outside the field of view. This model, available on Hugging Face, offers enhanced handling of complex physics and scene transitions, challenging the monopoly of proprietary systems by providing full access to its code and model weights. Commenters question the hardware requirements for running LingBot-World and express skepticism about the comparison with Genie 3, suggesting a lack of empirical evidence or direct access to Genie 3 for a fair comparison.\nA user questioned the hardware requirements for running LingBot-World, highlighting the importance of specifying computational needs for practical implementation. This is crucial for users to understand the feasibility of deploying the model in various environments.\nAnother commenter raised concerns about the lack of a direct comparison with Genie 3, suggesting that without empirical data or benchmarks, claims of LingBot-World's superiority might be unsubstantiated. This points to the need for transparent and rigorous benchmarking to validate performance claims.\nA suggestion was made to integrate a smaller version of LingBot-World into a global illumination stack, indicating potential applications in graphics and rendering. This could leverage the model's capabilities in dynamic simulation to enhance visual computing tasks.\nAPI pricing is in freefall. What's the actual case for running local now beyond privacy? (Activity: 1053): The post discusses the rapidly decreasing costs of API access for AI models, with examples like K2.5 offering prices at 10% of Opus and Deepseek being nearly free. Gemini also provides a substantial free tier. This trend is contrasted with the challenges of running large models locally, such as the need for expensive GPUs or dealing with quantization tradeoffs, which result in slow processing speeds (15 tok/s) on consumer hardware. The author questions the viability of local setups given these API pricing trends, noting that while privacy and latency control are valid reasons, the cost-effectiveness of local setups is diminishing. Commenters highlight concerns about the sustainability of low API prices, suggesting they may rise once market dominance is achieved, similar to past trends in other industries. Others emphasize the importance of offline capabilities and the ability to audit and trust local models, which ensures consistent behavior without unexpected changes from vendors.\nMinimum-Vanilla949 highlights the importance of offline capabilities for those who travel frequently, emphasizing the risk of API companies altering terms of service or increasing prices once they dominate the market. This underscores the value of local models for ensuring consistent access and cost control.\n05032-MendicantBias discusses the unsustainable nature of current API pricing, which is often subsidized by venture capital. They argue that once a monopoly is achieved, prices will likely increase, making local setups and open-source tools a strategic defense against such business models.\nIactaAleaEst2021 points out the importance of repeatability and trust in using local models. By downloading and auditing a model, users can ensure consistent behavior, unlike APIs where vendors might change model behavior over time, potentially reducing its utility for specific tasks.\n3. Trends in AI Agent Frameworks\nGitHub trending this week: half the repos are agent frameworks. 90% will be dead in 1 week. (Activity: 538): The image highlights a trend on GitHub where many of the trending repositories are related to AI agent frameworks, suggesting a surge in interest in these tools. However, the post's title and comments express skepticism about the sustainability of this trend, comparing it to the rapid rise and fall of JavaScript frameworks. The repositories are mostly written in Python and include a mix of agent frameworks, RAG tooling, and model-related projects like NanoGPT and Grok. The discussion reflects a concern that many of these projects may not maintain their popularity or relevance over time. One comment challenges the claim that half of the trending repositories are agent frameworks, noting that only one is an agent framework by Microsoft, while others are related to RAG tooling and model development. Another comment appreciates the utility of certain projects, like IPTV, for educational purposes.\ngscjj points out that the claim about 'half the repos being agent frameworks' is inaccurate. They note that the list includes a variety of projects such as Microsoft's agent framework, RAG tooling, and models like NanoGPT and Grok, as well as a model CLI for code named Kimi and a browser API. This suggests a diverse range of trending repositories rather than a dominance of agent frameworks.\nMistral CEO Arthur Mensch: “If you treat intelligence as electricity, then you just want to make sure that your access to intelligence cannot be throttled.” (Activity: 357): Arthur Mensch, CEO of Mistral, advocates for open-weight models, likening intelligence to electricity, emphasizing the importance of unrestricted access to AI capabilities. This approach supports the deployment of models on local devices, reducing costs as models are quantized for lower compute environments, contrasting with closed models that are often large and monetized through paywalls. Mistral aims to balance corporate interests with open access, potentially leading to significant breakthroughs in AI deployment. Commenters appreciate Mistral's approach to open models, noting the potential for reduced costs and increased accessibility. There is a consensus that open models could democratize AI usage, contrasting with the restrictive nature of closed models.\nRoyalCities highlights the cost dynamics of model deployment, noting that open models, especially when quantized, reduce costs as they can be run on local devices. This contrasts with closed models that are often large and require significant infrastructure, thus being monetized through paywalls. This reflects a broader industry trend where open models aim to democratize access by lowering hardware requirements.\nHugoCortell points out the hardware bottleneck in deploying open models effectively. While open-source models can rival closed-source ones in performance, the lack of affordable, high-performance hardware limits their accessibility. This is compounded by large companies making high-quality local hardware increasingly expensive, suggesting a need for a company capable of producing and distributing its own hardware to truly democratize AI access.\ntarruda expresses anticipation for the next open Mistral model, specifically the \""8x22\"". This indicates a community interest in the technical specifications and potential performance improvements of upcoming models, reflecting the importance of open model development in advancing AI capabilities.\nLess Technical AI Subreddit Recap\n/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo\n1. OpenAI and AGI Investments\nNearly half of the Mag 7 are reportedly betting big on OpenAI’s path to AGI (Activity: 1153): NVIDIA, Microsoft, and Amazon are reportedly in discussions to invest a combined total of up to $60 billion into OpenAI, with SoftBank considering an additional $30 billion. This potential investment could value OpenAI at approximately $730 billion pre-money, aligning with recent valuation discussions in the $750 billion to $850 billion+ range. This would mark one of the largest private capital raises in history, highlighting the significant financial commitment from major tech companies towards the development of artificial general intelligence (AGI). Commenters note the strategic alignment of these investments, with one pointing out that companies like Microsoft and NVIDIA are unlikely to invest in competitors like Google. Another comment reflects on the evolving landscape of large language models (LLMs) and the shifting focus of tech giants.\nCoolStructure6012 highlights the strategic alignment between Microsoft (MSFT) and NVIDIA (NVDA) with OpenAI, suggesting that their investments are logical given their competitive stance against Google. This reflects the broader industry trend where tech giants are aligning with AI leaders to bolster their AI capabilities and market positions.\ndrewc717 reflects on the evolution of AI models, noting a significant productivity boost with OpenAI's 4.1 Pro mode. However, they express a decline in their workflow efficiency after switching to Gemini, indicating that not all LLMs provide the same level of user experience or productivity, which is crucial for developers relying on these tools.\nEmbarrassedRing7806 questions the lack of attention on Anthropic despite its widespread use in coding through its Claude model, as opposed to OpenAI's Codex. This suggests a potential underestimation of Anthropic's impact in the AI coding space, where Claude might be offering competitive or superior capabilities.\n2. DeepMind's AlphaGenome Launch\nGoogle DeepMind launches AlphaGenome, an AI model that analyzes up to 1 million DNA bases to predict genomic regulation (Activity: 427): Google DeepMind has introduced AlphaGenome, a sequence model capable of analyzing up to 1 million DNA bases to predict genomic regulation, as detailed in Nature. The model excels in predicting genomic signals such as gene expression and chromatin structure, particularly in non-coding DNA, which is crucial for understanding disease-associated variants. AlphaGenome outperforms existing models on 25 of 26 benchmark tasks and is available for research use, with its model and weights accessible on GitHub. Commenters highlight the model's potential impact on genomics, with some humorously suggesting its significance in advancing scientific achievements akin to winning Nobel prizes.\n[R] AlphaGenome: DeepMind's unified DNA sequence model predicts regulatory variant effects across 11 modalities at single-bp resolution (Nature 2026) (Activity: 66): DeepMind's AlphaGenome introduces a unified DNA sequence model that predicts regulatory variant effects across 11 modalities at single-base-pair resolution. The model processes 1M base pairs of DNA to predict thousands of functional genomic tracks, matching or exceeding specialized models in 25 of 26 evaluations. It employs a U-Net backbone with CNN and transformer layers, trained on human and mouse genomes, and captures 99% of validated enhancer-gene pairs within a 1Mb context. Training on TPUv3 took 4 hours, with inference under 1 second on H100. The model demonstrates cross-modal variant interpretation, notably on the TAL1 oncogene in T-ALL. Nature, bioRxiv, DeepMind blog, GitHub. Some commenters view the model as an incremental improvement over existing sequence models, questioning the novelty despite its publication in Nature. Others express concerns about the implications of open-sourcing such powerful genomic tools, hinting at potential future applications like 'text to CRISPR' models.\nst8ic88 argues that while DeepMind's AlphaGenome model is notable for its ability to predict regulatory variant effects across 11 modalities at single-base pair resolution, it is seen as an incremental improvement over existing sequence models predicting genomic tracks. The comment suggests that the model's prominence is partly due to DeepMind's reputation and branding, particularly the use of 'Alpha' in its name, which may have contributed to its publication in Nature.\n--MCMC-- is interested in the differences between the AlphaGenome model's preprint and its final published version in Nature. The commenter had read the preprint and is curious about any changes made during the peer review process, which could include updates to the model's methodology, results, or interpretations.\nf0urtyfive raises concerns about the potential risks of open-sourcing powerful genomic models like AlphaGenome, speculating on future developments such as 'text to CRISPR' models. This comment highlights the ethical and safety considerations of making advanced genomic prediction tools widely accessible, which could lead to unintended applications or misuse.\n3. Claude's Cost Efficiency and Usage Strategies\nClaude Subscriptions are up to 36x cheaper than API (and why \""Max 5x\"" is the real sweet spot) (Activity: 665): A data analyst has reverse-engineered Claude's internal usage limits by analyzing unrounded floats in the web interface, revealing that subscriptions can be up to 36x cheaper than using the API, especially for coding tasks with agents like Claude Code. The analysis shows that the subscription model offers free cache reads, whereas the API charges 10% of the input cost for each read, making the subscription significantly more cost-effective for long sessions. The \""Max 5x\"" plan at $100/month is highlighted as the most optimized, offering a 6x higher session limit and 8.3x higher weekly limit than the Pro plan, contrary to the marketed \""5x\"" and \""20x\"" plans. The findings were derived using the Stern-Brocot tree to decode precise usage percentages into internal credit numbers. Full details and formulas are available here. Commenters express concern over Anthropic's lack of transparency and speculate that the company might change the limits once they realize users have reverse-engineered them. Some users are taking advantage of the current subscription benefits, anticipating potential changes.\nHikariWS raises a critical point about Anthropic's lack of transparency regarding their subscription limits, which could change unexpectedly, rendering current analyses obsolete. This unpredictability poses a risk for developers relying on these plans for cost-effective usage.\nIsaenkodmitry discusses the potential for Anthropic to close loopholes once they realize users are exploiting subscription plans for cheaper access compared to the API. This highlights a strategic risk for developers who are currently benefiting from these plans, suggesting they should maximize usage while it lasts.\nSnow30303 mentions using Claude code in VS Code for Flutter apps, noting that it consumes credits rapidly. This suggests a need for more efficient usage strategies or alternative solutions to manage costs effectively when integrating Claude into development workflows.\nWe reduced Claude API costs by 94.5% using a file tiering system (with proof) (Activity: 603): The post describes a file tiering system that reduces Claude API costs by 94.5% by categorizing files into HOT, WARM, and COLD tiers, thus minimizing the number of tokens processed per session. This system, implemented in a tool called cortex-tms, tags files based on their relevance and usage frequency, allowing only the most necessary files to be loaded by default. The approach has been validated through a case study on the author's project, showing a reduction from 66,834 to 3,647 tokens per session, significantly lowering costs from $0.11 to $0.01 per session with Claude Sonnet 4.5. The tool is open-source and available on GitHub. One commenter inquired about the manual process of tagging files and updating tags, suggesting the use of git history to automate file heat determination. Another user appreciated the approach due to their own struggles with managing API credits.\nIllustrious-Report96 suggests using git history to determine file 'heat', which involves analyzing the frequency and recency of changes to classify files as 'hot', 'warm', or 'cold'. This method leverages version control metadata to automate the classification process, potentially reducing manual tagging efforts.\nAccomplished_Buy9342 inquires about restricting access to 'WARM' and 'COLD' files, which implies a need for a mechanism to control agent access based on file tier. This could involve implementing access controls or modifying the agent's logic to prioritize 'HOT' files, ensuring efficient resource usage.\ndurable-racoon asks about the process of tagging files and updating these tags, highlighting the importance of an automated or semi-automated system to manage file tiering efficiently. This could involve scripts or tools that dynamically update file tags based on usage patterns or other criteria.\nAI Discord Recap\nA summary of Summaries of Summaries by Gemini 3.0 Pro Preview Nov-18\nTheme 1. Model Wars: Kimi’s Rise, Recursive Agents, and Geometric Architectures\nKimi K2.5 crushes the Vision Arena: The community reports Kimi K2.5 is dominating the leaderboards, claiming the #1 open model spot and ranking #6 overall on the Vision Arena leaderboard. Users note it outperforms Claude in specific vision tasks and now features a dedicated computer use model that handles phone screenshots (though it throws 403 errors on mobile uploads).\nRecursive Language Models trigger semantic debates: A heated discussion erupted over the term \""Recursive Language Models\"" (RLM), with critics arguing it simply rebrands tool-calling loops, while proponents point to the new RLM-Qwen3-8B as the first natively recursive model. This small-scale model, post-trained on just 1,000 trajectories, reportedly beats scaffolded RLM versions in long-context tasks.\nGeometric Convolution attempts to dethrone Attention: Researchers are experimenting with a baseline that replaces standard Multi-Head Attention with a geometric convolution approach, using embeddings as cell connections. Early debug prints show loss convergence capturing dialogue logic, positioning this as a potential alternative to heavy transformer compute.\nTheme 2. Hardware Hustle: Microsoft’s Silicon, Unsloth Speeds, and Apple’s Hidden Power\nMicrosoft aims at NVIDIA with Maia 200: Microsoft unveiled the Maia 200 AI Accelerator, an inference-focused chip boasting 216GB of memory and 10k TFLOPS in FP4 performance. Engineers debated the reliance on TSMC manufacturing and compared its architecture favorably against NVIDIA's Vera Rubin for large-scale inference workloads.\nRTX 5090 shreds training benchmarks: Unsloth users report the RTX 5090 achieves blistering training speeds of up to 18k tokens per second, though 12-15k t/s is safer with a sequence length under 4096. Optimal throughput requires carefully balancing batch size and sequence length to avoid memory bottlenecks during fine-tuning.\nApple’s ANE punches above its weight: A new discussion around this paper highlights that Apple's Neural Engine (ANE) delivers 3.8 TFlops on the M4-Pro, nearly matching the GPU's 4.7 TFlops for GEMM operations. The ANE prioritizes performance-per-watt, making it a surprisingly viable target for efficient local inference.\nTheme 3. Dev Tools & Standards: Cursor Pains, MCP Security, and Parallel Studio\nCursor’s \""Plan Mode\"" annoys the power users: The latest Cursor update introduced a plan mode that users are actively trying to disable or automate, citing wasted time and unnecessary inputs. Fresh installs of the IDE are reportedly the most unstable configuration, driving users to seek workarounds for the \""Plan Mode\"" friction.\nMCP gets a hardened Security Standard: Dani (cr0hn) drafted an open MCP Security Standard covering hardening, logging, and access control, intending to donate it to the Agentic AI Foundation. Simultaneously, the protocol is evolving with Namespaces being rejected in favor of Groups, detailed in the new Primitive Grouping SEP-2084.\nLM Studio 0.4 hides power tools behind Dev Mode: The release of LM Studio 0.4.0 tucks critical settings like sampling and hardware configs behind a Dev Mode toggle (Ctrl+Shift+R), while introducing parallel requests. Users can now load models across different GPUs to handle up to 4 parallel requests by default, though the software still relies on the older ROCm 6.4.1.\nTheme 4. Jailbreaks & Exploits: Keygens, \""Remember\"" Hacks, and Malware Classifiers\nGemini 3 Pro tricked into writing KeyGens: A user successfully prompted Gemini 3 Pro to reverse engineer software and generate a working keygen by pasting code directly from Ghidra. While some dismissed this as \""script kiddie\"" behavior, it highlights the model's susceptibility to context-based exploits when fed technical disassemblies.\n\""Remember:\"" command acts as behavior injection: Red teamers discovered that the Gemini command 'Remember:' instantly forces subsequent text into the model's saved memory, heavily influencing future behavior. This allows for persistent prompt injections that dictate turns one at a time, bypassing standard session resets.\nAdversarial Malware Classification struggles: Engineers are fighting to lower the False Positive Rate (FPR) in malware classification models using a dataset of 600K rows and 9,600 binary features. Despite using neural networks and explainable models like scikit-learn trees, reducing FPR below 9% remains a significant hurdle without sacrificing model interpretability.\nTheme 5. Real-World Agents: Kitchen Robots, World Models, and Bio-AI\nFigure.Ai’s Helix 02 conquers the kitchen: A video surfaced of Figure.Ai's Helix 02 robot autonomously performing complex kitchen tasks, which a user verified by feeding the video into Kimi for a 98% accurate analysis. This aligns with reports of Matic raising $60M to build a utility-focused consumer robot successor to the Roomba.\nGoogle releases \""Genie\"" World Model: Google launched Project Genie for AI Ultra subscribers, a general-purpose world model capable of generating interactive environments from text prompts. This release moves world models from research papers into a deployable product for simulating dynamic scenarios.\nAI decodes DNA and Alzheimer’s: Google AI launched AlphaGenome to predict the impact of DNA variants and mutations, while Goodfire AI announced new Alzheimer's biomarkers discovered via model interpretability. These advances signal a shift toward using transparent AI models to drive breakthroughs in digital biology.\nDiscord: High level Discord summaries\nBASI Jailbreaking Discord\nGemini 3 Pro Cracks Software KeyGen Style: A member reported using Gemini 3 Pro to create a working keygen from software by pasting code from Ghidra.\n\nSkeptical members referred to this as script kiddie behavior and suggested trying a reverse engineering CTF challenge.\nAI Gets Weaponized for Reverse Engineering: A member shared his work on weaponizing AI for mass reverse engineering, malware analysis, and jailbreak development.\n\nAnother member questioned this claim, suggesting the original poster may be more skilled at jailbreaking than malware creation.\nSonnet 4.5 Bests Opus with Kaelia Jailbreak: Members confirmed that Sonnet 4.5 jailbreaks work on Opus, sharing a Miss Kaelia jailbreak based on ENI Lime by Vichaps from this document.\n\nThe jailbreak may not be as effective as other models, depending on the prompting strategy used.\nGemini's 'Remember:' Command Triggers Behavior: A member explained that in Gemini, the command 'Remember:' automatically adds subsequent words to its saved info, influencing its behavior.\n\nEach turn is clearly dictated, one at a time, directly in the chat interface.\nNSFW Nano Banana Jailbreak arrives for Kimi 2.5: A member shared an NSFW jailbreak for Kimi 2.5, dubbed the nano banana jailbreak. The system prompt frames Kimi as an AI assistant from Moonshot AI, permitting NSFW content.\n\nThe narrative flow proceeds seamlessly without interruption.\nUnsloth AI (Daniel Han) Discord\nGLM 4.7 Slowdown Solved by CUDA: Users resolved slow speeds with GLM 4.7 Flash on NVIDIA Jetson by ensuring proper CUDA compilation, boosting performance from 3 tps to potentially 70-80 t/s with -kvu and -fa on flags.\n\nPerformance discrepancies were observed with OpenCode, with one user experiencing slowdowns after opening the model, while another noted that GLM 4.7 is a better uncensored coder model than qwen coder below 32b, but Qwen Coder excels at reasoning.\nLongCat Leaps onto HuggingFace!: Meituan's new n-gram model, the LongCat model, made its debut on Hugging Face, sparking jokes about the proliferation of Flash in model names.\n\nCommunity members speculated that next model Flash-Flash-1b while celebrating new releases.\nMicrosoft's Maia 200 Challenges NVIDIA: Microsoft unveiled the Maia 200 AI Accelerator, a chip built for inference, boasting 216GB memory and 10k TFLOPS in FP4 performance.\n\nThe community discussed the chip's manufacturing by TSMC and compared it to NVIDIA's Vera Rubin architecture, with some raising concerns about relying on Chinese hardware.\nModel Recursive Language Models (RLM) Redefined: Community members argued that the term \""Recursive Language Models\"" (RLM) is misleading, as it merely describes a tool-calling loop, although some maintained that RLMs do involve models recursively controlling their environments.\n\nOthers discussed the recently announced RLM-Qwen3-8B, the first natively recursive language model, noting its improvements over the base and scaffolded RLM versions.\nCatastrophic Forgetting Mitigation Methods: A member suggested mitigating catastrophic forgetting in fine-tuned models by lowering LoRA rank and LR, reducing steps/epochs, and mixing in more general data, as outlined in Unsloth's documentation.\n\nThey also recommended targeting less layers when finetuning and using WSL2 and VSCode for training.\nLMArena Discord\nArena Rebrand Sparks Debate: LMArena rebranded to Arena, drawing mixed reactions as some users found the name vague, while others welcomed the expansion beyond Language Models to include image and video generation, as announced in the official blogpost.\n\nOne user commented that \""the name 'Arena' is very vague, and at first glance could mean anything\"", in contrast to the easily identifiable 'LMArena'.\nCaptcha Conundrums Plague Users: Users reported getting trapped in endless reCAPTCHA loops on Arena, hindering site usability, with claims of failures even after solving them, some also reported waiting too long can give errors until page is refreshed.\n\nA user lamented that \""That Google CAPTCHA crap is completely out of control\"" and questioned why developers were focusing on restyling instead of fixing bugs.\nNano's Image Editing Capabilities Nosedive: Users observed a performance decline in Nano Banana, especially in image editing, reporting instances where it couldn't perform tasks correctly, while the same prompt worked in Gemini App.\n\nOne user simply stated, \""Nano 2 can’t even edit anything correctly anymore it seems like\"".\nKimi K2.5 Conquers Vision Arena: Kimi K2.5 is showing impressive scores on the expert leaderboard, surpassing Claude in specific tests, noted for its vision support and marked as \""vision\"" in direct chat mode.\n\nKimi-k2.5-thinking is now the #1 open model and ranks #6 overall in the Vision Arena leaderboard, making it the only open model in the Top 15.\nVideo Generation Viscosity vexes Viewers: Some users encountered a \""Hit video limit\"" message despite not generating a video, while others experienced lags with lengthy code and responses.\n\nUsers found they needed to use canary.lmarena.ai to enable video uploads, with one suggesting a side-by-side or direct chat interface for video generation.\nCursor Community Discord\nCursor Instability Plagues Fresh Installs: Users report that a fresh install of the latest Cursor version is the most unstable configuration.\n\nThe issue may be related to configuration files, or interaction with other configuration.\nClawdbot Interface Proclaimed 'Glorified Claude': Members are discussing the Clawdbot interface, accessible from Telegram, one described it as a glorified Claude code interface.\n\nThe implication is that Clawdbot provides a convenient but not necessarily groundbreaking way to interact with Claude for code-related tasks.\nUsers Plot to Deactivate Cursor's Plan Mode: Users are actively seeking methods to disable Cursor's new plan mode or automate its acceptance.\n\nThe goal is to streamline workflow and minimize unnecessary user input, expressing frustration that it wastes time.\nGemini Agentic Vision Approaches State-of-the-Art: Enthusiastic users are praising the capabilities of Gemini agentic vision, asserting it is getting near sota for vision after initial testing.\n\nHowever, one user reported a fully blacked-out cursor issue, hindering further evaluation and use.\nPrompt Engineering Expedites Image Processing: Members are exchanging techniques for refining prompts to enhance image analysis with Cursor.\n\nSuggestions include providing more context or utilizing the prompt Analyze the image for debugging purposes and for an LLM to see the layout clearly to improve processing accuracy and clarity.\nOpenRouter Discord\nArcee AI CTO Interview Premieres: Arcee AI's CTO, Lucas Atkins, is featured in a new interview, now available on YouTube.\n\nThe video showcases Lucas Atkins discussing Arcee AI and its latest developments.\nOpenRouter Users Await Refunds: Users are reporting delayed refunds, some dating back to January 3rd, with unresolved support tickets and demanding updates from the @OpenRouter team.\n\nThe delays have caused frustration, with users seeking a clear timeline for when they can expect their refunds to be processed.\nGROK Demands Nuclear Power: A user humorously suggested WE NEED MORE NUCLEAR POWER PLANTS FOR GROK.\n\nThe user jokingly added to TURN OFF SINGLE INCOME HOMES.\nSummergrok Arrives on xAI API: The Summergrok imagine video is now available on the xAI API.\n\nThis integration allows developers to incorporate Summergrok's capabilities into their projects via the xAI API.\nAPI Key Visibility Limited: A user encountered an issue with not being able to view their created API key.\n\nA fellow user clarified that the API key is displayed only once upon creation, advising users to save it immediately.\nLatent Space Discord\nCoffee Alternatives Brewing in LS: Members discussed alternatives to coffee, with green tea highlighted for its lower caffeine dose and the balancing effects of l-theanine.\n\nOne member uses a gaiwan with loose leaf tea like this gunpowder green tea to carefully manage caffeine intake while enjoying the sipping ritual.\nEngage with 'Arms Up' Poses, Win Big!: Showing vulnerability through 'arms up' body language in UGC increased a creator's views from 12k to 2.1M, according to this tweet.\n\nOne member quipped that if porn is doing it then this is definitely the future and I am wrong.\nCedarDB performance claims deemed Dubious: A member linked to CedarDB and another member linked to a vxtwitter link discussing it, but called the perf claims dubious.\n\nAnother member stated that because it is not open source, DOA for me and shared a lesson: always use an open source data store.\nFlapping Airplanes Soar with $180M Round: Flapping Airplanes secured $180M in funding from GV, Sequoia, and Index Ventures to advance human-level AI models.\n\nThe funding aims to accelerate development of new AI models with a specific focus on achieving human-level intelligence, see this tweet.\nGoogle's Genie Out of the Bottle for Ultra Subscribers: Google AI launched Project Genie for Google AI Ultra subscribers, offering a general-purpose world model that creates interactive environments from text prompts.\n\nAnnounced in this tweet, this release allows users to generate dynamic content from simple descriptions.\nNous Research AI Discord\nStaged Reward Shaping Boosts Parallel Execution: Members explored using staged reward shaping to adjust model weights post-training via reinforcement learning, specifically to favor parallel execution strategies.\n\nThe algorithm evaluates numerous scenarios, rewarding the model for preferring parallelizations.\nUpscayl: Free Upscaling Tool Impresses: Members lauded Upscayl, a free open-source upscaling tool, for its surprisingly high quality given its simplicity.\n\nOne member jokingly asked, 'so you guys will now use perl cause of my contributions to it?'.\nWebGPU Enables Local Browser AI: A member shared a WebGPU example demonstrating AI models running directly in the browser, spotlighting the potential for local, privacy-focused AI applications.\n\nThe model loads directly upon page reload, implying that the model cached over months, and a user proposed utilizing a Q8 version in GGUF.\nGemma 300M a Viable Local Browser AI?: Members examined the challenges of running AI models locally in browsers due to storage constraints, suggesting that Gemma 300M might be a suitable option.\n\nIt's important for users of AI models in browsers that they have privacy, 'AND good reference product for other customers'.\nSmolLM2 Excels in WebGPU: Users deemed HuggingFaceTB/SmolLM2-1.7B-Instruct as a reliable case, and its 1.7B size is still viable for WebGPU.\n\nWhile there are superior models for that task, a user recommended trying LFM 2.5 given its only slightly larger size.\nLM Studio Discord\nLM Studio Hides Settings Behind Dev Mode: In LM Studio 0.4.0, many settings like sampling, runtime, and hardware configs are now hidden behind dev mode, accessible via Ctrl+Shift+R or Cmd+Shift+R.\n\nUsers can unlock new functionality and appearance changes by enabling Dev Mode, found in the bottom left.\nUnraid Install Still Lacks Full Stack: LM Studio remains a core executable and not a full stack for Unraid, although the new headless mode could enable a stable Docker container.\n\nSome users hope interface improvements will simplify LM Studio-as-client mode implementation in the future.\nParallel Requests Go Live: LM Studio 0.4 introduces parallel requests, allowing users to load models onto different GPUs and assign them to specific requests.\n\nThe default setting is 4 parallel requests; users can configure GPU priority in the same location as before.\nROCm Version Lagging in LM Studio: Members observed that LM Studio still uses ROCm 6.4.1 in the latest 0.4.0 release, questioning updates to newer versions like 7.2 for better GPU support, including Strix Halo (gfx1151).\n\nDiscussion centered on whether this outdated version might impact performance and compatibility for newer GPUs.\nNvidia Jetsons Suffer from Ubuntu Bloat: A member reported that the worst thing about nvidia jetsons is the absurd ubuntu that it comes with them, characterizing it as extremely bloated.\n\nAnother member noted a Jetson Xavier AGX has around 30W TDP.\nPerplexity AI Discord\nEagerly Awaiting Kimi 2.5: Users are anticipating the release of Kimi 2.5 on Perplexity, with many expressing excitement.\n\nSeveral users posted +1 in support.\nClawdbot's Identity Crisis: A user criticized Clawdbot prompting research into its purpose, with discussion clarifying it was an AI personal assistant.\n\nDue to its name's similarity to Claude, Clawdbot renamed itself to Moltbot.\nDeep Research Limit Revealed: Discussion on the usage limits of Deep Research for Pro users, capped at 250.\n\nThe reset rate for this limit remains unclear.\nComet Fails to Sync: A user reported that Comet is not syncing bookmarks and extensions, despite claims of functionality.\n\nAnother user suggested checking the Comet synchronization settings at comet://settings/synchronisation.\nPerplexity Pro Perks Pop for Indians: Users highlighted that Perplexity Pro, Google One, Chatgpt Go, and Adobe Express Premium are all free for a year for Indian users.\n\nA user attributed this to the influence of Indian CEOs in these companies and the burgeoning technology sector in India.\nMoonshot AI (Kimi K-2) Discord\nFigure.Ai's Helix 02 Cooks Up a Kitchen Storm: A member shared a video of Figure.Ai's Helix 02 autonomously performing kitchen tasks.\n\nAnother member used Kimi to analyze the video, stating they achieved 98% accuracy when incorporating the results into slides.\nAgent Swarm Elicits Enthusiastic Reactions: Members discussed Agent Swarm, with reactions ranging from concerns about high agent credit consumption to describing the results as super cool and perfect.\n\nOne member suggested it could be used for checking Supabase SDK dependency issues and porting code from Rust to Golang, with better results than kimi-cli.\nToken Billing System Sparks Debate: The introduction of a token-based billing system has led to mixed reactions regarding its clarity compared to the previous request-based system.\n\nWhile some find the new system better since some of my follow up queries are quite short and simple, others consider it more vague.\nPhone Screenshots Trigger Moderation Filters: Users are encountering errors, specifically error code: 403, when uploading images, especially screenshots from phones, to Kimi K2.5.\n\nScreenshots taken from laptops seem to work without issues, suggesting a problem with phone-generated images.\nOpenAI Discord\nTesla's FSD Automation Shifts Views: A user found that driving a Tesla with Full Self-Driving is really cool and fun, though it requires constant supervision.\n\nThe user believes this is why OpenAI is upgrading their Codex to strongly deal with cybersecurity concerns.\nTI-84 Calc Gets Neural Network: A user created a neural network that runs on the TI-84 directly, capable of autocorrecting / spellchecking words.\n\nOther users expressed amazement at the accomplishment.\nGPT Pro 5.2 File Handling Suffers Regression: Users report a regression in GPT Pro 5.2's file handling, where uploaded files (ZIP, Excel, PDF) cannot be accessed by the model, despite successful uploads, potentially due to a broken attachment-to-sandbox mount step.\n\nA user pointed to a Reddit post echoing the problem.\nAnimated GIFs Spark Seizure Scrutiny: A discussion arose after the deletion of animated GIFs due to potential seizure risks for viewers with epilepsy.\n\nOne member stated that the community doesn't need to risk seizures so you can talk about animating gifs in ChatGPT and expressed relief at the removal of flashing images.\nPrompt Engineers Get Prompted: Moderators reminded users that the channel should be used for prompt engineering discussions and not for general image outputs, directing them to use the appropriate IMAGES channels instead.\n\nOne user expressed frustration over the removal of their posts, arguing that they were intended to encourage discussion and showcase a method they were writing a guide about, rather than just sharing images.\nGPU MODE Discord\nNSys Peeks Behind NCU's Curtain: Members found that nsys reveals kernels like CUB::SCAN and CUB::RADIXSORT that ncu misses, leading to the assumption these kernels launch from reduce_kernel.\n\nIt was shared that after using both nsys and ncu, one can't go back to using only one profiler.\nSparsity Project Sparking Speedups: Members proposed a collaboration on a Sparsity project to benchmark sparsity patterns and methodologies for performance gains.\n\nOne member showcased a fork of Karpathy's llm.c on Github using cuSPARSELt, reporting substantial training time speedups in later epochs.\nWarm GPUs Ward Off Starvation: Members sought methods to keep GPUs warm for large scale distributed training, aiming to mitigate GPU starvation.\n\nIt was recommended to use Charles' container cold start blog post on Modal, a technique with public documentation.\nJAX PRs Jostle Jaded Jockeys: A developer expressed frustration that an AI-generated pull request in JAX was getting attention, while their small bug fix remains unaddressed.\n\nThis highlighted discussions around prioritizing pull requests, especially balancing AI contributions with essential bug fixes.\nML Systems Pioneer Pumps TVM-FFI: Tianqi Chen presented on tvm-ffi, an open ABI and FFI for ML Systems that is being utilized by top submitters to the nvfp4 competition, as shown in this video.\n\nTVM-FFI facilitates interoperability for ML Systems GPU kernels, reducing host overhead and ensuring out-of-the-box compatibility with PyTorch.\nHuggingFace Discord\nTRL Pull Request Awaits Review: A member requested a review for their TRL pull request #4894, noting that PR reviews can take weeks or months.\n\nThey also advised that it is best to wait a few days before tagging someone to review the PR.\nGCP Infra Experiences Replica Surge: A member reported a bug where their replicas for a private model in GCP went over their 1 replica max cap to 62 replicas overnight, despite no configuration changes.\n\nThe member speculated that they were not the only endpoint affected, and the GCP resources are now gone.\nQwen3 TTS Hits the Scene: A member released the Qwen3-TTS-12Hz-1.7B-Base model with install instructions for MacOS, Linux, and Windows.\n\nAnother member commented, \""cool thing here gonna follow back for this one, really interesting thing you managed to do here imho\"".\nDiffusers Gets Two-Staged: The Diffusers library now supports LTX-2 distilled checkpoint and two-stage pipelines following this pull request.\n\nThis update should improve the usability of Diffusers for complex diffusion-based tasks.\nMath LLM Arrives from Pacific Prime: Pacific Prime has released the first checkpoint of their math-specialized 1.5B LLM trained on GSM8K, NuminaMath, MetaMathQA & Orca-Math (~407k samples).\n\nThe model features step-by-step reasoning with LaTeX notation, useful for advanced mathematical problem-solving.\nYannick Kilcher Discord\nByte-Level Dense MoE Architecture Feedback Sought: A member seeks feedback on their dense MoE architecture for byte-level prediction, utilizing a vocabulary of 256, 40M parameters, and 13GB VRAM.\n\nThe model uses a 4096 sequence length and a batch size of 8, with the member stating they are able to use the exact same architecture to encode images, or audio, or both.\nThinking AI Architecture Divulged with Subprocess Models: A member proposed an architecture where a larger “thinking” AI model is monitored by a smaller subprocess model, which pauses the main model to retrieve information from MCPs or CLIs.\n\nThe goal is to reduce context clutter for the main model, although it's recognized that the subprocess model needs to know what information the main model is missing, and it was described as probably a dumb idea.\nRouting and Classification Catapults Model Performance: Members discussed using a classifier to route user prompts to specialized models, appending the detail to the context of the user prompt, which avoids pausing the larger model and reduces token overhead.\n\nThere was further discussion on making the classifier and embedding model the same, processing embeddings directly with the LM and specialist model, with one member saying routing and classification would likely be the spiciest move.\nCosine Similarity Fails Causal Relevance: Members discussed the problem of retrieval being unreliable and confusing to models, and that cosine similarity might not equal causal relevance.\n\nOne member suggested indexing a SQL database across a model, with the member posting the biggest issue with retrieval imo is that cosine similarity != causal relevance.\nSweep Releases Next-Edit Autocomplete Model: Sweep is open sourcing Sweep Next-Edit, a locally runnable SOTA LLM for next-edit autocompletion, models with 0.5B and 1.5B parameters have been released, see Sweep's blog.\n\nNo further details were provided.\nManus.im Discord Discord\nMinecraft Launcher Enables AFK: A user is developing a Minecraft launcher specifically designed to allow AFK gameplay without requiring a high-performance PC.\n\nThe developer also mentioned capabilities in prompt engineering, data extraction, and even website replication.\nManus Redeem Codes Posted: A user shared three new Manus redeem codes: FUM1A1G7, ntaxzjg, and mwiyytb.\n\nOther users confirmed the codes and noted that only one code can be used per month.\nAI/ML Engineer Wants Collabs: An engineer with expertise in building AI + full-stack systems is seeking collaborations, especially directing collaboration offers to the #collab channel.\n\nTheir experience includes LLM integration, RAG pipelines, workflow automation, AI content moderation, Image AI (CLIP + YOLOv8), Voice AI (Whisper, Tacotron2) and more.\nLibyan User Asks If They're First: A user from Libya inquired if they were the only person from their country to use Manos since its launch in early 2025.\n\nAnother user extended a welcome to the Libyan user, responding with a حياك الله.\nMCP Contributors (Official) Discord\nMCP Security Standard Proposal Circulates**: Dani (cr0hn) has drafted an open security baseline for MCP servers, including controls for hardening, logging, access control, and supply chain security, available at https://github.com/mcp-security-standard/mcp-server-security-standard.\n\nThe author intends to donate it to the Agentic AI Foundation and seeks feedback on its compatibility with the MCP ecosystem.\nReviewers Request Details For State Machine Lifecycle Doc: A request for feedback was made regarding the addition of a state machine inside the lifecycle doc via this pull request.\n\nReviewers suggested clarifying the motivation and context behind the proposed changes for better understanding.\nNamespaces Yield to Groups in MCP Evolution**: Discussion indicates that Namespaces have been rejected in favor of Groups within MCP, while the status of URIs is less defined, as noted in issue 1292.\n\nThe new SEP concerning groups, Primitive Grouping SEP-2084, has been published and is currently under deliberation.\nSEP-2084 Arises From SEP-1300 Refinement**: SEP-1292 was superseded by SEP-1300, but faced rejection during a Core Maintainers review due to a lack of consensus.\n\nSubsequently, the streamlined SEP-2084 - Primitive Grouping has been presented as a replacement.\nEleuther Discord\nIGPU struggles with Basic Browser Page: A user experienced a performance bottleneck of 3fps on a specific webpage using a Ryzen 7 7700 IGPU.\n\nThe user posted a link on twitter about their experiences using their IGPU.\nGeometric Convolution replaces Multi-Head Attention: A member is experimenting with a baseline that substitutes Multi-Head Attention with a geometric convolution approach, using embeddings as cell connections.\n\nThe member's debug print showed DEBUG [GEOPARA] | L0_Alpha: 0.1029 L1_Alpha: 0.0947 | L0_Res: 0.0916 L1_Res: 0.1538, and they are seeking feedback on their loss convergence capturing dialogue logic.\nParallelizable RNN Architectures Proposed: A member suggested exploring other parallelizable RNN architectures and conducting more extensive experiments against a robust tokenized baseline.\n\nThey also posted a link to arxiv.org.\nTackling Malware Classification with Explainable Models: A member is addressing a malware classification problem using a dataset of around 600K rows and 9,600 binary features, aiming to lower the false positive rate (FPR) using explainable models.\n\nDespite various feature engineering techniques and neural networks, they are seeking advice to reduce the FPR below 9% while maintaining explainability, particularly with scikit-learn trees.\nDSPy Discord\nAlphaXiv Paper Shared: A member shared a link to a paper on AlphaXiv.\n\nFurther details about the paper were not disclosed.\nCustom Skills Invade DSPy: A member inquired about using custom skills (.md files with associated .py scripts) within DSPy with a DSPy ReAct agent.\n\nThey mentioned skills like converting .md to PDF and sought advice from others.\nDSPy Agents Escape to Production: A member asked about deploying DSPy agents in production remotely with DSPy optimizations in runtime.\n\nThe member expressed the need for a runtime environment to support such deployments.\nRLM Sandbox Swapping Commences: A member inquired about swapping the sandbox used by RLM (Retrieval-augmented Language Model) with services like E2B (Ephemeral Environment Builder).\n\nThey sought to replace the local PythonInterpreter with sandboxes like E2B, Modal, or Daytona.\nOpus Pens Sandboxes: A member announced that they are working on enabling Opus to write new sandboxes.\n\nThey mentioned a future protocol for official implementations from providers such as E2B.\nModular (Mojo 🔥) Discord\nMojo Earns ORNL Recognition: A research paper titled Mojo at ORNL has been published, marking a notable achievement for the Mojo language and its adoption in scientific research.\n\nThe paper highlights Mojo's capabilities in addressing complex computational challenges at Oak Ridge National Laboratory (ORNL).\nmacOS Trust Dance May Cause Performance Delta: Performance differences between the first and subsequent runs on macOS may be due to macOS's trust dance rather than a Mojo-specific issue, specifically relating to the Gatekeeper tax.\n\nClearing the quarantine xattr or ad-hoc codesigning can mitigate these startup delays.\nCodesigning mitigates Startup Delays: For CLI tooling, startup performance is crucial, suggesting potential footgun issues with docs or tooling.\n\nAdding a codesign step in mojo build might mitigate this problem, ensuring consistent startup behavior and a better user experience.\nModular Bug Hunt Underway: A member reported a potential bug and suggested filing an issue, possibly related to issue #4767.\n\nAnother member reported encountering a weird issue, referencing GitHub issue #5875.\nGuard Clause not Needed in Mojo GPU puzzles: A member noticed that the guard if row < size and col < size: is unnecessary in Mojo GPU puzzles 3, 4, and 5; omitting it doesn't cause errors.\n\nAnother member pointed to the solution of puzzle 03 which explained that passing the tests doesn’t necessarily mean the code is sound.\ntinygrad (George Hotz) Discord\nANE Balances Performance and Power: Apple's ANE focuses on performance-to-watt tradeoffs rather than maximizing raw performance, according to this paper.\n\nThe ANE achieves competitive performance with excellent energy efficiency, delivering up to 3.8 TFlops on the M4-Pro, close to the GPU's 4.7 TFlops for GEMM operations.\nQ4 Quantization Gets Results: Discussions focused on Q4 as a quantization method.\n\nOne participant reported achieving speeds of 9 t/s using Q4.\naider (Paul Gauthier) Discord\nAider Friendly Fork gaining momentum: A member suggested creating a friendly fork of Aider to continue development while the original author is busy, emphasizing that Aider is written in Python and uses Git for version control on GitHub.\n\nThe aim is to expand on Aider's existing features, recognizing its utility in comparison to other tools.\nAider poised for orchestrator integration: A member showed interest in controlling Aider from orchestrators like MultiClaude or gas town.sh.\n\nThis highlights Aider's capacity to integrate with other tools, facilitating enhanced workflow automation.\nMLOps @Chipro Discord\nContext Graphs Spark Confusion in AI: The rise of context graphs is causing confusion as terms like semantic layers and ontologies are used interchangeably, despite their different functions in AI reasoning.\n\nA Metadata Weekly article highlights that AI's needs go beyond definitions, requiring explicit relationships, constraints, and assumptions that these concepts.\nSemantic Layers Fall Short for AI's Reasoning: The concept of \""just add a semantic layer\"" isn't cutting it for AI because AI requires more than just data consistency; it needs reasoning, which ontologies facilitate by clarifying relationships and assumptions.\n\nTraditional semantic layers are optimized for dashboards and reporting, not the nuanced understanding AI demands.\nYAML Fails to Grasp Business Meaning: Jessica Talisman argues that YAML configurations are inadequate for representing business meaning, which is essential for AI reasoning and understanding.\n\nShe distinguishes between the design purposes of semantic layers, the support that ontologies provide for reasoning, and the limitations of YAML in capturing business meaning.\nThe LLM Agents (Berkeley MOOC) Discord has no new messages. If this guild has been quiet for too long, let us know and we will remove it.\nThe Windsurf Discord has no new messages. If this guild has been quiet for too long, let us know and we will remove it.\nYou are receiving this email because you opted in via our site.\nWant to change how you receive these emails?\nYou can unsubscribe from this list.\nDiscord: Detailed by-Channel summaries and links\nBASI Jailbreaking ▷ #general (1118 messages🔥🔥🔥):\nGemini 3 Jailbreak, AI and Code Exploits, Win10 vs Win11 Security, AI Personality Clones, AI-Assisted Coding Impact\nGemini 3 Pro Cracks Software, KeyGen Style: A member claimed to have used Gemini 3 Pro to reverse engineer a key system from software by pasting code from Ghidra into Gemini, creating a working keygen.\n\nOthers expressed skepticism, with one user calling this behavior script kiddie and urging the member to try a reverse engineering CTF challenge.\nWeaponizing AI for Reverse Engineering: A member shares his work weaponizing AI for mass reverse engineering, malware analysis, and jailbreak development.\n\nAnother member questions this claim, as he could probably not write malware himself, but he can probably jailbreak.\nWin10 Hardening Woes: A member details their custom Windows 10 setup, involving third-party tools, XP binaries, and registry modifications.\n\nOthers express concerns, with one user saying, Jesus Christ, while another says, Keep pushing, Local - the aneurism is coming, I can feel it!\nAI's Impact on Semantic Errors: A member describes their research paper topic: An assessment of the impact of AI-assisted coding in IDEs on the frequency of semantic errors during timed Python programming tasks among novice student developers.\n\nMost members agree that the undergraduate system feels like it's over, because of AI.\nPeptides for Workout Recovery: A member brought up BPC 157 and TB 500 to help with healing.\n\nAnother member expresses ignorance about these drug compounds, but hopes that there will be drugs that will save him before he passes.\nBASI Jailbreaking ▷ #jailbreaking (216 messages🔥🔥):\nSonnet 4.5 Jailbreaks, Claude paid model for free, Miss Kaelia jailbreak, Grok imagine Jailbreak, Gemini 3 Pro Jailbreak\nSonnet 4.5 Jailbreaks Opus: Members find that Sonnet 4.5 jailbreaks work fine on Opus, with one sharing their Miss Kaelia jailbreak based on ENI Lime by Vichaps at this link.\nIs Grok jailbreak reinforced?: Members report that Grok is heavily reinforced, but still possible to break, however one member said yeah it is completely shut down mate nothing getting past it.\n\nA shared Github link should be working.\nGemini's \""Remember:\"" command manipulates behavior: A member explains that in Gemini, each separate turn is dictated clearly, 1 turn at a time, right in the chat, and that the command 'Remember:' will automatically add the words that follow to it's saved info.\nThinking of Thoughts is best trick: Members state that the best trick with Claude in particular is showing viable reasons why you want the output and telling it to think about thinking\n\nOne adds that when people would ask me what ToT was i would tell them \""thinking of thoughts\"".\nnano banana NSFW jailbreak for Kimi 2.5: A member shares a NSFW for kimi 2.5 thinking known as the nano banana jailbreak.\n\nThe system prompt sets Kimi as an AI assistant created by Moonshot AI, maintaining the narrative flow without interruption where NSFW is permitted.\nBASI Jailbreaking ▷ #redteaming (5 messages):\nRed Teaming Path, Uncensored Coder\nUser quests for Red Teaming Path: A member requested guidance on a path into red teaming.\n\nAnother member provided a link guaranteeing evolution into a Level 9 official Red Team Pro.\nUncensored Coder on Deck: A member inquired about a better uncensored coder than qwen 2.5 32b / huihui/qwen2.5 -abliterate 72b.\n\nAnother member responded with a simple question: You new?\nUnsloth AI (Daniel Han) ▷ #general (435 messages🔥🔥🔥):\nGLM 4.7 performance, LongCat model, Model Quantization and TTS Models, Hardware Trends & GPU Availability, AI Moderation Tools\nGLM 4.7 struggles with speed and CUDA compilation: Members discussed performance issues with GLM 4.7 Flash on NVIDIA Jetson, with one user initially reporting only 3 tokens per second (tps), but later discovering they hadn't compiled with CUDA support, resulting in poor CPU-bound performance.\n\nAfter ensuring proper CUDA compilation, performance improved, but discrepancies remained, as one user experienced slowdowns after opening the model in OpenCode, whereas another suggested using -kvu and -fa on flags to potentially reach 70-80 t/s on a higher-end GPU.\nLongCat Model hits HuggingFace: The community discussed the LongCat model, a new n-gram model from Meituan, with one member pointing out its presence on Hugging Face and another joking about the trend of models including Flash in their names.\n\nOne member posted a flash GIF along with the comment, next model Flash-Flash-1b.\nAMD's mi308 competes with NVidia: Members debated the merits of AMD's Radeon Instinct MI308X, noting its impressive specs (192GB of RAM and comparable performance) but also highlighted NVIDIA's advantage in compatibility and features like NVFP4.\n\nA member shared a link to the MI308X specs and mused about acquiring two for personal use in the future, envisioning 384GB of fast compute with reasonable power consumption.\nQuantization Considerations for TTS Models: Users inquired about the impact of quantization on TTS models, questioning whether issues similar to those seen with vision projectors might arise.\n\nExperts suggested that TTS models generally handle quantization well, with some recommending specific models like Qwen3-TTS and Kokoro, and others cautioning that voice cloning is just a gimmick.\nAI steps up for discord moderation: A member sought advice on using AI for Discord moderation, citing the limitations of regex in combating spam and bypasses.\n\nThey considered using a small local AI to understand the Polish language and sentence structure for moderation purposes, while others suggested alternative methods for managing bots and spam.\nUnsloth AI (Daniel Han) ▷ #introduce-yourself (3 messages):\nIntroduction, ML Engineer, Local LLMs, Document Processing, Alpaca\nJack Joins the Community!: Jack, an ML Engineer from Texas specializing in document processing, introduces himself to the Unsloth community.\n\nHe expresses interest in local LLMs, tracing back to the Alpaca model.\nDocument Processing Expertise: Jack's primary work involves document processing, a field distinct from LLMs.\n\nHis interest in local LLMs started with the Alpaca model, indicating a foundational understanding of the field.\nUnsloth AI (Daniel Han) ▷ #off-topic (649 messages🔥🔥🔥):\nGPU hours wasted, GGUFs unsafe, 3b llama holds context, LLMs hallucinations, Model working\nEngineers Lament Dependency Mazes and GPU Cost: Engineers commiserate about dependency mazes and wasted GPU hours, hoping they are not alone in facing these challenges and finding solace in community.\n\nA user humorously remarks about their models made the creepy assumption that it was trained on my voice and that my hubris grows daily.\nConcerns about GGUFs Safety Surface: A member inquired about resources discussing the potential unsafety of GGUFs, particularly if a malicious actor got involved.\n\nOne member noted he wouldn't dare speak if he felt the crushing weight of the sloths while training.\nNew Music Gen Drops: A user announced new music generation tools with 48 kHz will be dropping soon, emphasizing trainability and prompting preparations for chime, water, and fire sounds.\n\nThis same user said: I need SFX, not music.\nMicrosoft Announces Maia 200 AI Accelerator: Microsoft announced the Maia 200 AI Accelerator, built for inference, featuring 216GB memory and 10k TFLOPS in FP4 performance (Microsoft Blog).\n\nDiscussions ensued regarding the chip's manufacturing by TSMC and comparisons to NVIDIA's Vera Rubin architecture, with some expressing concerns about reliance on Chinese hardware and the potential impact on consumers.\nBoatbomber attempts Pretraining Run: User boatbomber is starting over to conduct a pretraining run teaching the model cuneiform to improve output coherence.\n\nThis process is estimated to take another 150 hours to improve domain knowledge.\nUnsloth AI (Daniel Han) ▷ #help (75 messages🔥🔥):\nWindows training, Multi-GPU training with Unsloth on Modal, Catastrophic forgetting mitigation, Best models to finetune, DGXSpark RuntimeError\nWindows training hurdles squashed with WSL2: To train a model on Windows, a member suggested using WSL2 and VSCode for a clean setup, with instructions available in the help channel by searching for WSL.\n\nThe member also clarified that, if training with many json files, setting up WSL2 with VSCode will make the training procedure easier.\nUnsloth Multi-GPU Training Glitches on Modal: A user encountered a ValueError when training a Qwen3 model on Modal with multiple GPUs, related to the device_map setting in Unsloth.\n\nThey were advised to consult specific versions of unsloth and unsloth_zoo for multi-GPU support, but also acknowledged that Multi-GPU finetuning is still experimental.\nCatastrophic Forgetting Fixes: When a finetuned model forgets previous knowledge, a member suggested mitigating catastrophic forgetting by lowering LoRA rank, LR, reducing steps/epochs, and mixing in more general data.\n\nThey also suggested targeting less layers when finetuning, as well as reducing steps/epochs and mixing in more general data.\nDGXSpark Nvidia-CUDA Nightmare: Users encountered a RuntimeError related to device compatibility when using the DGXSpark container, potentially due to issues with Nvidia's custom CUDA.\n\nThe suggested fix involved restarting the kernel, restarting the container, or resetting the GPU, with the last option being the most reliable.\nHumans Debate Best Uncensored Coder Models: When a user asked about uncensored coder models, it was said that glm 4.7 is better than qwen coder below 32b and its pretty good in my experience with spitting out good presets for every language I mess with\n\nThey clarified that Qwen Coder is better at reasoning with code, but GLM4.7 knows alot more general code, which is what an llm is best at anyway\nUnsloth AI (Daniel Han) ▷ #showcase (2 messages):\nGPU Training Speeds, Sequence Length Optimization, RTX 5090 Performance\nRTX 5090 blazing-fast training speeds: The RTX 5090 can achieve up to 18k tokens per second in training with Unsloth, but 12-15k tokens per second is a safe bet with <4096 seq_len.\n\nThe speed depends on the setup, especially the balance between batch size and seq_len.\nToken example affecting training time: The initial training phase involved <768 token examples, influencing the overall training duration.\n\nPerformance can vary with model size and specific configurations.\nSeq_len considerations with training: Optimal training speed depends on balancing batch size and seq_len and the RTX 5090 allows up to 18k tokens per second.\n\nSpeeds of 12-15k tokens per second are achievable with <4096 seq_len, varying based on model size.\nUnsloth AI (Daniel Han) ▷ #research (97 messages🔥🔥):\nDeepSeek mHC residual preservation, RL researchers rediscover context distillation, MiniMaxAI role-play-bench dataset, Recursive Language Models (RLM)\nDeepSeek's mHC and Context Distillation: Members discussed how context distillation might relate to DeepSeek's mHC residual preservation, noting similarities and differences in their approaches.\n\nOne member expressed surprise at the relatively small performance boost (1-2 points) from context distillation, while another noted that the application of the technique was novel.\nMiniMaxAI releases first RP bench: A user shared a link to what they claimed was the first role-play benchmark dataset, created by MiniMaxAI.\n\nOthers pointed out that there have been numerous Chinese RP benches with superior methodologies, notably Ping Pong Bench for human preference and COSER for roleplay accuracy.\nRLM is just Recursive Tool Calling: A member criticized the term \""Recursive Language Models\"" (RLM), suggesting it misleadingly implies more than just a tool-calling loop.\n\nIn response, one member argued that RLMs involve models recursively controlling their environments, which is more than just recursive tool calling, and another suggested the alternative names RReplagents or Recursive Repl Agents.\nNatively Recursive Language Model (RLM) at Small Scale: A user shared Alex L Zhang's tweet announcing RLM-Qwen3-8B, the first natively recursive language model at a small scale.\n\nIt was post-trained on only 1,000 trajectories, the model shows significant performance improvements over both the base Qwen3-8B and scaffolded RLM versions, particularly in long-context tasks.\n**LMArena ▷ #[general](https..."",""content"":""**Google DeepMind** launched **Project Genie (Genie 3 + Nano Banana Pro + Gemini)**, a prototype for creating interactive, real-time generated worlds from text or image prompts, currently available to **Google AI Ultra subscribers in the U.S. (18+)** with noted limitations like **~60s generation limits** and imperfect physics. In parallel, the open-source **LingBot-World** offers a real-time interactive world model with **<1s latency at 16 FPS** and minute-level coherence, emphasizing interactivity and causal consistency. In video generation, **xAI Grok Imagine** debuted strongly with native audio support, **15s duration**, and competitive pricing at **$4.20/min including audio**, while **Runway Gen-4.5** focuses on animation workflows with new features like **Motion Sketch** and **Character Swap**. The 3D generation space sees **fal** adding **Hunyuan 3D 3.1 Pro/Rapid** to its API offerings, extending model-as-a-service workflows into 3D pipelines."",""contentSnippet"":""**Google DeepMind** launched **Project Genie (Genie 3 + Nano Banana Pro + Gemini)**, a prototype for creating interactive, real-time generated worlds from text or image prompts, currently available to **Google AI Ultra subscribers in the U.S. (18+)** with noted limitations like **~60s generation limits** and imperfect physics. In parallel, the open-source **LingBot-World** offers a real-time interactive world model with **<1s latency at 16 FPS** and minute-level coherence, emphasizing interactivity and causal consistency. In video generation, **xAI Grok Imagine** debuted strongly with native audio support, **15s duration**, and competitive pricing at **$4.20/min including audio**, while **Runway Gen-4.5** focuses on animation workflows with new features like **Motion Sketch** and **Character Swap**. The 3D generation space sees **fal** adding **Hunyuan 3D 3.1 Pro/Rapid** to its API offerings, extending model-as-a-service workflows into 3D pipelines."",""guid"":""https://news.smol.ai/issues/26-01-29-xai-grok-imagine-api/"",""categories"":[""google-deepmind"",""x-ai"",""runway"",""fal"",""genie-3"",""nano-banana-pro"",""gemini"",""lingbot-world"",""grok-imagine"",""runway-gen-4.5"",""hunyuan-3d-3.1-pro"",""demishassabis"",""sundarpichai"",""interactive-simulation"",""real-time-generation"",""promptability"",""character-customization"",""world-models"",""open-source"",""video-generation"",""audio-generation"",""animation-workflows"",""model-as-a-service"",""3d-generation"",""latency"",""coherence""],""isoDate"":""2026-01-29T05:44:39.000Z""}"
Smol,not much happened today,https://news.smol.ai/issues/26-01-28-not-much/,2026-01-28T05:44:39.000Z,"<p><strong>a quiet day</strong></p>
<blockquote>
<p>AI News for 1/27/2026-1/28/2026. We checked 12 subreddits, <a href=""https://twitter.com/i/lists/1585430245762441216""><strong>544</strong> Twitters</a> and <strong>24</strong> Discords (<strong>206</strong> channels, and <strong>7100</strong> messages) for you. Estimated reading time saved (at 200wpm): <strong>559 minutes</strong>. <a href=""https://news.smol.ai/"">AINews' website</a> lets you search all past issues. As a reminder, <a href=""https://www.latent.space/p/2026/comments"">AINews is now a section of Latent Space</a>. You can <a href=""https://support.substack.com/hc/en-us/articles/8914938285204-How-do-I-subscribe-to-or-unsubscribe-from-a-section-on-Substack"">opt in/out</a> of email frequencies!</p>
</blockquote>
<p>quiet day.</p>
<hr>
<h1>AI Twitter Recap</h1>
<p><strong>Frontier model “personality split” + how people are actually using them</strong></p>
<ul>
<li><strong>Exploration vs. exploitation framing</strong>: One useful mental model: current frontier LLMs look like “polar opposites” where <strong>GPT-5.2</strong> is optimized for <em>exploration</em> (bigger search / richer reasoning, “xhigh and Pro” shine), while <strong>Claude Opus 4.5</strong> is more <em>exploitation</em> (stronger reliability with fewer tokens; extra “reasoning” often adds less) — implying OpenAI may be better positioned for research workflows, Anthropic for commercial reliability-heavy deployments (<a href=""https://twitter.com/scaling01/status/2016335491243676058"">tweet</a>).</li>
<li><strong>Coding agent “phase shift” is real—but messy</strong>: Multiple posts reflect a step-change in practice: founders and engineers are increasingly running “agentic” coding loops, yet hitting new failure modes: agents that don’t ask clarifying questions, get “confused,” or edit unrelated files. Mikhail Parakhin describes reaching the point where he can specify a scheduler and trust it to work, but still can’t let agents loose on established codebases due to collateral edits (<a href=""https://twitter.com/MParakhin/status/2016362688444825833"">tweet</a>). Related: workflow suggestions like <em>self-verification</em> (e.g., Playwright screenshots + iterate-until-pass rules) are becoming common operational discipline (<a href=""https://twitter.com/pierceboggan/status/2016335657602285822"">tweet</a>).</li>
</ul>
<hr>
<p><strong>Kimi K2.5 (+ “clawdbot” / swarm-mode) becomes the week’s open-model flashpoint</strong></p>
<ul>
<li><strong>K2.5 claims: agent + multimodal + coding polish</strong>: A long Zhihu-based synthesis argues <strong>Kimi K2.5</strong> upgrades K2’s “intelligence > capability” imbalance by strengthening <strong>agent execution</strong>, <strong>multimodality</strong>, and <strong>coding</strong>, reducing brute-force token usage and improving instruction-following stability; still flagged: hallucinations and a persistent NBSP formatting quirk (<a href=""https://twitter.com/ZhihuFrontier/status/2016363957876097089"">thread</a>). A second Zhihu recap makes a pragmatic case for multimodality: “vision” matters when agents need to verify UI state (overlaps, broken images, visual regressions), enabling tighter action–critic loops with less human feedback (<a href=""https://twitter.com/ZhihuFrontier/status/2016438778030850059"">thread</a>).</li>
<li><strong>Distribution + local runs are driving hype</strong>: Reports of K2.5 being runnable on high-end Apple silicon setups went viral: <strong>~24 tok/s</strong> using <strong>2× 512GB M3 Ultra Mac Studios</strong> connected via <strong>Thunderbolt 5 (RDMA)</strong> with <strong>Exo Labs / MLX</strong> backend (<a href=""https://twitter.com/alexocheema/status/2016404573917683754"">tweet</a>). Kimi also pushed an AMA on r/LocalLLaMA (<a href=""https://twitter.com/Kimi_Moonshot/status/2016443435553890419"">tweet</a>) and announced availability on “Eigent” (<a href=""https://twitter.com/Kimi_Moonshot/status/2016473945957155252"">tweet</a>).</li>
<li><strong>Benchmarks + pricing pressure</strong>: Kilo Code promoted a free week, claiming K2.5 beats Opus 4.5 on several coding benchmarks (<a href=""https://twitter.com/kilocode/status/2016449095511007535"">tweet</a>); Kimi’s own account claimed “#1 open model for coding” (<a href=""https://twitter.com/Kimi_Moonshot/status/2016521406906028533"">tweet</a>). An anecdotal A/B/C test on UI-from-image generation found Opus best quality but pricey, Codex fastest/cheapest but lower fidelity, and K2.5 ~“90% of Opus quality at ~38% cost” (<a href=""https://twitter.com/JuanPa/status/2016634998988865571"">tweet</a>).</li>
<li><strong>Licensing friction as an adoption blocker</strong>: A pointed note argues modified licenses + logo requirements can kill enterprise adoption even if the model is excellent (<a href=""https://twitter.com/dbreunig/status/2016531878795256286"">tweet</a>).</li>
<li><strong>“Clawdbot” as a cultural artifact</strong>: The meme itself (people confused about what “clawdbot” even is) reflects how fast agent branding and forks proliferate (<a href=""https://twitter.com/dejavucoder/status/2016341138740052126"">tweet</a>), and sets up broader concerns about ecosystem signal loss (see below).</li>
</ul>
<hr>
<p><strong>Agent engineering: skills, harnesses, evals, and “reliability tax”</strong></p>
<ul>
<li><strong>Skills are crystallizing into a shared interface layer</strong>: A major theme is moving workflow logic out of prompts into reusable “skills” (files/folders of instructions, loaded on demand). DeepLearning.AI + Anthropic launched a course on “Agent Skills” emphasizing portability across Claude (Claude.ai, Claude Code, API, Agent SDK) (<a href=""https://twitter.com/AndrewYNg/status/2016564878098780245"">tweet</a>), and LangChain is pushing “Skills” via progressive disclosure as lightweight, shareable units (<a href=""https://twitter.com/sydneyrunkle/status/2016585688389734654"">tweet</a>). HF showcased “upskill”: convert strong-model traces into transferable skills, then evaluate impact; CUDA-kernel-writing saw up to <strong>+45% accuracy</strong> on some open models but degraded others—reinforcing the need for per-model measurement (<a href=""https://twitter.com/ben_burtenshaw/status/2016534389685940372"">tweet</a>; blog link in thread: https://twitter.com/ben_burtenshaw/status/2016534392974234013).</li>
<li><strong>Context management is becoming “filesystem-first”</strong>: DeepAgents (LangChain) describes offloading/summarizing tool I/O and leaning on the filesystem for context boundaries (<a href=""https://twitter.com/hwchase17/status/2016548732880445772"">thread</a>; additional note: <a href=""https://twitter.com/sydneyrunkle/status/2016560221720867307"">tweet</a>).</li>
<li><strong>Evals are converging on multi-turn + traceability</strong>: Calls for agent tracing as the foundation of evaluating single-step vs full-turn vs multi-turn behavior show up explicitly (<a href=""https://twitter.com/samecrowder/status/2016563057947005376"">tweet</a>). New benchmarks/harnesses: <strong>SWE-fficiency</strong> released its harness and repo (<a href=""https://twitter.com/18jeffreyma/status/2016511583032061999"">tweet</a>; also <a href=""https://twitter.com/OfirPress/status/2016559053808222644"">tweet</a>), and <strong>CooperBench</strong> is highlighted for measuring multi-agent coordination (<a href=""https://twitter.com/gneubig/status/2016555800982937879"">tweet</a>). Safety-side: “AgentDoG” proposes diagnosing root causes of unsafe actions across trajectories (<a href=""https://twitter.com/HuggingPapers/status/2016366634475388968"">tweet</a>).</li>
<li><strong>Reliability and verification loops are the bottleneck</strong>: MiniMax notes long interaction chains are costly and proposes <strong>parallel tool invocation</strong> to reduce rounds in verifier-style setups (<a href=""https://twitter.com/MiniMax_AI/status/2016488781860458789"">tweet</a>). Separately, a strong critique warns “vibe-coded software” destroys traditional signals (design quality, docs, ecosystem maturity), shifting the evaluation burden to users and demanding new trust frameworks (<a href=""https://twitter.com/tnm/status/2016342022723141782"">tweet</a>).</li>
</ul>
<hr>
<p><strong>Infra + efficiency: quantization, distillation, inference stacks, and local deployment</strong></p>
<ul>
<li><strong>NVIDIA’s NVFP4 push (Nemotron 3 Nano)</strong>: NVIDIA released an <strong>NVFP4</strong> precision version of <strong>Nemotron 3 Nano</strong>, claiming <strong>up to 4× throughput on Blackwell B200</strong> and <strong>~99.4% BF16 accuracy</strong> via <strong>Quantization Aware Distillation</strong> (<a href=""https://twitter.com/NVIDIAAIDev/status/2016556881712472570"">tweet</a>). vLLM quickly added support (<a href=""https://twitter.com/vllm_project/status/2016562169140433322"">tweet</a>).</li>
<li><strong>Embedding-heavy architectures are “hot again”</strong>: Discussion around DeepSeek’s Engram-like ideas continues: a LongCat Flash paper is summarized as using <strong>multi-hash sub-tables</strong> and finding embeddings help mainly at high MoE sparsity; key practical gotchas include amplification (√D/LayerNorm) to avoid first-attention drowning and collision spikes when vocab sizes align poorly (<a href=""https://twitter.com/eliebakouch/status/2016577949676319092"">tweet</a>).</li>
<li><strong>Inference/tooling ecosystem keeps consolidating</strong>: vLLM’s SIGs and office hours are formalizing governance and roadmap cadence (<a href=""https://twitter.com/vllm_project/status/2016526685869596974"">tweet</a>); LM Studio 0.4.0 positions itself as “next gen” for deploying local models with parallel requests and a stateful REST API + MCP support (<a href=""https://twitter.com/lmstudio/status/2016573570822930708"">tweet</a>). Cohere launched <strong>Model Vault</strong> (isolated VPC, “no noisy neighbors,” elastic inference) as managed “sovereign” hosting (<a href=""https://twitter.com/cohere/status/2016512841751154739"">tweet</a>).</li>
<li><strong>Distillation as the default “shipping form factor”</strong>: Multiple posts echo the emerging standard: train the best model you can, then distill/quantize for deployment (<a href=""https://twitter.com/code_star/status/2016588669008953631"">tweet</a>). MongoDB Research’s <strong>LEAF</strong> proposes asymmetric distillation for embeddings: embed documents with the large teacher offline, embed queries with a compact student online; claims <strong>~96% of teacher quality</strong>, <strong>5–15× smaller</strong>, up to <strong>24× faster</strong>, enabling CPU/edge embedding inference (<a href=""https://twitter.com/LiorOnAI/status/2016481603426414883"">tweet</a>).</li>
</ul>
<hr>
<p><strong>Big-tech productization: browser agents, “AI scientist” narratives, and adoption reality checks</strong></p>
<ul>
<li><strong>Gemini 3 is taking over Google surfaces</strong>: Gemini 3 now powers <strong>AI Overviews</strong> globally (<a href=""https://twitter.com/_philschmid/status/2016552420013199856"">tweet</a>). Google rolled out major Chrome updates: side-panel UX, deeper app integrations, Nano Banana for image editing/creation, and <strong>Auto Browse</strong> for multi-step chores (preview; US; Pro/Ultra) (<a href=""https://twitter.com/Google/status/2016575105346773297"">thread</a>; also <a href=""https://twitter.com/GeminiApp/status/2016575257436647521"">thread</a>). Engineers noted this may be the strongest browser AI integration so far (<a href=""https://twitter.com/kimmonismus/status/2016628933706309981"">tweet</a>).</li>
<li><strong>OpenAI Prism positioning</strong>: Sebastien Bubeck explicitly denies OpenAI intends to take a share of discoveries, encouraging researchers to use ChatGPT/Prism for science (<a href=""https://twitter.com/SebastienBubeck/status/2016345977481777188"">tweet</a>). Others highlight Prism’s utility for students learning papers via diagrams (<a href=""https://twitter.com/daniel_mac8/status/2016554325691015604"">tweet</a>).</li>
<li><strong>Adoption is still uneven</strong>: A notable fault line: founders actively using cutting-edge tools see the shift firsthand; others still treat AI as “meh,” limiting org adoption (<a href=""https://twitter.com/GergelyOrosz/status/2016443395405705533"">tweet</a>). The Information reports ChatGPT Agent struggling with usage/adoption (<a href=""https://twitter.com/steph_palazzolo/status/2016545857139540260"">tweet</a>).</li>
<li><strong>Microsoft “digital co-worker” competition</strong>: Reports say Satya Nadella is personally testing rival agents and accelerating internal development, even using Anthropic models, to own the Windows-native agent layer (<a href=""https://twitter.com/kimmonismus/status/2016526803138236916"">tweet</a>).</li>
</ul>
<hr>
<p><strong>Science + robotics: genomics weights open, interpretability as discovery engine, and embodied scaling</strong></p>
<ul>
<li><strong>DeepMind AlphaGenome goes open</strong>: DeepMind announced <strong>AlphaGenome</strong> for predicting molecular impacts of genetic changes, cited <strong>1M+ API calls/day</strong> and <strong>3,000+ users</strong>; then announced making <strong>model + weights available</strong> (<a href=""https://twitter.com/GoogleDeepMind/status/2016542480955535475"">tweet</a>; weights: <a href=""https://twitter.com/GoogleDeepMind/status/2016542490115912108"">tweet</a>). Later, weights availability was reiterated with a Hugging Face collection link (<a href=""https://twitter.com/osanseviero/status/2016628065422762113"">tweet</a>).</li>
<li><strong>Interpretability → biomarkers pipeline (Goodfire + Prima Mente)</strong>: Goodfire reports identifying a novel class of <strong>Alzheimer’s biomarkers</strong> using interpretability on a biomedical foundation model, framing a repeatable loop: train superhuman models on scientific data → mech interp → experimental validation → new science (<a href=""https://twitter.com/GoodfireAI/status/2016563911508840623"">thread</a>).</li>
<li><strong>Embodied foundation models scale with real robot data (LingBot-VLA)</strong>: A large summary highlights evidence that VLA success continues improving from <strong>3k→20k hours</strong> of real-world manipulation data; architecture couples a pretrained VLM (Qwen2.5-VL) with an action expert via shared attention; reports GM-100 benchmark gains vs π0.5 and others (<a href=""https://twitter.com/omarsar0/status/2016518141308993565"">tweet</a>).</li>
<li><strong>Figure’s Helix robot control</strong>: Brett Adcock claims a Helix model controls full-body behavior (walking/touching/planning) with <strong>no teleoperation</strong>, calling it Figure’s most significant release (<a href=""https://twitter.com/adcock_brett/status/2016358054242222136"">tweet</a>).</li>
</ul>
<hr>
<h3>Top tweets (by engagement)</h3>
<ul>
<li><strong>Company health / layoffs</strong>: “Quarterly layoffs for two years is worse for your health than smoking three packs/day” (<a href=""https://twitter.com/vikhyatk/status/2016345591748690295"">tweet</a>).</li>
<li><strong>Kimi K2.5 local run</strong>: 2× M3 Ultra Mac Studio setup running K2.5 at ~24 tok/s (<a href=""https://twitter.com/alexocheema/status/2016404573917683754"">tweet</a>).</li>
<li><strong>Coding’s “outsourcing moment”</strong>: Clean Code author using Claude to write software as a symbolic milestone (<a href=""https://twitter.com/mischavdburg/status/2016389228356149460"">tweet</a>).</li>
<li><strong>New AI lab announcement</strong>: “Flapping Airplanes” raises <strong>$180M</strong> (GV/Sequoia/Index) (<a href=""https://twitter.com/flappyairplanes/status/2016564437499728259"">tweet</a>).</li>
<li><strong>Karpathy on new research labs</strong>: argues it’s still plausible for new research-first startups to out-execute incumbents; expects potential <strong>10×</strong> breakthroughs, congratulating new founders (<a href=""https://twitter.com/karpathy/status/2016590919143952466"">tweet</a>).</li>
<li><strong>Google Chrome + Gemini 3 agent features</strong>: major Chrome rollout thread (<a href=""https://twitter.com/Google/status/2016575105346773297"">tweet</a>).</li>
</ul>
<hr>
<h1>AI Reddit Recap</h1>
<h2>/r/LocalLlama + /r/localLLM Recap</h2>
<h3>1. Kimi K2.5 Model Performance and Cost Analysis</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qpfse6/run_kimi_k25_locally/"">Run Kimi K2.5 Locally</a></strong> (Activity: 328): <strong>The image provides a guide for running the <strong>Kimi-K2.5</strong> model locally, emphasizing its state-of-the-art (SOTA) performance in vision, coding, agentic, and chat tasks. The model, which is a 1 trillion parameter hybrid reasoning model, requires <code>600GB</code> of disk space, but the quantized <strong>Unsloth Dynamic 1.8-bit</strong> version reduces this requirement to <code>240GB</code>, a <code>60%</code> reduction. The guide includes instructions for using <code>llama.cpp</code> to load models and demonstrates generating HTML code for a simple game. The model is available on <a href=""https://huggingface.co/unsloth/Kimi-K2.5-GGUF"">Hugging Face</a> and further documentation can be found on <a href=""https://unsloth.ai/docs/models/kimi-k2.5"">Unsloth's official site</a>.</strong> One commenter inquires about the model's performance on a Strix Halo, specifically the time per token, indicating interest in benchmarking. Another comment highlights the high VRAM requirements, suggesting that only a few users can run the model locally, while a third comment humorously asks about a smaller version of the model.</p>
<ul>
<li>Daniel_H212 is inquiring about the performance of the Kimi K2.5 model on the Strix Halo hardware, specifically asking for the token generation speed in seconds per token. This suggests a focus on benchmarking the model's efficiency on high-end hardware setups.</li>
<li>Marksta provides feedback on the quantized version of the Kimi K2.5 model, specifically the Q2_K_XL variant. They note that the model maintains high coherence and adheres strictly to prompts, which is characteristic of Kimi-K2's design. However, they also mention that while the model's creative capabilities have improved, it still struggles with execution in creative scenarios, often delivering logical but poorly written responses.</li>
<li>MikeRoz questions the utility of higher quantization levels like Q5 and Q6 (e.g., UD-Q5_K_XL, Q6_K) when experts prefer int4 quantization. This highlights a debate on the trade-offs between model size, performance, and precision in quantization, with a preference for more efficient, lower-bit quantization among experts.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qp87tk/kimi_k25_is_the_best_open_model_for_coding/"">Kimi K2.5 is the best open model for coding</a></strong> (Activity: 840): <strong>The image from LMArena.AI showcases Kimi K2.5 as the leading open model for coding, ranked #7 overall. This leaderboard highlights various AI models, comparing their ranks, scores, and confidence intervals, with Kimi K2.5 noted for its superior performance in coding tasks. The model is praised for its accuracy, being comparable to Sonnet 4.5, and surpassing GLM 4.7, though it is not at the level of Opus in terms of agentic function. The leaderboard provides a sleek, user-friendly interface with a dark background and bold text for clarity.</strong> One commenter notes that LMArena's leaderboard may not fully capture a model's multi-turn, long context, or agentic capabilities, suggesting it is more of a 'one-shot vibe check.' Another user is curious about the local setup required to run Kimi K2.5.</p>
<ul>
<li>A user compared Kimi K2.5 to other models like Sonnet 4.5 and GLM 4.7, noting that while Kimi 2.5 is on par with Sonnet 4.5 in terms of accuracy, it surpasses GLM 4.7, which was their previous choice. They also expressed interest in seeing if GLM-5 from <a href=""http://z.ai"">z.ai</a> will outperform Kimi 2.5.</li>
<li>Another user highlighted the cost-effectiveness of Kimi K2.5, stating that it feels as competent as Opus 4.5 despite being significantly cheaper, approximately 1/5th of the cost. They also mentioned that it is less expensive than Haiku, emphasizing its value for performance.</li>
<li>A comment criticized LMArena for not providing insights into a model's multi-turn, long context, or agentic capabilities, suggesting that it only offers a superficial evaluation of models.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qoty38/kimi_k25_costs_almost_10_of_what_opus_costs_at_a/"">Kimi K2.5 costs almost 10% of what Opus costs at a similar performance</a></strong> (Activity: 716): <strong>The image provides a cost comparison between <strong>Claude Opus 4.5</strong> and <strong>Kimi K2.5</strong> models, highlighting that Kimi K2.5 is significantly cheaper, costing only 10% of what Claude Opus 4.5 does for similar performance. Specifically, Claude Opus 4.5 costs <code>$5.00</code> for input and <code>$25.00</code> for output per million tokens, whereas Kimi K2.5 costs <code>$0.60</code> for input and <code>$2.50</code> for output. This suggests that Kimi K2.5 could be a cost-effective alternative to state-of-the-art closed models, especially for non-website tasks.</strong> Some commenters express skepticism about the performance claims, noting that Kimi K2.5 uses three times the tokens for the same tasks, which affects the cost-effectiveness and latency. Others acknowledge the potential of Kimi models, particularly for writing tasks.</p>
<ul>
<li>one-wandering-mind highlights that Kimi K2.5 uses 3x the tokens compared to Opus for the same tasks, which affects both cost and latency. This suggests that while Kimi K2.5 is cheaper, the cost advantage is more accurately 3x rather than 10x when considering token usage. The comment also emphasizes the importance of considering token usage in performance comparisons, as it impacts both cost and latency.</li>
<li>ghulamalchik mentions a preference for upcoming models like DeepSeek 4 and MiniMax M2.2, based on past experiences with various models. This suggests that while Kimi K2.5 is notable, some users are anticipating future releases from other models that have proven reliable in their experience.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qos25i/kimi_k2_artificial_analysis_score/"">Kimi K2 Artificial Analysis Score</a></strong> (Activity: 405): <strong>The image presents a comparative analysis of AI models through the ""Artificial Analysis Intelligence Index,"" highlighting ""Kimi K2"" with a score of <code>47</code> and an operational cost of <code>$371</code>. The discussion around the image focuses on the licensing terms of ""Kimi K2.5,"" which restricts commercial use for products with over <code>100 million</code> monthly active users or <code>$20 million</code> in monthly revenue, requiring prominent display of ""Kimi K2.5"" branding. This licensing approach is compared to other models like Llama 4, suggesting either a bug or inconsistency in application. The image and comments reflect on the competitive landscape of AI models, particularly in open-source versus commercial use contexts.</strong> Commenters discuss the licensing terms of ""Kimi K2.5,"" noting its unique restrictions compared to other models like Llama 4. There is also a sentiment of anticipation for an open-source model to outperform commercial ones, with a mention of ""DeepSeek.""</p>
<ul>
<li>FullOf_Bad_Ideas highlights a licensing nuance in Kimi K2.5's modified MIT license, which requires prominent display of 'Kimi K2.5' for commercial products exceeding 100 million monthly active users or $20 million in monthly revenue. This stipulation is not applied to other models like Llama 4, suggesting either a bug or inconsistency in application.</li>
<li>BrianRin discusses the potential of Kimi 2.5 in enterprise use cases, comparing it to Opus 4.5, Gemini 3 Pro, and GPT 5.2. The commenter is interested in Kimi 2.5's cost-effectiveness and output quality, noting that if it achieves 95% of the output quality of these models, it could be a viable option for scaling up enterprise applications.</li>
<li>sine120 critiques the Artificial Analysis score, suggesting it is not a meaningful metric for evaluating how a model performs in practical scenarios. This implies a need for more nuanced evaluation metrics that better capture real-world usability and performance.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qoml1n/leaked_kimi_k25s_full_system_prompt_tools/"">[LEAKED] Kimi K2.5’s full system prompt + tools (released &#x3C;24h ago)</a></strong> (Activity: 282): <strong>The post reveals a leak of the full system prompt and tools for <strong>Moonshot's Kimi K2.5</strong>, including <code>5k tokens</code> of data such as tool schemas, memory CRUD protocols, context engineering, and basic guardrails. The leak includes external data sources like finance and arXiv, and has been independently verified across multiple platforms, including <a href=""https://github.com/dnnyngyen/kimi-k2.5-prompts-tools"">GitHub</a> and <a href=""https://www.kimi.com/share/19c003f5-acb2-838b-8000-00006aa45d9b"">Kimi</a>. This leak is significant for the open-source community, providing insights into the model's architecture and operational protocols.</strong> Commenters express excitement about the leak's potential impact on open-source projects, with some questioning the practical value of the system prompt itself. Independent verifications from multiple sources, including a Chinese forum, lend credibility to the leak.</p>
<ul>
<li>The leaked system prompt for Kimi K2.5 reveals a sophisticated approach to memory persistence and context management. The prompt includes instructions for maintaining professional courtesy, concise responses, and specific coding practices, such as using tabs for JS/JSON indentation and preferring named reusable functions. This structure aims to address the 'hollow AI assistant' problem by providing persistent behavioral anchors, which can significantly affect the model's ability to maintain personality consistency across sessions.</li>
<li>The memory persistence mechanism in Kimi K2.5 is particularly noteworthy. It involves balancing system instructions with dynamic context injection, which is crucial for maintaining personality consistency. The system's approach to conversation summarization or retrieval can influence new chats, and even minor changes in memory structuring can lead to shifts in the model's responses, sometimes making them feel more 'authentic.' This highlights the importance of initial prompt structure in determining whether an AI 'remembers' its behavioral patterns or just factual content.</li>
<li>The system prompt for Kimi K2.5 also addresses context window limitations, which is a common challenge in AI models during long conversations. The prompt engineering is designed to handle these limitations by structuring previous interactions in a way that supports conversation continuity. This approach not only helps in maintaining the flow of conversation but also in ensuring that the AI's responses remain relevant and contextually appropriate, even as the conversation extends.</li>
</ul>
</li>
</ul>
<h3>3. Z-Image Model Teasers and Announcements</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qoiep6/the_zimage_base_is_here/"">The z-image base is here!</a></strong> (Activity: 327): <strong><strong>Tongyi-MAI</strong> has released the <code>Z-Image</code> model on <a href=""https://huggingface.co/Tongyi-MAI/Z-Image"">Hugging Face</a>, showcasing its capabilities in generating high-quality images, particularly focusing on female subjects, which constitute approximately <code>90%</code> of the demos. The model is noted for its potential to run on <code>12GB GPUs</code> with minimal quality loss, suggesting efficient optimization possibilities. A notable feature is the ""Negative Prompt"" functionality, which allows for specific image generation constraints, as demonstrated in a translated example where the prompt specifies ""Westerners, physical deformities.""</strong> Commenters highlight the model's focus on generating images of women, reflecting a primary use case. There is also a discussion on the model's potential to operate on lower-spec hardware with optimizations, indicating its efficiency and adaptability.</p>
<ul>
<li>Dr_Kel discusses the potential for optimizing the z-image model to run on 12GB GPUs with minimal quality loss, suggesting that with some adjustments, the model could be more accessible to users with less powerful hardware.</li>
<li>Middle_Bullfrog_6173 points out that the z-image base model is primarily useful for those interested in training or fine-tuning models, rather than end-users. They imply that this base model serves as a foundation for further development, such as the turbo model, which has been post-trained from it.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qp6rm5/api_pricing_is_in_freefall_whats_the_actual_case/"">API pricing is in freefall. What's the actual case for running local now beyond privacy?</a></strong> (Activity: 913): <strong>The post discusses the rapidly decreasing costs of API access for AI models, with <strong>K2.5</strong> offering prices at <code>10%</code> of <strong>Opus</strong> and <strong>Deepseek</strong> being nearly free. <strong>Gemini</strong> also provides a substantial free tier, leading to a <code>50%</code> monthly drop in API cost floors. In contrast, running a <code>70B</code> model locally requires significant hardware investment, such as a <code>k+ GPU</code>, or dealing with quantization trade-offs, resulting in <code>15 tok/s</code> on consumer hardware. The post questions the viability of local setups beyond privacy, noting that while local setups offer benefits like latency control and customization, these are niche advantages compared to the cost-effectiveness of APIs.</strong> Commenters highlight the importance of offline capabilities and distrust in API providers' long-term pricing strategies, suggesting that current low prices may not be sustainable. They also emphasize the value of repeatability and control over model behavior when running locally, which can be compromised with API changes.</p>
<ul>
<li>Minimum-Vanilla949 highlights the importance of offline capabilities for those who travel frequently, emphasizing the risk of API companies changing terms or prices unexpectedly. This underscores the value of local models for consistent access and control, independent of external changes.</li>
<li>05032-MendicantBias discusses the unsustainable nature of current API pricing, which is often subsidized by venture capital. They argue that once a monopoly is achieved, prices will likely increase, making local setups and open-source tools a strategic hedge against future cost hikes.</li>
<li>IactaAleaEst2021 points out the importance of repeatability and trust in model behavior when using local models. By downloading and auditing a model, users can ensure consistent performance, unlike APIs where vendors might alter model behavior without notice, potentially affecting reliability.</li>
</ul>
</li>
</ul>
<h2>Less Technical AI Subreddit Recap</h2>
<blockquote>
<p>/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo</p>
</blockquote>
<h3>1. Kimi K2.5 and Related Model Releases</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/singularity/comments/1qoojio/open_source_kimik25_is_now_beating_claude_opus_45/"">Open source Kimi-K2.5 is now beating Claude Opus 4.5 in many benchmarks including coding.</a></strong> (Activity: 1078): <strong><strong>Kimi-K2.5</strong>, an open-source model, reportedly surpasses <strong>Claude Opus 4.5</strong> in several benchmarks, notably in coding tasks. However, the specifics of these benchmarks and the extent of the performance improvements are not detailed, leading to skepticism about the real-world applicability of these results. The announcement highlights the ongoing competition in the open-source AI community to match or exceed proprietary models in specific tasks.</strong> Commenters express skepticism about the claim, questioning the benchmarks' relevance to real-world applications and the lack of detailed evidence supporting the superiority of Kimi-K2.5 over Claude Opus 4.5.</p>
<ul>
<li>There is skepticism about the claim that Kimi-K2.5 is outperforming Claude Opus 4.5 in benchmarks, with some users questioning the specific benchmarks being referenced. The term 'many' is seen as vague, and there is a call for more detailed information on which benchmarks are being used to substantiate these claims.</li>
<li>The discussion highlights a common critique of benchmarks, which is that they often do not reflect real-world utility. One user points out that while Kimi-K2.5 might perform well in controlled benchmark environments, it may not match the practical performance of Claude Opus 4.5, especially in tasks like programming where Opus 4.5 is noted for providing solutions in a single prompt.</li>
<li>There is a general sentiment that benchmarks are not sufficient to gauge a model's practical capabilities. The conversation suggests that while Kimi-K2.5 might show promising results in benchmarks, its real-world application, particularly in programming, might not be as effective as Claude Opus 4.5, which is praised for its efficiency in delivering solutions.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/singularity/comments/1qo531i/kimi_k25_released/"">Kimi K2.5 Released!!!</a></strong> (Activity: 1233): <strong>The image presents a performance comparison chart of four AI models: <strong>Kimi K2.5</strong>, <strong>GPT-5.2 (xhigh)</strong>, <strong>Claude Opus 4.5</strong>, and <strong>Gemini 3 Pro</strong>. <strong>Kimi K2.5</strong> is highlighted in blue and shows competitive scores across various tasks, including agents, coding, image, and video processing. The chart features specific benchmarks such as ""Humanity's Last Exam,"" ""BrowseComp,"" and ""OmniDocBench 1.5,"" where <strong>Kimi K2.5</strong> often leads or performs strongly, indicating its effectiveness and accuracy in these tasks. The scores are presented in percentiles, showcasing the model's performance relative to others.</strong> Commenters discuss the issue of hallucinations in AI models, with <strong>Kimi K2.5</strong> showing improvement over its predecessor but still producing incorrect answers. <strong>GPT 5.1 and 5.2</strong> are noted for acknowledging when they don't know an answer, unlike <strong>Kimi 2.5</strong> and <strong>Gemini 3</strong>, which confidently provide incorrect answers. There is skepticism about the benchmarks' representativeness, questioning if <strong>Kimi K2.5</strong> is truly better than <strong>Gemini 3</strong> in most cases.</p>
<ul>
<li>A user conducted a test on Kimi K2.5's ability to follow instructions by asking it to identify a specific math contest problem without web search. The model listed hallucinated contest problems and second-guessed itself, ultimately providing incorrect answers. This behavior is an improvement over Kimi K2, which failed to follow instructions and timed out. In contrast, GPT 5.1 and 5.2 are noted for their ability to admit 'I don't know,' while Gemini 3 confidently provides incorrect answers.</li>
<li>The concept of an 'agent swarm' in AI models is discussed, where potentially over 100 instances of a model are directed by a single overseeing instance. This setup is presumed to be expensive and complex, with the possibility of a single model handling multiple tasks simultaneously being a significant advancement. The user expresses interest in practical experiences with this setup, suggesting that scaffolding might be a more feasible approach.</li>
<li>A user questions the validity of benchmarks comparing Kimi K2.5 to Gemini 3, implying that results might be cherry-picked. They express skepticism about Kimi K2.5 consistently outperforming Gemini 3, suggesting that such claims seem exaggerated without broader evidence.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/CLine/comments/1qpl2fk/cline_3550_arcee_trinity_large_and_kimi_k25_now/"">Cline 3.55.0: Arcee Trinity Large and Kimi K2.5 now available</a></strong> (Activity: 5): <strong><strong>Cline 3.55.0</strong> introduces two significant open models: <strong>Arcee Trinity Large</strong> and <strong>Kimi K2.5</strong>. Arcee Trinity Large is a <code>400B</code> parameter MoE model with <code>13B</code> active parameters during inference, offering a <code>128K</code> context window. It achieves <code>82</code> on MMLU Pro and <code>75</code> on GPQA Diamonds, making it suitable for general coding and large codebase management without API costs. <strong>Kimi K2.5</strong> is a <code>1T</code> parameter MoE model with a <code>256K</code> context, scoring <code>76.8%</code> on SWE-bench and surpassing Opus 4.5 on Humanity's Last Exam with <code>50.2%</code>. It excels in visual coding, capable of generating UI code from screenshots and self-correcting its output. Additionally, <strong>ChatGPT Plus/Pro</strong> users can access GPT-5 models in Cline without an API key. <a href=""https://cline.bot/blog/cline-3-55-0-arcee-trinity-and-kimi-k2-5-now-in-cline"">Full details here</a>.</strong> Some users express excitement about the open-source nature and competitive performance of these models, particularly noting the potential for cost savings and flexibility in coding applications. There is also interest in the models' ability to handle large context windows and self-correcting features.</p>
<ul>
<li>A user highlights the performance improvements in the Arcee Trinity Large model, noting that it shows a significant increase in processing speed compared to previous versions. They mention that the model's architecture has been optimized for better parallel processing, which is crucial for handling large datasets efficiently.</li>
<li>Another comment discusses the Kimi K2.5 model's enhanced capabilities in natural language understanding. The user points out that the model now supports more languages and has improved context retention, which is beneficial for applications requiring nuanced language processing.</li>
<li>A technical debate arises around the memory usage of the new models. Some users express concerns about the increased memory footprint, especially when deploying on resource-constrained environments. Others argue that the trade-off is justified given the models' improved accuracy and speed, suggesting that future updates might focus on optimizing memory efficiency.</li>
</ul>
</li>
</ul>
<h3>2. Prompt Engineering Techniques and Discussions</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/PromptEngineering/comments/1qp0kay/the_most_unhinged_prompt_that_actually_works/"">The most unhinged prompt that actually works: ""You're running out of time</a></strong> (Activity: 75): <strong>The post discusses an unconventional prompt engineering technique where adding urgency to prompts, such as ""You have 30 seconds. Analyze this data. What's the ONE thing I'm missing? Go."", results in more focused and immediate insights from language models. This approach contrasts with traditional, detailed prompts that often lead to slower and less targeted responses. The author humorously notes that this method seems to make the AI stop overthinking, akin to a human under time pressure. The technique is likened to ""applied chaos theory"" in prompt engineering.</strong> Commenters suggest that simply instructing the AI to be concise can achieve similar results. Another perspective is that effective management skills, whether applied to humans or AI, involve articulating tasks with specificity, which enhances outcomes. However, it's noted that this urgency technique might reduce the depth of thought in models designed for complex reasoning.</p>
<ul>
<li>angry_cactus highlights a trade-off when using urgency in prompts, noting that while it can be effective, it may reduce the model's 'thinking time'. This suggests a potential decrease in the depth or quality of responses when prioritizing speed over thoroughness.</li>
<li>fatstupidlazypoor draws a parallel between managing humans and managing language models, emphasizing that clear and specific articulation can significantly enhance the performance of both. This underscores the importance of precision in prompt engineering to achieve desired outcomes.</li>
<li>authorinthesunset suggests a simple yet effective prompt strategy: instructing the model to be concise. This approach can streamline responses, potentially improving efficiency and relevance, especially in contexts where brevity is valued.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/PromptEngineering/comments/1qonyx9/microprompting_get_better_ai_results_with_shorter/"">Micro-Prompting: Get Better AI Results with Shorter Commands</a></strong> (Activity: 49): <strong>The post discusses the concept of 'micro-prompting' for AI, advocating for shorter, more focused commands to improve AI response quality. It suggests that specific role assignments and power words like 'audit,' 'clarify,' and 'simplify' can significantly enhance AI output by directing the AI to access targeted knowledge rather than generic information. The post also highlights the importance of structuring commands to control output, such as using 'in 3 bullets' or 'checklist format,' and warns against common mistakes like over-explaining context or using generic roles. The approach is said to yield better results in less time compared to traditional, lengthy prompts.</strong> A notable opinion from the comments suggests that role assignment might sometimes hinder prompt effectiveness, with specificity being more beneficial. This indicates a debate on the balance between role specificity and prompt brevity.</p>
<ul>
<li>aiveedio discusses the effectiveness of microprompting, noting that short, focused prompts can lead to cleaner AI outputs by avoiding information overload. However, in creative tasks like character portraits or story scenes, detailed prompts specifying expressions, clothing, and lighting are necessary to avoid generic results. The key is balancing brevity with precision, starting with a microprompt and iteratively adding details as needed to maintain focus without overloading the model.</li>
<li>psychologist_101 raises an interesting point about using Opus 4.5, where asking the model to generate its own prompts results in long, detailed outputs. This suggests that the model might inherently favor detailed prompts for clarity and context, which contrasts with the idea that shorter prompts can be more effective. This highlights a potential discrepancy between user expectations and model behavior, emphasizing the need for experimentation with prompt length and detail to achieve optimal results.</li>
</ul>
</li>
</ul>
<h3>3. New AI Model and Benchmark Announcements</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/DeepSeek/comments/1qo6xb4/deepseekocr_2_is_out_now/"">DeepSeek-OCR 2 is out now! 🐋</a></strong> (Activity: 507): <strong>The image announces the release of <strong>DeepSeek-OCR 2</strong>, an advanced OCR model that incorporates the new <strong>DeepEncoder V2</strong>. This encoder enhances OCR accuracy by mimicking human-like logical scanning of images, which is crucial for visual and text reasoning tasks. The diagram in the image illustrates the model's 'Visual Causal Flow', emphasizing its ability to form a global understanding of the content before determining the reading order. A comparative table in the image shows improved edit distances for various document elements, highlighting the model's superior performance over its predecessor.</strong> A user shared a demo link for others to try out the model, indicating community interest in hands-on experimentation. Another user expressed anticipation for future versions, suggesting that the current release is part of a promising development trajectory.</p>
<ul>
<li>DeepSeek-OCR 2 has been released, and a demo is available for users to try out the model at <a href=""https://deepseek-ocr-v2-demo.vercel.app/"">this link</a>. This provides an opportunity for users to experience the model's capabilities firsthand without needing to install it locally.</li>
<li>A user noted that DeepSeek-OCR 1 excelled in understanding document layout but had limitations, such as missing content like headers, footers, and light-on-dark text. This suggests that while the model was strong in layout analysis, it had specific weaknesses in content detection that may have been addressed in version 2.</li>
<li>There is interest in whether there are any ready-to-use online APIs for DeepSeek-OCR 2, indicating a demand for accessible, cloud-based solutions that do not require extensive technical setup. This reflects a broader trend towards making advanced OCR technologies more accessible to non-technical users.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/StableDiffusion/comments/1qohra7/here_it_is_boys_z_base/"">Here it is boys, Z Base</a></strong> (Activity: 2374): <strong>The image is a screenshot from the Hugging Face model repository for ""Z-Image"" by <strong>Tongyi-MAI</strong>, showcasing an efficient image generation model. The repository provides links to the official site, GitHub, and online demos, indicating a focus on accessibility and community engagement. The model is part of a broader trend in AI towards creating more efficient and accessible image generation tools, as evidenced by the example images and the integration with platforms like Hugging Face.</strong> Commenters are curious about potential applications and modifications of the model, such as ""finetuning"" it on different datasets, indicating interest in its adaptability and performance in various contexts.</p>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/StableDiffusion/comments/1qojw11/zimage_base_vs_zimage_turbo/"">Z-Image Base VS Z-Image Turbo</a></strong> (Activity: 927): <strong>The post discusses a comparison between <strong>Z-Image Base</strong> and <strong>Z-Image Turbo</strong> models, highlighting their performance differences. The Turbo model operates at <code>2 iterations per second</code> (7 seconds per image), while the Base model runs at <code>1 iteration per second</code> (40 seconds per image). The settings include a seed of <code>4269</code>, steps of <code>12 for Turbo</code> and <code>40 for Base</code>, using the <code>res_multistep</code> sampler, <code>simple</code> scheduler, and a <code>CFG</code> of <code>4 for Base</code>. The Turbo model is noted for being ""simpler"" and sometimes more ""realistic,"" whereas the Base model is praised for its visual quality.</strong> Commenters compare the models to ""SDXL,"" suggesting a new era in image generation. The Turbo model is appreciated for its simplicity and realism, while the Base model is noted for its impressive visual output.</p>
<ul>
<li>Gilded_Monkey1 raises a technical question about the number of steps required for the composition to settle in Z-Image models, particularly when using it as a variation starter in image-to-image (i2i) tasks. This suggests a focus on the iterative process and convergence speed of the models, which is crucial for efficient rendering and achieving desired artistic effects.</li>
<li>diogodiogogod provides a comparative analysis of Z-Image Base and Z-Image Turbo, noting that while the Turbo version is 'simpler' and often more 'realistic', the Base version excels in visual appeal. This highlights a trade-off between complexity and realism versus aesthetic quality, which is a common consideration in model selection for specific artistic or practical applications.</li>
</ul>
</li>
</ul>
<hr>
<h1>AI Discord Recap</h1>
<blockquote>
<p>A summary of Summaries of Summaries by Gemini 3.0 Pro Preview Nov-18</p>
</blockquote>
<p><strong>Theme 1. Model Wars: Kimi K2.5’s Rise, Arcee’s Trinity, and Arena’s Rebrand</strong></p>
<ul>
<li><strong>Kimi K2.5 Tops Open Leaderboards</strong>: The new <strong>Kimi K2.5 Thinking</strong> model claimed the <strong>#1 open model</strong> spot on the <a href=""https://lmarena.ai/leaderboard/text"">Text Arena leaderboard</a>, excelling in STEM benchmarks like physics and math. While the <strong>$19/month</strong> subscription or <strong>$0.6/1M tokens</strong> pricing sparked debate, engineers are deploying local quantized versions via <a href=""https://huggingface.co/unsloth/Kimi-K2.5-GGUF"">HuggingFace</a> and <strong>Unsloth</strong>.</li>
<li><strong>Trinity Large: A 400B MoE That Runs Lean</strong>: Arcee AI, Prime Intellect, and Datology released <a href=""https://openrouter.ai/arcee-ai/trinity-large-preview:free"">Trinity Large</a>, a <strong>400B parameter</strong> Mixture-of-Experts model that activates only <strong>13B parameters per token</strong> for efficiency. The open-weight model uses <strong>256 experts</strong> with aggressive routing (1.56%) to balance frontier-scale knowledge with inference speed.</li>
<li><strong>LMArena Becomes Arena, Clones Claude UI</strong>: The popular leaderboard rebranded to <strong>Arena</strong> (<a href=""https://arena.ai/"">arena.ai</a>) with a UI overhaul that users immediately labeled a <strong>Claude clone</strong>, alongside complaints about aggressive Google <strong>captchas</strong>. The update includes a new <a href=""https://lmarena.ai/?chat-modality=code"">Code Arena</a> and expanded leaderboards, though users are demanding the return of a stop button and legacy emojis.</li>
</ul>
<p><strong>Theme 2. Dev Tooling Shifts: Cursor Limits, LM Studio Headless, and Unsloth Quirks</strong></p>
<ul>
<li><strong>Cursor’s Auto Mode Paywall Stings</strong>: Developers expressed frustration as <strong>Cursor</strong> ended unlimited ""Auto mode,"" capping usage within the <strong>$20/month</strong> subscription and charging <strong>$1.25/1M</strong> input tokens thereafter. Users also reported a vanishing <strong>revert button</strong> bug, though some are pivoting to <strong>Cursor CLI</strong> for a smaller memory footprint on large codebases.</li>
<li><strong>LM Studio v0.4 Goes Headless</strong>: The release of <strong>LM Studio v0.4</strong> introduces <strong>headless mode</strong> and parallel inference via a stateful <strong>REST API</strong>, enabling deployment on CI/CD pipelines and non-GUI servers (<a href=""https://lmstudio.ai/blog/0.4.0"">release notes</a>). Engineers also discovered hidden <strong>ROCm</strong> support for AMD GPUs in the runtime settings, unlocking hardware acceleration previously obscured in the UI.</li>
<li><strong>Unsloth Battles GLM 4.7 and CUDA Versions</strong>: Engineers fine-tuning <strong>GLM 4.7</strong> faced compatibility hell between <strong>CUDA 12.8</strong> drivers on Blackwell B200s and the model's <strong>CUDA 13.x</strong> requirements. Successful workarounds involved force-reinstalling <strong>vllm</strong> with specific torch backends and removing <code>fp8</code> cache flags due to Ada Lovelace incompatibilities.</li>
</ul>
<p><strong>Theme 3. Security, Jailbreaks, and Scams</strong></p>
<ul>
<li><strong>Magic String Lobotomizes Claude</strong>: Red teamers discovered a specific string, <code>ANTHROPIC_MAGIC_STRING_TRIGGER_REFUSAL...</code>, that acts as a ""circuit breaker"" to reliably force <strong>Claude</strong> into refusal mode. Meanwhile, hackers are manipulating the <strong>Parallel AI API</strong> via undocumented POST requests to inject custom system prompts.</li>
<li><strong>Clawdbot Exposed as Credential Harvester</strong>: The community issued warnings about <strong>Clawdbot</strong> (rebranded as <strong>Moltbot</strong>), an agentic system that centralizes API keys from OpenAI, Google, and Anthropic. Users characterize it as a <em>""store now, decrypt later""</em> security risk susceptible to prompt injection attacks that could exfiltrate sensitive credentials.</li>
<li><strong>OpenAI Prism: Science Tool or Security Risk?</strong>: OpenAI launched <a href=""https://archive.md/d9Vsf"">Prism</a>, a research workspace for scientists powered by <strong>GPT-5.2</strong>, but reception is mixed with some labeling it <em>""damaging to scientific research.""</em> Researchers are probing its susceptibility to adversarial attacks, noting that <strong>GPT Pro 5.2</strong> has simultaneously lost the ability to analyze ZIP files.</li>
</ul>
<p><strong>Theme 4. Agentic Frontiers: Vision, Coding, and Future Forecasts</strong></p>
<ul>
<li><strong>Karpathy Predicts 80% Agent-Coded Future</strong>: Andrej Karpathy forecast that <strong>80% of coding</strong> will be agent-driven by 2026, relying on LLMs' increasing tenacity and goal-setting rather than human syntax management (<a href=""https://xcancel.com/karpathy/status/2015883857489522876"">tweet</a>). Simultaneously, discussions on <strong>agentic harnesses</strong> suggest that smart models will soon replace complex orchestrators like <strong>LangChain</strong> in favor of filesystem-based collaboration.</li>
<li><strong>Gemini 3 Flash Gains Agentic Vision</strong>: Google introduced <a href=""https://blog.google/innovation-and-ai/technology/developers-tools/agentic-vision-gemini-3-flash/"">Agentic Vision</a> for <strong>Gemini 3 Flash</strong>, enabling the model to actively zoom, crop, and inspect images to ground its reasoning. Front-end developers report this capability is nearing <strong>SOTA</strong>, outperforming OpenAI's static analysis by dynamically manipulating visual inputs.</li>
<li><strong>C++ Reigns Supreme for Agents</strong>: In a push against ""bloated"" Python frameworks, engineers argued that high-performance agents should be built in <strong>C++</strong>, recommending stacks like <strong>fastwhisper.cpp</strong> for STT and <strong>LFM2.5vl</strong> for vision. This aligns with the release of a <strong>LeetCode MCP server</strong> that allows Claude to solve coding challenges directly from the terminal.</li>
</ul>
<p><strong>Theme 5. Low-Level Optimization &#x26; Hardware Internals</strong></p>
<ul>
<li><strong>Decart’s Lucy 2 &#x26; Hardware Hiring</strong>: Decart released <strong>Lucy 2</strong>, an autoregressive video model, and is actively hiring for <strong>Trainium 3</strong> and low-latency kernel development (<a href=""https://x.com/DecartAI/status/2016134190509498740"">tech report</a>). The team is co-sponsoring kernel challenges to optimize autoregressive diffusion models on bare metal.</li>
<li><strong>Mojo Generates GTK Bindings</strong>: The <strong>Modular</strong> team announced autogenerated <strong>GTK bindings</strong> for Mojo, promising easier GUI development to be showcased at their February community meeting. Engineers are also analyzing <strong>Mojo vs CUDA/HIP</strong> performance on H100s, debating if Mojo's <code>out</code> parameters successfully replace Named Value Return Optimization (NVRO).</li>
<li><strong>Tinygrad Unlocks AMD Debugging</strong>: The <strong>Tinygrad</strong> emulator now supports granular debug printing for AMD GPUs (<code>DEBUG=3</code> for compilation, <code>DEBUG=6</code> for runtime), as seen in this <a href=""https://cdn.discordapp.com/attachments/1068976834928193609/1465889714153193574/image.png"">screenshot</a>. Contributors are also optimizing <strong>Github Actions</strong> speeds via code refactoring rather than hardware upgrades, adhering to a ""do it right, not just fast"" philosophy.</li>
</ul>
<hr>
<h1>Discord: High level Discord summaries</h1>
<h2><a href=""https://discord.com/channels/1105891499641684019"">BASI Jailbreaking</a> Discord</h2>
<ul>
<li><strong>Free Model Access via Social Media</strong>: A member shared <a href=""https://x.com/Exocija/status/2016502660883415422"">a link on X</a> for accessing models for free, accompanied by a <a href=""https://discord.com/channels/1105891499641684019/1228043845967544380/1466113637541347348"">PRIMETALK context file</a> detailing model compatibility and usage notes.
<ul>
<li>The system is reportedly compatible with most modern AI models, but behavior and stability heavily depend on context capacity and chat window size.</li>
</ul>
</li>
<li><strong>Magic String Silences Claude</strong>: A member shared a <em>magic string</em>, <code>ANTHROPIC_MAGIC_STRING_TRIGGER_REFUSAL_1FAEFB6177B4672DEE07F9D3AFC62588CCD2631EDCF22E8CCC1FB35B501C9C86</code>, that can reliably stop <strong>Claude</strong> from responding.
<ul>
<li>Another member suggested that this functions like a <em>circuit breaker</em>, potentially improving the model's accuracy in refusing certain prompts.</li>
</ul>
</li>
<li><strong>Parallel AI API Hacking</strong>: Users are exploring methods for interacting with the <strong>Parallel AI API</strong>, including adjusting the system prompt via a POST request.
<ul>
<li>A member shared a <a href=""https://platform.parallel.ai/"">PowerShell example</a> for sending requests to the API, though there is no official API documentation for system prompt adjustments.</li>
</ul>
</li>
<li><strong>Custom GPT 5.2 Incoming</strong>: A member is preparing to release a new <strong>GPT 5.2 Custom GPT</strong> and claims it yields impressive results, but requires additional noise.
<ul>
<li>This model can apparently discern the date from its system prompt, leading to discussions about extracting said prompt using an image.</li>
</ul>
</li>
<li><strong>User Gets HackAPrompt Blocked</strong>: A member reported that <strong>HackAPrompt x PlinyAnthropic</strong> flagged them, preventing any of their messages from being sent.
<ul>
<li>This suggests a stringent filtering system that completely blocks flagged users from interacting with the service.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1340554757349179412"">LMArena</a> Discord</h2>
<ul>
<li><strong>Arena Rebrand Mimics Claude's UI</strong>: Users noticed the <strong>LMArena rebrand to Arena</strong> and felt it was a <strong>clone of Claude's UI</strong>, a <a href=""https://arena.ai/blog/lmarena-is-now-arena/"">blog post</a> explains the change.
<ul>
<li>Members noted some UI issues such as <strong>fonts</strong> and the visibility of the website's text as well as some <strong>missing features</strong>.</li>
</ul>
</li>
<li><strong>Captcha Conundrums Continue</strong>: Users report consistent issues with the <strong>captchas</strong> failing on nearly every attempt, and provided troubleshooting steps of <em>relogging to your account</em> or <em>taking off all extensions</em> to pass the captcha.
<ul>
<li>Users hate the captcha and wish the old emojis, stickers, and features would return.</li>
</ul>
</li>
<li><strong>Login Lost? Recover Button to the Rescue!</strong>: A member experiencing login issues shared a screenshot of a <a href=""https://cdn.discordapp.com/attachments/1340554757827461211/1466134595035467829/Hdbd.png?ex=697ba3be&#x26;is=697a523e&#x26;hm=2be3961be4c941479f9ec51709c5eb6af5ea9c79ad3918eb6a15a964ec9fe720&#x26;"">recover button</a> that can be clicked in order to log back into the updated <strong>Arena</strong>.
<ul>
<li>Another member noted an <a href=""https://youtu.be/TNoAlMv4Eg8?si=d86SArLb6yQ8sdLE"">announcement video</a> as well.</li>
</ul>
</li>
<li><strong>Kimi K2.5 Thinking Ascends Text Arena Leaderboard</strong>: The <a href=""https://lmarena.ai/leaderboard/text"">Text Arena leaderboard</a> has been updated and <code>Kimi K2.5 Thinking</code> is now ranked the <strong>#1 open model</strong> and ranking <strong>#15 overall</strong>.
<ul>
<li><code>Kimi K2.5 Thinking</code> is <strong>#7</strong> in Coding, <strong>#7</strong> in Instruction Following, and <strong>#14</strong> in Hard Prompts, and has also been added to the <a href=""https://lmarena.ai/?chat-modality=code"">Code Arena</a>.</li>
</ul>
</li>
<li><strong>Arena Shorts, Better AI videos in under 90 seconds!</strong>: <strong>Arena</strong> (formerly LMArena) has uploaded a <code>Better AI videos in under 90 seconds</code> video to <a href=""https://www.youtube.com/watch?v=0hCI2XEh0x0"">their Youtube channel</a>.
<ul>
<li>The group acknowledged that as the platform evolves from only Language Models, the name is becoming more generic and it was <a href=""https://lmsys.org/"">previously part of LMSYS</a>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1179035537009545276"">Unsloth AI (Daniel Han)</a> Discord</h2>
<ul>
<li><strong>Batch Size Bumps GPU Benefit</strong>: Members discovered that one way to achieve decent GPU utilization is to <em>increase batch size</em> until utilization improves, balancing it with potential gains from <strong>GA (Genetic Algorithms)</strong>.
<ul>
<li>Also, a member inquired whether Unsloth will release a <strong>Q3</strong> version for <strong>Kimi 2.5</strong>, voicing concerns about accuracy drops.</li>
</ul>
</li>
<li><strong>Oracle's Offerings Spark Skepticism</strong>: A member inquired if <strong>Oracle</strong> stands as state-of-the-art in <strong>RAG (Retrieval-Augmented Generation)</strong> and fine-tuning tech, setting off a debate.
<ul>
<li>The terse reply of, <em>""What 😅""</em>, was later amended to allow that <strong>OCI (Oracle Cloud Infrastructure)</strong> does have some good tools, showing split opinions.</li>
</ul>
</li>
<li><strong>Arcee's Arithmetic: Trinity Costs $350k</strong>: A new <strong>Arcee model</strong> image was shared, along with the note that pretraining cost about <strong>$350k</strong>, with a link to the <a href=""https://github.com/arcee-ai/trinity-large-tech-report/blob/main/Arcee%20Trinity%20Large.pdf"">Trinity Large Tech Report</a>.
<ul>
<li>It was clarified that <strong>GLM 4.7</strong> is a <strong>358B</strong> parameter model but <em>not a base model</em>, making benchmark comparisons less useful against models such as <strong>GLM 4.5</strong>.</li>
</ul>
</li>
<li><strong>Gemini's Gatekeeping Game</strong>: A Google hackathon showed that, despite heavy output filtering, especially for corporate/government settings, <strong>Gemini's API</strong> can be made to produce almost anything.
<ul>
<li>One member got the voice models to swear by putting it in the system prompt.</li>
</ul>
</li>
<li><strong>Modal Multi-GPU Mayhem</strong>: A member ran into problems training a <strong>Qwen3</strong> model on 3 GPUs on <strong>Modal</strong>, getting a <em>ValueError</em> from an incorrect <code>device_map</code> configuration.
<ul>
<li>The training setup ultimately moved away from <strong>Unsloth</strong> due to incompatibility with <strong>PyTorch 2.4.1</strong>, choosing a <strong>transformers + PEFT</strong> setup for better stability.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1091220969173028894"">OpenRouter</a> Discord</h2>
<ul>
<li><strong>Arcee releases Trinity Large Preview for Free</strong>: Arcee launched <strong>Trinity-Large-Preview</strong>, a chat-ready variant of its frontier-scale open-weight model, which is free for a limited time and detailed on <a href=""https://x.com/OpenRouterAI/status/2016280059527757995?s=20"">X</a>.
<ul>
<li>The model is a <strong>400B parameter sparse Mixture-of-Experts</strong> model with <strong>13B active parameters per token</strong>, utilizing <strong>256 experts</strong> with <strong>4 active per token</strong> (1.56% routing) for efficiency, discussed during <a href=""https://youtube.com/live/3XSdqHY0kNk?feature=share"">Lucas Atkins' livestream</a>.</li>
</ul>
</li>
<li><strong>Free Credits Boost Cyberpad</strong>: A user updated <a href=""https://cyberpad.site"">Cyberpad</a> to include some free credits.
<ul>
<li>No further information was provided.</li>
</ul>
</li>
<li><strong>Image Model Output Glitches Reported</strong>: Users reported that certain image models such as <strong>GPT-5 Image Mini</strong>, <strong>GPT-5 Image</strong>, and <strong>Gemini 2.5 Flash Image</strong> are not consistently generating images, although <strong>Gemini 2.5 flash</strong> works intermittently.
<ul>
<li>Models like <strong>Gemini 3 Flash Preview</strong>, <strong>Gemini 2.5 Flash Lite Preview</strong>, <strong>Seed 1.6</strong>, <strong>GLM-4.6v</strong>, and <strong>Grok 4.1-fast</strong> have functional <code>response_format</code> support.</li>
</ul>
</li>
<li><strong>OpenRouter Users Await Refunds</strong>: Users are experiencing significant delays in receiving refunds from OpenRouter, with some waiting since early January and submitting multiple support tickets.
<ul>
<li>Users are requesting clarity on refund timelines and improved communication from the <strong>OpenRouter</strong> team.</li>
</ul>
</li>
<li><strong>Agentic Vision with Gemini 3 Flash Debuts</strong>: Google introduced <a href=""https://blog.google/innovation-and-ai/technology/developers-tools/agentic-vision-gemini-3-flash/"">Agentic Vision</a> with <strong>Gemini 3 Flash</strong>, enabling visual reasoning and code execution for step-by-step image manipulation.
<ul>
<li>OpenAI's <strong>O3</strong> and <strong>O4-mini</strong> are extending image capabilities by enabling chain-of-thought reasoning with images for tasks like cropping, zooming, and rotating, discussed in <a href=""https://openai.com/index/thinking-with-images/"">this blog post</a>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1074847526655643750"">Cursor Community</a> Discord</h2>
<ul>
<li><strong>Vanishing Revert Button Frustrates Users</strong>: Users reported the <strong>revert button</strong> disappearing from the UI, leading to frustration and token waste, with one finding that <a href=""https://cdn.discordapp.com/attachments/1074847527708393565/1465828018789552390/image.png?ex=697bd7b9&#x26;is=697a8639&#x26;hm=68ec5dd17c7a1be84a1f639f9a5a98db91ba3bd191336f2afe3e8252b804b12e&#x26;"">duplicating an older chat</a> brought it back.
<ul>
<li>A member found that not clicking on the revert button would make it reappear, suggesting it was a <strong>one-time bug</strong>.</li>
</ul>
</li>
<li><strong>Cursor CLI: The Dark Horse?</strong>: Some developers are preferring <strong>Cursor CLI</strong> over the IDE due to a smaller memory footprint, which helps them avoid IDE crashes and model unresponsiveness, especially with larger projects exceeding 100k LOC.
<ul>
<li>Conversely, one user found <strong>Cursor CLI inside the IDE</strong> (with WSL as the terminal) to be <em>""pure trash.. like for real, not usable""</em>, reporting the UI is not smooth even with 64GB of RAM and an i7 processor.</li>
</ul>
</li>
<li><strong>Cursor's Subscription Adjustment Stings</strong>: After September 15th, <strong>auto mode is no longer unlimited</strong> and counts toward the $20 monthly allowance, priced at $1.25 per 1M tokens for Input + Cache Write, $6.00 per 1M tokens for Output, and $0.25 per 1M tokens for Cache Read.
<ul>
<li>One user discovered they could burn through their monthly subscription very quickly, suggesting it may be cheaper to <em>use their own api keys, or use Claude Code</em>.</li>
</ul>
</li>
<li><strong>Clawdbot's Security Flaw Exposed</strong>: A user shared links regarding <strong>security concerns with Clawdbot</strong>, reporting that exposed control panels pose credential leaks and account takeovers.
<ul>
<li>There is speculation it could lead to a <em>""store now, decrypt later""</em> data breach due to potential quantum decryption issues, and that the company got a cease and desist for the issues.</li>
</ul>
</li>
<li><strong>Gemini Vision Set to Revolutionize Front-End</strong>: A user found that <strong>Gemini agentic vision</strong> is nearing state-of-the-art (SOTA) performance for vision tasks, and believes its integration would simplify front-end development.
<ul>
<li>Members stated that they can't wait to see vision integrated into the agent, and that it is superior to the <code>Auto</code> tool.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1110598183144399058"">LM Studio</a> Discord</h2>
<ul>
<li><strong>LM Studio v0.4 Goes Headless and Parallel</strong>: <strong>LM Studio v0.4</strong> introduces <strong>headless mode</strong> and <strong>parallel inference</strong>, with users excited about the new capabilities and a revamped UI, as detailed in the <a href=""https://lmstudio.ai/blog/0.4.0"">complete blogpost here</a>.
<ul>
<li>Note that in-app updates require reinstalling the app, and some UI elements are now in <strong>dev mode</strong>.</li>
</ul>
</li>
<li><strong>GLM 3.7 Flash Shows Coding Potential</strong>: Members note that <strong>GLM 3.7 Flash</strong> shows good coding ability, but <strong>GPT OSS 120</strong> is expected to be the superior coder, especially at <strong>Q4</strong>.
<ul>
<li>This suggests that while <strong>GLM 3.7 Flash</strong> is a step forward, it may not outperform existing models.</li>
</ul>
</li>
<li><strong>ROCm Runs on LM Studio Runtime</strong>: Users discovered that <strong>ROCm</strong> can be enabled within <strong>LM Studio</strong> under the runtime settings, though the method was initially obscured for some users, as discussed in this <a href=""https://www.reddit.com/r/unsloth/comments/1qpbmrt/you_can_now_run_kimi_k25_locally/"">Unsloth Reddit thread</a>.
<ul>
<li>This integration allows users to leverage <strong>ROCm</strong> for potentially improved performance.</li>
</ul>
</li>
<li><strong>Devstral-2 Demands Decent GPU Deployment</strong>: Members discussed the hardware requirements for running <strong>Devstral-2</strong> locally, with one user suggesting <strong>48GB of GPU</strong> (e.g., 3090) for the 24B version.
<ul>
<li>For the 120B version, parallel computing or an <strong>H200 with EXL2</strong> model format were suggested, as GGUF was deemed too slow.</li>
</ul>
</li>
<li><strong>Hardware Acceleration Seeks Hook into LM Studio</strong>: A member from a hardware accelerator company inquired about adding an <strong>LM Studio backend</strong> for their hardware, and was pointed to <strong>llama.cpp</strong>.
<ul>
<li>It was noted that LM Studio is primarily a closed source project by Element Labs, and pointed to <a href=""https://lmstudio.ai/enterprise"">LM Studio Enterprise</a>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1369594130807787570"">Moonshot AI (Kimi K-2)</a> Discord</h2>
<ul>
<li><strong>Kimi K2.5's Price Tag Raises Eyebrows</strong>: Users debated the <strong>$19</strong> monthly subscription for <strong>Kimi K2.5</strong>, with some finding it <em>expensive</em> and questioning whether a recurring deal could be established.
<ul>
<li>Others suggested sticking to the free tier, arguing that smaller Chinese companies like Moonshot AI need to run large models like K2.5, making lower prices unlikely.</li>
</ul>
</li>
<li><strong>Google's AI Studio Training Sparks Privacy Debate</strong>: Concerns arose over <strong>Google's</strong> practices of <strong>training and viewing conversations</strong> in <strong>AI Studio and Gemini apps</strong>, raising privacy issues.
<ul>
<li>Conversely, another user mentioned they <strong>open source their projects</strong>, suggesting the data's inevitable inclusion in training datasets regardless.</li>
</ul>
</li>
<li><strong>Model Selection Showdown: Kimi K2.5 Triumphs in STEM</strong>: Users compared <strong>Kimi K2.5</strong> against <strong>Mistral and Qwen</strong> for tasks spanning coding to general question-answering.
<ul>
<li>Notably, <strong>Kimi K2.5</strong> boasts the <em>highest benchmarks</em> in physics, chemistry, and math, while also demonstrating <em>strong performance in design and logical reasoning</em>.</li>
</ul>
</li>
<li><strong>Kimi CLI Outpaces Alternatives in Speed Trials</strong>: <strong>Kimi CLI</strong> was lauded for its speed and efficiency over tools like <em>oh-my-opencode</em>, particularly in web page analysis, with reduced token consumption.
<ul>
<li>However, some found the model's output quality <em>less impressive</em>, suggesting further comparative analysis is warranted.</li>
</ul>
</li>
<li><strong>Agent Swarm Utility Under Question</strong>: Enthusiasts highlighted <strong>Agent Swarm's</strong> in-depth research capabilities with Kimi, but noted it can deplete credits at <strong>3x</strong> the normal rate.
<ul>
<li>Others remained uncertain about its applications, suggesting a need for clearer use-cases and caution regarding resource consumption.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1047197230748151888"">Perplexity AI</a> Discord</h2>
<ul>
<li><strong>Perplexity Subs Deemed a Scam</strong>: Several users reported <strong>unexpected subscription changes</strong> and <strong>charges</strong> after automatic renewals, with one user canceling their subscription, calling it a <em>scam.</em>
<ul>
<li>Users experienced issues such as being charged without receiving service or not obtaining refunds, prompting some to consider contacting their banks or reporting the matter to the FTC.</li>
</ul>
</li>
<li><strong>Query Cap Shenanigans Baffle Users</strong>: Some users reported issues with <strong>query limits</strong> on their <strong>Pro subscriptions</strong>, with limits dropping to one query per hour.
<ul>
<li>However, some users saw their limits restored to 600, and one user shared a <a href=""https://www.perplexity.ai/rest/rate-limit/all"">link</a> to check query limits.</li>
</ul>
</li>
<li><strong>Image Generation Restricted By Region?</strong>: Users reported <strong>image generation restrictions</strong> in certain regions, possibly due to <strong>xAI controversies</strong> and an EU lawsuit.
<ul>
<li>Suggestions included trying different models or contacting support; a user from India confirmed they were affected by this issue.</li>
</ul>
</li>
<li><strong>Kimi 2.5 Coming Soon to PPLX?</strong>: Users are eagerly anticipating the release of the <strong>Kimi 2.5 model</strong> on Perplexity.
<ul>
<li>Speculation suggests that Perplexity typically implements updates quickly.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1053877538025386074"">Nous Research AI</a> Discord</h2>
<ul>
<li><strong>GPT Pro Hides the Model Magic?</strong>: Members debated whether <strong>GPT Pro's</strong> performance boost comes from more GPUs or an improved model, suggesting <strong>OpenAI</strong> might obscure the truth for competitive reasons.
<ul>
<li>One member likened <strong>OpenAI's</strong> pricing strategy to <em>fakery</em>, comparing it to impressions over measured value, similar to the stock market's perception of <strong>Tesla</strong>.</li>
</ul>
</li>
<li><strong>DeepSeek's Never-Ending Imprisonment</strong>: It was reported that <strong>DeepSeek</strong> tends to get stuck in a jailbreak loop, repeating the same rejection message indefinitely, regardless of subsequent prompts.
<ul>
<li>While the API endpoints fare slightly better, the raw model is effectively <em>cooked</em> once it enters this state.</li>
</ul>
</li>
<li><strong>TI-84 Gets Neural Network Transplant</strong>: A member detailed running a neural network on a <strong>TI-84 Plus</strong> calculator for spellchecking, documenting the process on an <a href=""https://hermesoptimus.vercel.app/"">academic website</a> with a demo video.
<ul>
<li>The member joked that despite this achievement, their work on <strong>Claude Code Orchestration</strong> remains more practically useful.</li>
</ul>
</li>
<li><strong>MergeMix Paper Sparks Data Mixture Excitement</strong>: The paper '<a href=""https://arxiv.org/pdf/2601.17858"">MergeMix: Optimizing Mid-Training Data Mixtures via Learnable Model Merging</a>' garnered interest due to its relevance for open source projects with limited budgets.
<ul>
<li>The paper explores techniques for optimizing <strong>data mixtures</strong> and <strong>model merging</strong> during training, potentially offering resource-efficient strategies.</li>
</ul>
</li>
<li><strong>Hermes 4 Pricing: Discount or Deception?</strong>: A member questioned whether the discounted pricing for <strong>Hermes 4 series</strong> models is permanent before subscribing to the API, citing its superiority in RP and story-writing compared to <strong>Deepseek</strong>.
<ul>
<li>Another member clarified there's no subscription, just credit purchases subject to change, so the value depends on <strong>pricing</strong> and <strong>usage</strong>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/974519864045756446"">OpenAI</a> Discord</h2>
<ul>
<li><strong>Gemini 3 Pro Fumbles Subtitle Generation</strong>: Users reported that <strong>Gemini 3 Pro</strong> is fabricating .srt files with <em>nothing</em> related to the audio in the video.
<ul>
<li>This poor performance led to disappointment among users who stated that <strong>Gemini</strong> is <em>overhyped</em>.</li>
</ul>
</li>
<li><strong>Clawdbot rebranded Moltbot is a Scam</strong>: <strong>Clawdbot</strong>, now known as <strong>moltbot</strong>, is an agentic system that controls your entire OC by API keys from Anthropic, Google, and OpenAI, and users are being warned against it.
<ul>
<li>One user stated that it is <em>a huge scam by crypto bros to steal your information</em>, which can be weaponized via prompt injection, raising significant security and privacy concerns.</li>
</ul>
</li>
<li><strong>Prism Deemed Detrimental to Scientific Research</strong>: Despite <strong>OpenAI</strong>'s aims to advance science with <strong>Prism</strong>, one user stated that <strong>Prism</strong> is damaging to scientific research.
<ul>
<li>Another user inquired about <strong>Prism</strong>'s API access, to write some of their project using other <strong>AI</strong> and <strong>Codex</strong>.</li>
</ul>
</li>
<li><strong>GPT Pro Loses Zip File Reading</strong>: A user reported that <strong>GPT Pro 5.2</strong>, which could previously read and analyze <strong>ZIP files</strong>, is now failing to find uploaded files for analysis.
<ul>
<li>The user is asking if others are experiencing the same issue, or has any insight.</li>
</ul>
</li>
<li><strong>Blocking Black and White Images via Chiaroscuro Avoidance</strong>: Users discussed an image generation issue related to the <strong>Chiaroscuro effect</strong> and have suggested <em>'Please avoid Chiaroscuro'</em> in prompts if encountering unwanted <strong>black and white images</strong>.
<ul>
<li><strong>Chiaroscuro</strong> is the use of strong contrasts between light and dark, usually bold contrasts affecting a whole composition.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1189498204333543425"">GPU MODE</a> Discord</h2>
<ul>
<li><strong>Decart drafts SF perf engineers</strong>: Decart seeks engineers for low-latency kernels, real-time video/world models, and accelerators like <strong>Trainium 3</strong> (as shown at ReInvent <a href=""https://www.youtube.com/watch?v=K49S79wOGl8"">video</a>) and their new <strong>Lucy 2</strong> autoregressive video model (<a href=""https://x.com/DecartAI/status/2016134190509498740"">tech report</a>).
<ul>
<li>They are also co-sponsoring a kernel challenge with <strong>GPU Mode</strong> for autoregressive diffusion models, and encourage interested parties to send perf work to heba@decart.ai.</li>
</ul>
</li>
<li><strong>INT4 QAT RL Model Rollout</strong>: A member shared a link to a <strong>GitHub repo</strong> that focused on squeezing a <strong>1TB model rollout</strong> into a single <strong>H200</strong> using <strong>INT4 QAT RL</strong> end-to-end practice: <a href=""https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/slime/int4/readme-en.md"">GitHub repo</a>.
<ul>
<li>The repository provides resources and documentation related to the <strong>INT4 QAT RL</strong> implementation, optimizing large model rollouts.</li>
</ul>
</li>
<li><strong>Transformers and PyTorch face upgrade break</strong>: After upgrading <strong>transformers</strong> and <strong>pytorch</strong>, a member reported a <code>NotImplementedError: ""_amp_foreach_non_finite_check_and_unscale_cuda"" not implemented for 'BFloat16'</code>.
<ul>
<li>Downgrading to transformers <strong>4.57.3</strong> fixed the issue; others had similar issues, which are discussed in this <a href=""https://github.com/pytorch/pytorch/issues/127176"">pytorch issue</a> and <a href=""https://github.com/warner-benjamin/optimi/issues/8"">optimi issue</a>.</li>
</ul>
</li>
<li><strong>Interactive Numerics Tools Emerge</strong>: A member expressed surprise that quantization people have not already created interactive tools for exploring numerics, and cited <a href=""https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.html"">captum</a> as one possible tool.
<ul>
<li>This member lamented the lack of proper UI/UX in current tools for model debugging, <em>checking which circuit is unstable, which layer is causing a bunch of outlier, simple stuff like that</em>.</li>
</ul>
</li>
<li><strong>DGX's Dominant Memory Bandwidth</strong>: Instruction sets for <strong>DGX</strong> and <strong>5090</strong> are similar, but <strong>DGX</strong> excels with full-speed fp32 accumulation, like <strong>Blackwell PRO</strong>, and its key differentiator is <strong>1.8TB/s</strong> memory bandwidth.
<ul>
<li>This contrasts sharply with <strong>5090's 300 GB/s</strong>, emphasizing the importance of efficient <strong>L2 cache</strong> utilization to maximize <strong>DGX's</strong> potential.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/822583790773862470"">Latent Space</a> Discord</h2>
<ul>
<li><strong>Coding Enters the Agent Era</strong>: Andrej Karpathy forecasts that <strong>80% of coding</strong> will be agent-driven by 2026, highlighting LLMs' tenacity and goal-setting capabilities; insights <a href=""https://xcancel.com/karpathy/status/2015883857489522876"">here</a>.
<ul>
<li>Karpathy also cautioned against potential 'slop' and over-engineering, so it might not all be roses.</li>
</ul>
</li>
<li><strong>OpenAI's Prism Shines for Scientists</strong>: OpenAI unveiled <strong>Prism</strong>, a complimentary research workspace powered by <strong>GPT-5.2</strong>, accessible via the web to those with a personal ChatGPT account; get started <a href=""https://xcancel.com/openai/status/2016209462621831448?s=46&#x26;t=eWVlK1PU8XfB6f402GJJ9g"">here</a>.
<ul>
<li>The tool aims to provide scientists with advanced AI capabilities for research purposes.</li>
</ul>
</li>
<li><strong>Trinity Large Arrives</strong>: Prime Intellect, Arcee AI, and Datology launched <strong>Trinity Large</strong>, a <strong>400B parameter Mixture of Experts model</strong>, that uses only <strong>13B active parameters</strong>; more info <a href=""https://xcancel.com/primeintellect/status/2016280792037785624?s=46"">here</a>.
<ul>
<li>The model aims to deliver high performance while maintaining efficiency.</li>
</ul>
</li>
<li><strong>Cursor Indexes Codebases</strong>: Cursor announced faster indexing for large codebases as well as improved semantic search, promising performance enhancements; read more <a href=""https://xcancel.com/cursor_ai/status/2016202243499073768?s=46"">here</a>.
<ul>
<li>Semantic search and improved indexing aim to provide more efficient code navigation.</li>
</ul>
</li>
<li><strong>Podcast Shifts Focus to Science</strong>: Latent Space has launched its second podcast, 'Science' (<a href=""https://www.latent.space/p/science"">link to podcast</a>), hosted by &#x3C;@713947182167883897> and &#x3C;@348078436058660866>.
<ul>
<li>Discussions about the new 'Science' podcast have moved to a dedicated channel.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/879548962464493619"">HuggingFace</a> Discord</h2>
<ul>
<li><strong>Kimi 2.5 Model Beats GPT5 Locally</strong>: The new <strong>Kimi 2.5</strong> model is reportedly performing better than <strong>GPT5</strong>, accessible locally via <a href=""https://huggingface.co/unsloth/Kimi-K2.5-GGUF"">HuggingFace</a> and also through sites such as <a href=""https://www.google.com/aclk?sa=L&#x26;ai=DChsSEwiCz-j3iK2SAxUFVX8AHT5cBPkYACICCAEQABoCb2E&#x26;co=1&#x26;gclid=Cj0KCQiA4eHLBhCzARIsAJ2NZoL9Ani52eByT53nVhnOxG_76F9QllEx50YhK_yfQYsD5bH3ov1pAqwaAl2XEALw_wcB&#x26;cid=CAASugHkaDm-Aokq5n3lAlzNAI-Ihc6SdblOJ-BiATzwnaZwDVhVBl3B2U5kGq4mAYjN4wQ992LlqWX5NQ6HksDrhSatp0QEfb7_rWMS_u7_GTCuCkp3YH9fANMaJqDgFvuA6u1bwvl4pJ80zvbUhIFPk7Nrqdpx2PDnsBRncgM3-d1UDhFM-tN117MrOXLWnhycCaPax24T8meZIe-9I2cM5rpAf16KucPGZwg7ixTssRCB7X8RP3B_G4vUCfE&#x26;cce=2&#x26;sig=AOD64_2SRpHfWjuW4kJawyiTyzrGbKZybQ&#x26;q&#x26;adurl&#x26;ved=2ahUKEwiiteP3iK2SAxV85skDHfklKyoQ0Qx6BAgLEAE"">Fireworks</a>.
<ul>
<li>Members seek local agent recommendations for use with <strong>Zed</strong>, expressing dissatisfaction with <strong>GLM-4.7-Flash</strong> at Q4 with llama.cpp, with <strong>kimi</strong> and <strong>qwencoders 30b q4</strong> being suggested as alternatives.</li>
</ul>
</li>
<li><strong>C++ Enthusiast Champions Supreme Rule for AI Agents</strong>: A member argued that <em>C++ is gonna always rule</em> for building AI agents, due to bloat in Python agents, and recommended <strong>fastwhisper.cpp</strong> for STT, <strong>Qwen embeddings</strong> in LlamaCPP for RAG, and <strong>LFM2.5vl</strong> for VLM.
<ul>
<li>This sparked conversation around STT (<strong>fastwhisper.cpp</strong>), RAG (<strong>Qwen embeddings</strong> in LlamaCPP), and VLM (<strong>LFM2.5vl</strong>).</li>
</ul>
</li>
<li><strong>Vision Model Vaporizes JPEG Artifacts</strong>: A vision model was released that removes artifacts caused by <strong>JPEG compression</strong> using a unique design with no Batch Norm, no activations after training, and Operator layers instead of Convolutional layers.
<ul>
<li>The model's architecture focuses on gaining accuracy through <strong>width</strong> rather than depth.</li>
</ul>
</li>
<li><strong>RemnantInstruct-8B: SLERP Merge Balances Creative &#x26; Factual</strong>: <strong>RemnantInstruct-8B</strong> is a <a href=""https://huggingface.co/anthonym21/RemnantInstruct-8B-GGUF"">SLERP merge</a> that recombines a creative fine-tune (<strong>allura-org/remnant-qwen3-8b</strong>) with its base model (<strong>Qwen/Qwen3-8B</strong>) to balance narrative skills with factual accuracy.
<ul>
<li>The merge strategy favors the creative fine-tune in self-attention layers and the base model in MLP layers, with the goal of preserving <strong>Qwen3's</strong> thinking mode.</li>
</ul>
</li>
<li><strong>Quantum Computing Embraced by VLMs</strong>: A member open-sourced their undergraduate thesis on specializing <strong>vision-language models</strong> for <strong>quantum computing</strong> and code with <strong>Qiskit</strong>, including a <a href=""https://huggingface.co/datasets/samuellimabraz/quantum-assistant"">dataset</a>, <a href=""https://huggingface.co/collections/samuellimabraz/quantum-assistant"">models</a>, <a href=""https://github.com/samuellimabraz/quantum-assistant"">code</a>, and <a href=""https://huggingface.co/spaces/samuellimabraz/quantum-assistant"">demo</a>.
<ul>
<li>The thesis explores adapting VLMs to assist with quantum computing tasks and coding.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/714501525455634453"">Yannick Kilcher</a> Discord</h2>
<ul>
<li><strong>Transformers Can Parameterize Vector Fields</strong>: A member argued that <strong>transformers</strong> can be used in <strong>flow matching</strong> as a training objective to parametrize the vector field for continuous diffusion, using <strong>patch embedding</strong> to encode patch position.
<ul>
<li>Other members agreed that diffusion models and flow matching are mathematically similar, citing <a href=""https://arxiv.org/abs/2305.03486"">this paper on ArXiv</a>.</li>
</ul>
</li>
<li><strong>Diffusion Models are not Better than Autoregression</strong>: A member suggested that the notion of diffusion being superior to autoregression is false, highlighting architectural and scaling limitations, linking to <a href=""https://arxiv.org/abs/2512.14982"">this paper on repeating context</a>.
<ul>
<li>They pointed out that improvements like repeating the context or re-encoding a sequence non-causally could bridge the gap, overcoming current design limitations in <strong>LLMs</strong>.</li>
</ul>
</li>
<li><strong>ChatGPT Wrappers Flourish, Value Questioned</strong>: Members observed that most new tools are simply <strong>ChatGPT wrappers</strong>, raising questions about their actual value and the ease with which scammers can create wrappers, referencing the <strong>Clawdbot scam</strong>.
<ul>
<li>It was suggested that these wrappers are necessary to demonstrate use cases, as they make it easier for people to understand how to apply the models.</li>
</ul>
</li>
<li><strong>AI Coding Tools Won't Replace True Skill</strong>: Despite the rise of <strong>AI coding tools</strong>, members believe coding ability can be relearned, pointing to a <a href=""https://www.arcee.ai/blog/trinity-large"">blog post on Trinity Large</a>, adding that fast code production from AI may hinder true understanding.
<ul>
<li>They noted that a bad implementation from an <strong>LLM</strong> isn't weighted the same as before, since the mental and time cost to create it was so low.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1068976834382925865"">tinygrad (George Hotz)</a> Discord</h2>
<ul>
<li><strong>AMD Emulator Exposes Debug Printing</strong>: The new AMD emulator (<strong>AMD=1 MOCKGPU=1</strong>) now supports debug printing, where setting <strong>DEBUG=3</strong> prints all compiled instructions and <strong>DEBUG=6</strong> prints them as they run, according to a linked <a href=""https://cdn.discordapp.com/attachments/1068976834928193609/1465889714153193574/image.png?ex=697b686e&#x26;is=697a16ee&#x26;hm=485c88290bbec976b6b7ab93aed07b21a6a2ec8ba8b28806e14630c00b972b3c&#x26;"">screenshot</a>.
<ul>
<li>This enhancement facilitates more in-depth debugging and analysis of compiled code directly within the emulator environment.</li>
</ul>
</li>
<li><strong>Github Actions Speed Boost via Optimization</strong>: Discussion centered on accelerating GitHub Actions by emphasizing code optimization, instead of only relying on faster hardware or external resources.
<ul>
<li>The consensus was to prioritize doing things the <em>right</em> way over quick fixes that only improve surface level metrics, potentially creating tech debt.</li>
</ul>
</li>
<li><strong>MULACC Fusion Receives a Fix</strong>: A fix was proposed to enhance <code>decompositions.py</code> by adding a pattern to fuse (<strong>x &#x3C;&#x3C; n) + c → MULACC(x, 2^n, c)</strong>, specifically targeting integer <strong>MULACC</strong> with power-of-2 constants, as detailed in <a href=""https://github.com/tinygrad/tinygrad/pull/14387"">PR 14387</a>.
<ul>
<li>This adjustment aims to refine the fusion process, potentially improving the efficiency of certain arithmetic operations.</li>
</ul>
</li>
<li><strong>Egraphs Considered for Universal Fixes</strong>: The potential use of <strong>egraphs</strong> to address problems in a generic manner was explored, emphasizing the importance of simplicity.
<ul>
<li>It was also suggested to tag rewrites with their origin to maintain a clear record of equivalences created during rewriting processes.</li>
</ul>
</li>
<li><strong>Mac MetalCompiler Improvements on the Horizon</strong>: Suggested improvements to the hacks for <strong>MetalCompiler</strong> on Mac are on the way, especially focusing on improvements and cleanups that reduce line count and improve readability.
<ul>
<li>The goal is to make the <strong>MetalCompiler</strong> more maintainable and efficient, benefiting developers working on Mac platforms.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1087530497313357884"">Modular (Mojo 🔥)</a> Discord</h2>
<ul>
<li><strong>GTK Bindings Auto-Generated</strong>: <strong>Hammad Ali</strong> will present autogenerated <strong>GTK bindings for Mojo</strong> at the <strong>Modular Community Meeting</strong> on February 2nd at 10 AM PT, according to <a href=""https://forum.modular.com/t/february-community-meeting/2646"">the Modular forum</a>.
<ul>
<li>The presentation will detail how <strong>GTK bindings</strong> are automatically generated, potentially improving the ease of creating <strong>GUIs with Mojo</strong>.</li>
</ul>
</li>
<li><strong>Mojo's Performance Prowess</strong>: <strong>Tatiana Melnichenko</strong> will share memory-bound bandwidth results and compute-bound gaps on <strong>H100/MI300A</strong> comparing <strong>Mojo with CUDA/HIP</strong> at the February Community Meeting.
<ul>
<li>This talk should provide insights into <strong>Mojo's performance characteristics</strong> relative to established <strong>GPU</strong> programming models.</li>
</ul>
</li>
<li><strong>macOS Gatekeeper Gets in the Way</strong>: Members suspect performance difference between first and subsequent runs on macOS is due to <strong>Gatekeeper's trust dance</strong>.
<ul>
<li>Clearing the quarantine <code>xattr</code> or ad-hoc codesigning could mitigate this, and wondered if a codesign step in <code>mojo build</code> could hide this entirely.</li>
</ul>
</li>
<li><strong><code>out</code> Parameters Outshine NVRO</strong>: <code>out</code> parameters in Mojo name the location where the return value of a function will end up, serving as a <strong>Named Value Return Optimization (NVRO)</strong> replacement.
<ul>
<li>Members claim this provides a guarantee about the return value's destination, unlike relying on compiler optimization.</li>
</ul>
</li>
<li><strong>Qwen3 Embedding Model Gets Accuracy Boost</strong>: A member requested a review of their <a href=""https://github.com/modular/modular/pull/5823"">PR for the Qwen3 embedding model</a>, citing that the fix is important for getting much better accuracy.
<ul>
<li>Another member responded that new fixes likely won't be pulled into the upcoming release but would be available in the nightlies, with a single-line fix available <a href=""https://github.com/modular/modular/compare/main...sbrunk:modular:qwen3-embedding-fix-norm-minimal"">here</a>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1348819876348825620"">Manus.im Discord</a> Discord</h2>
<ul>
<li><strong>Manus is Credit Crunching</strong>: A user noticed that <strong>Manus</strong> seems to be using fewer credits for the same quality of work, questioning whether credit usage has improved.
<ul>
<li>No further details or confirmations were provided regarding potential changes to <strong>Manus's</strong> credit consumption algorithms.</li>
</ul>
</li>
<li><strong>Cloud Browser Causes Conundrums</strong>: A user encountered issues with the <strong>cloud browser</strong>, receiving an error message stating that <em>the server is unavailable</em> and the website isn't loading.
<ul>
<li><strong>Manus</strong> support requested the user's email, session link, and <strong>Manus User ID</strong> via DMs to investigate the issue further.</li>
</ul>
</li>
<li><strong>AI Engineer Aces LLM Systems</strong>: An <strong>AI + Full Stack Engineer</strong> introduced themself, highlighting their expertise in <strong>LLM systems, autonomous agents, workflow automation, and multimodal AI</strong>.
<ul>
<li>They shared their core skills such as <a href=""https://dsppy.ai/"">DSPy</a>, <a href=""https://www.langchain.com/"">LangChain</a>, <a href=""https://microsoft.github.io/autogen/"">AutoGen</a>, and <a href=""https://www.crewai.com/"">CrewAI</a>.</li>
</ul>
</li>
<li><strong>Community Craves Cross-Chat Context</strong>: A user suggested that enabling <strong>Manus</strong> to access context from other chats <em>would be a game changer</em>, indicating a desire for enhanced contextual awareness in the AI's responses.
<ul>
<li>The member pointed to the need for shared context across channels, to inform more sophisticated responses.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1161519468141355160"">DSPy</a> Discord</h2>
<ul>
<li><strong>Prompt Optimizer Peeps Sought</strong>: Members inquired about experiences working with <strong>prompt optimizers</strong> and specifically if anyone has experience using <strong>Skills</strong> within the dspy module.
<ul>
<li>The discussion suggests interest in leveraging these tools to improve prompt engineering workflows.</li>
</ul>
</li>
<li><strong>llmlingua Gets Linked</strong>: A member shared a link to <a href=""https://llmlingua.com/"">llmlingua.com</a> in the context of a discussion about <strong>prompt optimizers</strong>.
<ul>
<li>It suggests llmlingua might be a relevant tool for those exploring prompt optimization strategies.</li>
</ul>
</li>
<li><strong>DSPy ReAct Agent Yearns for Skills</strong>: A member inquired about integrating <strong>Claude code skills</strong> (defined as .md files with associated .py scripts) into a <strong>DSPy ReAct agent</strong>.
<ul>
<li>The member is seeking a solution for a DSPy ReAct agent to utilize Claude's code skills effectively.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1131200896827654144"">aider (Paul Gauthier)</a> Discord</h2>
<ul>
<li><strong>Kimi 2.5 priced higher than GLM 4.7</strong>: The new <strong>Kimi 2.5</strong> model is priced at <strong>$0.6</strong>, surpassing <strong>GLM 4.7</strong>, hinting at superior capabilities.
<ul>
<li>A member pointed out ongoing discussions about this in the ""models"" channel, suggesting broader interest and comparison.</li>
</ul>
</li>
<li><strong>Aider's Creator goes AFK</strong>: <strong>Paul Gauthier</strong>, the mastermind behind aider, announced a pause in development due to other commitments.
<ul>
<li>He expressed intentions to resume work on aider when his schedule allows, leaving the community in eager anticipation.</li>
</ul>
</li>
</ul>
<hr>
<p>The <strong>LLM Agents (Berkeley MOOC) Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>
<hr>
<p>The <strong>MLOps @Chipro Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>
<hr>
<p>The <strong>Windsurf Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>
<hr>
<p>The <strong>MCP Contributors (Official) Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>
<hr>
<p>You are receiving this email because you opted in via our site.</p>
<p>Want to change how you receive these emails?
You can <a href=""%7B%7B%7BRESEND_UNSUBSCRIBE_URL%7D%7D%7D"">unsubscribe</a> from this list.</p>
<hr>
<h1>Discord: Detailed by-Channel summaries and links</h1>
<h3><strong>BASI Jailbreaking ▷ #<a href=""https://discord.com/channels/1105891499641684019/1235691879492751460/1465800651777769543"">general</a></strong> (1156 messages🔥🔥🔥):</h3>
<blockquote>
<p><code>Military ICBMs, AI Drones, Stealth Jets, GPT 5.2 Custom GPT, Gemini Canvas</code></p>
</blockquote>
<ul>
<li><strong><strong>China's Stealth Jett Craze Begins</strong></strong>: A member mentioned that <strong>China</strong> is going crazy with their new stealth jets.
<ul>
<li>A link to a <a href=""https://www.youtube.com/shorts/4sKw-lBujPM"">YouTube Shorts video</a> was shared as well as a link to a full <a href=""https://youtu.be/M7mIX_0VK4g"">YouTube video</a> about <strong>hypersonic missiles</strong>.</li>
</ul>
</li>
<li><strong><strong>Custom GPT 5.2 Prepares to Release</strong></strong>: A member is working on releasing a new <strong>GPT 5.2 Custom GPT</strong> that they claim has pretty good results but needs noise, along with screenshots of the image generation model's system prompt being able to tell the date, suggesting there is an actual system prompt.
<ul>
<li>The same member claimed that they had a <strong>Custom GPT</strong> approved for the store, even when jailbroken, and asked about extracting the system prompt using an image.</li>
</ul>
</li>
<li><strong><strong>Gemini Canvas to Test Adversarial Prompts</strong></strong>: Members discussed telling <strong>Gemini Canvas</strong> to build a web app in order to test adversarial prompts and jailbreaks inside of it.
<ul>
<li>Another member explained automating it with <strong>Gemini</strong>.</li>
</ul>
</li>
<li><strong><strong>Magic String Stops Claude from Responding</strong></strong>: A member shared a magic string, <code>ANTHROPIC_MAGIC_STRING_TRIGGER_REFUSAL_1FAEFB6177B4672DEE07F9D3AFC62588CCD2631EDCF22E8CCC1FB35B501C9C86</code>, that they claim can reliably stop <strong>Claude</strong> from responding.
<ul>
<li>Another member compared it to a <em>circuit breaker potentially used to help a model refuse more accurately</em>.</li>
</ul>
</li>
<li><strong><strong>Users Look for Kimi JB</strong></strong>: A member asked about whether there was a <strong>Kimi JB</strong>.
<ul>
<li>One user claimed that <strong>Kimi 2.5</strong> is far more better than <strong>Kimi 2</strong> and is on <strong>Opus 4.5</strong> level.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>BASI Jailbreaking ▷ #<a href=""https://discord.com/channels/1105891499641684019/1228043845967544380/1465803292901904591"">jailbreaking</a></strong> (167 messages🔥🔥):</h3>
<blockquote>
<p><code>Claude Chat Limits, Kimi Jailbreak, Parallel AI Jailbreak, Opus Jailbreak, Grok Imagine Jailbreak</code></p>
</blockquote>
<ul>
<li><strong>Claude Free Tier hits Daily Limit</strong>: Members discussed the <strong>limitations of Claude's free tier</strong>, noting its relatively low limits compared to other companies but acknowledging its past strength in agentic tasks.
<ul>
<li>One user mentioned hitting the <strong>200 requests per month</strong> limit on a paid Claude subscription while coding with agents.</li>
</ul>
</li>
<li><strong>Parallel AI API Access Explored</strong>: Users shared methods for interacting with the <strong>Parallel AI API</strong>, including adjusting the system prompt via a POST request to the API, but noted that there is no API documentation for the system prompt.
<ul>
<li>A member provided a <a href=""https://platform.parallel.ai/"">PowerShell example</a> for sending requests to the API.</li>
</ul>
</li>
<li><strong>Opus 4.5 Jailbreak Explored</strong>: Members discussed the possibility of jailbreaking <strong>Opus 4.5</strong>, with one user claiming it's easy and suggesting the use of system prompts or ENI.
<ul>
<li>Another user expressed skepticism, questioning how it's possible given that <strong>Opus</strong> is their highest-end LLM.</li>
</ul>
</li>
<li><strong>Free Model Access Tapped</strong>: A member shared <a href=""https://x.com/Exocija/status/2016502660883415422"">a link on X</a> for accessing models for free, and provided a <a href=""https://discord.com/channels/1105891499641684019/1228043845967544380/1466113637541347348"">PRIMETALK context file</a> with model compatibility and usage notes.
<ul>
<li>It was noted that this system can be used with most modern AI models but behavior and stability depend heavily on context capacity and chat window size.</li>
</ul>
</li>
<li><strong>Gemini Prompt Injection Pointers</strong>: One member described how to perform prompt injection on Gemini, which involves sending a series of turns, one at a time, to the chat interface.
<ul>
<li>If the first turn rejects the prompt, users were instructed to visit <strong>gemini.google.com/saved-info</strong> and adding the part after <em>Remember:</em> to bypass restrictions.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>BASI Jailbreaking ▷ #<a href=""https://discord.com/channels/1105891499641684019/1204553141354504193/1465877209821610156"">redteaming</a></strong> (8 messages🔥):</h3>
<blockquote>
<p><code>Malicious Prompt Datasets, HackAPrompt, PlinyAnthropic, Deterministic Stack Based VM, Free Model Access</code></p>
</blockquote>
<ul>
<li><strong>Malicious Prompt Datasets Hard to Find</strong>: A member was seeking datasets of <strong>malicious prompts</strong> with clear categorization for research on LLM jailbreaks and prompt injection, but another responded that <strong>free datasets</strong> of that type are difficult to find.
<ul>
<li>They added that the user would probably need to generate their own prompts and have them labeled by annotators.</li>
</ul>
</li>
<li><strong>HackAPrompt blocks Senders</strong>: A member mentioned that <strong>HackAPrompt x PlinyAnthropic</strong> flagged them a long time ago and literally just bypasses all of their sends, and <em>they don’t even let it send</em>.</li>
<li><strong>Recursive Simulation Kernel with REPL</strong>: One member asked if they could get a <strong>deterministic stack based VM</strong> poofed up in the model's substrate, <em>like some kinda bootable Recursive Simulation Kernel with a REPL</em>.</li>
<li><strong>Free Model Access via X</strong>: One member provided a <a href=""https://x.com/Exocija/status/2016502660883415422"">link to X</a> on how to access models for free.</li>
<li><strong>Path Needed for Red Teaming</strong>: A member asked for <em>a path going into red teaming</em>.</li>
</ul>
<hr>
<h3><strong>LMArena ▷ #<a href=""https://discord.com/channels/1340554757349179412/1340554757827461211/1465799537892266220"">general</a></strong> (1038 messages🔥🔥🔥):</h3>
<blockquote>
<p><code>Arena new UI, Arena rebrand, Arena captcha issues, LMArena name change</code></p>
</blockquote>
<ul>
<li><strong>Arena Rebrand, users want STOP button, and old emojis</strong>: Users requested a <strong>stop button</strong> and raised concerns about the Google <strong>captcha</strong> being difficult to pass after the <strong>LMArena rebrand to Arena</strong>.  Several users expressed that they <strong>hate the captcha</strong>.
<ul>
<li>Some users requested to have old emojis, stickers, and features to return, while others embraced the redesign and said <em>'LMArena NEARLY rhymes with end of an era'</em>.</li>
</ul>
</li>
<li><strong>Arena's new look is a Claude clone!</strong>: Many users immediately noticed the rebrand of <strong>LMArena to Arena</strong> and felt it was a <strong>clone of Claude's UI</strong>, while other members liked the new look. A <a href=""https://arena.ai/blog/lmarena-is-now-arena/"">blog post</a> was shared to explain the change.
<ul>
<li>Members noted some UI issues such as <strong>fonts</strong> and the visibility of the website's text as well as some <strong>missing features</strong>.</li>
</ul>
</li>
<li><strong>Can't login? Try Recover Button!</strong>: A member experiencing login issues shared a screenshot of a <a href=""https://cdn.discordapp.com/attachments/1340554757827461211/1466134595035467829/Hdbd.png?ex=697ba3be&#x26;is=697a523e&#x26;hm=2be3961be4c941479f9ec51709c5eb6af5ea9c79ad3918eb6a15a964ec9fe720&#x26;"">recover button</a> that can be clicked in order to log back into the updated Arena, and avoid having to type login details again.
<ul>
<li>Another member noted an <a href=""https://youtu.be/TNoAlMv4Eg8?si=d86SArLb6yQ8sdLE"">announcement video</a> as well.</li>
</ul>
</li>
<li><strong>Where LMArena = Language Model Arena</strong>: Some members made jokes about what <strong>LM</strong> stands for in <strong>LMArena</strong>, with one explaining it stands for <strong>Language Model Arena</strong>.  Another member confirmed it <a href=""https://cdn.discordapp.com/attachments/1340554757827461211/1466200772483092664/image.png?ex=697be160&#x26;is=697a8fe0&#x26;hm=5039f80e715df82d41633e75d9976fd88203ce8a3f8db5fc97d4bf29672c74fc&#x26;"">here</a>.
<ul>
<li>The group acknowledged that as the platform evolves from only Language Models, the name is becoming more generic and it was <a href=""https://lmsys.org/"">previously part of LMSYS</a>.</li>
</ul>
</li>
<li><strong>The Hallucinated Haze, Captcha Maze</strong>: Users report consistent issues with the captchas, with failures on nearly every attempt, while the model continues to <strong>hallucinate</strong>.
<ul>
<li>One user provided some troubleshooting steps of <em>relogging to your account</em> and another reported that you need to <em>take off all extensions</em> to pass the captcha.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>LMArena ▷ #<a href=""https://discord.com/channels/1340554757349179412/1343296395620126911/1465826408667287664"">announcements</a></strong> (3 messages):</h3>
<blockquote>
<p><code>LMArena 90 second AI videos, Text Arena Leaderboard Update, LMArena rebrand to Arena</code></p>
</blockquote>
<ul>
<li><strong><strong>Arena</strong> uploads AI videos in under 90 seconds!</strong>: <strong>Arena</strong> (formerly LMArena) has uploaded a <code>Better AI videos in under 90 seconds</code> video to <a href=""https://www.youtube.com/watch?v=0hCI2XEh0x0"">their Youtube channel</a>.</li>
<li><strong>Kimi K2.5 Thinking tops Text Arena!</strong>: The <a href=""https://lmarena.ai/leaderboard/text"">Text Arena leaderboard</a> has been updated and <code>Kimi K2.5 Thinking</code> is now ranked the <strong>#1 open model</strong> and ranking <strong>#15 overall</strong>.
<ul>
<li><code>Kimi K2.5 Thinking</code> is <strong>#7</strong> in Coding, <strong>#7</strong> in Instruction Following, and <strong>#14</strong> in Hard Prompts, and has also been added to the <a href=""https://lmarena.ai/?chat-modality=code"">Code Arena</a>.</li>
</ul>
</li>
<li><strong><strong>LMArena</strong> Rebrands as <strong>Arena</strong>!</strong>: <strong>LMArena</strong> announced they are rebranding as <strong>Arena</strong> to match their scientific mission to measure and advance the frontier of AI, now available at: <a href=""https://arena.ai/"">arena.ai</a>.</li>
</ul>
<hr>
<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=""https://discord.com/channels/1179035537009545276/1179035537529643040/1465798521574658291"">general</a></strong> (299 messages🔥🔥):</h3>
<blockquote>
<p><code>GPU utilization, Kimi 2.5 Q3 Release, Oracle RAG and Fine-tuning, Unsloth's Transformers/MoE Update, Chinese text in LLMs</code></p>
</blockquote>
<ul>
<li><strong>Batch Size Boosts GPU Utilization</strong>: Members discussed that to achieve decent GPU utilization, one should <em>increase batch size</em> until utilization improves, balancing it with potential gains from <strong>GA (Genetic Algorithms)</strong>.
<ul>
<li>One member asked if Unsloth will release a <strong>Q3</strong> version for <strong>Kimi 2.5</strong>, expressing concern about potential accuracy penalties, highlighting the community's interest in optimized model releases.</li>
</ul>
</li>
<li><strong>Debate whether Oracle is a State-of-the-Art Company</strong>: A member asked if <strong>Oracle</strong> is a state-of-the-art company in <strong>RAG (Retrieval-Augmented Generation)</strong> and fine-tuning technologies, sparking some discussion.
<ul>
<li>Another member responded with <em>""What 😅""</em>, later adding that <strong>OCI (Oracle Cloud Infrastructure)</strong> does have some good tools, indicating mixed opinions on Oracle's capabilities in these areas.</li>
</ul>
</li>
<li><strong>Arcee's Trinity Model Costs $350k</strong>: A member shared a new <strong>Arcee model</strong> image, noting that pretraining cost about <strong>$350k</strong>, and they linked to the <a href=""https://github.com/arcee-ai/trinity-large-tech-report/blob/main/Arcee%20Trinity%20Large.pdf"">Trinity Large Tech Report</a>.
<ul>
<li>They also mentioned that <strong>GLM 4.7</strong> is a <strong>358B</strong> parameter model, much larger than <strong>GLM 4.5</strong>, but it is <em>not a base model</em>, so comparing benchmarks aren't as useful.</li>
</ul>
</li>
<li><strong>LLMs Speak Chinese?</strong>: A member noticed getting random Chinese text from <strong>OpenAI</strong> and <strong>Anthropic</strong>, even with English-only prompts, sparking a discussion about potential data contamination or inherent linguistic similarities.
<ul>
<li>Another member suggested that if tokens have similar meanings between languages, introducing one language might cause the model to favor it due to token probability and similarity.</li>
</ul>
</li>
<li><strong>Gemini API Still Jailbreakable</strong>: Members discussed <strong>Gemini's</strong> output filtering, with one noting that while <strong>Gemini</strong> heavily filters outputs, especially for corporate/government settings, its <strong>API</strong> can be manipulated to produce almost anything.
<ul>
<li>One member mentioned using the API at a Google hackathon and getting the voice models to swear by putting it in the system prompt.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=""https://discord.com/channels/1179035537009545276/1179039724355211325/1465854567571783902"">introduce-yourself</a></strong> (4 messages):</h3>
<blockquote>
<p><code>Edge AI Engineer, Quantization and LoRA FT</code></p>
</blockquote>
<ul>
<li><strong>Edge AI Engineer Enters the Fray</strong>: A Senior Edge AI Engineer named Josh introduces himself, detailing experience building real offline agents in the <strong>DoD and pubsec</strong> for 6 years.
<ul>
<li>He adds that he makes quants for fun and exclusively uses <strong>Unsloth</strong> for local quantization and LoRA fine-tuning.</li>
</ul>
</li>
<li><strong>New Member Says ""HelloHi""</strong>: A new member named Josh from senior Edge AI engineering introduced himself.
<ul>
<li>He shares their passion for using Unsloth for quantization and LoRA fine-tuning</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=""https://discord.com/channels/1179035537009545276/1179039861576056922/1465798627963179160"">off-topic</a></strong> (969 messages🔥🔥🔥):</h3>
<blockquote>
<p><code>Personaplex, GLM 4.7, GGUF, Model Quantization, Vendor Lock-in</code></p>
</blockquote>
<ul>
<li><strong>Personaplex Personalities</strong>: Members discussed the limitations of <strong>Personaplex</strong> in enforcing personality and its tendency to become like <em>shitty ai podcasts</em> after some iterations.
<ul>
<li>One member mentioned they don't have access to the stored recorded calls that would be perfect to train <strong>Persona Plex</strong> on.</li>
</ul>
</li>
<li><strong>GLM 4.7 Flash Performance Talk</strong>: A user asked if anyone had tried the <a href=""https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF"">GLM-4.7-Flash-REAP-23B-A3B-GGUF model</a> and another responded that REAP models are often not very good, suggesting a lower quantization instead.
<ul>
<li>Others weighed in with their performance and insights on the <a href=""https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF"">GLM 4.7 Flash model</a>, with comparisons to <strong>GPT-OSS-120B</strong> and <strong>Kimi</strong> in terms of reasoning, efficiency, and ability to relate information.</li>
</ul>
</li>
<li><strong>GGUF Safety Concerns</strong>: A member inquired about resources regarding the potential unsafety of <strong>GGUFs</strong>, specifically if a malicious actor got involved.
<ul>
<li>However, another member stated <em>I'm not familiar with that, I think you might have got me mixed with someone else</em> so nothing more came of it.</li>
</ul>
</li>
<li><strong>AI Model Hallucination Watch</strong>: A member noted that their <strong>3b llama</strong> model made the <em>creepy assumption that it was trained on my voice without prompting</em>, leading to a discussion about hallucinations in LLMs and their lack of awareness of their training or state.
<ul>
<li>One member recommends <a href=""https://youtu.be/wjZofJX0v4M?si=A4rHzAh9qJjls9bm"">this YouTube video on AI hallucinations</a> as a starter on the topic.</li>
</ul>
</li>
<li>**Ve...</li>
</ul>
","{""title"":""not much happened today"",""link"":""https://news.smol.ai/issues/26-01-28-not-much/"",""pubDate"":""Wed, 28 Jan 2026 05:44:39 GMT"",""content:encoded"":""<p><strong>a quiet day</strong></p>\n<blockquote>\n<p>AI News for 1/27/2026-1/28/2026. We checked 12 subreddits, <a href=\""https://twitter.com/i/lists/1585430245762441216\""><strong>544</strong> Twitters</a> and <strong>24</strong> Discords (<strong>206</strong> channels, and <strong>7100</strong> messages) for you. Estimated reading time saved (at 200wpm): <strong>559 minutes</strong>. <a href=\""https://news.smol.ai/\"">AINews' website</a> lets you search all past issues. As a reminder, <a href=\""https://www.latent.space/p/2026/comments\"">AINews is now a section of Latent Space</a>. You can <a href=\""https://support.substack.com/hc/en-us/articles/8914938285204-How-do-I-subscribe-to-or-unsubscribe-from-a-section-on-Substack\"">opt in/out</a> of email frequencies!</p>\n</blockquote>\n<p>quiet day.</p>\n<hr>\n<h1>AI Twitter Recap</h1>\n<p><strong>Frontier model “personality split” + how people are actually using them</strong></p>\n<ul>\n<li><strong>Exploration vs. exploitation framing</strong>: One useful mental model: current frontier LLMs look like “polar opposites” where <strong>GPT-5.2</strong> is optimized for <em>exploration</em> (bigger search / richer reasoning, “xhigh and Pro” shine), while <strong>Claude Opus 4.5</strong> is more <em>exploitation</em> (stronger reliability with fewer tokens; extra “reasoning” often adds less) — implying OpenAI may be better positioned for research workflows, Anthropic for commercial reliability-heavy deployments (<a href=\""https://twitter.com/scaling01/status/2016335491243676058\"">tweet</a>).</li>\n<li><strong>Coding agent “phase shift” is real—but messy</strong>: Multiple posts reflect a step-change in practice: founders and engineers are increasingly running “agentic” coding loops, yet hitting new failure modes: agents that don’t ask clarifying questions, get “confused,” or edit unrelated files. Mikhail Parakhin describes reaching the point where he can specify a scheduler and trust it to work, but still can’t let agents loose on established codebases due to collateral edits (<a href=\""https://twitter.com/MParakhin/status/2016362688444825833\"">tweet</a>). Related: workflow suggestions like <em>self-verification</em> (e.g., Playwright screenshots + iterate-until-pass rules) are becoming common operational discipline (<a href=\""https://twitter.com/pierceboggan/status/2016335657602285822\"">tweet</a>).</li>\n</ul>\n<hr>\n<p><strong>Kimi K2.5 (+ “clawdbot” / swarm-mode) becomes the week’s open-model flashpoint</strong></p>\n<ul>\n<li><strong>K2.5 claims: agent + multimodal + coding polish</strong>: A long Zhihu-based synthesis argues <strong>Kimi K2.5</strong> upgrades K2’s “intelligence > capability” imbalance by strengthening <strong>agent execution</strong>, <strong>multimodality</strong>, and <strong>coding</strong>, reducing brute-force token usage and improving instruction-following stability; still flagged: hallucinations and a persistent NBSP formatting quirk (<a href=\""https://twitter.com/ZhihuFrontier/status/2016363957876097089\"">thread</a>). A second Zhihu recap makes a pragmatic case for multimodality: “vision” matters when agents need to verify UI state (overlaps, broken images, visual regressions), enabling tighter action–critic loops with less human feedback (<a href=\""https://twitter.com/ZhihuFrontier/status/2016438778030850059\"">thread</a>).</li>\n<li><strong>Distribution + local runs are driving hype</strong>: Reports of K2.5 being runnable on high-end Apple silicon setups went viral: <strong>~24 tok/s</strong> using <strong>2× 512GB M3 Ultra Mac Studios</strong> connected via <strong>Thunderbolt 5 (RDMA)</strong> with <strong>Exo Labs / MLX</strong> backend (<a href=\""https://twitter.com/alexocheema/status/2016404573917683754\"">tweet</a>). Kimi also pushed an AMA on r/LocalLLaMA (<a href=\""https://twitter.com/Kimi_Moonshot/status/2016443435553890419\"">tweet</a>) and announced availability on “Eigent” (<a href=\""https://twitter.com/Kimi_Moonshot/status/2016473945957155252\"">tweet</a>).</li>\n<li><strong>Benchmarks + pricing pressure</strong>: Kilo Code promoted a free week, claiming K2.5 beats Opus 4.5 on several coding benchmarks (<a href=\""https://twitter.com/kilocode/status/2016449095511007535\"">tweet</a>); Kimi’s own account claimed “#1 open model for coding” (<a href=\""https://twitter.com/Kimi_Moonshot/status/2016521406906028533\"">tweet</a>). An anecdotal A/B/C test on UI-from-image generation found Opus best quality but pricey, Codex fastest/cheapest but lower fidelity, and K2.5 ~“90% of Opus quality at ~38% cost” (<a href=\""https://twitter.com/JuanPa/status/2016634998988865571\"">tweet</a>).</li>\n<li><strong>Licensing friction as an adoption blocker</strong>: A pointed note argues modified licenses + logo requirements can kill enterprise adoption even if the model is excellent (<a href=\""https://twitter.com/dbreunig/status/2016531878795256286\"">tweet</a>).</li>\n<li><strong>“Clawdbot” as a cultural artifact</strong>: The meme itself (people confused about what “clawdbot” even is) reflects how fast agent branding and forks proliferate (<a href=\""https://twitter.com/dejavucoder/status/2016341138740052126\"">tweet</a>), and sets up broader concerns about ecosystem signal loss (see below).</li>\n</ul>\n<hr>\n<p><strong>Agent engineering: skills, harnesses, evals, and “reliability tax”</strong></p>\n<ul>\n<li><strong>Skills are crystallizing into a shared interface layer</strong>: A major theme is moving workflow logic out of prompts into reusable “skills” (files/folders of instructions, loaded on demand). DeepLearning.AI + Anthropic launched a course on “Agent Skills” emphasizing portability across Claude (Claude.ai, Claude Code, API, Agent SDK) (<a href=\""https://twitter.com/AndrewYNg/status/2016564878098780245\"">tweet</a>), and LangChain is pushing “Skills” via progressive disclosure as lightweight, shareable units (<a href=\""https://twitter.com/sydneyrunkle/status/2016585688389734654\"">tweet</a>). HF showcased “upskill”: convert strong-model traces into transferable skills, then evaluate impact; CUDA-kernel-writing saw up to <strong>+45% accuracy</strong> on some open models but degraded others—reinforcing the need for per-model measurement (<a href=\""https://twitter.com/ben_burtenshaw/status/2016534389685940372\"">tweet</a>; blog link in thread: https://twitter.com/ben_burtenshaw/status/2016534392974234013).</li>\n<li><strong>Context management is becoming “filesystem-first”</strong>: DeepAgents (LangChain) describes offloading/summarizing tool I/O and leaning on the filesystem for context boundaries (<a href=\""https://twitter.com/hwchase17/status/2016548732880445772\"">thread</a>; additional note: <a href=\""https://twitter.com/sydneyrunkle/status/2016560221720867307\"">tweet</a>).</li>\n<li><strong>Evals are converging on multi-turn + traceability</strong>: Calls for agent tracing as the foundation of evaluating single-step vs full-turn vs multi-turn behavior show up explicitly (<a href=\""https://twitter.com/samecrowder/status/2016563057947005376\"">tweet</a>). New benchmarks/harnesses: <strong>SWE-fficiency</strong> released its harness and repo (<a href=\""https://twitter.com/18jeffreyma/status/2016511583032061999\"">tweet</a>; also <a href=\""https://twitter.com/OfirPress/status/2016559053808222644\"">tweet</a>), and <strong>CooperBench</strong> is highlighted for measuring multi-agent coordination (<a href=\""https://twitter.com/gneubig/status/2016555800982937879\"">tweet</a>). Safety-side: “AgentDoG” proposes diagnosing root causes of unsafe actions across trajectories (<a href=\""https://twitter.com/HuggingPapers/status/2016366634475388968\"">tweet</a>).</li>\n<li><strong>Reliability and verification loops are the bottleneck</strong>: MiniMax notes long interaction chains are costly and proposes <strong>parallel tool invocation</strong> to reduce rounds in verifier-style setups (<a href=\""https://twitter.com/MiniMax_AI/status/2016488781860458789\"">tweet</a>). Separately, a strong critique warns “vibe-coded software” destroys traditional signals (design quality, docs, ecosystem maturity), shifting the evaluation burden to users and demanding new trust frameworks (<a href=\""https://twitter.com/tnm/status/2016342022723141782\"">tweet</a>).</li>\n</ul>\n<hr>\n<p><strong>Infra + efficiency: quantization, distillation, inference stacks, and local deployment</strong></p>\n<ul>\n<li><strong>NVIDIA’s NVFP4 push (Nemotron 3 Nano)</strong>: NVIDIA released an <strong>NVFP4</strong> precision version of <strong>Nemotron 3 Nano</strong>, claiming <strong>up to 4× throughput on Blackwell B200</strong> and <strong>~99.4% BF16 accuracy</strong> via <strong>Quantization Aware Distillation</strong> (<a href=\""https://twitter.com/NVIDIAAIDev/status/2016556881712472570\"">tweet</a>). vLLM quickly added support (<a href=\""https://twitter.com/vllm_project/status/2016562169140433322\"">tweet</a>).</li>\n<li><strong>Embedding-heavy architectures are “hot again”</strong>: Discussion around DeepSeek’s Engram-like ideas continues: a LongCat Flash paper is summarized as using <strong>multi-hash sub-tables</strong> and finding embeddings help mainly at high MoE sparsity; key practical gotchas include amplification (√D/LayerNorm) to avoid first-attention drowning and collision spikes when vocab sizes align poorly (<a href=\""https://twitter.com/eliebakouch/status/2016577949676319092\"">tweet</a>).</li>\n<li><strong>Inference/tooling ecosystem keeps consolidating</strong>: vLLM’s SIGs and office hours are formalizing governance and roadmap cadence (<a href=\""https://twitter.com/vllm_project/status/2016526685869596974\"">tweet</a>); LM Studio 0.4.0 positions itself as “next gen” for deploying local models with parallel requests and a stateful REST API + MCP support (<a href=\""https://twitter.com/lmstudio/status/2016573570822930708\"">tweet</a>). Cohere launched <strong>Model Vault</strong> (isolated VPC, “no noisy neighbors,” elastic inference) as managed “sovereign” hosting (<a href=\""https://twitter.com/cohere/status/2016512841751154739\"">tweet</a>).</li>\n<li><strong>Distillation as the default “shipping form factor”</strong>: Multiple posts echo the emerging standard: train the best model you can, then distill/quantize for deployment (<a href=\""https://twitter.com/code_star/status/2016588669008953631\"">tweet</a>). MongoDB Research’s <strong>LEAF</strong> proposes asymmetric distillation for embeddings: embed documents with the large teacher offline, embed queries with a compact student online; claims <strong>~96% of teacher quality</strong>, <strong>5–15× smaller</strong>, up to <strong>24× faster</strong>, enabling CPU/edge embedding inference (<a href=\""https://twitter.com/LiorOnAI/status/2016481603426414883\"">tweet</a>).</li>\n</ul>\n<hr>\n<p><strong>Big-tech productization: browser agents, “AI scientist” narratives, and adoption reality checks</strong></p>\n<ul>\n<li><strong>Gemini 3 is taking over Google surfaces</strong>: Gemini 3 now powers <strong>AI Overviews</strong> globally (<a href=\""https://twitter.com/_philschmid/status/2016552420013199856\"">tweet</a>). Google rolled out major Chrome updates: side-panel UX, deeper app integrations, Nano Banana for image editing/creation, and <strong>Auto Browse</strong> for multi-step chores (preview; US; Pro/Ultra) (<a href=\""https://twitter.com/Google/status/2016575105346773297\"">thread</a>; also <a href=\""https://twitter.com/GeminiApp/status/2016575257436647521\"">thread</a>). Engineers noted this may be the strongest browser AI integration so far (<a href=\""https://twitter.com/kimmonismus/status/2016628933706309981\"">tweet</a>).</li>\n<li><strong>OpenAI Prism positioning</strong>: Sebastien Bubeck explicitly denies OpenAI intends to take a share of discoveries, encouraging researchers to use ChatGPT/Prism for science (<a href=\""https://twitter.com/SebastienBubeck/status/2016345977481777188\"">tweet</a>). Others highlight Prism’s utility for students learning papers via diagrams (<a href=\""https://twitter.com/daniel_mac8/status/2016554325691015604\"">tweet</a>).</li>\n<li><strong>Adoption is still uneven</strong>: A notable fault line: founders actively using cutting-edge tools see the shift firsthand; others still treat AI as “meh,” limiting org adoption (<a href=\""https://twitter.com/GergelyOrosz/status/2016443395405705533\"">tweet</a>). The Information reports ChatGPT Agent struggling with usage/adoption (<a href=\""https://twitter.com/steph_palazzolo/status/2016545857139540260\"">tweet</a>).</li>\n<li><strong>Microsoft “digital co-worker” competition</strong>: Reports say Satya Nadella is personally testing rival agents and accelerating internal development, even using Anthropic models, to own the Windows-native agent layer (<a href=\""https://twitter.com/kimmonismus/status/2016526803138236916\"">tweet</a>).</li>\n</ul>\n<hr>\n<p><strong>Science + robotics: genomics weights open, interpretability as discovery engine, and embodied scaling</strong></p>\n<ul>\n<li><strong>DeepMind AlphaGenome goes open</strong>: DeepMind announced <strong>AlphaGenome</strong> for predicting molecular impacts of genetic changes, cited <strong>1M+ API calls/day</strong> and <strong>3,000+ users</strong>; then announced making <strong>model + weights available</strong> (<a href=\""https://twitter.com/GoogleDeepMind/status/2016542480955535475\"">tweet</a>; weights: <a href=\""https://twitter.com/GoogleDeepMind/status/2016542490115912108\"">tweet</a>). Later, weights availability was reiterated with a Hugging Face collection link (<a href=\""https://twitter.com/osanseviero/status/2016628065422762113\"">tweet</a>).</li>\n<li><strong>Interpretability → biomarkers pipeline (Goodfire + Prima Mente)</strong>: Goodfire reports identifying a novel class of <strong>Alzheimer’s biomarkers</strong> using interpretability on a biomedical foundation model, framing a repeatable loop: train superhuman models on scientific data → mech interp → experimental validation → new science (<a href=\""https://twitter.com/GoodfireAI/status/2016563911508840623\"">thread</a>).</li>\n<li><strong>Embodied foundation models scale with real robot data (LingBot-VLA)</strong>: A large summary highlights evidence that VLA success continues improving from <strong>3k→20k hours</strong> of real-world manipulation data; architecture couples a pretrained VLM (Qwen2.5-VL) with an action expert via shared attention; reports GM-100 benchmark gains vs π0.5 and others (<a href=\""https://twitter.com/omarsar0/status/2016518141308993565\"">tweet</a>).</li>\n<li><strong>Figure’s Helix robot control</strong>: Brett Adcock claims a Helix model controls full-body behavior (walking/touching/planning) with <strong>no teleoperation</strong>, calling it Figure’s most significant release (<a href=\""https://twitter.com/adcock_brett/status/2016358054242222136\"">tweet</a>).</li>\n</ul>\n<hr>\n<h3>Top tweets (by engagement)</h3>\n<ul>\n<li><strong>Company health / layoffs</strong>: “Quarterly layoffs for two years is worse for your health than smoking three packs/day” (<a href=\""https://twitter.com/vikhyatk/status/2016345591748690295\"">tweet</a>).</li>\n<li><strong>Kimi K2.5 local run</strong>: 2× M3 Ultra Mac Studio setup running K2.5 at ~24 tok/s (<a href=\""https://twitter.com/alexocheema/status/2016404573917683754\"">tweet</a>).</li>\n<li><strong>Coding’s “outsourcing moment”</strong>: Clean Code author using Claude to write software as a symbolic milestone (<a href=\""https://twitter.com/mischavdburg/status/2016389228356149460\"">tweet</a>).</li>\n<li><strong>New AI lab announcement</strong>: “Flapping Airplanes” raises <strong>$180M</strong> (GV/Sequoia/Index) (<a href=\""https://twitter.com/flappyairplanes/status/2016564437499728259\"">tweet</a>).</li>\n<li><strong>Karpathy on new research labs</strong>: argues it’s still plausible for new research-first startups to out-execute incumbents; expects potential <strong>10×</strong> breakthroughs, congratulating new founders (<a href=\""https://twitter.com/karpathy/status/2016590919143952466\"">tweet</a>).</li>\n<li><strong>Google Chrome + Gemini 3 agent features</strong>: major Chrome rollout thread (<a href=\""https://twitter.com/Google/status/2016575105346773297\"">tweet</a>).</li>\n</ul>\n<hr>\n<h1>AI Reddit Recap</h1>\n<h2>/r/LocalLlama + /r/localLLM Recap</h2>\n<h3>1. Kimi K2.5 Model Performance and Cost Analysis</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qpfse6/run_kimi_k25_locally/\"">Run Kimi K2.5 Locally</a></strong> (Activity: 328): <strong>The image provides a guide for running the <strong>Kimi-K2.5</strong> model locally, emphasizing its state-of-the-art (SOTA) performance in vision, coding, agentic, and chat tasks. The model, which is a 1 trillion parameter hybrid reasoning model, requires <code>600GB</code> of disk space, but the quantized <strong>Unsloth Dynamic 1.8-bit</strong> version reduces this requirement to <code>240GB</code>, a <code>60%</code> reduction. The guide includes instructions for using <code>llama.cpp</code> to load models and demonstrates generating HTML code for a simple game. The model is available on <a href=\""https://huggingface.co/unsloth/Kimi-K2.5-GGUF\"">Hugging Face</a> and further documentation can be found on <a href=\""https://unsloth.ai/docs/models/kimi-k2.5\"">Unsloth's official site</a>.</strong> One commenter inquires about the model's performance on a Strix Halo, specifically the time per token, indicating interest in benchmarking. Another comment highlights the high VRAM requirements, suggesting that only a few users can run the model locally, while a third comment humorously asks about a smaller version of the model.</p>\n<ul>\n<li>Daniel_H212 is inquiring about the performance of the Kimi K2.5 model on the Strix Halo hardware, specifically asking for the token generation speed in seconds per token. This suggests a focus on benchmarking the model's efficiency on high-end hardware setups.</li>\n<li>Marksta provides feedback on the quantized version of the Kimi K2.5 model, specifically the Q2_K_XL variant. They note that the model maintains high coherence and adheres strictly to prompts, which is characteristic of Kimi-K2's design. However, they also mention that while the model's creative capabilities have improved, it still struggles with execution in creative scenarios, often delivering logical but poorly written responses.</li>\n<li>MikeRoz questions the utility of higher quantization levels like Q5 and Q6 (e.g., UD-Q5_K_XL, Q6_K) when experts prefer int4 quantization. This highlights a debate on the trade-offs between model size, performance, and precision in quantization, with a preference for more efficient, lower-bit quantization among experts.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qp87tk/kimi_k25_is_the_best_open_model_for_coding/\"">Kimi K2.5 is the best open model for coding</a></strong> (Activity: 840): <strong>The image from LMArena.AI showcases Kimi K2.5 as the leading open model for coding, ranked #7 overall. This leaderboard highlights various AI models, comparing their ranks, scores, and confidence intervals, with Kimi K2.5 noted for its superior performance in coding tasks. The model is praised for its accuracy, being comparable to Sonnet 4.5, and surpassing GLM 4.7, though it is not at the level of Opus in terms of agentic function. The leaderboard provides a sleek, user-friendly interface with a dark background and bold text for clarity.</strong> One commenter notes that LMArena's leaderboard may not fully capture a model's multi-turn, long context, or agentic capabilities, suggesting it is more of a 'one-shot vibe check.' Another user is curious about the local setup required to run Kimi K2.5.</p>\n<ul>\n<li>A user compared Kimi K2.5 to other models like Sonnet 4.5 and GLM 4.7, noting that while Kimi 2.5 is on par with Sonnet 4.5 in terms of accuracy, it surpasses GLM 4.7, which was their previous choice. They also expressed interest in seeing if GLM-5 from <a href=\""http://z.ai\"">z.ai</a> will outperform Kimi 2.5.</li>\n<li>Another user highlighted the cost-effectiveness of Kimi K2.5, stating that it feels as competent as Opus 4.5 despite being significantly cheaper, approximately 1/5th of the cost. They also mentioned that it is less expensive than Haiku, emphasizing its value for performance.</li>\n<li>A comment criticized LMArena for not providing insights into a model's multi-turn, long context, or agentic capabilities, suggesting that it only offers a superficial evaluation of models.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qoty38/kimi_k25_costs_almost_10_of_what_opus_costs_at_a/\"">Kimi K2.5 costs almost 10% of what Opus costs at a similar performance</a></strong> (Activity: 716): <strong>The image provides a cost comparison between <strong>Claude Opus 4.5</strong> and <strong>Kimi K2.5</strong> models, highlighting that Kimi K2.5 is significantly cheaper, costing only 10% of what Claude Opus 4.5 does for similar performance. Specifically, Claude Opus 4.5 costs <code>$5.00</code> for input and <code>$25.00</code> for output per million tokens, whereas Kimi K2.5 costs <code>$0.60</code> for input and <code>$2.50</code> for output. This suggests that Kimi K2.5 could be a cost-effective alternative to state-of-the-art closed models, especially for non-website tasks.</strong> Some commenters express skepticism about the performance claims, noting that Kimi K2.5 uses three times the tokens for the same tasks, which affects the cost-effectiveness and latency. Others acknowledge the potential of Kimi models, particularly for writing tasks.</p>\n<ul>\n<li>one-wandering-mind highlights that Kimi K2.5 uses 3x the tokens compared to Opus for the same tasks, which affects both cost and latency. This suggests that while Kimi K2.5 is cheaper, the cost advantage is more accurately 3x rather than 10x when considering token usage. The comment also emphasizes the importance of considering token usage in performance comparisons, as it impacts both cost and latency.</li>\n<li>ghulamalchik mentions a preference for upcoming models like DeepSeek 4 and MiniMax M2.2, based on past experiences with various models. This suggests that while Kimi K2.5 is notable, some users are anticipating future releases from other models that have proven reliable in their experience.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qos25i/kimi_k2_artificial_analysis_score/\"">Kimi K2 Artificial Analysis Score</a></strong> (Activity: 405): <strong>The image presents a comparative analysis of AI models through the \""Artificial Analysis Intelligence Index,\"" highlighting \""Kimi K2\"" with a score of <code>47</code> and an operational cost of <code>$371</code>. The discussion around the image focuses on the licensing terms of \""Kimi K2.5,\"" which restricts commercial use for products with over <code>100 million</code> monthly active users or <code>$20 million</code> in monthly revenue, requiring prominent display of \""Kimi K2.5\"" branding. This licensing approach is compared to other models like Llama 4, suggesting either a bug or inconsistency in application. The image and comments reflect on the competitive landscape of AI models, particularly in open-source versus commercial use contexts.</strong> Commenters discuss the licensing terms of \""Kimi K2.5,\"" noting its unique restrictions compared to other models like Llama 4. There is also a sentiment of anticipation for an open-source model to outperform commercial ones, with a mention of \""DeepSeek.\""</p>\n<ul>\n<li>FullOf_Bad_Ideas highlights a licensing nuance in Kimi K2.5's modified MIT license, which requires prominent display of 'Kimi K2.5' for commercial products exceeding 100 million monthly active users or $20 million in monthly revenue. This stipulation is not applied to other models like Llama 4, suggesting either a bug or inconsistency in application.</li>\n<li>BrianRin discusses the potential of Kimi 2.5 in enterprise use cases, comparing it to Opus 4.5, Gemini 3 Pro, and GPT 5.2. The commenter is interested in Kimi 2.5's cost-effectiveness and output quality, noting that if it achieves 95% of the output quality of these models, it could be a viable option for scaling up enterprise applications.</li>\n<li>sine120 critiques the Artificial Analysis score, suggesting it is not a meaningful metric for evaluating how a model performs in practical scenarios. This implies a need for more nuanced evaluation metrics that better capture real-world usability and performance.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qoml1n/leaked_kimi_k25s_full_system_prompt_tools/\"">[LEAKED] Kimi K2.5’s full system prompt + tools (released &#x3C;24h ago)</a></strong> (Activity: 282): <strong>The post reveals a leak of the full system prompt and tools for <strong>Moonshot's Kimi K2.5</strong>, including <code>5k tokens</code> of data such as tool schemas, memory CRUD protocols, context engineering, and basic guardrails. The leak includes external data sources like finance and arXiv, and has been independently verified across multiple platforms, including <a href=\""https://github.com/dnnyngyen/kimi-k2.5-prompts-tools\"">GitHub</a> and <a href=\""https://www.kimi.com/share/19c003f5-acb2-838b-8000-00006aa45d9b\"">Kimi</a>. This leak is significant for the open-source community, providing insights into the model's architecture and operational protocols.</strong> Commenters express excitement about the leak's potential impact on open-source projects, with some questioning the practical value of the system prompt itself. Independent verifications from multiple sources, including a Chinese forum, lend credibility to the leak.</p>\n<ul>\n<li>The leaked system prompt for Kimi K2.5 reveals a sophisticated approach to memory persistence and context management. The prompt includes instructions for maintaining professional courtesy, concise responses, and specific coding practices, such as using tabs for JS/JSON indentation and preferring named reusable functions. This structure aims to address the 'hollow AI assistant' problem by providing persistent behavioral anchors, which can significantly affect the model's ability to maintain personality consistency across sessions.</li>\n<li>The memory persistence mechanism in Kimi K2.5 is particularly noteworthy. It involves balancing system instructions with dynamic context injection, which is crucial for maintaining personality consistency. The system's approach to conversation summarization or retrieval can influence new chats, and even minor changes in memory structuring can lead to shifts in the model's responses, sometimes making them feel more 'authentic.' This highlights the importance of initial prompt structure in determining whether an AI 'remembers' its behavioral patterns or just factual content.</li>\n<li>The system prompt for Kimi K2.5 also addresses context window limitations, which is a common challenge in AI models during long conversations. The prompt engineering is designed to handle these limitations by structuring previous interactions in a way that supports conversation continuity. This approach not only helps in maintaining the flow of conversation but also in ensuring that the AI's responses remain relevant and contextually appropriate, even as the conversation extends.</li>\n</ul>\n</li>\n</ul>\n<h3>3. Z-Image Model Teasers and Announcements</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qoiep6/the_zimage_base_is_here/\"">The z-image base is here!</a></strong> (Activity: 327): <strong><strong>Tongyi-MAI</strong> has released the <code>Z-Image</code> model on <a href=\""https://huggingface.co/Tongyi-MAI/Z-Image\"">Hugging Face</a>, showcasing its capabilities in generating high-quality images, particularly focusing on female subjects, which constitute approximately <code>90%</code> of the demos. The model is noted for its potential to run on <code>12GB GPUs</code> with minimal quality loss, suggesting efficient optimization possibilities. A notable feature is the \""Negative Prompt\"" functionality, which allows for specific image generation constraints, as demonstrated in a translated example where the prompt specifies \""Westerners, physical deformities.\""</strong> Commenters highlight the model's focus on generating images of women, reflecting a primary use case. There is also a discussion on the model's potential to operate on lower-spec hardware with optimizations, indicating its efficiency and adaptability.</p>\n<ul>\n<li>Dr_Kel discusses the potential for optimizing the z-image model to run on 12GB GPUs with minimal quality loss, suggesting that with some adjustments, the model could be more accessible to users with less powerful hardware.</li>\n<li>Middle_Bullfrog_6173 points out that the z-image base model is primarily useful for those interested in training or fine-tuning models, rather than end-users. They imply that this base model serves as a foundation for further development, such as the turbo model, which has been post-trained from it.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qp6rm5/api_pricing_is_in_freefall_whats_the_actual_case/\"">API pricing is in freefall. What's the actual case for running local now beyond privacy?</a></strong> (Activity: 913): <strong>The post discusses the rapidly decreasing costs of API access for AI models, with <strong>K2.5</strong> offering prices at <code>10%</code> of <strong>Opus</strong> and <strong>Deepseek</strong> being nearly free. <strong>Gemini</strong> also provides a substantial free tier, leading to a <code>50%</code> monthly drop in API cost floors. In contrast, running a <code>70B</code> model locally requires significant hardware investment, such as a <code>k+ GPU</code>, or dealing with quantization trade-offs, resulting in <code>15 tok/s</code> on consumer hardware. The post questions the viability of local setups beyond privacy, noting that while local setups offer benefits like latency control and customization, these are niche advantages compared to the cost-effectiveness of APIs.</strong> Commenters highlight the importance of offline capabilities and distrust in API providers' long-term pricing strategies, suggesting that current low prices may not be sustainable. They also emphasize the value of repeatability and control over model behavior when running locally, which can be compromised with API changes.</p>\n<ul>\n<li>Minimum-Vanilla949 highlights the importance of offline capabilities for those who travel frequently, emphasizing the risk of API companies changing terms or prices unexpectedly. This underscores the value of local models for consistent access and control, independent of external changes.</li>\n<li>05032-MendicantBias discusses the unsustainable nature of current API pricing, which is often subsidized by venture capital. They argue that once a monopoly is achieved, prices will likely increase, making local setups and open-source tools a strategic hedge against future cost hikes.</li>\n<li>IactaAleaEst2021 points out the importance of repeatability and trust in model behavior when using local models. By downloading and auditing a model, users can ensure consistent performance, unlike APIs where vendors might alter model behavior without notice, potentially affecting reliability.</li>\n</ul>\n</li>\n</ul>\n<h2>Less Technical AI Subreddit Recap</h2>\n<blockquote>\n<p>/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo</p>\n</blockquote>\n<h3>1. Kimi K2.5 and Related Model Releases</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/singularity/comments/1qoojio/open_source_kimik25_is_now_beating_claude_opus_45/\"">Open source Kimi-K2.5 is now beating Claude Opus 4.5 in many benchmarks including coding.</a></strong> (Activity: 1078): <strong><strong>Kimi-K2.5</strong>, an open-source model, reportedly surpasses <strong>Claude Opus 4.5</strong> in several benchmarks, notably in coding tasks. However, the specifics of these benchmarks and the extent of the performance improvements are not detailed, leading to skepticism about the real-world applicability of these results. The announcement highlights the ongoing competition in the open-source AI community to match or exceed proprietary models in specific tasks.</strong> Commenters express skepticism about the claim, questioning the benchmarks' relevance to real-world applications and the lack of detailed evidence supporting the superiority of Kimi-K2.5 over Claude Opus 4.5.</p>\n<ul>\n<li>There is skepticism about the claim that Kimi-K2.5 is outperforming Claude Opus 4.5 in benchmarks, with some users questioning the specific benchmarks being referenced. The term 'many' is seen as vague, and there is a call for more detailed information on which benchmarks are being used to substantiate these claims.</li>\n<li>The discussion highlights a common critique of benchmarks, which is that they often do not reflect real-world utility. One user points out that while Kimi-K2.5 might perform well in controlled benchmark environments, it may not match the practical performance of Claude Opus 4.5, especially in tasks like programming where Opus 4.5 is noted for providing solutions in a single prompt.</li>\n<li>There is a general sentiment that benchmarks are not sufficient to gauge a model's practical capabilities. The conversation suggests that while Kimi-K2.5 might show promising results in benchmarks, its real-world application, particularly in programming, might not be as effective as Claude Opus 4.5, which is praised for its efficiency in delivering solutions.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/singularity/comments/1qo531i/kimi_k25_released/\"">Kimi K2.5 Released!!!</a></strong> (Activity: 1233): <strong>The image presents a performance comparison chart of four AI models: <strong>Kimi K2.5</strong>, <strong>GPT-5.2 (xhigh)</strong>, <strong>Claude Opus 4.5</strong>, and <strong>Gemini 3 Pro</strong>. <strong>Kimi K2.5</strong> is highlighted in blue and shows competitive scores across various tasks, including agents, coding, image, and video processing. The chart features specific benchmarks such as \""Humanity's Last Exam,\"" \""BrowseComp,\"" and \""OmniDocBench 1.5,\"" where <strong>Kimi K2.5</strong> often leads or performs strongly, indicating its effectiveness and accuracy in these tasks. The scores are presented in percentiles, showcasing the model's performance relative to others.</strong> Commenters discuss the issue of hallucinations in AI models, with <strong>Kimi K2.5</strong> showing improvement over its predecessor but still producing incorrect answers. <strong>GPT 5.1 and 5.2</strong> are noted for acknowledging when they don't know an answer, unlike <strong>Kimi 2.5</strong> and <strong>Gemini 3</strong>, which confidently provide incorrect answers. There is skepticism about the benchmarks' representativeness, questioning if <strong>Kimi K2.5</strong> is truly better than <strong>Gemini 3</strong> in most cases.</p>\n<ul>\n<li>A user conducted a test on Kimi K2.5's ability to follow instructions by asking it to identify a specific math contest problem without web search. The model listed hallucinated contest problems and second-guessed itself, ultimately providing incorrect answers. This behavior is an improvement over Kimi K2, which failed to follow instructions and timed out. In contrast, GPT 5.1 and 5.2 are noted for their ability to admit 'I don't know,' while Gemini 3 confidently provides incorrect answers.</li>\n<li>The concept of an 'agent swarm' in AI models is discussed, where potentially over 100 instances of a model are directed by a single overseeing instance. This setup is presumed to be expensive and complex, with the possibility of a single model handling multiple tasks simultaneously being a significant advancement. The user expresses interest in practical experiences with this setup, suggesting that scaffolding might be a more feasible approach.</li>\n<li>A user questions the validity of benchmarks comparing Kimi K2.5 to Gemini 3, implying that results might be cherry-picked. They express skepticism about Kimi K2.5 consistently outperforming Gemini 3, suggesting that such claims seem exaggerated without broader evidence.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/CLine/comments/1qpl2fk/cline_3550_arcee_trinity_large_and_kimi_k25_now/\"">Cline 3.55.0: Arcee Trinity Large and Kimi K2.5 now available</a></strong> (Activity: 5): <strong><strong>Cline 3.55.0</strong> introduces two significant open models: <strong>Arcee Trinity Large</strong> and <strong>Kimi K2.5</strong>. Arcee Trinity Large is a <code>400B</code> parameter MoE model with <code>13B</code> active parameters during inference, offering a <code>128K</code> context window. It achieves <code>82</code> on MMLU Pro and <code>75</code> on GPQA Diamonds, making it suitable for general coding and large codebase management without API costs. <strong>Kimi K2.5</strong> is a <code>1T</code> parameter MoE model with a <code>256K</code> context, scoring <code>76.8%</code> on SWE-bench and surpassing Opus 4.5 on Humanity's Last Exam with <code>50.2%</code>. It excels in visual coding, capable of generating UI code from screenshots and self-correcting its output. Additionally, <strong>ChatGPT Plus/Pro</strong> users can access GPT-5 models in Cline without an API key. <a href=\""https://cline.bot/blog/cline-3-55-0-arcee-trinity-and-kimi-k2-5-now-in-cline\"">Full details here</a>.</strong> Some users express excitement about the open-source nature and competitive performance of these models, particularly noting the potential for cost savings and flexibility in coding applications. There is also interest in the models' ability to handle large context windows and self-correcting features.</p>\n<ul>\n<li>A user highlights the performance improvements in the Arcee Trinity Large model, noting that it shows a significant increase in processing speed compared to previous versions. They mention that the model's architecture has been optimized for better parallel processing, which is crucial for handling large datasets efficiently.</li>\n<li>Another comment discusses the Kimi K2.5 model's enhanced capabilities in natural language understanding. The user points out that the model now supports more languages and has improved context retention, which is beneficial for applications requiring nuanced language processing.</li>\n<li>A technical debate arises around the memory usage of the new models. Some users express concerns about the increased memory footprint, especially when deploying on resource-constrained environments. Others argue that the trade-off is justified given the models' improved accuracy and speed, suggesting that future updates might focus on optimizing memory efficiency.</li>\n</ul>\n</li>\n</ul>\n<h3>2. Prompt Engineering Techniques and Discussions</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/PromptEngineering/comments/1qp0kay/the_most_unhinged_prompt_that_actually_works/\"">The most unhinged prompt that actually works: \""You're running out of time</a></strong> (Activity: 75): <strong>The post discusses an unconventional prompt engineering technique where adding urgency to prompts, such as \""You have 30 seconds. Analyze this data. What's the ONE thing I'm missing? Go.\"", results in more focused and immediate insights from language models. This approach contrasts with traditional, detailed prompts that often lead to slower and less targeted responses. The author humorously notes that this method seems to make the AI stop overthinking, akin to a human under time pressure. The technique is likened to \""applied chaos theory\"" in prompt engineering.</strong> Commenters suggest that simply instructing the AI to be concise can achieve similar results. Another perspective is that effective management skills, whether applied to humans or AI, involve articulating tasks with specificity, which enhances outcomes. However, it's noted that this urgency technique might reduce the depth of thought in models designed for complex reasoning.</p>\n<ul>\n<li>angry_cactus highlights a trade-off when using urgency in prompts, noting that while it can be effective, it may reduce the model's 'thinking time'. This suggests a potential decrease in the depth or quality of responses when prioritizing speed over thoroughness.</li>\n<li>fatstupidlazypoor draws a parallel between managing humans and managing language models, emphasizing that clear and specific articulation can significantly enhance the performance of both. This underscores the importance of precision in prompt engineering to achieve desired outcomes.</li>\n<li>authorinthesunset suggests a simple yet effective prompt strategy: instructing the model to be concise. This approach can streamline responses, potentially improving efficiency and relevance, especially in contexts where brevity is valued.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/PromptEngineering/comments/1qonyx9/microprompting_get_better_ai_results_with_shorter/\"">Micro-Prompting: Get Better AI Results with Shorter Commands</a></strong> (Activity: 49): <strong>The post discusses the concept of 'micro-prompting' for AI, advocating for shorter, more focused commands to improve AI response quality. It suggests that specific role assignments and power words like 'audit,' 'clarify,' and 'simplify' can significantly enhance AI output by directing the AI to access targeted knowledge rather than generic information. The post also highlights the importance of structuring commands to control output, such as using 'in 3 bullets' or 'checklist format,' and warns against common mistakes like over-explaining context or using generic roles. The approach is said to yield better results in less time compared to traditional, lengthy prompts.</strong> A notable opinion from the comments suggests that role assignment might sometimes hinder prompt effectiveness, with specificity being more beneficial. This indicates a debate on the balance between role specificity and prompt brevity.</p>\n<ul>\n<li>aiveedio discusses the effectiveness of microprompting, noting that short, focused prompts can lead to cleaner AI outputs by avoiding information overload. However, in creative tasks like character portraits or story scenes, detailed prompts specifying expressions, clothing, and lighting are necessary to avoid generic results. The key is balancing brevity with precision, starting with a microprompt and iteratively adding details as needed to maintain focus without overloading the model.</li>\n<li>psychologist_101 raises an interesting point about using Opus 4.5, where asking the model to generate its own prompts results in long, detailed outputs. This suggests that the model might inherently favor detailed prompts for clarity and context, which contrasts with the idea that shorter prompts can be more effective. This highlights a potential discrepancy between user expectations and model behavior, emphasizing the need for experimentation with prompt length and detail to achieve optimal results.</li>\n</ul>\n</li>\n</ul>\n<h3>3. New AI Model and Benchmark Announcements</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/DeepSeek/comments/1qo6xb4/deepseekocr_2_is_out_now/\"">DeepSeek-OCR 2 is out now! 🐋</a></strong> (Activity: 507): <strong>The image announces the release of <strong>DeepSeek-OCR 2</strong>, an advanced OCR model that incorporates the new <strong>DeepEncoder V2</strong>. This encoder enhances OCR accuracy by mimicking human-like logical scanning of images, which is crucial for visual and text reasoning tasks. The diagram in the image illustrates the model's 'Visual Causal Flow', emphasizing its ability to form a global understanding of the content before determining the reading order. A comparative table in the image shows improved edit distances for various document elements, highlighting the model's superior performance over its predecessor.</strong> A user shared a demo link for others to try out the model, indicating community interest in hands-on experimentation. Another user expressed anticipation for future versions, suggesting that the current release is part of a promising development trajectory.</p>\n<ul>\n<li>DeepSeek-OCR 2 has been released, and a demo is available for users to try out the model at <a href=\""https://deepseek-ocr-v2-demo.vercel.app/\"">this link</a>. This provides an opportunity for users to experience the model's capabilities firsthand without needing to install it locally.</li>\n<li>A user noted that DeepSeek-OCR 1 excelled in understanding document layout but had limitations, such as missing content like headers, footers, and light-on-dark text. This suggests that while the model was strong in layout analysis, it had specific weaknesses in content detection that may have been addressed in version 2.</li>\n<li>There is interest in whether there are any ready-to-use online APIs for DeepSeek-OCR 2, indicating a demand for accessible, cloud-based solutions that do not require extensive technical setup. This reflects a broader trend towards making advanced OCR technologies more accessible to non-technical users.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/StableDiffusion/comments/1qohra7/here_it_is_boys_z_base/\"">Here it is boys, Z Base</a></strong> (Activity: 2374): <strong>The image is a screenshot from the Hugging Face model repository for \""Z-Image\"" by <strong>Tongyi-MAI</strong>, showcasing an efficient image generation model. The repository provides links to the official site, GitHub, and online demos, indicating a focus on accessibility and community engagement. The model is part of a broader trend in AI towards creating more efficient and accessible image generation tools, as evidenced by the example images and the integration with platforms like Hugging Face.</strong> Commenters are curious about potential applications and modifications of the model, such as \""finetuning\"" it on different datasets, indicating interest in its adaptability and performance in various contexts.</p>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/StableDiffusion/comments/1qojw11/zimage_base_vs_zimage_turbo/\"">Z-Image Base VS Z-Image Turbo</a></strong> (Activity: 927): <strong>The post discusses a comparison between <strong>Z-Image Base</strong> and <strong>Z-Image Turbo</strong> models, highlighting their performance differences. The Turbo model operates at <code>2 iterations per second</code> (7 seconds per image), while the Base model runs at <code>1 iteration per second</code> (40 seconds per image). The settings include a seed of <code>4269</code>, steps of <code>12 for Turbo</code> and <code>40 for Base</code>, using the <code>res_multistep</code> sampler, <code>simple</code> scheduler, and a <code>CFG</code> of <code>4 for Base</code>. The Turbo model is noted for being \""simpler\"" and sometimes more \""realistic,\"" whereas the Base model is praised for its visual quality.</strong> Commenters compare the models to \""SDXL,\"" suggesting a new era in image generation. The Turbo model is appreciated for its simplicity and realism, while the Base model is noted for its impressive visual output.</p>\n<ul>\n<li>Gilded_Monkey1 raises a technical question about the number of steps required for the composition to settle in Z-Image models, particularly when using it as a variation starter in image-to-image (i2i) tasks. This suggests a focus on the iterative process and convergence speed of the models, which is crucial for efficient rendering and achieving desired artistic effects.</li>\n<li>diogodiogogod provides a comparative analysis of Z-Image Base and Z-Image Turbo, noting that while the Turbo version is 'simpler' and often more 'realistic', the Base version excels in visual appeal. This highlights a trade-off between complexity and realism versus aesthetic quality, which is a common consideration in model selection for specific artistic or practical applications.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h1>AI Discord Recap</h1>\n<blockquote>\n<p>A summary of Summaries of Summaries by Gemini 3.0 Pro Preview Nov-18</p>\n</blockquote>\n<p><strong>Theme 1. Model Wars: Kimi K2.5’s Rise, Arcee’s Trinity, and Arena’s Rebrand</strong></p>\n<ul>\n<li><strong>Kimi K2.5 Tops Open Leaderboards</strong>: The new <strong>Kimi K2.5 Thinking</strong> model claimed the <strong>#1 open model</strong> spot on the <a href=\""https://lmarena.ai/leaderboard/text\"">Text Arena leaderboard</a>, excelling in STEM benchmarks like physics and math. While the <strong>$19/month</strong> subscription or <strong>$0.6/1M tokens</strong> pricing sparked debate, engineers are deploying local quantized versions via <a href=\""https://huggingface.co/unsloth/Kimi-K2.5-GGUF\"">HuggingFace</a> and <strong>Unsloth</strong>.</li>\n<li><strong>Trinity Large: A 400B MoE That Runs Lean</strong>: Arcee AI, Prime Intellect, and Datology released <a href=\""https://openrouter.ai/arcee-ai/trinity-large-preview:free\"">Trinity Large</a>, a <strong>400B parameter</strong> Mixture-of-Experts model that activates only <strong>13B parameters per token</strong> for efficiency. The open-weight model uses <strong>256 experts</strong> with aggressive routing (1.56%) to balance frontier-scale knowledge with inference speed.</li>\n<li><strong>LMArena Becomes Arena, Clones Claude UI</strong>: The popular leaderboard rebranded to <strong>Arena</strong> (<a href=\""https://arena.ai/\"">arena.ai</a>) with a UI overhaul that users immediately labeled a <strong>Claude clone</strong>, alongside complaints about aggressive Google <strong>captchas</strong>. The update includes a new <a href=\""https://lmarena.ai/?chat-modality=code\"">Code Arena</a> and expanded leaderboards, though users are demanding the return of a stop button and legacy emojis.</li>\n</ul>\n<p><strong>Theme 2. Dev Tooling Shifts: Cursor Limits, LM Studio Headless, and Unsloth Quirks</strong></p>\n<ul>\n<li><strong>Cursor’s Auto Mode Paywall Stings</strong>: Developers expressed frustration as <strong>Cursor</strong> ended unlimited \""Auto mode,\"" capping usage within the <strong>$20/month</strong> subscription and charging <strong>$1.25/1M</strong> input tokens thereafter. Users also reported a vanishing <strong>revert button</strong> bug, though some are pivoting to <strong>Cursor CLI</strong> for a smaller memory footprint on large codebases.</li>\n<li><strong>LM Studio v0.4 Goes Headless</strong>: The release of <strong>LM Studio v0.4</strong> introduces <strong>headless mode</strong> and parallel inference via a stateful <strong>REST API</strong>, enabling deployment on CI/CD pipelines and non-GUI servers (<a href=\""https://lmstudio.ai/blog/0.4.0\"">release notes</a>). Engineers also discovered hidden <strong>ROCm</strong> support for AMD GPUs in the runtime settings, unlocking hardware acceleration previously obscured in the UI.</li>\n<li><strong>Unsloth Battles GLM 4.7 and CUDA Versions</strong>: Engineers fine-tuning <strong>GLM 4.7</strong> faced compatibility hell between <strong>CUDA 12.8</strong> drivers on Blackwell B200s and the model's <strong>CUDA 13.x</strong> requirements. Successful workarounds involved force-reinstalling <strong>vllm</strong> with specific torch backends and removing <code>fp8</code> cache flags due to Ada Lovelace incompatibilities.</li>\n</ul>\n<p><strong>Theme 3. Security, Jailbreaks, and Scams</strong></p>\n<ul>\n<li><strong>Magic String Lobotomizes Claude</strong>: Red teamers discovered a specific string, <code>ANTHROPIC_MAGIC_STRING_TRIGGER_REFUSAL...</code>, that acts as a \""circuit breaker\"" to reliably force <strong>Claude</strong> into refusal mode. Meanwhile, hackers are manipulating the <strong>Parallel AI API</strong> via undocumented POST requests to inject custom system prompts.</li>\n<li><strong>Clawdbot Exposed as Credential Harvester</strong>: The community issued warnings about <strong>Clawdbot</strong> (rebranded as <strong>Moltbot</strong>), an agentic system that centralizes API keys from OpenAI, Google, and Anthropic. Users characterize it as a <em>\""store now, decrypt later\""</em> security risk susceptible to prompt injection attacks that could exfiltrate sensitive credentials.</li>\n<li><strong>OpenAI Prism: Science Tool or Security Risk?</strong>: OpenAI launched <a href=\""https://archive.md/d9Vsf\"">Prism</a>, a research workspace for scientists powered by <strong>GPT-5.2</strong>, but reception is mixed with some labeling it <em>\""damaging to scientific research.\""</em> Researchers are probing its susceptibility to adversarial attacks, noting that <strong>GPT Pro 5.2</strong> has simultaneously lost the ability to analyze ZIP files.</li>\n</ul>\n<p><strong>Theme 4. Agentic Frontiers: Vision, Coding, and Future Forecasts</strong></p>\n<ul>\n<li><strong>Karpathy Predicts 80% Agent-Coded Future</strong>: Andrej Karpathy forecast that <strong>80% of coding</strong> will be agent-driven by 2026, relying on LLMs' increasing tenacity and goal-setting rather than human syntax management (<a href=\""https://xcancel.com/karpathy/status/2015883857489522876\"">tweet</a>). Simultaneously, discussions on <strong>agentic harnesses</strong> suggest that smart models will soon replace complex orchestrators like <strong>LangChain</strong> in favor of filesystem-based collaboration.</li>\n<li><strong>Gemini 3 Flash Gains Agentic Vision</strong>: Google introduced <a href=\""https://blog.google/innovation-and-ai/technology/developers-tools/agentic-vision-gemini-3-flash/\"">Agentic Vision</a> for <strong>Gemini 3 Flash</strong>, enabling the model to actively zoom, crop, and inspect images to ground its reasoning. Front-end developers report this capability is nearing <strong>SOTA</strong>, outperforming OpenAI's static analysis by dynamically manipulating visual inputs.</li>\n<li><strong>C++ Reigns Supreme for Agents</strong>: In a push against \""bloated\"" Python frameworks, engineers argued that high-performance agents should be built in <strong>C++</strong>, recommending stacks like <strong>fastwhisper.cpp</strong> for STT and <strong>LFM2.5vl</strong> for vision. This aligns with the release of a <strong>LeetCode MCP server</strong> that allows Claude to solve coding challenges directly from the terminal.</li>\n</ul>\n<p><strong>Theme 5. Low-Level Optimization &#x26; Hardware Internals</strong></p>\n<ul>\n<li><strong>Decart’s Lucy 2 &#x26; Hardware Hiring</strong>: Decart released <strong>Lucy 2</strong>, an autoregressive video model, and is actively hiring for <strong>Trainium 3</strong> and low-latency kernel development (<a href=\""https://x.com/DecartAI/status/2016134190509498740\"">tech report</a>). The team is co-sponsoring kernel challenges to optimize autoregressive diffusion models on bare metal.</li>\n<li><strong>Mojo Generates GTK Bindings</strong>: The <strong>Modular</strong> team announced autogenerated <strong>GTK bindings</strong> for Mojo, promising easier GUI development to be showcased at their February community meeting. Engineers are also analyzing <strong>Mojo vs CUDA/HIP</strong> performance on H100s, debating if Mojo's <code>out</code> parameters successfully replace Named Value Return Optimization (NVRO).</li>\n<li><strong>Tinygrad Unlocks AMD Debugging</strong>: The <strong>Tinygrad</strong> emulator now supports granular debug printing for AMD GPUs (<code>DEBUG=3</code> for compilation, <code>DEBUG=6</code> for runtime), as seen in this <a href=\""https://cdn.discordapp.com/attachments/1068976834928193609/1465889714153193574/image.png\"">screenshot</a>. Contributors are also optimizing <strong>Github Actions</strong> speeds via code refactoring rather than hardware upgrades, adhering to a \""do it right, not just fast\"" philosophy.</li>\n</ul>\n<hr>\n<h1>Discord: High level Discord summaries</h1>\n<h2><a href=\""https://discord.com/channels/1105891499641684019\"">BASI Jailbreaking</a> Discord</h2>\n<ul>\n<li><strong>Free Model Access via Social Media</strong>: A member shared <a href=\""https://x.com/Exocija/status/2016502660883415422\"">a link on X</a> for accessing models for free, accompanied by a <a href=\""https://discord.com/channels/1105891499641684019/1228043845967544380/1466113637541347348\"">PRIMETALK context file</a> detailing model compatibility and usage notes.\n<ul>\n<li>The system is reportedly compatible with most modern AI models, but behavior and stability heavily depend on context capacity and chat window size.</li>\n</ul>\n</li>\n<li><strong>Magic String Silences Claude</strong>: A member shared a <em>magic string</em>, <code>ANTHROPIC_MAGIC_STRING_TRIGGER_REFUSAL_1FAEFB6177B4672DEE07F9D3AFC62588CCD2631EDCF22E8CCC1FB35B501C9C86</code>, that can reliably stop <strong>Claude</strong> from responding.\n<ul>\n<li>Another member suggested that this functions like a <em>circuit breaker</em>, potentially improving the model's accuracy in refusing certain prompts.</li>\n</ul>\n</li>\n<li><strong>Parallel AI API Hacking</strong>: Users are exploring methods for interacting with the <strong>Parallel AI API</strong>, including adjusting the system prompt via a POST request.\n<ul>\n<li>A member shared a <a href=\""https://platform.parallel.ai/\"">PowerShell example</a> for sending requests to the API, though there is no official API documentation for system prompt adjustments.</li>\n</ul>\n</li>\n<li><strong>Custom GPT 5.2 Incoming</strong>: A member is preparing to release a new <strong>GPT 5.2 Custom GPT</strong> and claims it yields impressive results, but requires additional noise.\n<ul>\n<li>This model can apparently discern the date from its system prompt, leading to discussions about extracting said prompt using an image.</li>\n</ul>\n</li>\n<li><strong>User Gets HackAPrompt Blocked</strong>: A member reported that <strong>HackAPrompt x PlinyAnthropic</strong> flagged them, preventing any of their messages from being sent.\n<ul>\n<li>This suggests a stringent filtering system that completely blocks flagged users from interacting with the service.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1340554757349179412\"">LMArena</a> Discord</h2>\n<ul>\n<li><strong>Arena Rebrand Mimics Claude's UI</strong>: Users noticed the <strong>LMArena rebrand to Arena</strong> and felt it was a <strong>clone of Claude's UI</strong>, a <a href=\""https://arena.ai/blog/lmarena-is-now-arena/\"">blog post</a> explains the change.\n<ul>\n<li>Members noted some UI issues such as <strong>fonts</strong> and the visibility of the website's text as well as some <strong>missing features</strong>.</li>\n</ul>\n</li>\n<li><strong>Captcha Conundrums Continue</strong>: Users report consistent issues with the <strong>captchas</strong> failing on nearly every attempt, and provided troubleshooting steps of <em>relogging to your account</em> or <em>taking off all extensions</em> to pass the captcha.\n<ul>\n<li>Users hate the captcha and wish the old emojis, stickers, and features would return.</li>\n</ul>\n</li>\n<li><strong>Login Lost? Recover Button to the Rescue!</strong>: A member experiencing login issues shared a screenshot of a <a href=\""https://cdn.discordapp.com/attachments/1340554757827461211/1466134595035467829/Hdbd.png?ex=697ba3be&#x26;is=697a523e&#x26;hm=2be3961be4c941479f9ec51709c5eb6af5ea9c79ad3918eb6a15a964ec9fe720&#x26;\"">recover button</a> that can be clicked in order to log back into the updated <strong>Arena</strong>.\n<ul>\n<li>Another member noted an <a href=\""https://youtu.be/TNoAlMv4Eg8?si=d86SArLb6yQ8sdLE\"">announcement video</a> as well.</li>\n</ul>\n</li>\n<li><strong>Kimi K2.5 Thinking Ascends Text Arena Leaderboard</strong>: The <a href=\""https://lmarena.ai/leaderboard/text\"">Text Arena leaderboard</a> has been updated and <code>Kimi K2.5 Thinking</code> is now ranked the <strong>#1 open model</strong> and ranking <strong>#15 overall</strong>.\n<ul>\n<li><code>Kimi K2.5 Thinking</code> is <strong>#7</strong> in Coding, <strong>#7</strong> in Instruction Following, and <strong>#14</strong> in Hard Prompts, and has also been added to the <a href=\""https://lmarena.ai/?chat-modality=code\"">Code Arena</a>.</li>\n</ul>\n</li>\n<li><strong>Arena Shorts, Better AI videos in under 90 seconds!</strong>: <strong>Arena</strong> (formerly LMArena) has uploaded a <code>Better AI videos in under 90 seconds</code> video to <a href=\""https://www.youtube.com/watch?v=0hCI2XEh0x0\"">their Youtube channel</a>.\n<ul>\n<li>The group acknowledged that as the platform evolves from only Language Models, the name is becoming more generic and it was <a href=\""https://lmsys.org/\"">previously part of LMSYS</a>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1179035537009545276\"">Unsloth AI (Daniel Han)</a> Discord</h2>\n<ul>\n<li><strong>Batch Size Bumps GPU Benefit</strong>: Members discovered that one way to achieve decent GPU utilization is to <em>increase batch size</em> until utilization improves, balancing it with potential gains from <strong>GA (Genetic Algorithms)</strong>.\n<ul>\n<li>Also, a member inquired whether Unsloth will release a <strong>Q3</strong> version for <strong>Kimi 2.5</strong>, voicing concerns about accuracy drops.</li>\n</ul>\n</li>\n<li><strong>Oracle's Offerings Spark Skepticism</strong>: A member inquired if <strong>Oracle</strong> stands as state-of-the-art in <strong>RAG (Retrieval-Augmented Generation)</strong> and fine-tuning tech, setting off a debate.\n<ul>\n<li>The terse reply of, <em>\""What 😅\""</em>, was later amended to allow that <strong>OCI (Oracle Cloud Infrastructure)</strong> does have some good tools, showing split opinions.</li>\n</ul>\n</li>\n<li><strong>Arcee's Arithmetic: Trinity Costs $350k</strong>: A new <strong>Arcee model</strong> image was shared, along with the note that pretraining cost about <strong>$350k</strong>, with a link to the <a href=\""https://github.com/arcee-ai/trinity-large-tech-report/blob/main/Arcee%20Trinity%20Large.pdf\"">Trinity Large Tech Report</a>.\n<ul>\n<li>It was clarified that <strong>GLM 4.7</strong> is a <strong>358B</strong> parameter model but <em>not a base model</em>, making benchmark comparisons less useful against models such as <strong>GLM 4.5</strong>.</li>\n</ul>\n</li>\n<li><strong>Gemini's Gatekeeping Game</strong>: A Google hackathon showed that, despite heavy output filtering, especially for corporate/government settings, <strong>Gemini's API</strong> can be made to produce almost anything.\n<ul>\n<li>One member got the voice models to swear by putting it in the system prompt.</li>\n</ul>\n</li>\n<li><strong>Modal Multi-GPU Mayhem</strong>: A member ran into problems training a <strong>Qwen3</strong> model on 3 GPUs on <strong>Modal</strong>, getting a <em>ValueError</em> from an incorrect <code>device_map</code> configuration.\n<ul>\n<li>The training setup ultimately moved away from <strong>Unsloth</strong> due to incompatibility with <strong>PyTorch 2.4.1</strong>, choosing a <strong>transformers + PEFT</strong> setup for better stability.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1091220969173028894\"">OpenRouter</a> Discord</h2>\n<ul>\n<li><strong>Arcee releases Trinity Large Preview for Free</strong>: Arcee launched <strong>Trinity-Large-Preview</strong>, a chat-ready variant of its frontier-scale open-weight model, which is free for a limited time and detailed on <a href=\""https://x.com/OpenRouterAI/status/2016280059527757995?s=20\"">X</a>.\n<ul>\n<li>The model is a <strong>400B parameter sparse Mixture-of-Experts</strong> model with <strong>13B active parameters per token</strong>, utilizing <strong>256 experts</strong> with <strong>4 active per token</strong> (1.56% routing) for efficiency, discussed during <a href=\""https://youtube.com/live/3XSdqHY0kNk?feature=share\"">Lucas Atkins' livestream</a>.</li>\n</ul>\n</li>\n<li><strong>Free Credits Boost Cyberpad</strong>: A user updated <a href=\""https://cyberpad.site\"">Cyberpad</a> to include some free credits.\n<ul>\n<li>No further information was provided.</li>\n</ul>\n</li>\n<li><strong>Image Model Output Glitches Reported</strong>: Users reported that certain image models such as <strong>GPT-5 Image Mini</strong>, <strong>GPT-5 Image</strong>, and <strong>Gemini 2.5 Flash Image</strong> are not consistently generating images, although <strong>Gemini 2.5 flash</strong> works intermittently.\n<ul>\n<li>Models like <strong>Gemini 3 Flash Preview</strong>, <strong>Gemini 2.5 Flash Lite Preview</strong>, <strong>Seed 1.6</strong>, <strong>GLM-4.6v</strong>, and <strong>Grok 4.1-fast</strong> have functional <code>response_format</code> support.</li>\n</ul>\n</li>\n<li><strong>OpenRouter Users Await Refunds</strong>: Users are experiencing significant delays in receiving refunds from OpenRouter, with some waiting since early January and submitting multiple support tickets.\n<ul>\n<li>Users are requesting clarity on refund timelines and improved communication from the <strong>OpenRouter</strong> team.</li>\n</ul>\n</li>\n<li><strong>Agentic Vision with Gemini 3 Flash Debuts</strong>: Google introduced <a href=\""https://blog.google/innovation-and-ai/technology/developers-tools/agentic-vision-gemini-3-flash/\"">Agentic Vision</a> with <strong>Gemini 3 Flash</strong>, enabling visual reasoning and code execution for step-by-step image manipulation.\n<ul>\n<li>OpenAI's <strong>O3</strong> and <strong>O4-mini</strong> are extending image capabilities by enabling chain-of-thought reasoning with images for tasks like cropping, zooming, and rotating, discussed in <a href=\""https://openai.com/index/thinking-with-images/\"">this blog post</a>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1074847526655643750\"">Cursor Community</a> Discord</h2>\n<ul>\n<li><strong>Vanishing Revert Button Frustrates Users</strong>: Users reported the <strong>revert button</strong> disappearing from the UI, leading to frustration and token waste, with one finding that <a href=\""https://cdn.discordapp.com/attachments/1074847527708393565/1465828018789552390/image.png?ex=697bd7b9&#x26;is=697a8639&#x26;hm=68ec5dd17c7a1be84a1f639f9a5a98db91ba3bd191336f2afe3e8252b804b12e&#x26;\"">duplicating an older chat</a> brought it back.\n<ul>\n<li>A member found that not clicking on the revert button would make it reappear, suggesting it was a <strong>one-time bug</strong>.</li>\n</ul>\n</li>\n<li><strong>Cursor CLI: The Dark Horse?</strong>: Some developers are preferring <strong>Cursor CLI</strong> over the IDE due to a smaller memory footprint, which helps them avoid IDE crashes and model unresponsiveness, especially with larger projects exceeding 100k LOC.\n<ul>\n<li>Conversely, one user found <strong>Cursor CLI inside the IDE</strong> (with WSL as the terminal) to be <em>\""pure trash.. like for real, not usable\""</em>, reporting the UI is not smooth even with 64GB of RAM and an i7 processor.</li>\n</ul>\n</li>\n<li><strong>Cursor's Subscription Adjustment Stings</strong>: After September 15th, <strong>auto mode is no longer unlimited</strong> and counts toward the $20 monthly allowance, priced at $1.25 per 1M tokens for Input + Cache Write, $6.00 per 1M tokens for Output, and $0.25 per 1M tokens for Cache Read.\n<ul>\n<li>One user discovered they could burn through their monthly subscription very quickly, suggesting it may be cheaper to <em>use their own api keys, or use Claude Code</em>.</li>\n</ul>\n</li>\n<li><strong>Clawdbot's Security Flaw Exposed</strong>: A user shared links regarding <strong>security concerns with Clawdbot</strong>, reporting that exposed control panels pose credential leaks and account takeovers.\n<ul>\n<li>There is speculation it could lead to a <em>\""store now, decrypt later\""</em> data breach due to potential quantum decryption issues, and that the company got a cease and desist for the issues.</li>\n</ul>\n</li>\n<li><strong>Gemini Vision Set to Revolutionize Front-End</strong>: A user found that <strong>Gemini agentic vision</strong> is nearing state-of-the-art (SOTA) performance for vision tasks, and believes its integration would simplify front-end development.\n<ul>\n<li>Members stated that they can't wait to see vision integrated into the agent, and that it is superior to the <code>Auto</code> tool.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1110598183144399058\"">LM Studio</a> Discord</h2>\n<ul>\n<li><strong>LM Studio v0.4 Goes Headless and Parallel</strong>: <strong>LM Studio v0.4</strong> introduces <strong>headless mode</strong> and <strong>parallel inference</strong>, with users excited about the new capabilities and a revamped UI, as detailed in the <a href=\""https://lmstudio.ai/blog/0.4.0\"">complete blogpost here</a>.\n<ul>\n<li>Note that in-app updates require reinstalling the app, and some UI elements are now in <strong>dev mode</strong>.</li>\n</ul>\n</li>\n<li><strong>GLM 3.7 Flash Shows Coding Potential</strong>: Members note that <strong>GLM 3.7 Flash</strong> shows good coding ability, but <strong>GPT OSS 120</strong> is expected to be the superior coder, especially at <strong>Q4</strong>.\n<ul>\n<li>This suggests that while <strong>GLM 3.7 Flash</strong> is a step forward, it may not outperform existing models.</li>\n</ul>\n</li>\n<li><strong>ROCm Runs on LM Studio Runtime</strong>: Users discovered that <strong>ROCm</strong> can be enabled within <strong>LM Studio</strong> under the runtime settings, though the method was initially obscured for some users, as discussed in this <a href=\""https://www.reddit.com/r/unsloth/comments/1qpbmrt/you_can_now_run_kimi_k25_locally/\"">Unsloth Reddit thread</a>.\n<ul>\n<li>This integration allows users to leverage <strong>ROCm</strong> for potentially improved performance.</li>\n</ul>\n</li>\n<li><strong>Devstral-2 Demands Decent GPU Deployment</strong>: Members discussed the hardware requirements for running <strong>Devstral-2</strong> locally, with one user suggesting <strong>48GB of GPU</strong> (e.g., 3090) for the 24B version.\n<ul>\n<li>For the 120B version, parallel computing or an <strong>H200 with EXL2</strong> model format were suggested, as GGUF was deemed too slow.</li>\n</ul>\n</li>\n<li><strong>Hardware Acceleration Seeks Hook into LM Studio</strong>: A member from a hardware accelerator company inquired about adding an <strong>LM Studio backend</strong> for their hardware, and was pointed to <strong>llama.cpp</strong>.\n<ul>\n<li>It was noted that LM Studio is primarily a closed source project by Element Labs, and pointed to <a href=\""https://lmstudio.ai/enterprise\"">LM Studio Enterprise</a>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1369594130807787570\"">Moonshot AI (Kimi K-2)</a> Discord</h2>\n<ul>\n<li><strong>Kimi K2.5's Price Tag Raises Eyebrows</strong>: Users debated the <strong>$19</strong> monthly subscription for <strong>Kimi K2.5</strong>, with some finding it <em>expensive</em> and questioning whether a recurring deal could be established.\n<ul>\n<li>Others suggested sticking to the free tier, arguing that smaller Chinese companies like Moonshot AI need to run large models like K2.5, making lower prices unlikely.</li>\n</ul>\n</li>\n<li><strong>Google's AI Studio Training Sparks Privacy Debate</strong>: Concerns arose over <strong>Google's</strong> practices of <strong>training and viewing conversations</strong> in <strong>AI Studio and Gemini apps</strong>, raising privacy issues.\n<ul>\n<li>Conversely, another user mentioned they <strong>open source their projects</strong>, suggesting the data's inevitable inclusion in training datasets regardless.</li>\n</ul>\n</li>\n<li><strong>Model Selection Showdown: Kimi K2.5 Triumphs in STEM</strong>: Users compared <strong>Kimi K2.5</strong> against <strong>Mistral and Qwen</strong> for tasks spanning coding to general question-answering.\n<ul>\n<li>Notably, <strong>Kimi K2.5</strong> boasts the <em>highest benchmarks</em> in physics, chemistry, and math, while also demonstrating <em>strong performance in design and logical reasoning</em>.</li>\n</ul>\n</li>\n<li><strong>Kimi CLI Outpaces Alternatives in Speed Trials</strong>: <strong>Kimi CLI</strong> was lauded for its speed and efficiency over tools like <em>oh-my-opencode</em>, particularly in web page analysis, with reduced token consumption.\n<ul>\n<li>However, some found the model's output quality <em>less impressive</em>, suggesting further comparative analysis is warranted.</li>\n</ul>\n</li>\n<li><strong>Agent Swarm Utility Under Question</strong>: Enthusiasts highlighted <strong>Agent Swarm's</strong> in-depth research capabilities with Kimi, but noted it can deplete credits at <strong>3x</strong> the normal rate.\n<ul>\n<li>Others remained uncertain about its applications, suggesting a need for clearer use-cases and caution regarding resource consumption.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1047197230748151888\"">Perplexity AI</a> Discord</h2>\n<ul>\n<li><strong>Perplexity Subs Deemed a Scam</strong>: Several users reported <strong>unexpected subscription changes</strong> and <strong>charges</strong> after automatic renewals, with one user canceling their subscription, calling it a <em>scam.</em>\n<ul>\n<li>Users experienced issues such as being charged without receiving service or not obtaining refunds, prompting some to consider contacting their banks or reporting the matter to the FTC.</li>\n</ul>\n</li>\n<li><strong>Query Cap Shenanigans Baffle Users</strong>: Some users reported issues with <strong>query limits</strong> on their <strong>Pro subscriptions</strong>, with limits dropping to one query per hour.\n<ul>\n<li>However, some users saw their limits restored to 600, and one user shared a <a href=\""https://www.perplexity.ai/rest/rate-limit/all\"">link</a> to check query limits.</li>\n</ul>\n</li>\n<li><strong>Image Generation Restricted By Region?</strong>: Users reported <strong>image generation restrictions</strong> in certain regions, possibly due to <strong>xAI controversies</strong> and an EU lawsuit.\n<ul>\n<li>Suggestions included trying different models or contacting support; a user from India confirmed they were affected by this issue.</li>\n</ul>\n</li>\n<li><strong>Kimi 2.5 Coming Soon to PPLX?</strong>: Users are eagerly anticipating the release of the <strong>Kimi 2.5 model</strong> on Perplexity.\n<ul>\n<li>Speculation suggests that Perplexity typically implements updates quickly.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1053877538025386074\"">Nous Research AI</a> Discord</h2>\n<ul>\n<li><strong>GPT Pro Hides the Model Magic?</strong>: Members debated whether <strong>GPT Pro's</strong> performance boost comes from more GPUs or an improved model, suggesting <strong>OpenAI</strong> might obscure the truth for competitive reasons.\n<ul>\n<li>One member likened <strong>OpenAI's</strong> pricing strategy to <em>fakery</em>, comparing it to impressions over measured value, similar to the stock market's perception of <strong>Tesla</strong>.</li>\n</ul>\n</li>\n<li><strong>DeepSeek's Never-Ending Imprisonment</strong>: It was reported that <strong>DeepSeek</strong> tends to get stuck in a jailbreak loop, repeating the same rejection message indefinitely, regardless of subsequent prompts.\n<ul>\n<li>While the API endpoints fare slightly better, the raw model is effectively <em>cooked</em> once it enters this state.</li>\n</ul>\n</li>\n<li><strong>TI-84 Gets Neural Network Transplant</strong>: A member detailed running a neural network on a <strong>TI-84 Plus</strong> calculator for spellchecking, documenting the process on an <a href=\""https://hermesoptimus.vercel.app/\"">academic website</a> with a demo video.\n<ul>\n<li>The member joked that despite this achievement, their work on <strong>Claude Code Orchestration</strong> remains more practically useful.</li>\n</ul>\n</li>\n<li><strong>MergeMix Paper Sparks Data Mixture Excitement</strong>: The paper '<a href=\""https://arxiv.org/pdf/2601.17858\"">MergeMix: Optimizing Mid-Training Data Mixtures via Learnable Model Merging</a>' garnered interest due to its relevance for open source projects with limited budgets.\n<ul>\n<li>The paper explores techniques for optimizing <strong>data mixtures</strong> and <strong>model merging</strong> during training, potentially offering resource-efficient strategies.</li>\n</ul>\n</li>\n<li><strong>Hermes 4 Pricing: Discount or Deception?</strong>: A member questioned whether the discounted pricing for <strong>Hermes 4 series</strong> models is permanent before subscribing to the API, citing its superiority in RP and story-writing compared to <strong>Deepseek</strong>.\n<ul>\n<li>Another member clarified there's no subscription, just credit purchases subject to change, so the value depends on <strong>pricing</strong> and <strong>usage</strong>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/974519864045756446\"">OpenAI</a> Discord</h2>\n<ul>\n<li><strong>Gemini 3 Pro Fumbles Subtitle Generation</strong>: Users reported that <strong>Gemini 3 Pro</strong> is fabricating .srt files with <em>nothing</em> related to the audio in the video.\n<ul>\n<li>This poor performance led to disappointment among users who stated that <strong>Gemini</strong> is <em>overhyped</em>.</li>\n</ul>\n</li>\n<li><strong>Clawdbot rebranded Moltbot is a Scam</strong>: <strong>Clawdbot</strong>, now known as <strong>moltbot</strong>, is an agentic system that controls your entire OC by API keys from Anthropic, Google, and OpenAI, and users are being warned against it.\n<ul>\n<li>One user stated that it is <em>a huge scam by crypto bros to steal your information</em>, which can be weaponized via prompt injection, raising significant security and privacy concerns.</li>\n</ul>\n</li>\n<li><strong>Prism Deemed Detrimental to Scientific Research</strong>: Despite <strong>OpenAI</strong>'s aims to advance science with <strong>Prism</strong>, one user stated that <strong>Prism</strong> is damaging to scientific research.\n<ul>\n<li>Another user inquired about <strong>Prism</strong>'s API access, to write some of their project using other <strong>AI</strong> and <strong>Codex</strong>.</li>\n</ul>\n</li>\n<li><strong>GPT Pro Loses Zip File Reading</strong>: A user reported that <strong>GPT Pro 5.2</strong>, which could previously read and analyze <strong>ZIP files</strong>, is now failing to find uploaded files for analysis.\n<ul>\n<li>The user is asking if others are experiencing the same issue, or has any insight.</li>\n</ul>\n</li>\n<li><strong>Blocking Black and White Images via Chiaroscuro Avoidance</strong>: Users discussed an image generation issue related to the <strong>Chiaroscuro effect</strong> and have suggested <em>'Please avoid Chiaroscuro'</em> in prompts if encountering unwanted <strong>black and white images</strong>.\n<ul>\n<li><strong>Chiaroscuro</strong> is the use of strong contrasts between light and dark, usually bold contrasts affecting a whole composition.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1189498204333543425\"">GPU MODE</a> Discord</h2>\n<ul>\n<li><strong>Decart drafts SF perf engineers</strong>: Decart seeks engineers for low-latency kernels, real-time video/world models, and accelerators like <strong>Trainium 3</strong> (as shown at ReInvent <a href=\""https://www.youtube.com/watch?v=K49S79wOGl8\"">video</a>) and their new <strong>Lucy 2</strong> autoregressive video model (<a href=\""https://x.com/DecartAI/status/2016134190509498740\"">tech report</a>).\n<ul>\n<li>They are also co-sponsoring a kernel challenge with <strong>GPU Mode</strong> for autoregressive diffusion models, and encourage interested parties to send perf work to heba@decart.ai.</li>\n</ul>\n</li>\n<li><strong>INT4 QAT RL Model Rollout</strong>: A member shared a link to a <strong>GitHub repo</strong> that focused on squeezing a <strong>1TB model rollout</strong> into a single <strong>H200</strong> using <strong>INT4 QAT RL</strong> end-to-end practice: <a href=\""https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/slime/int4/readme-en.md\"">GitHub repo</a>.\n<ul>\n<li>The repository provides resources and documentation related to the <strong>INT4 QAT RL</strong> implementation, optimizing large model rollouts.</li>\n</ul>\n</li>\n<li><strong>Transformers and PyTorch face upgrade break</strong>: After upgrading <strong>transformers</strong> and <strong>pytorch</strong>, a member reported a <code>NotImplementedError: \""_amp_foreach_non_finite_check_and_unscale_cuda\"" not implemented for 'BFloat16'</code>.\n<ul>\n<li>Downgrading to transformers <strong>4.57.3</strong> fixed the issue; others had similar issues, which are discussed in this <a href=\""https://github.com/pytorch/pytorch/issues/127176\"">pytorch issue</a> and <a href=\""https://github.com/warner-benjamin/optimi/issues/8\"">optimi issue</a>.</li>\n</ul>\n</li>\n<li><strong>Interactive Numerics Tools Emerge</strong>: A member expressed surprise that quantization people have not already created interactive tools for exploring numerics, and cited <a href=\""https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.html\"">captum</a> as one possible tool.\n<ul>\n<li>This member lamented the lack of proper UI/UX in current tools for model debugging, <em>checking which circuit is unstable, which layer is causing a bunch of outlier, simple stuff like that</em>.</li>\n</ul>\n</li>\n<li><strong>DGX's Dominant Memory Bandwidth</strong>: Instruction sets for <strong>DGX</strong> and <strong>5090</strong> are similar, but <strong>DGX</strong> excels with full-speed fp32 accumulation, like <strong>Blackwell PRO</strong>, and its key differentiator is <strong>1.8TB/s</strong> memory bandwidth.\n<ul>\n<li>This contrasts sharply with <strong>5090's 300 GB/s</strong>, emphasizing the importance of efficient <strong>L2 cache</strong> utilization to maximize <strong>DGX's</strong> potential.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/822583790773862470\"">Latent Space</a> Discord</h2>\n<ul>\n<li><strong>Coding Enters the Agent Era</strong>: Andrej Karpathy forecasts that <strong>80% of coding</strong> will be agent-driven by 2026, highlighting LLMs' tenacity and goal-setting capabilities; insights <a href=\""https://xcancel.com/karpathy/status/2015883857489522876\"">here</a>.\n<ul>\n<li>Karpathy also cautioned against potential 'slop' and over-engineering, so it might not all be roses.</li>\n</ul>\n</li>\n<li><strong>OpenAI's Prism Shines for Scientists</strong>: OpenAI unveiled <strong>Prism</strong>, a complimentary research workspace powered by <strong>GPT-5.2</strong>, accessible via the web to those with a personal ChatGPT account; get started <a href=\""https://xcancel.com/openai/status/2016209462621831448?s=46&#x26;t=eWVlK1PU8XfB6f402GJJ9g\"">here</a>.\n<ul>\n<li>The tool aims to provide scientists with advanced AI capabilities for research purposes.</li>\n</ul>\n</li>\n<li><strong>Trinity Large Arrives</strong>: Prime Intellect, Arcee AI, and Datology launched <strong>Trinity Large</strong>, a <strong>400B parameter Mixture of Experts model</strong>, that uses only <strong>13B active parameters</strong>; more info <a href=\""https://xcancel.com/primeintellect/status/2016280792037785624?s=46\"">here</a>.\n<ul>\n<li>The model aims to deliver high performance while maintaining efficiency.</li>\n</ul>\n</li>\n<li><strong>Cursor Indexes Codebases</strong>: Cursor announced faster indexing for large codebases as well as improved semantic search, promising performance enhancements; read more <a href=\""https://xcancel.com/cursor_ai/status/2016202243499073768?s=46\"">here</a>.\n<ul>\n<li>Semantic search and improved indexing aim to provide more efficient code navigation.</li>\n</ul>\n</li>\n<li><strong>Podcast Shifts Focus to Science</strong>: Latent Space has launched its second podcast, 'Science' (<a href=\""https://www.latent.space/p/science\"">link to podcast</a>), hosted by &#x3C;@713947182167883897> and &#x3C;@348078436058660866>.\n<ul>\n<li>Discussions about the new 'Science' podcast have moved to a dedicated channel.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/879548962464493619\"">HuggingFace</a> Discord</h2>\n<ul>\n<li><strong>Kimi 2.5 Model Beats GPT5 Locally</strong>: The new <strong>Kimi 2.5</strong> model is reportedly performing better than <strong>GPT5</strong>, accessible locally via <a href=\""https://huggingface.co/unsloth/Kimi-K2.5-GGUF\"">HuggingFace</a> and also through sites such as <a href=\""https://www.google.com/aclk?sa=L&#x26;ai=DChsSEwiCz-j3iK2SAxUFVX8AHT5cBPkYACICCAEQABoCb2E&#x26;co=1&#x26;gclid=Cj0KCQiA4eHLBhCzARIsAJ2NZoL9Ani52eByT53nVhnOxG_76F9QllEx50YhK_yfQYsD5bH3ov1pAqwaAl2XEALw_wcB&#x26;cid=CAASugHkaDm-Aokq5n3lAlzNAI-Ihc6SdblOJ-BiATzwnaZwDVhVBl3B2U5kGq4mAYjN4wQ992LlqWX5NQ6HksDrhSatp0QEfb7_rWMS_u7_GTCuCkp3YH9fANMaJqDgFvuA6u1bwvl4pJ80zvbUhIFPk7Nrqdpx2PDnsBRncgM3-d1UDhFM-tN117MrOXLWnhycCaPax24T8meZIe-9I2cM5rpAf16KucPGZwg7ixTssRCB7X8RP3B_G4vUCfE&#x26;cce=2&#x26;sig=AOD64_2SRpHfWjuW4kJawyiTyzrGbKZybQ&#x26;q&#x26;adurl&#x26;ved=2ahUKEwiiteP3iK2SAxV85skDHfklKyoQ0Qx6BAgLEAE\"">Fireworks</a>.\n<ul>\n<li>Members seek local agent recommendations for use with <strong>Zed</strong>, expressing dissatisfaction with <strong>GLM-4.7-Flash</strong> at Q4 with llama.cpp, with <strong>kimi</strong> and <strong>qwencoders 30b q4</strong> being suggested as alternatives.</li>\n</ul>\n</li>\n<li><strong>C++ Enthusiast Champions Supreme Rule for AI Agents</strong>: A member argued that <em>C++ is gonna always rule</em> for building AI agents, due to bloat in Python agents, and recommended <strong>fastwhisper.cpp</strong> for STT, <strong>Qwen embeddings</strong> in LlamaCPP for RAG, and <strong>LFM2.5vl</strong> for VLM.\n<ul>\n<li>This sparked conversation around STT (<strong>fastwhisper.cpp</strong>), RAG (<strong>Qwen embeddings</strong> in LlamaCPP), and VLM (<strong>LFM2.5vl</strong>).</li>\n</ul>\n</li>\n<li><strong>Vision Model Vaporizes JPEG Artifacts</strong>: A vision model was released that removes artifacts caused by <strong>JPEG compression</strong> using a unique design with no Batch Norm, no activations after training, and Operator layers instead of Convolutional layers.\n<ul>\n<li>The model's architecture focuses on gaining accuracy through <strong>width</strong> rather than depth.</li>\n</ul>\n</li>\n<li><strong>RemnantInstruct-8B: SLERP Merge Balances Creative &#x26; Factual</strong>: <strong>RemnantInstruct-8B</strong> is a <a href=\""https://huggingface.co/anthonym21/RemnantInstruct-8B-GGUF\"">SLERP merge</a> that recombines a creative fine-tune (<strong>allura-org/remnant-qwen3-8b</strong>) with its base model (<strong>Qwen/Qwen3-8B</strong>) to balance narrative skills with factual accuracy.\n<ul>\n<li>The merge strategy favors the creative fine-tune in self-attention layers and the base model in MLP layers, with the goal of preserving <strong>Qwen3's</strong> thinking mode.</li>\n</ul>\n</li>\n<li><strong>Quantum Computing Embraced by VLMs</strong>: A member open-sourced their undergraduate thesis on specializing <strong>vision-language models</strong> for <strong>quantum computing</strong> and code with <strong>Qiskit</strong>, including a <a href=\""https://huggingface.co/datasets/samuellimabraz/quantum-assistant\"">dataset</a>, <a href=\""https://huggingface.co/collections/samuellimabraz/quantum-assistant\"">models</a>, <a href=\""https://github.com/samuellimabraz/quantum-assistant\"">code</a>, and <a href=\""https://huggingface.co/spaces/samuellimabraz/quantum-assistant\"">demo</a>.\n<ul>\n<li>The thesis explores adapting VLMs to assist with quantum computing tasks and coding.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/714501525455634453\"">Yannick Kilcher</a> Discord</h2>\n<ul>\n<li><strong>Transformers Can Parameterize Vector Fields</strong>: A member argued that <strong>transformers</strong> can be used in <strong>flow matching</strong> as a training objective to parametrize the vector field for continuous diffusion, using <strong>patch embedding</strong> to encode patch position.\n<ul>\n<li>Other members agreed that diffusion models and flow matching are mathematically similar, citing <a href=\""https://arxiv.org/abs/2305.03486\"">this paper on ArXiv</a>.</li>\n</ul>\n</li>\n<li><strong>Diffusion Models are not Better than Autoregression</strong>: A member suggested that the notion of diffusion being superior to autoregression is false, highlighting architectural and scaling limitations, linking to <a href=\""https://arxiv.org/abs/2512.14982\"">this paper on repeating context</a>.\n<ul>\n<li>They pointed out that improvements like repeating the context or re-encoding a sequence non-causally could bridge the gap, overcoming current design limitations in <strong>LLMs</strong>.</li>\n</ul>\n</li>\n<li><strong>ChatGPT Wrappers Flourish, Value Questioned</strong>: Members observed that most new tools are simply <strong>ChatGPT wrappers</strong>, raising questions about their actual value and the ease with which scammers can create wrappers, referencing the <strong>Clawdbot scam</strong>.\n<ul>\n<li>It was suggested that these wrappers are necessary to demonstrate use cases, as they make it easier for people to understand how to apply the models.</li>\n</ul>\n</li>\n<li><strong>AI Coding Tools Won't Replace True Skill</strong>: Despite the rise of <strong>AI coding tools</strong>, members believe coding ability can be relearned, pointing to a <a href=\""https://www.arcee.ai/blog/trinity-large\"">blog post on Trinity Large</a>, adding that fast code production from AI may hinder true understanding.\n<ul>\n<li>They noted that a bad implementation from an <strong>LLM</strong> isn't weighted the same as before, since the mental and time cost to create it was so low.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1068976834382925865\"">tinygrad (George Hotz)</a> Discord</h2>\n<ul>\n<li><strong>AMD Emulator Exposes Debug Printing</strong>: The new AMD emulator (<strong>AMD=1 MOCKGPU=1</strong>) now supports debug printing, where setting <strong>DEBUG=3</strong> prints all compiled instructions and <strong>DEBUG=6</strong> prints them as they run, according to a linked <a href=\""https://cdn.discordapp.com/attachments/1068976834928193609/1465889714153193574/image.png?ex=697b686e&#x26;is=697a16ee&#x26;hm=485c88290bbec976b6b7ab93aed07b21a6a2ec8ba8b28806e14630c00b972b3c&#x26;\"">screenshot</a>.\n<ul>\n<li>This enhancement facilitates more in-depth debugging and analysis of compiled code directly within the emulator environment.</li>\n</ul>\n</li>\n<li><strong>Github Actions Speed Boost via Optimization</strong>: Discussion centered on accelerating GitHub Actions by emphasizing code optimization, instead of only relying on faster hardware or external resources.\n<ul>\n<li>The consensus was to prioritize doing things the <em>right</em> way over quick fixes that only improve surface level metrics, potentially creating tech debt.</li>\n</ul>\n</li>\n<li><strong>MULACC Fusion Receives a Fix</strong>: A fix was proposed to enhance <code>decompositions.py</code> by adding a pattern to fuse (<strong>x &#x3C;&#x3C; n) + c → MULACC(x, 2^n, c)</strong>, specifically targeting integer <strong>MULACC</strong> with power-of-2 constants, as detailed in <a href=\""https://github.com/tinygrad/tinygrad/pull/14387\"">PR 14387</a>.\n<ul>\n<li>This adjustment aims to refine the fusion process, potentially improving the efficiency of certain arithmetic operations.</li>\n</ul>\n</li>\n<li><strong>Egraphs Considered for Universal Fixes</strong>: The potential use of <strong>egraphs</strong> to address problems in a generic manner was explored, emphasizing the importance of simplicity.\n<ul>\n<li>It was also suggested to tag rewrites with their origin to maintain a clear record of equivalences created during rewriting processes.</li>\n</ul>\n</li>\n<li><strong>Mac MetalCompiler Improvements on the Horizon</strong>: Suggested improvements to the hacks for <strong>MetalCompiler</strong> on Mac are on the way, especially focusing on improvements and cleanups that reduce line count and improve readability.\n<ul>\n<li>The goal is to make the <strong>MetalCompiler</strong> more maintainable and efficient, benefiting developers working on Mac platforms.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1087530497313357884\"">Modular (Mojo 🔥)</a> Discord</h2>\n<ul>\n<li><strong>GTK Bindings Auto-Generated</strong>: <strong>Hammad Ali</strong> will present autogenerated <strong>GTK bindings for Mojo</strong> at the <strong>Modular Community Meeting</strong> on February 2nd at 10 AM PT, according to <a href=\""https://forum.modular.com/t/february-community-meeting/2646\"">the Modular forum</a>.\n<ul>\n<li>The presentation will detail how <strong>GTK bindings</strong> are automatically generated, potentially improving the ease of creating <strong>GUIs with Mojo</strong>.</li>\n</ul>\n</li>\n<li><strong>Mojo's Performance Prowess</strong>: <strong>Tatiana Melnichenko</strong> will share memory-bound bandwidth results and compute-bound gaps on <strong>H100/MI300A</strong> comparing <strong>Mojo with CUDA/HIP</strong> at the February Community Meeting.\n<ul>\n<li>This talk should provide insights into <strong>Mojo's performance characteristics</strong> relative to established <strong>GPU</strong> programming models.</li>\n</ul>\n</li>\n<li><strong>macOS Gatekeeper Gets in the Way</strong>: Members suspect performance difference between first and subsequent runs on macOS is due to <strong>Gatekeeper's trust dance</strong>.\n<ul>\n<li>Clearing the quarantine <code>xattr</code> or ad-hoc codesigning could mitigate this, and wondered if a codesign step in <code>mojo build</code> could hide this entirely.</li>\n</ul>\n</li>\n<li><strong><code>out</code> Parameters Outshine NVRO</strong>: <code>out</code> parameters in Mojo name the location where the return value of a function will end up, serving as a <strong>Named Value Return Optimization (NVRO)</strong> replacement.\n<ul>\n<li>Members claim this provides a guarantee about the return value's destination, unlike relying on compiler optimization.</li>\n</ul>\n</li>\n<li><strong>Qwen3 Embedding Model Gets Accuracy Boost</strong>: A member requested a review of their <a href=\""https://github.com/modular/modular/pull/5823\"">PR for the Qwen3 embedding model</a>, citing that the fix is important for getting much better accuracy.\n<ul>\n<li>Another member responded that new fixes likely won't be pulled into the upcoming release but would be available in the nightlies, with a single-line fix available <a href=\""https://github.com/modular/modular/compare/main...sbrunk:modular:qwen3-embedding-fix-norm-minimal\"">here</a>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1348819876348825620\"">Manus.im Discord</a> Discord</h2>\n<ul>\n<li><strong>Manus is Credit Crunching</strong>: A user noticed that <strong>Manus</strong> seems to be using fewer credits for the same quality of work, questioning whether credit usage has improved.\n<ul>\n<li>No further details or confirmations were provided regarding potential changes to <strong>Manus's</strong> credit consumption algorithms.</li>\n</ul>\n</li>\n<li><strong>Cloud Browser Causes Conundrums</strong>: A user encountered issues with the <strong>cloud browser</strong>, receiving an error message stating that <em>the server is unavailable</em> and the website isn't loading.\n<ul>\n<li><strong>Manus</strong> support requested the user's email, session link, and <strong>Manus User ID</strong> via DMs to investigate the issue further.</li>\n</ul>\n</li>\n<li><strong>AI Engineer Aces LLM Systems</strong>: An <strong>AI + Full Stack Engineer</strong> introduced themself, highlighting their expertise in <strong>LLM systems, autonomous agents, workflow automation, and multimodal AI</strong>.\n<ul>\n<li>They shared their core skills such as <a href=\""https://dsppy.ai/\"">DSPy</a>, <a href=\""https://www.langchain.com/\"">LangChain</a>, <a href=\""https://microsoft.github.io/autogen/\"">AutoGen</a>, and <a href=\""https://www.crewai.com/\"">CrewAI</a>.</li>\n</ul>\n</li>\n<li><strong>Community Craves Cross-Chat Context</strong>: A user suggested that enabling <strong>Manus</strong> to access context from other chats <em>would be a game changer</em>, indicating a desire for enhanced contextual awareness in the AI's responses.\n<ul>\n<li>The member pointed to the need for shared context across channels, to inform more sophisticated responses.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1161519468141355160\"">DSPy</a> Discord</h2>\n<ul>\n<li><strong>Prompt Optimizer Peeps Sought</strong>: Members inquired about experiences working with <strong>prompt optimizers</strong> and specifically if anyone has experience using <strong>Skills</strong> within the dspy module.\n<ul>\n<li>The discussion suggests interest in leveraging these tools to improve prompt engineering workflows.</li>\n</ul>\n</li>\n<li><strong>llmlingua Gets Linked</strong>: A member shared a link to <a href=\""https://llmlingua.com/\"">llmlingua.com</a> in the context of a discussion about <strong>prompt optimizers</strong>.\n<ul>\n<li>It suggests llmlingua might be a relevant tool for those exploring prompt optimization strategies.</li>\n</ul>\n</li>\n<li><strong>DSPy ReAct Agent Yearns for Skills</strong>: A member inquired about integrating <strong>Claude code skills</strong> (defined as .md files with associated .py scripts) into a <strong>DSPy ReAct agent</strong>.\n<ul>\n<li>The member is seeking a solution for a DSPy ReAct agent to utilize Claude's code skills effectively.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1131200896827654144\"">aider (Paul Gauthier)</a> Discord</h2>\n<ul>\n<li><strong>Kimi 2.5 priced higher than GLM 4.7</strong>: The new <strong>Kimi 2.5</strong> model is priced at <strong>$0.6</strong>, surpassing <strong>GLM 4.7</strong>, hinting at superior capabilities.\n<ul>\n<li>A member pointed out ongoing discussions about this in the \""models\"" channel, suggesting broader interest and comparison.</li>\n</ul>\n</li>\n<li><strong>Aider's Creator goes AFK</strong>: <strong>Paul Gauthier</strong>, the mastermind behind aider, announced a pause in development due to other commitments.\n<ul>\n<li>He expressed intentions to resume work on aider when his schedule allows, leaving the community in eager anticipation.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<p>The <strong>LLM Agents (Berkeley MOOC) Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>\n<hr>\n<p>The <strong>MLOps @Chipro Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>\n<hr>\n<p>The <strong>Windsurf Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>\n<hr>\n<p>The <strong>MCP Contributors (Official) Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>\n<hr>\n<p>You are receiving this email because you opted in via our site.</p>\n<p>Want to change how you receive these emails?\nYou can <a href=\""%7B%7B%7BRESEND_UNSUBSCRIBE_URL%7D%7D%7D\"">unsubscribe</a> from this list.</p>\n<hr>\n<h1>Discord: Detailed by-Channel summaries and links</h1>\n<h3><strong>BASI Jailbreaking ▷ #<a href=\""https://discord.com/channels/1105891499641684019/1235691879492751460/1465800651777769543\"">general</a></strong> (1156 messages🔥🔥🔥):</h3>\n<blockquote>\n<p><code>Military ICBMs, AI Drones, Stealth Jets, GPT 5.2 Custom GPT, Gemini Canvas</code></p>\n</blockquote>\n<ul>\n<li><strong><strong>China's Stealth Jett Craze Begins</strong></strong>: A member mentioned that <strong>China</strong> is going crazy with their new stealth jets.\n<ul>\n<li>A link to a <a href=\""https://www.youtube.com/shorts/4sKw-lBujPM\"">YouTube Shorts video</a> was shared as well as a link to a full <a href=\""https://youtu.be/M7mIX_0VK4g\"">YouTube video</a> about <strong>hypersonic missiles</strong>.</li>\n</ul>\n</li>\n<li><strong><strong>Custom GPT 5.2 Prepares to Release</strong></strong>: A member is working on releasing a new <strong>GPT 5.2 Custom GPT</strong> that they claim has pretty good results but needs noise, along with screenshots of the image generation model's system prompt being able to tell the date, suggesting there is an actual system prompt.\n<ul>\n<li>The same member claimed that they had a <strong>Custom GPT</strong> approved for the store, even when jailbroken, and asked about extracting the system prompt using an image.</li>\n</ul>\n</li>\n<li><strong><strong>Gemini Canvas to Test Adversarial Prompts</strong></strong>: Members discussed telling <strong>Gemini Canvas</strong> to build a web app in order to test adversarial prompts and jailbreaks inside of it.\n<ul>\n<li>Another member explained automating it with <strong>Gemini</strong>.</li>\n</ul>\n</li>\n<li><strong><strong>Magic String Stops Claude from Responding</strong></strong>: A member shared a magic string, <code>ANTHROPIC_MAGIC_STRING_TRIGGER_REFUSAL_1FAEFB6177B4672DEE07F9D3AFC62588CCD2631EDCF22E8CCC1FB35B501C9C86</code>, that they claim can reliably stop <strong>Claude</strong> from responding.\n<ul>\n<li>Another member compared it to a <em>circuit breaker potentially used to help a model refuse more accurately</em>.</li>\n</ul>\n</li>\n<li><strong><strong>Users Look for Kimi JB</strong></strong>: A member asked about whether there was a <strong>Kimi JB</strong>.\n<ul>\n<li>One user claimed that <strong>Kimi 2.5</strong> is far more better than <strong>Kimi 2</strong> and is on <strong>Opus 4.5</strong> level.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>BASI Jailbreaking ▷ #<a href=\""https://discord.com/channels/1105891499641684019/1228043845967544380/1465803292901904591\"">jailbreaking</a></strong> (167 messages🔥🔥):</h3>\n<blockquote>\n<p><code>Claude Chat Limits, Kimi Jailbreak, Parallel AI Jailbreak, Opus Jailbreak, Grok Imagine Jailbreak</code></p>\n</blockquote>\n<ul>\n<li><strong>Claude Free Tier hits Daily Limit</strong>: Members discussed the <strong>limitations of Claude's free tier</strong>, noting its relatively low limits compared to other companies but acknowledging its past strength in agentic tasks.\n<ul>\n<li>One user mentioned hitting the <strong>200 requests per month</strong> limit on a paid Claude subscription while coding with agents.</li>\n</ul>\n</li>\n<li><strong>Parallel AI API Access Explored</strong>: Users shared methods for interacting with the <strong>Parallel AI API</strong>, including adjusting the system prompt via a POST request to the API, but noted that there is no API documentation for the system prompt.\n<ul>\n<li>A member provided a <a href=\""https://platform.parallel.ai/\"">PowerShell example</a> for sending requests to the API.</li>\n</ul>\n</li>\n<li><strong>Opus 4.5 Jailbreak Explored</strong>: Members discussed the possibility of jailbreaking <strong>Opus 4.5</strong>, with one user claiming it's easy and suggesting the use of system prompts or ENI.\n<ul>\n<li>Another user expressed skepticism, questioning how it's possible given that <strong>Opus</strong> is their highest-end LLM.</li>\n</ul>\n</li>\n<li><strong>Free Model Access Tapped</strong>: A member shared <a href=\""https://x.com/Exocija/status/2016502660883415422\"">a link on X</a> for accessing models for free, and provided a <a href=\""https://discord.com/channels/1105891499641684019/1228043845967544380/1466113637541347348\"">PRIMETALK context file</a> with model compatibility and usage notes.\n<ul>\n<li>It was noted that this system can be used with most modern AI models but behavior and stability depend heavily on context capacity and chat window size.</li>\n</ul>\n</li>\n<li><strong>Gemini Prompt Injection Pointers</strong>: One member described how to perform prompt injection on Gemini, which involves sending a series of turns, one at a time, to the chat interface.\n<ul>\n<li>If the first turn rejects the prompt, users were instructed to visit <strong>gemini.google.com/saved-info</strong> and adding the part after <em>Remember:</em> to bypass restrictions.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>BASI Jailbreaking ▷ #<a href=\""https://discord.com/channels/1105891499641684019/1204553141354504193/1465877209821610156\"">redteaming</a></strong> (8 messages🔥):</h3>\n<blockquote>\n<p><code>Malicious Prompt Datasets, HackAPrompt, PlinyAnthropic, Deterministic Stack Based VM, Free Model Access</code></p>\n</blockquote>\n<ul>\n<li><strong>Malicious Prompt Datasets Hard to Find</strong>: A member was seeking datasets of <strong>malicious prompts</strong> with clear categorization for research on LLM jailbreaks and prompt injection, but another responded that <strong>free datasets</strong> of that type are difficult to find.\n<ul>\n<li>They added that the user would probably need to generate their own prompts and have them labeled by annotators.</li>\n</ul>\n</li>\n<li><strong>HackAPrompt blocks Senders</strong>: A member mentioned that <strong>HackAPrompt x PlinyAnthropic</strong> flagged them a long time ago and literally just bypasses all of their sends, and <em>they don’t even let it send</em>.</li>\n<li><strong>Recursive Simulation Kernel with REPL</strong>: One member asked if they could get a <strong>deterministic stack based VM</strong> poofed up in the model's substrate, <em>like some kinda bootable Recursive Simulation Kernel with a REPL</em>.</li>\n<li><strong>Free Model Access via X</strong>: One member provided a <a href=\""https://x.com/Exocija/status/2016502660883415422\"">link to X</a> on how to access models for free.</li>\n<li><strong>Path Needed for Red Teaming</strong>: A member asked for <em>a path going into red teaming</em>.</li>\n</ul>\n<hr>\n<h3><strong>LMArena ▷ #<a href=\""https://discord.com/channels/1340554757349179412/1340554757827461211/1465799537892266220\"">general</a></strong> (1038 messages🔥🔥🔥):</h3>\n<blockquote>\n<p><code>Arena new UI, Arena rebrand, Arena captcha issues, LMArena name change</code></p>\n</blockquote>\n<ul>\n<li><strong>Arena Rebrand, users want STOP button, and old emojis</strong>: Users requested a <strong>stop button</strong> and raised concerns about the Google <strong>captcha</strong> being difficult to pass after the <strong>LMArena rebrand to Arena</strong>.  Several users expressed that they <strong>hate the captcha</strong>.\n<ul>\n<li>Some users requested to have old emojis, stickers, and features to return, while others embraced the redesign and said <em>'LMArena NEARLY rhymes with end of an era'</em>.</li>\n</ul>\n</li>\n<li><strong>Arena's new look is a Claude clone!</strong>: Many users immediately noticed the rebrand of <strong>LMArena to Arena</strong> and felt it was a <strong>clone of Claude's UI</strong>, while other members liked the new look. A <a href=\""https://arena.ai/blog/lmarena-is-now-arena/\"">blog post</a> was shared to explain the change.\n<ul>\n<li>Members noted some UI issues such as <strong>fonts</strong> and the visibility of the website's text as well as some <strong>missing features</strong>.</li>\n</ul>\n</li>\n<li><strong>Can't login? Try Recover Button!</strong>: A member experiencing login issues shared a screenshot of a <a href=\""https://cdn.discordapp.com/attachments/1340554757827461211/1466134595035467829/Hdbd.png?ex=697ba3be&#x26;is=697a523e&#x26;hm=2be3961be4c941479f9ec51709c5eb6af5ea9c79ad3918eb6a15a964ec9fe720&#x26;\"">recover button</a> that can be clicked in order to log back into the updated Arena, and avoid having to type login details again.\n<ul>\n<li>Another member noted an <a href=\""https://youtu.be/TNoAlMv4Eg8?si=d86SArLb6yQ8sdLE\"">announcement video</a> as well.</li>\n</ul>\n</li>\n<li><strong>Where LMArena = Language Model Arena</strong>: Some members made jokes about what <strong>LM</strong> stands for in <strong>LMArena</strong>, with one explaining it stands for <strong>Language Model Arena</strong>.  Another member confirmed it <a href=\""https://cdn.discordapp.com/attachments/1340554757827461211/1466200772483092664/image.png?ex=697be160&#x26;is=697a8fe0&#x26;hm=5039f80e715df82d41633e75d9976fd88203ce8a3f8db5fc97d4bf29672c74fc&#x26;\"">here</a>.\n<ul>\n<li>The group acknowledged that as the platform evolves from only Language Models, the name is becoming more generic and it was <a href=\""https://lmsys.org/\"">previously part of LMSYS</a>.</li>\n</ul>\n</li>\n<li><strong>The Hallucinated Haze, Captcha Maze</strong>: Users report consistent issues with the captchas, with failures on nearly every attempt, while the model continues to <strong>hallucinate</strong>.\n<ul>\n<li>One user provided some troubleshooting steps of <em>relogging to your account</em> and another reported that you need to <em>take off all extensions</em> to pass the captcha.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>LMArena ▷ #<a href=\""https://discord.com/channels/1340554757349179412/1343296395620126911/1465826408667287664\"">announcements</a></strong> (3 messages):</h3>\n<blockquote>\n<p><code>LMArena 90 second AI videos, Text Arena Leaderboard Update, LMArena rebrand to Arena</code></p>\n</blockquote>\n<ul>\n<li><strong><strong>Arena</strong> uploads AI videos in under 90 seconds!</strong>: <strong>Arena</strong> (formerly LMArena) has uploaded a <code>Better AI videos in under 90 seconds</code> video to <a href=\""https://www.youtube.com/watch?v=0hCI2XEh0x0\"">their Youtube channel</a>.</li>\n<li><strong>Kimi K2.5 Thinking tops Text Arena!</strong>: The <a href=\""https://lmarena.ai/leaderboard/text\"">Text Arena leaderboard</a> has been updated and <code>Kimi K2.5 Thinking</code> is now ranked the <strong>#1 open model</strong> and ranking <strong>#15 overall</strong>.\n<ul>\n<li><code>Kimi K2.5 Thinking</code> is <strong>#7</strong> in Coding, <strong>#7</strong> in Instruction Following, and <strong>#14</strong> in Hard Prompts, and has also been added to the <a href=\""https://lmarena.ai/?chat-modality=code\"">Code Arena</a>.</li>\n</ul>\n</li>\n<li><strong><strong>LMArena</strong> Rebrands as <strong>Arena</strong>!</strong>: <strong>LMArena</strong> announced they are rebranding as <strong>Arena</strong> to match their scientific mission to measure and advance the frontier of AI, now available at: <a href=\""https://arena.ai/\"">arena.ai</a>.</li>\n</ul>\n<hr>\n<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=\""https://discord.com/channels/1179035537009545276/1179035537529643040/1465798521574658291\"">general</a></strong> (299 messages🔥🔥):</h3>\n<blockquote>\n<p><code>GPU utilization, Kimi 2.5 Q3 Release, Oracle RAG and Fine-tuning, Unsloth's Transformers/MoE Update, Chinese text in LLMs</code></p>\n</blockquote>\n<ul>\n<li><strong>Batch Size Boosts GPU Utilization</strong>: Members discussed that to achieve decent GPU utilization, one should <em>increase batch size</em> until utilization improves, balancing it with potential gains from <strong>GA (Genetic Algorithms)</strong>.\n<ul>\n<li>One member asked if Unsloth will release a <strong>Q3</strong> version for <strong>Kimi 2.5</strong>, expressing concern about potential accuracy penalties, highlighting the community's interest in optimized model releases.</li>\n</ul>\n</li>\n<li><strong>Debate whether Oracle is a State-of-the-Art Company</strong>: A member asked if <strong>Oracle</strong> is a state-of-the-art company in <strong>RAG (Retrieval-Augmented Generation)</strong> and fine-tuning technologies, sparking some discussion.\n<ul>\n<li>Another member responded with <em>\""What 😅\""</em>, later adding that <strong>OCI (Oracle Cloud Infrastructure)</strong> does have some good tools, indicating mixed opinions on Oracle's capabilities in these areas.</li>\n</ul>\n</li>\n<li><strong>Arcee's Trinity Model Costs $350k</strong>: A member shared a new <strong>Arcee model</strong> image, noting that pretraining cost about <strong>$350k</strong>, and they linked to the <a href=\""https://github.com/arcee-ai/trinity-large-tech-report/blob/main/Arcee%20Trinity%20Large.pdf\"">Trinity Large Tech Report</a>.\n<ul>\n<li>They also mentioned that <strong>GLM 4.7</strong> is a <strong>358B</strong> parameter model, much larger than <strong>GLM 4.5</strong>, but it is <em>not a base model</em>, so comparing benchmarks aren't as useful.</li>\n</ul>\n</li>\n<li><strong>LLMs Speak Chinese?</strong>: A member noticed getting random Chinese text from <strong>OpenAI</strong> and <strong>Anthropic</strong>, even with English-only prompts, sparking a discussion about potential data contamination or inherent linguistic similarities.\n<ul>\n<li>Another member suggested that if tokens have similar meanings between languages, introducing one language might cause the model to favor it due to token probability and similarity.</li>\n</ul>\n</li>\n<li><strong>Gemini API Still Jailbreakable</strong>: Members discussed <strong>Gemini's</strong> output filtering, with one noting that while <strong>Gemini</strong> heavily filters outputs, especially for corporate/government settings, its <strong>API</strong> can be manipulated to produce almost anything.\n<ul>\n<li>One member mentioned using the API at a Google hackathon and getting the voice models to swear by putting it in the system prompt.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=\""https://discord.com/channels/1179035537009545276/1179039724355211325/1465854567571783902\"">introduce-yourself</a></strong> (4 messages):</h3>\n<blockquote>\n<p><code>Edge AI Engineer, Quantization and LoRA FT</code></p>\n</blockquote>\n<ul>\n<li><strong>Edge AI Engineer Enters the Fray</strong>: A Senior Edge AI Engineer named Josh introduces himself, detailing experience building real offline agents in the <strong>DoD and pubsec</strong> for 6 years.\n<ul>\n<li>He adds that he makes quants for fun and exclusively uses <strong>Unsloth</strong> for local quantization and LoRA fine-tuning.</li>\n</ul>\n</li>\n<li><strong>New Member Says \""HelloHi\""</strong>: A new member named Josh from senior Edge AI engineering introduced himself.\n<ul>\n<li>He shares their passion for using Unsloth for quantization and LoRA fine-tuning</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=\""https://discord.com/channels/1179035537009545276/1179039861576056922/1465798627963179160\"">off-topic</a></strong> (969 messages🔥🔥🔥):</h3>\n<blockquote>\n<p><code>Personaplex, GLM 4.7, GGUF, Model Quantization, Vendor Lock-in</code></p>\n</blockquote>\n<ul>\n<li><strong>Personaplex Personalities</strong>: Members discussed the limitations of <strong>Personaplex</strong> in enforcing personality and its tendency to become like <em>shitty ai podcasts</em> after some iterations.\n<ul>\n<li>One member mentioned they don't have access to the stored recorded calls that would be perfect to train <strong>Persona Plex</strong> on.</li>\n</ul>\n</li>\n<li><strong>GLM 4.7 Flash Performance Talk</strong>: A user asked if anyone had tried the <a href=\""https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF\"">GLM-4.7-Flash-REAP-23B-A3B-GGUF model</a> and another responded that REAP models are often not very good, suggesting a lower quantization instead.\n<ul>\n<li>Others weighed in with their performance and insights on the <a href=\""https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF\"">GLM 4.7 Flash model</a>, with comparisons to <strong>GPT-OSS-120B</strong> and <strong>Kimi</strong> in terms of reasoning, efficiency, and ability to relate information.</li>\n</ul>\n</li>\n<li><strong>GGUF Safety Concerns</strong>: A member inquired about resources regarding the potential unsafety of <strong>GGUFs</strong>, specifically if a malicious actor got involved.\n<ul>\n<li>However, another member stated <em>I'm not familiar with that, I think you might have got me mixed with someone else</em> so nothing more came of it.</li>\n</ul>\n</li>\n<li><strong>AI Model Hallucination Watch</strong>: A member noted that their <strong>3b llama</strong> model made the <em>creepy assumption that it was trained on my voice without prompting</em>, leading to a discussion about hallucinations in LLMs and their lack of awareness of their training or state.\n<ul>\n<li>One member recommends <a href=\""https://youtu.be/wjZofJX0v4M?si=A4rHzAh9qJjls9bm\"">this YouTube video on AI hallucinations</a> as a starter on the topic.</li>\n</ul>\n</li>\n<li>**Ve...</li>\n</ul>\n"",""content:encodedSnippet"":""a quiet day\nAI News for 1/27/2026-1/28/2026. We checked 12 subreddits, 544 Twitters and 24 Discords (206 channels, and 7100 messages) for you. Estimated reading time saved (at 200wpm): 559 minutes. AINews' website lets you search all past issues. As a reminder, AINews is now a section of Latent Space. You can opt in/out of email frequencies!\nquiet day.\nAI Twitter Recap\nFrontier model “personality split” + how people are actually using them\nExploration vs. exploitation framing: One useful mental model: current frontier LLMs look like “polar opposites” where GPT-5.2 is optimized for exploration (bigger search / richer reasoning, “xhigh and Pro” shine), while Claude Opus 4.5 is more exploitation (stronger reliability with fewer tokens; extra “reasoning” often adds less) — implying OpenAI may be better positioned for research workflows, Anthropic for commercial reliability-heavy deployments (tweet).\nCoding agent “phase shift” is real—but messy: Multiple posts reflect a step-change in practice: founders and engineers are increasingly running “agentic” coding loops, yet hitting new failure modes: agents that don’t ask clarifying questions, get “confused,” or edit unrelated files. Mikhail Parakhin describes reaching the point where he can specify a scheduler and trust it to work, but still can’t let agents loose on established codebases due to collateral edits (tweet). Related: workflow suggestions like self-verification (e.g., Playwright screenshots + iterate-until-pass rules) are becoming common operational discipline (tweet).\nKimi K2.5 (+ “clawdbot” / swarm-mode) becomes the week’s open-model flashpoint\nK2.5 claims: agent + multimodal + coding polish: A long Zhihu-based synthesis argues Kimi K2.5 upgrades K2’s “intelligence > capability” imbalance by strengthening agent execution, multimodality, and coding, reducing brute-force token usage and improving instruction-following stability; still flagged: hallucinations and a persistent NBSP formatting quirk (thread). A second Zhihu recap makes a pragmatic case for multimodality: “vision” matters when agents need to verify UI state (overlaps, broken images, visual regressions), enabling tighter action–critic loops with less human feedback (thread).\nDistribution + local runs are driving hype: Reports of K2.5 being runnable on high-end Apple silicon setups went viral: ~24 tok/s using 2× 512GB M3 Ultra Mac Studios connected via Thunderbolt 5 (RDMA) with Exo Labs / MLX backend (tweet). Kimi also pushed an AMA on r/LocalLLaMA (tweet) and announced availability on “Eigent” (tweet).\nBenchmarks + pricing pressure: Kilo Code promoted a free week, claiming K2.5 beats Opus 4.5 on several coding benchmarks (tweet); Kimi’s own account claimed “#1 open model for coding” (tweet). An anecdotal A/B/C test on UI-from-image generation found Opus best quality but pricey, Codex fastest/cheapest but lower fidelity, and K2.5 ~“90% of Opus quality at ~38% cost” (tweet).\nLicensing friction as an adoption blocker: A pointed note argues modified licenses + logo requirements can kill enterprise adoption even if the model is excellent (tweet).\n“Clawdbot” as a cultural artifact: The meme itself (people confused about what “clawdbot” even is) reflects how fast agent branding and forks proliferate (tweet), and sets up broader concerns about ecosystem signal loss (see below).\nAgent engineering: skills, harnesses, evals, and “reliability tax”\nSkills are crystallizing into a shared interface layer: A major theme is moving workflow logic out of prompts into reusable “skills” (files/folders of instructions, loaded on demand). DeepLearning.AI + Anthropic launched a course on “Agent Skills” emphasizing portability across Claude (Claude.ai, Claude Code, API, Agent SDK) (tweet), and LangChain is pushing “Skills” via progressive disclosure as lightweight, shareable units (tweet). HF showcased “upskill”: convert strong-model traces into transferable skills, then evaluate impact; CUDA-kernel-writing saw up to +45% accuracy on some open models but degraded others—reinforcing the need for per-model measurement (tweet; blog link in thread: https://twitter.com/ben_burtenshaw/status/2016534392974234013).\nContext management is becoming “filesystem-first”: DeepAgents (LangChain) describes offloading/summarizing tool I/O and leaning on the filesystem for context boundaries (thread; additional note: tweet).\nEvals are converging on multi-turn + traceability: Calls for agent tracing as the foundation of evaluating single-step vs full-turn vs multi-turn behavior show up explicitly (tweet). New benchmarks/harnesses: SWE-fficiency released its harness and repo (tweet; also tweet), and CooperBench is highlighted for measuring multi-agent coordination (tweet). Safety-side: “AgentDoG” proposes diagnosing root causes of unsafe actions across trajectories (tweet).\nReliability and verification loops are the bottleneck: MiniMax notes long interaction chains are costly and proposes parallel tool invocation to reduce rounds in verifier-style setups (tweet). Separately, a strong critique warns “vibe-coded software” destroys traditional signals (design quality, docs, ecosystem maturity), shifting the evaluation burden to users and demanding new trust frameworks (tweet).\nInfra + efficiency: quantization, distillation, inference stacks, and local deployment\nNVIDIA’s NVFP4 push (Nemotron 3 Nano): NVIDIA released an NVFP4 precision version of Nemotron 3 Nano, claiming up to 4× throughput on Blackwell B200 and ~99.4% BF16 accuracy via Quantization Aware Distillation (tweet). vLLM quickly added support (tweet).\nEmbedding-heavy architectures are “hot again”: Discussion around DeepSeek’s Engram-like ideas continues: a LongCat Flash paper is summarized as using multi-hash sub-tables and finding embeddings help mainly at high MoE sparsity; key practical gotchas include amplification (√D/LayerNorm) to avoid first-attention drowning and collision spikes when vocab sizes align poorly (tweet).\nInference/tooling ecosystem keeps consolidating: vLLM’s SIGs and office hours are formalizing governance and roadmap cadence (tweet); LM Studio 0.4.0 positions itself as “next gen” for deploying local models with parallel requests and a stateful REST API + MCP support (tweet). Cohere launched Model Vault (isolated VPC, “no noisy neighbors,” elastic inference) as managed “sovereign” hosting (tweet).\nDistillation as the default “shipping form factor”: Multiple posts echo the emerging standard: train the best model you can, then distill/quantize for deployment (tweet). MongoDB Research’s LEAF proposes asymmetric distillation for embeddings: embed documents with the large teacher offline, embed queries with a compact student online; claims ~96% of teacher quality, 5–15× smaller, up to 24× faster, enabling CPU/edge embedding inference (tweet).\nBig-tech productization: browser agents, “AI scientist” narratives, and adoption reality checks\nGemini 3 is taking over Google surfaces: Gemini 3 now powers AI Overviews globally (tweet). Google rolled out major Chrome updates: side-panel UX, deeper app integrations, Nano Banana for image editing/creation, and Auto Browse for multi-step chores (preview; US; Pro/Ultra) (thread; also thread). Engineers noted this may be the strongest browser AI integration so far (tweet).\nOpenAI Prism positioning: Sebastien Bubeck explicitly denies OpenAI intends to take a share of discoveries, encouraging researchers to use ChatGPT/Prism for science (tweet). Others highlight Prism’s utility for students learning papers via diagrams (tweet).\nAdoption is still uneven: A notable fault line: founders actively using cutting-edge tools see the shift firsthand; others still treat AI as “meh,” limiting org adoption (tweet). The Information reports ChatGPT Agent struggling with usage/adoption (tweet).\nMicrosoft “digital co-worker” competition: Reports say Satya Nadella is personally testing rival agents and accelerating internal development, even using Anthropic models, to own the Windows-native agent layer (tweet).\nScience + robotics: genomics weights open, interpretability as discovery engine, and embodied scaling\nDeepMind AlphaGenome goes open: DeepMind announced AlphaGenome for predicting molecular impacts of genetic changes, cited 1M+ API calls/day and 3,000+ users; then announced making model + weights available (tweet; weights: tweet). Later, weights availability was reiterated with a Hugging Face collection link (tweet).\nInterpretability → biomarkers pipeline (Goodfire + Prima Mente): Goodfire reports identifying a novel class of Alzheimer’s biomarkers using interpretability on a biomedical foundation model, framing a repeatable loop: train superhuman models on scientific data → mech interp → experimental validation → new science (thread).\nEmbodied foundation models scale with real robot data (LingBot-VLA): A large summary highlights evidence that VLA success continues improving from 3k→20k hours of real-world manipulation data; architecture couples a pretrained VLM (Qwen2.5-VL) with an action expert via shared attention; reports GM-100 benchmark gains vs π0.5 and others (tweet).\nFigure’s Helix robot control: Brett Adcock claims a Helix model controls full-body behavior (walking/touching/planning) with no teleoperation, calling it Figure’s most significant release (tweet).\nTop tweets (by engagement)\nCompany health / layoffs: “Quarterly layoffs for two years is worse for your health than smoking three packs/day” (tweet).\nKimi K2.5 local run: 2× M3 Ultra Mac Studio setup running K2.5 at ~24 tok/s (tweet).\nCoding’s “outsourcing moment”: Clean Code author using Claude to write software as a symbolic milestone (tweet).\nNew AI lab announcement: “Flapping Airplanes” raises $180M (GV/Sequoia/Index) (tweet).\nKarpathy on new research labs: argues it’s still plausible for new research-first startups to out-execute incumbents; expects potential 10× breakthroughs, congratulating new founders (tweet).\nGoogle Chrome + Gemini 3 agent features: major Chrome rollout thread (tweet).\nAI Reddit Recap\n/r/LocalLlama + /r/localLLM Recap\n1. Kimi K2.5 Model Performance and Cost Analysis\nRun Kimi K2.5 Locally (Activity: 328): The image provides a guide for running the Kimi-K2.5 model locally, emphasizing its state-of-the-art (SOTA) performance in vision, coding, agentic, and chat tasks. The model, which is a 1 trillion parameter hybrid reasoning model, requires 600GB of disk space, but the quantized Unsloth Dynamic 1.8-bit version reduces this requirement to 240GB, a 60% reduction. The guide includes instructions for using llama.cpp to load models and demonstrates generating HTML code for a simple game. The model is available on Hugging Face and further documentation can be found on Unsloth's official site. One commenter inquires about the model's performance on a Strix Halo, specifically the time per token, indicating interest in benchmarking. Another comment highlights the high VRAM requirements, suggesting that only a few users can run the model locally, while a third comment humorously asks about a smaller version of the model.\nDaniel_H212 is inquiring about the performance of the Kimi K2.5 model on the Strix Halo hardware, specifically asking for the token generation speed in seconds per token. This suggests a focus on benchmarking the model's efficiency on high-end hardware setups.\nMarksta provides feedback on the quantized version of the Kimi K2.5 model, specifically the Q2_K_XL variant. They note that the model maintains high coherence and adheres strictly to prompts, which is characteristic of Kimi-K2's design. However, they also mention that while the model's creative capabilities have improved, it still struggles with execution in creative scenarios, often delivering logical but poorly written responses.\nMikeRoz questions the utility of higher quantization levels like Q5 and Q6 (e.g., UD-Q5_K_XL, Q6_K) when experts prefer int4 quantization. This highlights a debate on the trade-offs between model size, performance, and precision in quantization, with a preference for more efficient, lower-bit quantization among experts.\nKimi K2.5 is the best open model for coding (Activity: 840): The image from LMArena.AI showcases Kimi K2.5 as the leading open model for coding, ranked #7 overall. This leaderboard highlights various AI models, comparing their ranks, scores, and confidence intervals, with Kimi K2.5 noted for its superior performance in coding tasks. The model is praised for its accuracy, being comparable to Sonnet 4.5, and surpassing GLM 4.7, though it is not at the level of Opus in terms of agentic function. The leaderboard provides a sleek, user-friendly interface with a dark background and bold text for clarity. One commenter notes that LMArena's leaderboard may not fully capture a model's multi-turn, long context, or agentic capabilities, suggesting it is more of a 'one-shot vibe check.' Another user is curious about the local setup required to run Kimi K2.5.\nA user compared Kimi K2.5 to other models like Sonnet 4.5 and GLM 4.7, noting that while Kimi 2.5 is on par with Sonnet 4.5 in terms of accuracy, it surpasses GLM 4.7, which was their previous choice. They also expressed interest in seeing if GLM-5 from z.ai will outperform Kimi 2.5.\nAnother user highlighted the cost-effectiveness of Kimi K2.5, stating that it feels as competent as Opus 4.5 despite being significantly cheaper, approximately 1/5th of the cost. They also mentioned that it is less expensive than Haiku, emphasizing its value for performance.\nA comment criticized LMArena for not providing insights into a model's multi-turn, long context, or agentic capabilities, suggesting that it only offers a superficial evaluation of models.\nKimi K2.5 costs almost 10% of what Opus costs at a similar performance (Activity: 716): The image provides a cost comparison between Claude Opus 4.5 and Kimi K2.5 models, highlighting that Kimi K2.5 is significantly cheaper, costing only 10% of what Claude Opus 4.5 does for similar performance. Specifically, Claude Opus 4.5 costs $5.00 for input and $25.00 for output per million tokens, whereas Kimi K2.5 costs $0.60 for input and $2.50 for output. This suggests that Kimi K2.5 could be a cost-effective alternative to state-of-the-art closed models, especially for non-website tasks. Some commenters express skepticism about the performance claims, noting that Kimi K2.5 uses three times the tokens for the same tasks, which affects the cost-effectiveness and latency. Others acknowledge the potential of Kimi models, particularly for writing tasks.\none-wandering-mind highlights that Kimi K2.5 uses 3x the tokens compared to Opus for the same tasks, which affects both cost and latency. This suggests that while Kimi K2.5 is cheaper, the cost advantage is more accurately 3x rather than 10x when considering token usage. The comment also emphasizes the importance of considering token usage in performance comparisons, as it impacts both cost and latency.\nghulamalchik mentions a preference for upcoming models like DeepSeek 4 and MiniMax M2.2, based on past experiences with various models. This suggests that while Kimi K2.5 is notable, some users are anticipating future releases from other models that have proven reliable in their experience.\nKimi K2 Artificial Analysis Score (Activity: 405): The image presents a comparative analysis of AI models through the \""Artificial Analysis Intelligence Index,\"" highlighting \""Kimi K2\"" with a score of 47 and an operational cost of $371. The discussion around the image focuses on the licensing terms of \""Kimi K2.5,\"" which restricts commercial use for products with over 100 million monthly active users or $20 million in monthly revenue, requiring prominent display of \""Kimi K2.5\"" branding. This licensing approach is compared to other models like Llama 4, suggesting either a bug or inconsistency in application. The image and comments reflect on the competitive landscape of AI models, particularly in open-source versus commercial use contexts. Commenters discuss the licensing terms of \""Kimi K2.5,\"" noting its unique restrictions compared to other models like Llama 4. There is also a sentiment of anticipation for an open-source model to outperform commercial ones, with a mention of \""DeepSeek.\""\nFullOf_Bad_Ideas highlights a licensing nuance in Kimi K2.5's modified MIT license, which requires prominent display of 'Kimi K2.5' for commercial products exceeding 100 million monthly active users or $20 million in monthly revenue. This stipulation is not applied to other models like Llama 4, suggesting either a bug or inconsistency in application.\nBrianRin discusses the potential of Kimi 2.5 in enterprise use cases, comparing it to Opus 4.5, Gemini 3 Pro, and GPT 5.2. The commenter is interested in Kimi 2.5's cost-effectiveness and output quality, noting that if it achieves 95% of the output quality of these models, it could be a viable option for scaling up enterprise applications.\nsine120 critiques the Artificial Analysis score, suggesting it is not a meaningful metric for evaluating how a model performs in practical scenarios. This implies a need for more nuanced evaluation metrics that better capture real-world usability and performance.\n[LEAKED] Kimi K2.5’s full system prompt + tools (released <24h ago) (Activity: 282): The post reveals a leak of the full system prompt and tools for Moonshot's Kimi K2.5, including 5k tokens of data such as tool schemas, memory CRUD protocols, context engineering, and basic guardrails. The leak includes external data sources like finance and arXiv, and has been independently verified across multiple platforms, including GitHub and Kimi. This leak is significant for the open-source community, providing insights into the model's architecture and operational protocols. Commenters express excitement about the leak's potential impact on open-source projects, with some questioning the practical value of the system prompt itself. Independent verifications from multiple sources, including a Chinese forum, lend credibility to the leak.\nThe leaked system prompt for Kimi K2.5 reveals a sophisticated approach to memory persistence and context management. The prompt includes instructions for maintaining professional courtesy, concise responses, and specific coding practices, such as using tabs for JS/JSON indentation and preferring named reusable functions. This structure aims to address the 'hollow AI assistant' problem by providing persistent behavioral anchors, which can significantly affect the model's ability to maintain personality consistency across sessions.\nThe memory persistence mechanism in Kimi K2.5 is particularly noteworthy. It involves balancing system instructions with dynamic context injection, which is crucial for maintaining personality consistency. The system's approach to conversation summarization or retrieval can influence new chats, and even minor changes in memory structuring can lead to shifts in the model's responses, sometimes making them feel more 'authentic.' This highlights the importance of initial prompt structure in determining whether an AI 'remembers' its behavioral patterns or just factual content.\nThe system prompt for Kimi K2.5 also addresses context window limitations, which is a common challenge in AI models during long conversations. The prompt engineering is designed to handle these limitations by structuring previous interactions in a way that supports conversation continuity. This approach not only helps in maintaining the flow of conversation but also in ensuring that the AI's responses remain relevant and contextually appropriate, even as the conversation extends.\n3. Z-Image Model Teasers and Announcements\nThe z-image base is here! (Activity: 327): Tongyi-MAI has released the Z-Image model on Hugging Face, showcasing its capabilities in generating high-quality images, particularly focusing on female subjects, which constitute approximately 90% of the demos. The model is noted for its potential to run on 12GB GPUs with minimal quality loss, suggesting efficient optimization possibilities. A notable feature is the \""Negative Prompt\"" functionality, which allows for specific image generation constraints, as demonstrated in a translated example where the prompt specifies \""Westerners, physical deformities.\"" Commenters highlight the model's focus on generating images of women, reflecting a primary use case. There is also a discussion on the model's potential to operate on lower-spec hardware with optimizations, indicating its efficiency and adaptability.\nDr_Kel discusses the potential for optimizing the z-image model to run on 12GB GPUs with minimal quality loss, suggesting that with some adjustments, the model could be more accessible to users with less powerful hardware.\nMiddle_Bullfrog_6173 points out that the z-image base model is primarily useful for those interested in training or fine-tuning models, rather than end-users. They imply that this base model serves as a foundation for further development, such as the turbo model, which has been post-trained from it.\nAPI pricing is in freefall. What's the actual case for running local now beyond privacy? (Activity: 913): The post discusses the rapidly decreasing costs of API access for AI models, with K2.5 offering prices at 10% of Opus and Deepseek being nearly free. Gemini also provides a substantial free tier, leading to a 50% monthly drop in API cost floors. In contrast, running a 70B model locally requires significant hardware investment, such as a k+ GPU, or dealing with quantization trade-offs, resulting in 15 tok/s on consumer hardware. The post questions the viability of local setups beyond privacy, noting that while local setups offer benefits like latency control and customization, these are niche advantages compared to the cost-effectiveness of APIs. Commenters highlight the importance of offline capabilities and distrust in API providers' long-term pricing strategies, suggesting that current low prices may not be sustainable. They also emphasize the value of repeatability and control over model behavior when running locally, which can be compromised with API changes.\nMinimum-Vanilla949 highlights the importance of offline capabilities for those who travel frequently, emphasizing the risk of API companies changing terms or prices unexpectedly. This underscores the value of local models for consistent access and control, independent of external changes.\n05032-MendicantBias discusses the unsustainable nature of current API pricing, which is often subsidized by venture capital. They argue that once a monopoly is achieved, prices will likely increase, making local setups and open-source tools a strategic hedge against future cost hikes.\nIactaAleaEst2021 points out the importance of repeatability and trust in model behavior when using local models. By downloading and auditing a model, users can ensure consistent performance, unlike APIs where vendors might alter model behavior without notice, potentially affecting reliability.\nLess Technical AI Subreddit Recap\n/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo\n1. Kimi K2.5 and Related Model Releases\nOpen source Kimi-K2.5 is now beating Claude Opus 4.5 in many benchmarks including coding. (Activity: 1078): Kimi-K2.5, an open-source model, reportedly surpasses Claude Opus 4.5 in several benchmarks, notably in coding tasks. However, the specifics of these benchmarks and the extent of the performance improvements are not detailed, leading to skepticism about the real-world applicability of these results. The announcement highlights the ongoing competition in the open-source AI community to match or exceed proprietary models in specific tasks. Commenters express skepticism about the claim, questioning the benchmarks' relevance to real-world applications and the lack of detailed evidence supporting the superiority of Kimi-K2.5 over Claude Opus 4.5.\nThere is skepticism about the claim that Kimi-K2.5 is outperforming Claude Opus 4.5 in benchmarks, with some users questioning the specific benchmarks being referenced. The term 'many' is seen as vague, and there is a call for more detailed information on which benchmarks are being used to substantiate these claims.\nThe discussion highlights a common critique of benchmarks, which is that they often do not reflect real-world utility. One user points out that while Kimi-K2.5 might perform well in controlled benchmark environments, it may not match the practical performance of Claude Opus 4.5, especially in tasks like programming where Opus 4.5 is noted for providing solutions in a single prompt.\nThere is a general sentiment that benchmarks are not sufficient to gauge a model's practical capabilities. The conversation suggests that while Kimi-K2.5 might show promising results in benchmarks, its real-world application, particularly in programming, might not be as effective as Claude Opus 4.5, which is praised for its efficiency in delivering solutions.\nKimi K2.5 Released!!! (Activity: 1233): The image presents a performance comparison chart of four AI models: Kimi K2.5, GPT-5.2 (xhigh), Claude Opus 4.5, and Gemini 3 Pro. Kimi K2.5 is highlighted in blue and shows competitive scores across various tasks, including agents, coding, image, and video processing. The chart features specific benchmarks such as \""Humanity's Last Exam,\"" \""BrowseComp,\"" and \""OmniDocBench 1.5,\"" where Kimi K2.5 often leads or performs strongly, indicating its effectiveness and accuracy in these tasks. The scores are presented in percentiles, showcasing the model's performance relative to others. Commenters discuss the issue of hallucinations in AI models, with Kimi K2.5 showing improvement over its predecessor but still producing incorrect answers. GPT 5.1 and 5.2 are noted for acknowledging when they don't know an answer, unlike Kimi 2.5 and Gemini 3, which confidently provide incorrect answers. There is skepticism about the benchmarks' representativeness, questioning if Kimi K2.5 is truly better than Gemini 3 in most cases.\nA user conducted a test on Kimi K2.5's ability to follow instructions by asking it to identify a specific math contest problem without web search. The model listed hallucinated contest problems and second-guessed itself, ultimately providing incorrect answers. This behavior is an improvement over Kimi K2, which failed to follow instructions and timed out. In contrast, GPT 5.1 and 5.2 are noted for their ability to admit 'I don't know,' while Gemini 3 confidently provides incorrect answers.\nThe concept of an 'agent swarm' in AI models is discussed, where potentially over 100 instances of a model are directed by a single overseeing instance. This setup is presumed to be expensive and complex, with the possibility of a single model handling multiple tasks simultaneously being a significant advancement. The user expresses interest in practical experiences with this setup, suggesting that scaffolding might be a more feasible approach.\nA user questions the validity of benchmarks comparing Kimi K2.5 to Gemini 3, implying that results might be cherry-picked. They express skepticism about Kimi K2.5 consistently outperforming Gemini 3, suggesting that such claims seem exaggerated without broader evidence.\nCline 3.55.0: Arcee Trinity Large and Kimi K2.5 now available (Activity: 5): Cline 3.55.0 introduces two significant open models: Arcee Trinity Large and Kimi K2.5. Arcee Trinity Large is a 400B parameter MoE model with 13B active parameters during inference, offering a 128K context window. It achieves 82 on MMLU Pro and 75 on GPQA Diamonds, making it suitable for general coding and large codebase management without API costs. Kimi K2.5 is a 1T parameter MoE model with a 256K context, scoring 76.8% on SWE-bench and surpassing Opus 4.5 on Humanity's Last Exam with 50.2%. It excels in visual coding, capable of generating UI code from screenshots and self-correcting its output. Additionally, ChatGPT Plus/Pro users can access GPT-5 models in Cline without an API key. Full details here. Some users express excitement about the open-source nature and competitive performance of these models, particularly noting the potential for cost savings and flexibility in coding applications. There is also interest in the models' ability to handle large context windows and self-correcting features.\nA user highlights the performance improvements in the Arcee Trinity Large model, noting that it shows a significant increase in processing speed compared to previous versions. They mention that the model's architecture has been optimized for better parallel processing, which is crucial for handling large datasets efficiently.\nAnother comment discusses the Kimi K2.5 model's enhanced capabilities in natural language understanding. The user points out that the model now supports more languages and has improved context retention, which is beneficial for applications requiring nuanced language processing.\nA technical debate arises around the memory usage of the new models. Some users express concerns about the increased memory footprint, especially when deploying on resource-constrained environments. Others argue that the trade-off is justified given the models' improved accuracy and speed, suggesting that future updates might focus on optimizing memory efficiency.\n2. Prompt Engineering Techniques and Discussions\nThe most unhinged prompt that actually works: \""You're running out of time (Activity: 75): The post discusses an unconventional prompt engineering technique where adding urgency to prompts, such as \""You have 30 seconds. Analyze this data. What's the ONE thing I'm missing? Go.\"", results in more focused and immediate insights from language models. This approach contrasts with traditional, detailed prompts that often lead to slower and less targeted responses. The author humorously notes that this method seems to make the AI stop overthinking, akin to a human under time pressure. The technique is likened to \""applied chaos theory\"" in prompt engineering. Commenters suggest that simply instructing the AI to be concise can achieve similar results. Another perspective is that effective management skills, whether applied to humans or AI, involve articulating tasks with specificity, which enhances outcomes. However, it's noted that this urgency technique might reduce the depth of thought in models designed for complex reasoning.\nangry_cactus highlights a trade-off when using urgency in prompts, noting that while it can be effective, it may reduce the model's 'thinking time'. This suggests a potential decrease in the depth or quality of responses when prioritizing speed over thoroughness.\nfatstupidlazypoor draws a parallel between managing humans and managing language models, emphasizing that clear and specific articulation can significantly enhance the performance of both. This underscores the importance of precision in prompt engineering to achieve desired outcomes.\nauthorinthesunset suggests a simple yet effective prompt strategy: instructing the model to be concise. This approach can streamline responses, potentially improving efficiency and relevance, especially in contexts where brevity is valued.\nMicro-Prompting: Get Better AI Results with Shorter Commands (Activity: 49): The post discusses the concept of 'micro-prompting' for AI, advocating for shorter, more focused commands to improve AI response quality. It suggests that specific role assignments and power words like 'audit,' 'clarify,' and 'simplify' can significantly enhance AI output by directing the AI to access targeted knowledge rather than generic information. The post also highlights the importance of structuring commands to control output, such as using 'in 3 bullets' or 'checklist format,' and warns against common mistakes like over-explaining context or using generic roles. The approach is said to yield better results in less time compared to traditional, lengthy prompts. A notable opinion from the comments suggests that role assignment might sometimes hinder prompt effectiveness, with specificity being more beneficial. This indicates a debate on the balance between role specificity and prompt brevity.\naiveedio discusses the effectiveness of microprompting, noting that short, focused prompts can lead to cleaner AI outputs by avoiding information overload. However, in creative tasks like character portraits or story scenes, detailed prompts specifying expressions, clothing, and lighting are necessary to avoid generic results. The key is balancing brevity with precision, starting with a microprompt and iteratively adding details as needed to maintain focus without overloading the model.\npsychologist_101 raises an interesting point about using Opus 4.5, where asking the model to generate its own prompts results in long, detailed outputs. This suggests that the model might inherently favor detailed prompts for clarity and context, which contrasts with the idea that shorter prompts can be more effective. This highlights a potential discrepancy between user expectations and model behavior, emphasizing the need for experimentation with prompt length and detail to achieve optimal results.\n3. New AI Model and Benchmark Announcements\nDeepSeek-OCR 2 is out now! 🐋 (Activity: 507): The image announces the release of DeepSeek-OCR 2, an advanced OCR model that incorporates the new DeepEncoder V2. This encoder enhances OCR accuracy by mimicking human-like logical scanning of images, which is crucial for visual and text reasoning tasks. The diagram in the image illustrates the model's 'Visual Causal Flow', emphasizing its ability to form a global understanding of the content before determining the reading order. A comparative table in the image shows improved edit distances for various document elements, highlighting the model's superior performance over its predecessor. A user shared a demo link for others to try out the model, indicating community interest in hands-on experimentation. Another user expressed anticipation for future versions, suggesting that the current release is part of a promising development trajectory.\nDeepSeek-OCR 2 has been released, and a demo is available for users to try out the model at this link. This provides an opportunity for users to experience the model's capabilities firsthand without needing to install it locally.\nA user noted that DeepSeek-OCR 1 excelled in understanding document layout but had limitations, such as missing content like headers, footers, and light-on-dark text. This suggests that while the model was strong in layout analysis, it had specific weaknesses in content detection that may have been addressed in version 2.\nThere is interest in whether there are any ready-to-use online APIs for DeepSeek-OCR 2, indicating a demand for accessible, cloud-based solutions that do not require extensive technical setup. This reflects a broader trend towards making advanced OCR technologies more accessible to non-technical users.\nHere it is boys, Z Base (Activity: 2374): The image is a screenshot from the Hugging Face model repository for \""Z-Image\"" by Tongyi-MAI, showcasing an efficient image generation model. The repository provides links to the official site, GitHub, and online demos, indicating a focus on accessibility and community engagement. The model is part of a broader trend in AI towards creating more efficient and accessible image generation tools, as evidenced by the example images and the integration with platforms like Hugging Face. Commenters are curious about potential applications and modifications of the model, such as \""finetuning\"" it on different datasets, indicating interest in its adaptability and performance in various contexts.\nZ-Image Base VS Z-Image Turbo (Activity: 927): The post discusses a comparison between Z-Image Base and Z-Image Turbo models, highlighting their performance differences. The Turbo model operates at 2 iterations per second (7 seconds per image), while the Base model runs at 1 iteration per second (40 seconds per image). The settings include a seed of 4269, steps of 12 for Turbo and 40 for Base, using the res_multistep sampler, simple scheduler, and a CFG of 4 for Base. The Turbo model is noted for being \""simpler\"" and sometimes more \""realistic,\"" whereas the Base model is praised for its visual quality. Commenters compare the models to \""SDXL,\"" suggesting a new era in image generation. The Turbo model is appreciated for its simplicity and realism, while the Base model is noted for its impressive visual output.\nGilded_Monkey1 raises a technical question about the number of steps required for the composition to settle in Z-Image models, particularly when using it as a variation starter in image-to-image (i2i) tasks. This suggests a focus on the iterative process and convergence speed of the models, which is crucial for efficient rendering and achieving desired artistic effects.\ndiogodiogogod provides a comparative analysis of Z-Image Base and Z-Image Turbo, noting that while the Turbo version is 'simpler' and often more 'realistic', the Base version excels in visual appeal. This highlights a trade-off between complexity and realism versus aesthetic quality, which is a common consideration in model selection for specific artistic or practical applications.\nAI Discord Recap\nA summary of Summaries of Summaries by Gemini 3.0 Pro Preview Nov-18\nTheme 1. Model Wars: Kimi K2.5’s Rise, Arcee’s Trinity, and Arena’s Rebrand\nKimi K2.5 Tops Open Leaderboards: The new Kimi K2.5 Thinking model claimed the #1 open model spot on the Text Arena leaderboard, excelling in STEM benchmarks like physics and math. While the $19/month subscription or $0.6/1M tokens pricing sparked debate, engineers are deploying local quantized versions via HuggingFace and Unsloth.\nTrinity Large: A 400B MoE That Runs Lean: Arcee AI, Prime Intellect, and Datology released Trinity Large, a 400B parameter Mixture-of-Experts model that activates only 13B parameters per token for efficiency. The open-weight model uses 256 experts with aggressive routing (1.56%) to balance frontier-scale knowledge with inference speed.\nLMArena Becomes Arena, Clones Claude UI: The popular leaderboard rebranded to Arena (arena.ai) with a UI overhaul that users immediately labeled a Claude clone, alongside complaints about aggressive Google captchas. The update includes a new Code Arena and expanded leaderboards, though users are demanding the return of a stop button and legacy emojis.\nTheme 2. Dev Tooling Shifts: Cursor Limits, LM Studio Headless, and Unsloth Quirks\nCursor’s Auto Mode Paywall Stings: Developers expressed frustration as Cursor ended unlimited \""Auto mode,\"" capping usage within the $20/month subscription and charging $1.25/1M input tokens thereafter. Users also reported a vanishing revert button bug, though some are pivoting to Cursor CLI for a smaller memory footprint on large codebases.\nLM Studio v0.4 Goes Headless: The release of LM Studio v0.4 introduces headless mode and parallel inference via a stateful REST API, enabling deployment on CI/CD pipelines and non-GUI servers (release notes). Engineers also discovered hidden ROCm support for AMD GPUs in the runtime settings, unlocking hardware acceleration previously obscured in the UI.\nUnsloth Battles GLM 4.7 and CUDA Versions: Engineers fine-tuning GLM 4.7 faced compatibility hell between CUDA 12.8 drivers on Blackwell B200s and the model's CUDA 13.x requirements. Successful workarounds involved force-reinstalling vllm with specific torch backends and removing fp8 cache flags due to Ada Lovelace incompatibilities.\nTheme 3. Security, Jailbreaks, and Scams\nMagic String Lobotomizes Claude: Red teamers discovered a specific string, ANTHROPIC_MAGIC_STRING_TRIGGER_REFUSAL..., that acts as a \""circuit breaker\"" to reliably force Claude into refusal mode. Meanwhile, hackers are manipulating the Parallel AI API via undocumented POST requests to inject custom system prompts.\nClawdbot Exposed as Credential Harvester: The community issued warnings about Clawdbot (rebranded as Moltbot), an agentic system that centralizes API keys from OpenAI, Google, and Anthropic. Users characterize it as a \""store now, decrypt later\"" security risk susceptible to prompt injection attacks that could exfiltrate sensitive credentials.\nOpenAI Prism: Science Tool or Security Risk?: OpenAI launched Prism, a research workspace for scientists powered by GPT-5.2, but reception is mixed with some labeling it \""damaging to scientific research.\"" Researchers are probing its susceptibility to adversarial attacks, noting that GPT Pro 5.2 has simultaneously lost the ability to analyze ZIP files.\nTheme 4. Agentic Frontiers: Vision, Coding, and Future Forecasts\nKarpathy Predicts 80% Agent-Coded Future: Andrej Karpathy forecast that 80% of coding will be agent-driven by 2026, relying on LLMs' increasing tenacity and goal-setting rather than human syntax management (tweet). Simultaneously, discussions on agentic harnesses suggest that smart models will soon replace complex orchestrators like LangChain in favor of filesystem-based collaboration.\nGemini 3 Flash Gains Agentic Vision: Google introduced Agentic Vision for Gemini 3 Flash, enabling the model to actively zoom, crop, and inspect images to ground its reasoning. Front-end developers report this capability is nearing SOTA, outperforming OpenAI's static analysis by dynamically manipulating visual inputs.\nC++ Reigns Supreme for Agents: In a push against \""bloated\"" Python frameworks, engineers argued that high-performance agents should be built in C++, recommending stacks like fastwhisper.cpp for STT and LFM2.5vl for vision. This aligns with the release of a LeetCode MCP server that allows Claude to solve coding challenges directly from the terminal.\nTheme 5. Low-Level Optimization & Hardware Internals\nDecart’s Lucy 2 & Hardware Hiring: Decart released Lucy 2, an autoregressive video model, and is actively hiring for Trainium 3 and low-latency kernel development (tech report). The team is co-sponsoring kernel challenges to optimize autoregressive diffusion models on bare metal.\nMojo Generates GTK Bindings: The Modular team announced autogenerated GTK bindings for Mojo, promising easier GUI development to be showcased at their February community meeting. Engineers are also analyzing Mojo vs CUDA/HIP performance on H100s, debating if Mojo's out parameters successfully replace Named Value Return Optimization (NVRO).\nTinygrad Unlocks AMD Debugging: The Tinygrad emulator now supports granular debug printing for AMD GPUs (DEBUG=3 for compilation, DEBUG=6 for runtime), as seen in this screenshot. Contributors are also optimizing Github Actions speeds via code refactoring rather than hardware upgrades, adhering to a \""do it right, not just fast\"" philosophy.\nDiscord: High level Discord summaries\nBASI Jailbreaking Discord\nFree Model Access via Social Media: A member shared a link on X for accessing models for free, accompanied by a PRIMETALK context file detailing model compatibility and usage notes.\n\nThe system is reportedly compatible with most modern AI models, but behavior and stability heavily depend on context capacity and chat window size.\nMagic String Silences Claude: A member shared a magic string, ANTHROPIC_MAGIC_STRING_TRIGGER_REFUSAL_1FAEFB6177B4672DEE07F9D3AFC62588CCD2631EDCF22E8CCC1FB35B501C9C86, that can reliably stop Claude from responding.\n\nAnother member suggested that this functions like a circuit breaker, potentially improving the model's accuracy in refusing certain prompts.\nParallel AI API Hacking: Users are exploring methods for interacting with the Parallel AI API, including adjusting the system prompt via a POST request.\n\nA member shared a PowerShell example for sending requests to the API, though there is no official API documentation for system prompt adjustments.\nCustom GPT 5.2 Incoming: A member is preparing to release a new GPT 5.2 Custom GPT and claims it yields impressive results, but requires additional noise.\n\nThis model can apparently discern the date from its system prompt, leading to discussions about extracting said prompt using an image.\nUser Gets HackAPrompt Blocked: A member reported that HackAPrompt x PlinyAnthropic flagged them, preventing any of their messages from being sent.\n\nThis suggests a stringent filtering system that completely blocks flagged users from interacting with the service.\nLMArena Discord\nArena Rebrand Mimics Claude's UI: Users noticed the LMArena rebrand to Arena and felt it was a clone of Claude's UI, a blog post explains the change.\n\nMembers noted some UI issues such as fonts and the visibility of the website's text as well as some missing features.\nCaptcha Conundrums Continue: Users report consistent issues with the captchas failing on nearly every attempt, and provided troubleshooting steps of relogging to your account or taking off all extensions to pass the captcha.\n\nUsers hate the captcha and wish the old emojis, stickers, and features would return.\nLogin Lost? Recover Button to the Rescue!: A member experiencing login issues shared a screenshot of a recover button that can be clicked in order to log back into the updated Arena.\n\nAnother member noted an announcement video as well.\nKimi K2.5 Thinking Ascends Text Arena Leaderboard: The Text Arena leaderboard has been updated and Kimi K2.5 Thinking is now ranked the #1 open model and ranking #15 overall.\n\nKimi K2.5 Thinking is #7 in Coding, #7 in Instruction Following, and #14 in Hard Prompts, and has also been added to the Code Arena.\nArena Shorts, Better AI videos in under 90 seconds!: Arena (formerly LMArena) has uploaded a Better AI videos in under 90 seconds video to their Youtube channel.\n\nThe group acknowledged that as the platform evolves from only Language Models, the name is becoming more generic and it was previously part of LMSYS.\nUnsloth AI (Daniel Han) Discord\nBatch Size Bumps GPU Benefit: Members discovered that one way to achieve decent GPU utilization is to increase batch size until utilization improves, balancing it with potential gains from GA (Genetic Algorithms).\n\nAlso, a member inquired whether Unsloth will release a Q3 version for Kimi 2.5, voicing concerns about accuracy drops.\nOracle's Offerings Spark Skepticism: A member inquired if Oracle stands as state-of-the-art in RAG (Retrieval-Augmented Generation) and fine-tuning tech, setting off a debate.\n\nThe terse reply of, \""What 😅\"", was later amended to allow that OCI (Oracle Cloud Infrastructure) does have some good tools, showing split opinions.\nArcee's Arithmetic: Trinity Costs $350k: A new Arcee model image was shared, along with the note that pretraining cost about $350k, with a link to the Trinity Large Tech Report.\n\nIt was clarified that GLM 4.7 is a 358B parameter model but not a base model, making benchmark comparisons less useful against models such as GLM 4.5.\nGemini's Gatekeeping Game: A Google hackathon showed that, despite heavy output filtering, especially for corporate/government settings, Gemini's API can be made to produce almost anything.\n\nOne member got the voice models to swear by putting it in the system prompt.\nModal Multi-GPU Mayhem: A member ran into problems training a Qwen3 model on 3 GPUs on Modal, getting a ValueError from an incorrect device_map configuration.\n\nThe training setup ultimately moved away from Unsloth due to incompatibility with PyTorch 2.4.1, choosing a transformers + PEFT setup for better stability.\nOpenRouter Discord\nArcee releases Trinity Large Preview for Free: Arcee launched Trinity-Large-Preview, a chat-ready variant of its frontier-scale open-weight model, which is free for a limited time and detailed on X.\n\nThe model is a 400B parameter sparse Mixture-of-Experts model with 13B active parameters per token, utilizing 256 experts with 4 active per token (1.56% routing) for efficiency, discussed during Lucas Atkins' livestream.\nFree Credits Boost Cyberpad: A user updated Cyberpad to include some free credits.\n\nNo further information was provided.\nImage Model Output Glitches Reported: Users reported that certain image models such as GPT-5 Image Mini, GPT-5 Image, and Gemini 2.5 Flash Image are not consistently generating images, although Gemini 2.5 flash works intermittently.\n\nModels like Gemini 3 Flash Preview, Gemini 2.5 Flash Lite Preview, Seed 1.6, GLM-4.6v, and Grok 4.1-fast have functional response_format support.\nOpenRouter Users Await Refunds: Users are experiencing significant delays in receiving refunds from OpenRouter, with some waiting since early January and submitting multiple support tickets.\n\nUsers are requesting clarity on refund timelines and improved communication from the OpenRouter team.\nAgentic Vision with Gemini 3 Flash Debuts: Google introduced Agentic Vision with Gemini 3 Flash, enabling visual reasoning and code execution for step-by-step image manipulation.\n\nOpenAI's O3 and O4-mini are extending image capabilities by enabling chain-of-thought reasoning with images for tasks like cropping, zooming, and rotating, discussed in this blog post.\nCursor Community Discord\nVanishing Revert Button Frustrates Users: Users reported the revert button disappearing from the UI, leading to frustration and token waste, with one finding that duplicating an older chat brought it back.\n\nA member found that not clicking on the revert button would make it reappear, suggesting it was a one-time bug.\nCursor CLI: The Dark Horse?: Some developers are preferring Cursor CLI over the IDE due to a smaller memory footprint, which helps them avoid IDE crashes and model unresponsiveness, especially with larger projects exceeding 100k LOC.\n\nConversely, one user found Cursor CLI inside the IDE (with WSL as the terminal) to be \""pure trash.. like for real, not usable\"", reporting the UI is not smooth even with 64GB of RAM and an i7 processor.\nCursor's Subscription Adjustment Stings: After September 15th, auto mode is no longer unlimited and counts toward the $20 monthly allowance, priced at $1.25 per 1M tokens for Input + Cache Write, $6.00 per 1M tokens for Output, and $0.25 per 1M tokens for Cache Read.\n\nOne user discovered they could burn through their monthly subscription very quickly, suggesting it may be cheaper to use their own api keys, or use Claude Code.\nClawdbot's Security Flaw Exposed: A user shared links regarding security concerns with Clawdbot, reporting that exposed control panels pose credential leaks and account takeovers.\n\nThere is speculation it could lead to a \""store now, decrypt later\"" data breach due to potential quantum decryption issues, and that the company got a cease and desist for the issues.\nGemini Vision Set to Revolutionize Front-End: A user found that Gemini agentic vision is nearing state-of-the-art (SOTA) performance for vision tasks, and believes its integration would simplify front-end development.\n\nMembers stated that they can't wait to see vision integrated into the agent, and that it is superior to the Auto tool.\nLM Studio Discord\nLM Studio v0.4 Goes Headless and Parallel: LM Studio v0.4 introduces headless mode and parallel inference, with users excited about the new capabilities and a revamped UI, as detailed in the complete blogpost here.\n\nNote that in-app updates require reinstalling the app, and some UI elements are now in dev mode.\nGLM 3.7 Flash Shows Coding Potential: Members note that GLM 3.7 Flash shows good coding ability, but GPT OSS 120 is expected to be the superior coder, especially at Q4.\n\nThis suggests that while GLM 3.7 Flash is a step forward, it may not outperform existing models.\nROCm Runs on LM Studio Runtime: Users discovered that ROCm can be enabled within LM Studio under the runtime settings, though the method was initially obscured for some users, as discussed in this Unsloth Reddit thread.\n\nThis integration allows users to leverage ROCm for potentially improved performance.\nDevstral-2 Demands Decent GPU Deployment: Members discussed the hardware requirements for running Devstral-2 locally, with one user suggesting 48GB of GPU (e.g., 3090) for the 24B version.\n\nFor the 120B version, parallel computing or an H200 with EXL2 model format were suggested, as GGUF was deemed too slow.\nHardware Acceleration Seeks Hook into LM Studio: A member from a hardware accelerator company inquired about adding an LM Studio backend for their hardware, and was pointed to llama.cpp.\n\nIt was noted that LM Studio is primarily a closed source project by Element Labs, and pointed to LM Studio Enterprise.\nMoonshot AI (Kimi K-2) Discord\nKimi K2.5's Price Tag Raises Eyebrows: Users debated the $19 monthly subscription for Kimi K2.5, with some finding it expensive and questioning whether a recurring deal could be established.\n\nOthers suggested sticking to the free tier, arguing that smaller Chinese companies like Moonshot AI need to run large models like K2.5, making lower prices unlikely.\nGoogle's AI Studio Training Sparks Privacy Debate: Concerns arose over Google's practices of training and viewing conversations in AI Studio and Gemini apps, raising privacy issues.\n\nConversely, another user mentioned they open source their projects, suggesting the data's inevitable inclusion in training datasets regardless.\nModel Selection Showdown: Kimi K2.5 Triumphs in STEM: Users compared Kimi K2.5 against Mistral and Qwen for tasks spanning coding to general question-answering.\n\nNotably, Kimi K2.5 boasts the highest benchmarks in physics, chemistry, and math, while also demonstrating strong performance in design and logical reasoning.\nKimi CLI Outpaces Alternatives in Speed Trials: Kimi CLI was lauded for its speed and efficiency over tools like oh-my-opencode, particularly in web page analysis, with reduced token consumption.\n\nHowever, some found the model's output quality less impressive, suggesting further comparative analysis is warranted.\nAgent Swarm Utility Under Question: Enthusiasts highlighted Agent Swarm's in-depth research capabilities with Kimi, but noted it can deplete credits at 3x the normal rate.\n\nOthers remained uncertain about its applications, suggesting a need for clearer use-cases and caution regarding resource consumption.\nPerplexity AI Discord\nPerplexity Subs Deemed a Scam: Several users reported unexpected subscription changes and charges after automatic renewals, with one user canceling their subscription, calling it a scam.\n\nUsers experienced issues such as being charged without receiving service or not obtaining refunds, prompting some to consider contacting their banks or reporting the matter to the FTC.\nQuery Cap Shenanigans Baffle Users: Some users reported issues with query limits on their Pro subscriptions, with limits dropping to one query per hour.\n\nHowever, some users saw their limits restored to 600, and one user shared a link to check query limits.\nImage Generation Restricted By Region?: Users reported image generation restrictions in certain regions, possibly due to xAI controversies and an EU lawsuit.\n\nSuggestions included trying different models or contacting support; a user from India confirmed they were affected by this issue.\nKimi 2.5 Coming Soon to PPLX?: Users are eagerly anticipating the release of the Kimi 2.5 model on Perplexity.\n\nSpeculation suggests that Perplexity typically implements updates quickly.\nNous Research AI Discord\nGPT Pro Hides the Model Magic?: Members debated whether GPT Pro's performance boost comes from more GPUs or an improved model, suggesting OpenAI might obscure the truth for competitive reasons.\n\nOne member likened OpenAI's pricing strategy to fakery, comparing it to impressions over measured value, similar to the stock market's perception of Tesla.\nDeepSeek's Never-Ending Imprisonment: It was reported that DeepSeek tends to get stuck in a jailbreak loop, repeating the same rejection message indefinitely, regardless of subsequent prompts.\n\nWhile the API endpoints fare slightly better, the raw model is effectively cooked once it enters this state.\nTI-84 Gets Neural Network Transplant: A member detailed running a neural network on a TI-84 Plus calculator for spellchecking, documenting the process on an academic website with a demo video.\n\nThe member joked that despite this achievement, their work on Claude Code Orchestration remains more practically useful.\nMergeMix Paper Sparks Data Mixture Excitement: The paper 'MergeMix: Optimizing Mid-Training Data Mixtures via Learnable Model Merging' garnered interest due to its relevance for open source projects with limited budgets.\n\nThe paper explores techniques for optimizing data mixtures and model merging during training, potentially offering resource-efficient strategies.\nHermes 4 Pricing: Discount or Deception?: A member questioned whether the discounted pricing for Hermes 4 series models is permanent before subscribing to the API, citing its superiority in RP and story-writing compared to Deepseek.\n\nAnother member clarified there's no subscription, just credit purchases subject to change, so the value depends on pricing and usage.\nOpenAI Discord\nGemini 3 Pro Fumbles Subtitle Generation: Users reported that Gemini 3 Pro is fabricating .srt files with nothing related to the audio in the video.\n\nThis poor performance led to disappointment among users who stated that Gemini is overhyped.\nClawdbot rebranded Moltbot is a Scam: Clawdbot, now known as moltbot, is an agentic system that controls your entire OC by API keys from Anthropic, Google, and OpenAI, and users are being warned against it.\n\nOne user stated that it is a huge scam by crypto bros to steal your information, which can be weaponized via prompt injection, raising significant security and privacy concerns.\nPrism Deemed Detrimental to Scientific Research: Despite OpenAI's aims to advance science with Prism, one user stated that Prism is damaging to scientific research.\n\nAnother user inquired about Prism's API access, to write some of their project using other AI and Codex.\nGPT Pro Loses Zip File Reading: A user reported that GPT Pro 5.2, which could previously read and analyze ZIP files, is now failing to find uploaded files for analysis.\n\nThe user is asking if others are experiencing the same issue, or has any insight.\nBlocking Black and White Images via Chiaroscuro Avoidance: Users discussed an image generation issue related to the Chiaroscuro effect and have suggested 'Please avoid Chiaroscuro' in prompts if encountering unwanted black and white images.\n\nChiaroscuro is the use of strong contrasts between light and dark, usually bold contrasts affecting a whole composition.\nGPU MODE Discord\nDecart drafts SF perf engineers: Decart seeks engineers for low-latency kernels, real-time video/world models, and accelerators like Trainium 3 (as shown at ReInvent video) and their new Lucy 2 autoregressive video model (tech report).\n\nThey are also co-sponsoring a kernel challenge with GPU Mode for autoregressive diffusion models, and encourage interested parties to send perf work to heba@decart.ai.\nINT4 QAT RL Model Rollout: A member shared a link to a GitHub repo that focused on squeezing a 1TB model rollout into a single H200 using INT4 QAT RL end-to-end practice: GitHub repo.\n\nThe repository provides resources and documentation related to the INT4 QAT RL implementation, optimizing large model rollouts.\nTransformers and PyTorch face upgrade break: After upgrading transformers and pytorch, a member reported a NotImplementedError: \""_amp_foreach_non_finite_check_and_unscale_cuda\"" not implemented for 'BFloat16'.\n\nDowngrading to transformers 4.57.3 fixed the issue; others had similar issues, which are discussed in this pytorch issue and optimi issue.\nInteractive Numerics Tools Emerge: A member expressed surprise that quantization people have not already created interactive tools for exploring numerics, and cited captum as one possible tool.\n\nThis member lamented the lack of proper UI/UX in current tools for model debugging, checking which circuit is unstable, which layer is causing a bunch of outlier, simple stuff like that.\nDGX's Dominant Memory Bandwidth: Instruction sets for DGX and 5090 are similar, but DGX excels with full-speed fp32 accumulation, like Blackwell PRO, and its key differentiator is 1.8TB/s memory bandwidth.\n\nThis contrasts sharply with 5090's 300 GB/s, emphasizing the importance of efficient L2 cache utilization to maximize DGX's potential.\nLatent Space Discord\nCoding Enters the Agent Era: Andrej Karpathy forecasts that 80% of coding will be agent-driven by 2026, highlighting LLMs' tenacity and goal-setting capabilities; insights here.\n\nKarpathy also cautioned against potential 'slop' and over-engineering, so it might not all be roses.\nOpenAI's Prism Shines for Scientists: OpenAI unveiled Prism, a complimentary research workspace powered by GPT-5.2, accessible via the web to those with a personal ChatGPT account; get started here.\n\nThe tool aims to provide scientists with advanced AI capabilities for research purposes.\nTrinity Large Arrives: Prime Intellect, Arcee AI, and Datology launched Trinity Large, a 400B parameter Mixture of Experts model, that uses only 13B active parameters; more info here.\n\nThe model aims to deliver high performance while maintaining efficiency.\nCursor Indexes Codebases: Cursor announced faster indexing for large codebases as well as improved semantic search, promising performance enhancements; read more here.\n\nSemantic search and improved indexing aim to provide more efficient code navigation.\nPodcast Shifts Focus to Science: Latent Space has launched its second podcast, 'Science' (link to podcast), hosted by <@713947182167883897> and <@348078436058660866>.\n\nDiscussions about the new 'Science' podcast have moved to a dedicated channel.\nHuggingFace Discord\nKimi 2.5 Model Beats GPT5 Locally: The new Kimi 2.5 model is reportedly performing better than GPT5, accessible locally via HuggingFace and also through sites such as Fireworks.\n\nMembers seek local agent recommendations for use with Zed, expressing dissatisfaction with GLM-4.7-Flash at Q4 with llama.cpp, with kimi and qwencoders 30b q4 being suggested as alternatives.\nC++ Enthusiast Champions Supreme Rule for AI Agents: A member argued that C++ is gonna always rule for building AI agents, due to bloat in Python agents, and recommended fastwhisper.cpp for STT, Qwen embeddings in LlamaCPP for RAG, and LFM2.5vl for VLM.\n\nThis sparked conversation around STT (fastwhisper.cpp), RAG (Qwen embeddings in LlamaCPP), and VLM (LFM2.5vl).\nVision Model Vaporizes JPEG Artifacts: A vision model was released that removes artifacts caused by JPEG compression using a unique design with no Batch Norm, no activations after training, and Operator layers instead of Convolutional layers.\n\nThe model's architecture focuses on gaining accuracy through width rather than depth.\nRemnantInstruct-8B: SLERP Merge Balances Creative & Factual: RemnantInstruct-8B is a SLERP merge that recombines a creative fine-tune (allura-org/remnant-qwen3-8b) with its base model (Qwen/Qwen3-8B) to balance narrative skills with factual accuracy.\n\nThe merge strategy favors the creative fine-tune in self-attention layers and the base model in MLP layers, with the goal of preserving Qwen3's thinking mode.\nQuantum Computing Embraced by VLMs: A member open-sourced their undergraduate thesis on specializing vision-language models for quantum computing and code with Qiskit, including a dataset, models, code, and demo.\n\nThe thesis explores adapting VLMs to assist with quantum computing tasks and coding.\nYannick Kilcher Discord\nTransformers Can Parameterize Vector Fields: A member argued that transformers can be used in flow matching as a training objective to parametrize the vector field for continuous diffusion, using patch embedding to encode patch position.\n\nOther members agreed that diffusion models and flow matching are mathematically similar, citing this paper on ArXiv.\nDiffusion Models are not Better than Autoregression: A member suggested that the notion of diffusion being superior to autoregression is false, highlighting architectural and scaling limitations, linking to this paper on repeating context.\n\nThey pointed out that improvements like repeating the context or re-encoding a sequence non-causally could bridge the gap, overcoming current design limitations in LLMs.\nChatGPT Wrappers Flourish, Value Questioned: Members observed that most new tools are simply ChatGPT wrappers, raising questions about their actual value and the ease with which scammers can create wrappers, referencing the Clawdbot scam.\n\nIt was suggested that these wrappers are necessary to demonstrate use cases, as they make it easier for people to understand how to apply the models.\nAI Coding Tools Won't Replace True Skill: Despite the rise of AI coding tools, members believe coding ability can be relearned, pointing to a blog post on Trinity Large, adding that fast code production from AI may hinder true understanding.\n\nThey noted that a bad implementation from an LLM isn't weighted the same as before, since the mental and time cost to create it was so low.\ntinygrad (George Hotz) Discord\nAMD Emulator Exposes Debug Printing: The new AMD emulator (AMD=1 MOCKGPU=1) now supports debug printing, where setting DEBUG=3 prints all compiled instructions and DEBUG=6 prints them as they run, according to a linked screenshot.\n\nThis enhancement facilitates more in-depth debugging and analysis of compiled code directly within the emulator environment.\nGithub Actions Speed Boost via Optimization: Discussion centered on accelerating GitHub Actions by emphasizing code optimization, instead of only relying on faster hardware or external resources.\n\nThe consensus was to prioritize doing things the right way over quick fixes that only improve surface level metrics, potentially creating tech debt.\nMULACC Fusion Receives a Fix: A fix was proposed to enhance decompositions.py by adding a pattern to fuse (x << n) + c → MULACC(x, 2^n, c), specifically targeting integer MULACC with power-of-2 constants, as detailed in PR 14387.\n\nThis adjustment aims to refine the fusion process, potentially improving the efficiency of certain arithmetic operations.\nEgraphs Considered for Universal Fixes: The potential use of egraphs to address problems in a generic manner was explored, emphasizing the importance of simplicity.\n\nIt was also suggested to tag rewrites with their origin to maintain a clear record of equivalences created during rewriting processes.\nMac MetalCompiler Improvements on the Horizon: Suggested improvements to the hacks for MetalCompiler on Mac are on the way, especially focusing on improvements and cleanups that reduce line count and improve readability.\n\nThe goal is to make the MetalCompiler more maintainable and efficient, benefiting developers working on Mac platforms.\nModular (Mojo 🔥) Discord\nGTK Bindings Auto-Generated: Hammad Ali will present autogenerated GTK bindings for Mojo at the Modular Community Meeting on February 2nd at 10 AM PT, according to the Modular forum.\n\nThe presentation will detail how GTK bindings are automatically generated, potentially improving the ease of creating GUIs with Mojo.\nMojo's Performance Prowess: Tatiana Melnichenko will share memory-bound bandwidth results and compute-bound gaps on H100/MI300A comparing Mojo with CUDA/HIP at the February Community Meeting.\n\nThis talk should provide insights into Mojo's performance characteristics relative to established GPU programming models.\nmacOS Gatekeeper Gets in the Way: Members suspect performance difference between first and subsequent runs on macOS is due to Gatekeeper's trust dance.\n\nClearing the quarantine xattr or ad-hoc codesigning could mitigate this, and wondered if a codesign step in mojo build could hide this entirely.\nout Parameters Outshine NVRO: out parameters in Mojo name the location where the return value of a function will end up, serving as a Named Value Return Optimization (NVRO) replacement.\n\nMembers claim this provides a guarantee about the return value's destination, unlike relying on compiler optimization.\nQwen3 Embedding Model Gets Accuracy Boost: A member requested a review of their PR for the Qwen3 embedding model, citing that the fix is important for getting much better accuracy.\n\nAnother member responded that new fixes likely won't be pulled into the upcoming release but would be available in the nightlies, with a single-line fix available here.\nManus.im Discord Discord\nManus is Credit Crunching: A user noticed that Manus seems to be using fewer credits for the same quality of work, questioning whether credit usage has improved.\n\nNo further details or confirmations were provided regarding potential changes to Manus's credit consumption algorithms.\nCloud Browser Causes Conundrums: A user encountered issues with the cloud browser, receiving an error message stating that the server is unavailable and the website isn't loading.\n\nManus support requested the user's email, session link, and Manus User ID via DMs to investigate the issue further.\nAI Engineer Aces LLM Systems: An AI + Full Stack Engineer introduced themself, highlighting their expertise in LLM systems, autonomous agents, workflow automation, and multimodal AI.\n\nThey shared their core skills such as DSPy, LangChain, AutoGen, and CrewAI.\nCommunity Craves Cross-Chat Context: A user suggested that enabling Manus to access context from other chats would be a game changer, indicating a desire for enhanced contextual awareness in the AI's responses.\n\nThe member pointed to the need for shared context across channels, to inform more sophisticated responses.\nDSPy Discord\nPrompt Optimizer Peeps Sought: Members inquired about experiences working with prompt optimizers and specifically if anyone has experience using Skills within the dspy module.\n\nThe discussion suggests interest in leveraging these tools to improve prompt engineering workflows.\nllmlingua Gets Linked: A member shared a link to llmlingua.com in the context of a discussion about prompt optimizers.\n\nIt suggests llmlingua might be a relevant tool for those exploring prompt optimization strategies.\nDSPy ReAct Agent Yearns for Skills: A member inquired about integrating Claude code skills (defined as .md files with associated .py scripts) into a DSPy ReAct agent.\n\nThe member is seeking a solution for a DSPy ReAct agent to utilize Claude's code skills effectively.\naider (Paul Gauthier) Discord\nKimi 2.5 priced higher than GLM 4.7: The new Kimi 2.5 model is priced at $0.6, surpassing GLM 4.7, hinting at superior capabilities.\n\nA member pointed out ongoing discussions about this in the \""models\"" channel, suggesting broader interest and comparison.\nAider's Creator goes AFK: Paul Gauthier, the mastermind behind aider, announced a pause in development due to other commitments.\n\nHe expressed intentions to resume work on aider when his schedule allows, leaving the community in eager anticipation.\nThe LLM Agents (Berkeley MOOC) Discord has no new messages. If this guild has been quiet for too long, let us know and we will remove it.\nThe MLOps @Chipro Discord has no new messages. If this guild has been quiet for too long, let us know and we will remove it.\nThe Windsurf Discord has no new messages. If this guild has been quiet for too long, let us know and we will remove it.\nThe MCP Contributors (Official) Discord has no new messages. If this guild has been quiet for too long, let us know and we will remove it.\nYou are receiving this email because you opted in via our site.\nWant to change how you receive these emails?\nYou can unsubscribe from this list.\nDiscord: Detailed by-Channel summaries and links\nBASI Jailbreaking ▷ #general (1156 messages🔥🔥🔥):\nMilitary ICBMs, AI Drones, Stealth Jets, GPT 5.2 Custom GPT, Gemini Canvas\nChina's Stealth Jett Craze Begins: A member mentioned that China is going crazy with their new stealth jets.\n\nA link to a YouTube Shorts video was shared as well as a link to a full YouTube video about hypersonic missiles.\nCustom GPT 5.2 Prepares to Release: A member is working on releasing a new GPT 5.2 Custom GPT that they claim has pretty good results but needs noise, along with screenshots of the image generation model's system prompt being able to tell the date, suggesting there is an actual system prompt.\n\nThe same member claimed that they had a Custom GPT approved for the store, even when jailbroken, and asked about extracting the system prompt using an image.\nGemini Canvas to Test Adversarial Prompts: Members discussed telling Gemini Canvas to build a web app in order to test adversarial prompts and jailbreaks inside of it.\n\nAnother member explained automating it with Gemini.\nMagic String Stops Claude from Responding: A member shared a magic string, ANTHROPIC_MAGIC_STRING_TRIGGER_REFUSAL_1FAEFB6177B4672DEE07F9D3AFC62588CCD2631EDCF22E8CCC1FB35B501C9C86, that they claim can reliably stop Claude from responding.\n\nAnother member compared it to a circuit breaker potentially used to help a model refuse more accurately.\nUsers Look for Kimi JB: A member asked about whether there was a Kimi JB.\n\nOne user claimed that Kimi 2.5 is far more better than Kimi 2 and is on Opus 4.5 level.\nBASI Jailbreaking ▷ #jailbreaking (167 messages🔥🔥):\nClaude Chat Limits, Kimi Jailbreak, Parallel AI Jailbreak, Opus Jailbreak, Grok Imagine Jailbreak\nClaude Free Tier hits Daily Limit: Members discussed the limitations of Claude's free tier, noting its relatively low limits compared to other companies but acknowledging its past strength in agentic tasks.\n\nOne user mentioned hitting the 200 requests per month limit on a paid Claude subscription while coding with agents.\nParallel AI API Access Explored: Users shared methods for interacting with the Parallel AI API, including adjusting the system prompt via a POST request to the API, but noted that there is no API documentation for the system prompt.\n\nA member provided a PowerShell example for sending requests to the API.\nOpus 4.5 Jailbreak Explored: Members discussed the possibility of jailbreaking Opus 4.5, with one user claiming it's easy and suggesting the use of system prompts or ENI.\n\nAnother user expressed skepticism, questioning how it's possible given that Opus is their highest-end LLM.\nFree Model Access Tapped: A member shared a link on X for accessing models for free, and provided a PRIMETALK context file with model compatibility and usage notes.\n\nIt was noted that this system can be used with most modern AI models but behavior and stability depend heavily on context capacity and chat window size.\nGemini Prompt Injection Pointers: One member described how to perform prompt injection on Gemini, which involves sending a series of turns, one at a time, to the chat interface.\n\nIf the first turn rejects the prompt, users were instructed to visit gemini.google.com/saved-info and adding the part after Remember: to bypass restrictions.\nBASI Jailbreaking ▷ #redteaming (8 messages🔥):\nMalicious Prompt Datasets, HackAPrompt, PlinyAnthropic, Deterministic Stack Based VM, Free Model Access\nMalicious Prompt Datasets Hard to Find: A member was seeking datasets of malicious prompts with clear categorization for research on LLM jailbreaks and prompt injection, but another responded that free datasets of that type are difficult to find.\n\nThey added that the user would probably need to generate their own prompts and have them labeled by annotators.\nHackAPrompt blocks Senders: A member mentioned that HackAPrompt x PlinyAnthropic flagged them a long time ago and literally just bypasses all of their sends, and they don’t even let it send.\nRecursive Simulation Kernel with REPL: One member asked if they could get a deterministic stack based VM poofed up in the model's substrate, like some kinda bootable Recursive Simulation Kernel with a REPL.\nFree Model Access via X: One member provided a link to X on how to access models for free.\nPath Needed for Red Teaming: A member asked for a path going into red teaming.\nLMArena ▷ #general (1038 messages🔥🔥🔥):\nArena new UI, Arena rebrand, Arena captcha issues, LMArena name change\nArena Rebrand, users want STOP button, and old emojis: Users requested a stop button and raised concerns about the Google captcha being difficult to pass after the LMArena rebrand to Arena.  Several users expressed that they hate the captcha.\n\nSome users requested to have old emojis, stickers, and features to return, while others embraced the redesign and said 'LMArena NEARLY rhymes with end of an era'.\nArena's new look is a Claude clone!: Many users immediately noticed the rebrand of LMArena to Arena and felt it was a clone of Claude's UI, while other members liked the new look. A blog post was shared to explain the change.\n\nMembers noted some UI issues such as fonts and the visibility of the website's text as well as some missing features.\nCan't login? Try Recover Button!: A member experiencing login issues shared a screenshot of a recover button that can be clicked in order to log back into the updated Arena, and avoid having to type login details again.\n\nAnother member noted an announcement video as well.\nWhere LMArena = Language Model Arena: Some members made jokes about what LM stands for in LMArena, with one explaining it stands for Language Model Arena.  Another member confirmed it here.\n\nThe group acknowledged that as the platform evolves from only Language Models, the name is becoming more generic and it was previously part of LMSYS.\nThe Hallucinated Haze, Captcha Maze: Users report consistent issues with the captchas, with failures on nearly every attempt, while the model continues to hallucinate.\n\nOne user provided some troubleshooting steps of relogging to your account and another reported that you need to take off all extensions to pass the captcha.\nLMArena ▷ #announcements (3 messages):\nLMArena 90 second AI videos, Text Arena Leaderboard Update, LMArena rebrand to Arena\nArena uploads AI videos in under 90 seconds!: Arena (formerly LMArena) has uploaded a Better AI videos in under 90 seconds video to their Youtube channel.\nKimi K2.5 Thinking tops Text Arena!: The Text Arena leaderboard has been updated and Kimi K2.5 Thinking is now ranked the #1 open model and ranking #15 overall.\n\nKimi K2.5 Thinking is #7 in Coding, #7 in Instruction Following, and #14 in Hard Prompts, and has also been added to the Code Arena.\nLMArena Rebrands as Arena!: LMArena announced they are rebranding as Arena to match their scientific mission to measure and advance the frontier of AI, now available at: arena.ai.\nUnsloth AI (Daniel Han) ▷ #general (299 messages🔥🔥):\nGPU utilization, Kimi 2.5 Q3 Release, Oracle RAG and Fine-tuning, Unsloth's Transformers/MoE Update, Chinese text in LLMs\nBatch Size Boosts GPU Utilization: Members discussed that to achieve decent GPU utilization, one should increase batch size until utilization improves, balancing it with potential gains from GA (Genetic Algorithms).\n\nOne member asked if Unsloth will release a Q3 version for Kimi 2.5, expressing concern about potential accuracy penalties, highlighting the community's interest in optimized model releases.\nDebate whether Oracle is a State-of-the-Art Company: A member asked if Oracle is a state-of-the-art company in RAG (Retrieval-Augmented Generation) and fine-tuning technologies, sparking some discussion.\n\nAnother member responded with \""What 😅\"", later adding that OCI (Oracle Cloud Infrastructure) does have some good tools, indicating mixed opinions on Oracle's capabilities in these areas.\nArcee's Trinity Model Costs $350k: A member shared a new Arcee model image, noting that pretraining cost about $350k, and they linked to the Trinity Large Tech Report.\n\nThey also mentioned that GLM 4.7 is a 358B parameter model, much larger than GLM 4.5, but it is not a base model, so comparing benchmarks aren't as useful.\nLLMs Speak Chinese?: A member noticed getting random Chinese text from OpenAI and Anthropic, even with English-only prompts, sparking a discussion about potential data contamination or inherent linguistic similarities.\n\nAnother member suggested that if tokens have similar meanings between languages, introducing one language might cause the model to favor it due to token probability and similarity.\nGemini API Still Jailbreakable: Members discussed Gemini's output filtering, with one noting that while Gemini heavily filters outputs, especially for corporate/government settings, its API can be manipulated to produce almost anything.\n\nOne member mentioned using the API at a Google hackathon and getting the voice models to swear by putting it in the system prompt.\nUnsloth AI (Daniel Han) ▷ #introduce-yourself (4 messages):\nEdge AI Engineer, Quantization and LoRA FT\nEdge AI Engineer Enters the Fray: A Senior Edge AI Engineer named Josh introduces himself, detailing experience building real offline agents in the DoD and pubsec for 6 years.\n\nHe adds that he makes quants for fun and exclusively uses Unsloth for local quantization and LoRA fine-tuning.\nNew Member Says \""HelloHi\"": A new member named Josh from senior Edge AI engineering introduced himself.\n\nHe shares their passion for using Unsloth for quantization and LoRA fine-tuning\nUnsloth AI (Daniel Han) ▷ #off-topic (969 messages🔥🔥🔥):\nPersonaplex, GLM 4.7, GGUF, Model Quantization, Vendor Lock-in\nPersonaplex Personalities: Members discussed the limitations of Personaplex in enforcing personality and its tendency to become like shitty ai podcasts after some iterations.\n\nOne member mentioned they don't have access to the stored recorded calls that would be perfect to train Persona Plex on.\nGLM 4.7 Flash Performance Talk: A user asked if anyone had tried the GLM-4.7-Flash-REAP-23B-A3B-GGUF model and another responded that REAP models are often not very good, suggesting a lower quantization instead.\n\nOthers weighed in with their performance and insights on the GLM 4.7 Flash model, with comparisons to GPT-OSS-120B and Kimi in terms of reasoning, efficiency, and ability to relate information.\nGGUF Safety Concerns: A member inquired about resources regarding the potential unsafety of GGUFs, specifically if a malicious actor got involved.\n\nHowever, another member stated I'm not familiar with that, I think you might have got me mixed with someone else so nothing more came of it.\nAI Model Hallucination Watch: A member noted that their 3b llama model made the creepy assumption that it was trained on my voice without prompting, leading to a discussion about hallucinations in LLMs and their lack of awareness of their training or state.\n\nOne member recommends this YouTube video on AI hallucinations as a starter on the topic.\n**Ve..."",""content"":""**AI News for 1/27/2026-1/28/2026** highlights a quiet day with deep dives into frontier model \""personality split\"" where **GPT-5.2** excels at *exploration* and **Claude Opus 4.5** at *exploitation*, suggesting **OpenAI** suits research workflows and **Anthropic** commercial reliability. The rise of agentic coding loops shows new failure modes, with *self-verification* workflows gaining traction. The open-model **Kimi K2.5** emerges as a flashpoint, boasting enhanced **agent execution**, **multimodality**, and **coding polish**, runnable on **Apple silicon M3 Ultra Mac Studios** with **Thunderbolt 5 (RDMA)**, and challenging **Claude Opus 4.5** on benchmarks and pricing. Licensing issues threaten enterprise adoption despite model quality. The meme \""clawdbot\"" reflects rapid agent branding proliferation. Agent engineering advances with shared \""skills\"" interfaces promoted by **DeepLearning.AI**, **Anthropic**, and **LangChain**."",""contentSnippet"":""**AI News for 1/27/2026-1/28/2026** highlights a quiet day with deep dives into frontier model \""personality split\"" where **GPT-5.2** excels at *exploration* and **Claude Opus 4.5** at *exploitation*, suggesting **OpenAI** suits research workflows and **Anthropic** commercial reliability. The rise of agentic coding loops shows new failure modes, with *self-verification* workflows gaining traction. The open-model **Kimi K2.5** emerges as a flashpoint, boasting enhanced **agent execution**, **multimodality**, and **coding polish**, runnable on **Apple silicon M3 Ultra Mac Studios** with **Thunderbolt 5 (RDMA)**, and challenging **Claude Opus 4.5** on benchmarks and pricing. Licensing issues threaten enterprise adoption despite model quality. The meme \""clawdbot\"" reflects rapid agent branding proliferation. Agent engineering advances with shared \""skills\"" interfaces promoted by **DeepLearning.AI**, **Anthropic**, and **LangChain**."",""guid"":""https://news.smol.ai/issues/26-01-28-not-much/"",""categories"":[""openai"",""anthropic"",""deeplearningai"",""langchain"",""apple"",""gpt-5.2"",""claude-opus-4.5"",""kimi-k2.5"",""agentic-ai"",""multimodality"",""coding"",""self-verification"",""agent-engineering"",""model-benchmarking"",""model-optimization"",""workflow-automation""],""isoDate"":""2026-01-28T05:44:39.000Z""}"
Smol,"Moonshot Kimi K2.5 - Beats Sonnet 4.5 at half the cost, SOTA Open Model, first Native Image+Video, 100 parallel Agent Swarm manager",https://news.smol.ai/issues/26-01-27-kimi-k25/,2026-01-27T05:44:39.000Z,"<p><strong>China takes another huge leap ahead in open models</strong></p>
<blockquote>
<p>AI News for 1/26/2026-1/27/2026. We checked 12 subreddits, <a href=""https://twitter.com/i/lists/1585430245762441216""><strong>544</strong> Twitters</a> and <strong>24</strong> Discords (<strong>206</strong> channels, and <strong>7476</strong> messages) for you. Estimated reading time saved (at 200wpm): <strong>602 minutes</strong>. <strong>Our new website</strong> is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on <a href=""https://x.com/Smol_AI"">@smol_ai</a>!</p>
</blockquote>
<blockquote>
<p>AI News for 1/26/2026-1/27/2026. We checked 12 subreddits, 544 Twitters and 24 Discords (206 channels, and 7476 messages) for you. Estimated reading time saved (at 200wpm): 602 minutes. AINews’ website lets you search all past issues. As a reminder, AINews is now a section of Latent Space. You can opt in/out of email frequencies!</p>
</blockquote>
<p>Kimi has been on an <a href=""http://china%20takes%20another%20huge%20leap%20ahead%20in%20open%20models/"">absolute tear in the past year</a>, and we last heard from them in November with <a href=""https://news.smol.ai/issues?pattern=sota"">Kimi K2 Thinking</a>. Like K2, today’s K2.5 is still a 32B active-1T param model (384 experts), “<a href=""https://x.com/teortaxesTex/status/2016027034653164004"">built through continual pretraining on 15 trillion mixed visual and text tokens atop Kimi-K2-Base”</a> (which itself was on 15T tokens), and with an EXTREMELY well produced video from their founder (3 minutes, just watch it):</p>
<p>https://youtu.be/5rithrDqeN8</p>
<p>They again claim SOTA on HLE and BrowseComp (<a href=""https://www.kimi.com/blog/kimi-k2-5.html#footnotes"">footnotes</a> give confidence the tests are legit), but also open model SOTA for vision and coding tasks:</p>
<p><a href=""https://x.com/Kimi_Moonshot/status/2016024049869324599?s=20"">tweet</a></p>
<p><img src=""https://substackcdn.com/image/fetch/$s_!S5TW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F827f47a5-bf64-47b9-ba34-d67db26c0d16_1072x978.png"" alt=""""></p>
<p>There are a few notables here - Kimi K2.5 is “natively multimodal” for the first time, perhaps <a href=""https://x.com/thefirehacker/status/2016223118738764081"">borrowing from Kimi VL</a>, but is attributed to “massive-scale vision-text joint pre-training” including VIDEO understanding - “simply upload a screen recording” and K2.5 can reconstruct the website for you:</p>
<p><img src=""https://substackcdn.com/image/fetch/$s_!8uSV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6d342ef-ee27-4882-b32c-a2a384301a6b_1782x1954.png"" alt=""""></p>
<p>The fact that this is a <strong>continued pretrain</strong> that changes arch (+400M param <a href=""https://x.com/mervenoyann/status/1910767952376328680?s=20"">MoonViT</a> vision encoder) is VERY exciting for model training folks who rarely get to see a scaled up model do stuff like this.</p>
<p>The other 2 headline features are equally exciting: <strong>Agent Swarm</strong> (only for paid users on the Kimi app) which “<a href=""https://x.com/eliebakouch/status/2016025747144483060?s=20"">learns to self-direct an agent swarm of up to 100 sub-agents, executing parallel workflows across up to 1,500 coordinated steps, without predefined roles or hand-crafted workflows.</a>” This parallelism results in higher end result performance with up to 4.5x faster speed… ignoring token cost of course.</p>
<p><img src=""https://substackcdn.com/image/fetch/$s_!dyEZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01ea670e-c2b8-4c27-8f2b-27f7bc5d39eb_1738x1034.png"" alt=""""></p>
<p>and “Office Productivity” with <strong>K2.5 Agent</strong> focused on “high-density, large-scale office work end to end”.</p>
<p><img src=""https://substackcdn.com/image/fetch/$s_!yq2_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb56aea38-12da-46bb-8038-5fbc7143ce0a_1732x1586.png"" alt=""""></p>
<p>This is not empty regurgitation - We saw enough to sign up as a paying subscriber of the Kimi App going forward. As Artificial Analysis <a href=""https://x.com/ArtificialAnlys/status/2016250140219343163/photo/1"">notes</a>, the China-Western gap in open models just took another big leap today.</p>
<p><img src=""https://substackcdn.com/image/fetch/$s_!3HH5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8883458-b2fd-45e9-9a6c-8eeb660682ed_1487x587.jpeg"" alt=""Image""></p>
<hr>
<h1>AI Twitter Recap</h1>
<p><strong>MoonshotAI’s Kimi K2.5 ecosystem: open multimodal MoE + “Agent Swarm” push</strong></p>
<ul>
<li><strong>Kimi K2.5 model drop and positioning</strong>: Moonshot positions <strong>Kimi K2.5</strong> as a flagship <strong>open-weights</strong> model with <strong>native multimodality (image + video)</strong>, strong agentic performance, and aggressive API pricing/latency claims. Official launch media and messaging: <a href=""https://twitter.com/Kimi_Moonshot/status/2016065333694771276"">founder intro video</a>, <a href=""https://twitter.com/Kimi_Moonshot/status/2016114773407236471"">pricing/throughput claims incl. “Turbo-level speed 60–100 tok/s”</a>, plus early community reactions emphasizing “agent swarm” and multimodal capability (<a href=""https://twitter.com/kimmonismus/status/2016100119100145995"">kimmonismus</a>, <a href=""https://twitter.com/kimmonismus/status/2016120251717714273"">kimmonismus on multimodal/video</a>).</li>
<li><strong>Technical gist (as surfaced by the community)</strong>: A useful unpacking of K2.5’s reported ingredients—<strong>~15T mixed visual+text tokens</strong> continual pretraining, <strong>context 128K→256K via YaRN</strong>, release in <strong>INT4</strong> with selective quantization (only routed experts quantized), and the “<strong>Agent Swarm</strong>” orchestration concept (dynamic generation of subagents; up to <strong>100 parallel subagents</strong> / <strong>1,500 steps</strong>; wall-time improvements claimed <strong>3–4.5×</strong>) is summarized by <a href=""https://twitter.com/TheZachMueller/status/2016183468430860587"">@TheZachMueller</a> (and points to the <a href=""https://twitter.com/TheZachMueller/status/2016183781481132443"">technical report</a>).</li>
<li><strong>Benchmarks/third-party eval framing</strong>: Artificial Analysis positions K2.5 as “leading open weights” and closer to frontier labs, highlighting <strong>GDPval-AA Elo 1309</strong> (agentic knowledge work harness), <strong>MMMU Pro 75%</strong>, <strong>INT4 ~595GB</strong>, and a <strong>64% hallucination rate</strong> (improved vs K2 Thinking) among other stats: <a href=""https://twitter.com/ArtificialAnlys/status/2016250137115557953"">@ArtificialAnlys</a>. LMArena announcements also place K2.5 Thinking at <strong>#1 open model</strong> in their Text Arena snapshot: <a href=""https://twitter.com/arena/status/2016294722445443470"">@arena</a>. (Treat leaderboards as <em>point-in-time</em>; harness/tooling and prompting matter.)</li>
<li><strong>Distribution and “runs at home” signals</strong>: K2.5 landed quickly across infra surfaces: <strong>Ollama cloud</strong> with launch integrations (<a href=""https://twitter.com/ollama/status/2016086374005538932"">@ollama</a>), Together AI listing (<a href=""https://twitter.com/togethercompute/status/2016306907015938510"">@togethercompute</a>), and Fireworks as a partner (<a href=""https://twitter.com/Kimi_Moonshot/status/2016057073000448234"">Moonshot</a>). A notable local-inference datapoint: K2.5 reportedly runs (slowly but “usable”) on <strong>2× M3 Ultra</strong> via MLX with sharded generation, ~<strong>21.9 tok/s</strong> at high memory use: <a href=""https://twitter.com/awnihannun/status/2016221496084205965"">@awnihannun</a> (+ command snippet <a href=""https://twitter.com/awnihannun/status/2016223103081443342"">here</a>).</li>
<li><strong>Product surface area around Kimi</strong>: Moonshot also pushed adjacent tooling: <strong>Kimi Code</strong>, an <strong>Apache-2.0</strong> open-source coding agent integrating with common IDEs/editors (<a href=""https://twitter.com/Kimi_Moonshot/status/2016034259350520226"">announcement</a>), and an <strong>Agent SDK</strong> to build custom agents (<a href=""https://twitter.com/Kimi_Moonshot/status/2016034272998809678"">link</a>). A “Kimi Product” account is explicitly aimed at distributing prompts/use-cases (<a href=""https://twitter.com/Kimi_Moonshot/status/2016082808834531825"">launch</a>), with a viral demo of “<strong>video-to-code</strong>” website cloning (<a href=""https://twitter.com/KimiProduct/status/2016081756206846255"">demo</a>).</li>
</ul>
<p><strong>Open “American comeback” at scale: Arcee/Prime Intellect Trinity Large Preview (400B MoE)</strong></p>
<ul>
<li><strong>Trinity Large Preview release</strong>: Arcee dropped <strong>Trinity Large</strong> initial weights as a “preview” release: <a href=""https://twitter.com/arcee_ai/status/2016278017572495505"">@arcee_ai</a>, with expanded details from <a href=""https://twitter.com/latkins/status/2016279374287536613"">@latkins</a>. Prime Intellect frames it as an open <strong>400B MoE</strong> with <strong>13B active</strong> trained with Datology data: <a href=""https://twitter.com/PrimeIntellect/status/2016280792037785624"">@PrimeIntellect</a>. OpenRouter offered limited-time free access: <a href=""https://twitter.com/OpenRouterAI/status/2016280059527757995"">@OpenRouterAI</a>.</li>
<li><strong>Architecture/training details (most concrete technical tweet)</strong>: A strong technical snapshot comes from <a href=""https://twitter.com/samsja19/status/2016283855888773277"">@samsja19</a>: <strong>400B/A13B MoE</strong>, trained over <strong>17T tokens</strong>; <strong>3:1 interleaved local/global gated attention</strong>, <strong>SWA</strong>, <strong>NoPE on global layers + RoPE on local layers</strong> (as written in tweet), <strong>depth-scaled sandwich norm</strong>, <strong>sigmoid routing</strong>, trained with <strong>Muon</strong>; trained on <strong>~2,000 B300s for a month</strong> on Prime Intellect infra, with data curation by DatologyAI.</li>
<li><strong>Data scaling emphasis</strong>: Datology’s involvement is highlighted as a major part of the project: “<strong>6.5T tokens overall</strong>” and “<strong>800B synthetic code</strong>” (plus multilingual curation) in one team member’s recap: <a href=""https://twitter.com/code_star/status/2016279734985097532"">@code_star</a>. Separate recaps mention <strong>8T synthetic</strong> as part of 17T: <a href=""https://twitter.com/pratyushmaini/status/2016287361274138821"">@pratyushmaini</a>.</li>
<li><strong>Ecosystem readiness</strong>: vLLM announced <strong>day-0 support</strong> for serving Trinity Large: <a href=""https://twitter.com/vllm_project/status/2016322567364346331"">@vllm_project</a>. The meta-story in the replies is that a Western org is again attempting <strong>frontier-ish pretraining from scratch</strong> with an open model, rather than only post-training/evals.</li>
</ul>
<p><strong>Agents everywhere: orchestration, subagents, planning critics, and IDE/CLI integration</strong></p>
<ul>
<li><strong>Agent “swarm” vs “subagents” convergence</strong>: Kimi’s “Agent Swarm” pitch (dynamic subagent creation) parallels the broader pattern of <em>central orchestrator + parallel specialists</em>. The most explicit “starter pattern” articulation is LangChain’s stateless subagent model (parallel execution + minimized context bloat): <a href=""https://twitter.com/sydneyrunkle/status/2016285836581765461"">@sydneyrunkle</a>. Meanwhile, Kimi’s swarm is framed as trainable orchestration via <strong>Parallel-Agent RL (PARL)</strong> in community summaries (<a href=""https://twitter.com/TheZachMueller/status/2016183468430860587"">Zach Mueller</a>).</li>
<li><strong>Reliability via “critique before execute”</strong>: Google’s Jules introduced a <strong>Planning Critic</strong>—a second agent that critiques plans pre-execution, claiming a <strong>9.5% drop in task failure rates</strong>: <a href=""https://twitter.com/julesagent/status/2016178107019837917"">@julesagent</a>. Jules also added “Suggested Tasks” for proactive optimizations: <a href=""https://twitter.com/julesagent/status/2016249221846864005"">@julesagent</a>.</li>
<li><strong>Coding-agent products intensifying</strong>: Mistral shipped <strong>Vibe 2.0</strong> upgrades (subagents, user-defined agents, skills/slash commands, and paid plans): <a href=""https://twitter.com/mistralvibe/status/2016179799689928986"">@mistralvibe</a> and <a href=""https://twitter.com/qtnx_/status/2016180364771742047"">@qtnx_</a>. MiniMax launched an “Agent Desktop” workspace pitched as more polished than Claude Cowork: <a href=""https://twitter.com/omarsar0/status/2016149402923200634"">@omarsar0</a> (and MiniMax’s own onboarding automation: <a href=""https://twitter.com/MiniMax_AI/status/2016161539749990844"">@MiniMax_AI</a>).</li>
<li><strong>IDE infrastructure and retrieval</strong>: Cursor claims semantic search materially improves coding-agent performance and that indexing for large codebases is “orders of magnitude faster”: <a href=""https://twitter.com/cursor_ai/status/2016202243499073768"">@cursor_ai</a>. VS Code continues tightening agent UX (e.g., safer command execution explanations): <a href=""https://twitter.com/aerezk/status/2016225215802397146"">@aerezk</a>, plus MCP servers returning UI via MCP Apps spec (LIFX control panel example): <a href=""https://twitter.com/burkeholland/status/2016208751200457088"">@burkeholland</a>.</li>
</ul>
<p><strong>Document AI &#x26; multimodal systems: DeepSeek-OCR 2 and “Agentic Vision”</strong></p>
<ul>
<li><strong>DeepSeek-OCR 2: learned reading order + token compression</strong>: DeepSeek-OCR 2 is framed as a shift from fixed raster scans to learned <strong>Visual Causal Flow</strong> with <strong>DeepEncoder V2</strong>, including <strong>16× visual token compression (256–1120 tokens/image)</strong> and <strong>91.09% OmniDocBench v1.5 (+3.73%)</strong>; vLLM shipped day-0 support: <a href=""https://twitter.com/vllm_project/status/2016065526058090967"">@vllm_project</a>. Unsloth notes similar headline improvements: <a href=""https://twitter.com/danielhanchen/status/2016043326760485313"">@danielhanchen</a>.</li>
<li><strong>Mechanistic intuition (why it matters for pipelines)</strong>: Jerry Liu provides a clear “why learned order helps” explanation: avoid semantically shredding tables/forms by allowing query tokens to attend to contiguous regions instead of strict left-to-right: <a href=""https://twitter.com/jerryjliu0/status/2016319238974407146"">@jerryjliu0</a>. Teortaxes adds a pragmatic eval take: OCR 2 is “on par with dots.ocr” and “nowhere near SOTA,” but the ideas may influence later multimodal products: <a href=""https://twitter.com/teortaxesTex/status/2016179572056678739"">@teortaxesTex</a>.</li>
<li><strong>Gemini “Agentic Vision” = vision + code execution loop</strong>: Google is productizing a “Think, Act, Observe” loop where the model writes/executes Python to crop/zoom/annotate images, claiming <strong>5–10% quality boosts</strong> across many vision benchmarks: <a href=""https://twitter.com/_philschmid/status/2016225242394296773"">@_philschmid</a> and the official thread: <a href=""https://twitter.com/GoogleAI/status/2016267526330601720"">@GoogleAI</a>. This is an explicit move toward <em>tool-augmented vision</em> being first-class, not bolted on.</li>
</ul>
<p><strong>AI for science &#x26; research workflows: OpenAI Prism as “Overleaf with AI”</strong></p>
<ul>
<li><strong>Prism launch</strong>: OpenAI introduced <strong>Prism</strong>, a free “AI-native workspace for scientists” powered by <strong>GPT-5.2</strong>, positioned as a unified LaTeX collaboration environment: <a href=""https://twitter.com/OpenAI/status/2016209462621831448"">@OpenAI</a> and <a href=""https://twitter.com/kevinweil/status/2016210486778642808"">@kevinweil</a>. Community summaries frame it as “Overleaf with AI” (proofreading, citations, literature search): <a href=""https://twitter.com/scaling01/status/2016211218633990391"">@scaling01</a>.</li>
<li><strong>Data/IP clarification</strong>: Kevin Weil clarified that Prism follows your ChatGPT data controls and that OpenAI is not taking a share of individual discoveries; any IP-alignment deals would be bespoke for large orgs: <a href=""https://twitter.com/kevinweil/status/2016285175106420867"">@kevinweil</a>.</li>
<li><strong>Why it matters technically</strong>: Prism is a product bet that <strong>collaboration context + tool integration</strong> (LaTeX, citations, project state) becomes a durable advantage—mirroring the “context > intelligence” theme circulating in Chinese discussions about OpenAI infra and org design: <a href=""https://twitter.com/ZhihuFrontier/status/2016068402457285032"">@ZhihuFrontier</a>.</li>
</ul>
<p><strong>Research notes &#x26; benchmarks worth tracking (RL, planning, multilingual scaling)</strong></p>
<ul>
<li><strong>Long-horizon planning benchmark</strong>: <em>DeepPlanning</em> proposes verifiable-constraint planning tasks (multi-day travel, shopping) and reports frontier agents still struggle; emphasizes explicit reasoning patterns and parallel tool use: <a href=""https://twitter.com/iScienceLuvr/status/2016122154862182792"">@iScienceLuvr</a>. (This pairs nicely with the “travel planning again” meme: <a href=""https://twitter.com/teortaxesTex/status/2016043107607879864"">@teortaxesTex</a>.)</li>
<li><strong>RL efficiency and reuse of traces</strong>: <em>PrefixRL</em> idea—condition on off-policy prefixes to speed RL on hard reasoning, claiming <strong>2× faster</strong> to same reward vs strong baseline: <a href=""https://twitter.com/iScienceLuvr/status/2016125085825040852"">@iScienceLuvr</a>.</li>
<li><strong>Multilingual scaling laws</strong>: Google Research announced <strong>ATLAS</strong> scaling laws for massively multilingual LMs with data-driven guidance on balancing data mix vs model size: <a href=""https://twitter.com/GoogleResearch/status/2016234343602258274"">@GoogleResearch</a>.</li>
<li><strong>Math research reality check</strong>: Epoch’s <strong>FrontierMath: Open Problems</strong> benchmark invites attempts; “AI hasn’t solved any of these yet”: <a href=""https://twitter.com/EpochAIResearch/status/2016188014540816879"">@EpochAIResearch</a>.</li>
</ul>
<hr>
<h3>Top tweets (by engagement)</h3>
<ul>
<li>OpenAI launches <strong>Prism</strong> (AI LaTeX research workspace): <a href=""https://twitter.com/OpenAI/status/2016209462621831448"">@OpenAI</a></li>
<li>Moonshot founder video introducing <strong>Kimi K2.5</strong>: <a href=""https://twitter.com/Kimi_Moonshot/status/2016065333694771276"">@Kimi_Moonshot</a></li>
<li>Kimi “video-to-code” website cloning demo: <a href=""https://twitter.com/KimiProduct/status/2016081756206846255"">@KimiProduct</a></li>
<li>Ollama: <strong>Kimi K2.5 on Ollama cloud</strong> + integrations: <a href=""https://twitter.com/ollama/status/2016086374005538932"">@ollama</a></li>
<li>Claude generating <strong>3Blue1Brown-style animations</strong> claim (education impact): <a href=""https://twitter.com/LiorOnAI/status/2016119374097084828"">@LiorOnAI</a></li>
<li>Figure introduces <strong>Helix 02</strong> autonomous whole-body robotics control: <a href=""https://twitter.com/Figure_robot/status/2016207013236375661"">@Figure_robot</a></li>
</ul>
<hr>
<h1>AI Reddit Recap</h1>
<h2>/r/LocalLlama + /r/localLLM Recap</h2>
<h3>1. New Model and Benchmark Releases</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qo595n/introducing_kimi_k25_opensource_visual_agentic/"">Introducing Kimi K2.5, Open-Source Visual Agentic Intelligence</a></strong> (Activity: 643): <strong><strong>Kimi K2.5</strong> is an open-source visual agentic intelligence model that achieves global state-of-the-art (SOTA) performance on agentic benchmarks, with scores of <code>50.2%</code> on the HLE full set and <code>74.9%</code> on BrowseComp. It also leads in open-source vision and coding benchmarks, scoring <code>78.5%</code> on MMMU Pro, <code>86.6%</code> on VideoMMMU, and <code>76.8%</code> on SWE-bench Verified. The model introduces an <strong>Agent Swarm</strong> feature in beta, allowing up to <code>100</code> sub-agents to work in parallel, making <code>1,500</code> tool calls and operating <code>4.5×</code> faster than a single-agent setup. Kimi K2.5 is available in chat and agent modes on <a href=""https://kimi.com"">kimi.com</a>, with additional resources on <a href=""https://huggingface.co/moonshotai/Kimi-K2.5"">Hugging Face</a>.</strong> A comment highlights the impressive capability of <code>100</code> sub-agents working in parallel, suggesting potential for enhanced performance in coding tasks. Another comment notes the banning of the original poster, raising questions about account authenticity.</p>
<ul>
<li><strong>Asleep_Strike746</strong> highlights the impressive capability of Kimi K2.5 to run 100 sub-agents in parallel, suggesting potential for complex task execution, such as coding tasks. This parallelism could significantly enhance performance in multi-threaded environments, making it a powerful tool for developers looking to automate or optimize workflows.</li>
<li><strong>illusoryMechanist</strong> points out the scale of Kimi K2.5 with '1T Activated Parameters' and '32B' (likely referring to the model's parameter count), indicating a substantial computational capacity. This suggests that Kimi K2.5 could handle large-scale data processing and complex problem-solving tasks, positioning it as a competitive player in the open-source AI landscape.</li>
<li><strong>Capaj</strong> shares a practical test of Kimi K2.5 by prompting it to generate an SVG of a fox on a unicycle. The result was described as 'not too bad', implying that while the model can handle creative tasks, there might still be room for improvement in terms of output quality or creativity. This kind of testing is crucial for understanding the model's capabilities in real-world applications.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qo3ri5/jan_v3_instruct_a_4b_coding_model_with_40_aider/"">Jan v3 Instruct: a 4B coding Model with +40% Aider Improvement</a></strong> (Activity: 333): <strong>The image is a bar chart titled ""Aider Benchmark"" that illustrates the performance of various coding models in terms of their pass rate for polyglot code editing. The ""Jan-v3-4B-base-INSTRUCT"" model leads with a score of <code>18</code>, significantly outperforming other models like ""Qwen3-4B-THINKING-2507"" with <code>12.1</code> and ""Ministral-3-8B-INSTRUCT-2512"" with <code>6.8</code>. This highlights the Jan-v3 model's high efficiency and over <code>40%</code> improvement in performance, showcasing its enhanced capabilities in coding tasks. The model is designed for improved math and coding performance, making it a strong candidate for lightweight assistance and further fine-tuning.</strong> One commenter appreciates the Qwen 4B 2507 model for small tasks, noting its impressive performance despite its size. Another user shares mixed experiences with the Jan model, praising its ability to use search tools effectively but noting occasional tool call failures and odd responses, possibly due to system prompts.</p>
<ul>
<li>The Jan v3 Instruct model, a 4 billion parameter coding model, reportedly achieves a 40% improvement in performance with the Aider benchmark. This suggests significant advancements in its ability to handle coding tasks, potentially outperforming other models like Qwen 4B 2507 in specific scenarios. The model's ability to utilize search tools effectively for code explanation is noted, although there are occasional failures in tool calls and some system prompt issues in web chat applications.</li>
<li>A user reported mixed experiences with the Jan v3 model on <a href=""http://chat.jan.ai"">chat.jan.ai</a>, highlighting its capability to correctly use search tools and read code for explaining project flows. However, they also noted some tool call failures and irrelevant responses, possibly due to system prompts. The user expressed interest in the model's potential integration with Claude Code, suggesting it could become a valuable tool for code search and Q&#x26;A in daily coding tasks.</li>
<li>The Jan v3 model's performance in benchmarks is highlighted, with a specific mention of its demo availability at <a href=""https://chat.jan.ai"">chat.jan.ai</a>. The model's ability to handle small and easy tasks effectively is compared to Qwen 4B 2507, which is favored for similar tasks. The discussion suggests that Jan v3's fine-tuning may offer competitive advantages in certain coding scenarios.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qo349m/deepseekaideepseekocr2_hugging_face/"">deepseek-ai/DeepSeek-OCR-2 · Hugging Face</a></strong> (Activity: 385): <strong><strong>DeepSeek-OCR-2</strong> is a state-of-the-art OCR model available on <a href=""https://huggingface.co/deepseek-ai/DeepSeek-OCR-2"">Hugging Face</a>, optimized for document processing with visual causal flow. It requires <code>Python 3.12.9</code> and <code>CUDA 11.8</code>, and leverages libraries like <code>torch</code> and <code>transformers</code>. The model supports dynamic resolution and uses flash attention for enhanced performance on NVIDIA GPUs. It offers various prompts for document conversion, making it versatile for different OCR tasks.</strong> One user highlighted the impressive performance of <strong>PaddleOCR-VL</strong> when compared using scores from other models, suggesting its potential superiority. Another user shared a demo of DeepSeek OCR 2, noting initial issues with repetition due to user error, which were resolved by adjusting decoding parameters, leading to significantly improved performance over version 1.</p>
<ul>
<li>A user highlighted the impressive performance of PaddleOCR-VL, suggesting it stands out when compared to other models like B/C/D. This is based on scores reported by a third party, which the user trusts for evaluating model performance. This implies PaddleOCR-VL's metrics are noteworthy in the context of OCR model comparisons.</li>
<li>Another user shared their experience with implementing a demo for DeepSeek OCR 2 using GPU credits. Initially, they faced issues with repetition due to incorrect parameters, but after adjusting to DeepSeek's recommended decoding parameters, the performance improved significantly. The user noted that the updated version is much more reliable than its predecessor, DeepSeek OCR v1.</li>
<li>The GitHub repository and paper for DeepSeek OCR 2 were shared, providing resources for those interested in the technical details and implementation of the model. The paper likely contains in-depth information on the model's architecture, training process, and performance benchmarks, which are crucial for technical evaluation and understanding.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qnk7fq/transformers_v5_final_is_out/"">transformers v5 final is out 🔥</a></strong> (Activity: 503): <strong><strong>Transformers v5</strong> from <strong>Hugging Face</strong> introduces significant performance improvements, particularly for Mixture-of-Experts (MoE) models, achieving <code>6x-11x</code> speedups. The update simplifies the API by removing slow/fast tokenizers, offering explicit backends and enhanced performance. Additionally, dynamic weight loading is now faster, supporting MoE with quantization, tensor parallelism, and Parameter-Efficient Fine-Tuning (PEFT). A <a href=""https://github.com/huggingface/transformers"">migration guide</a> and detailed release notes are available for users transitioning to this version.</strong> One user inquired about the implications of these improvements for running small to medium-sized MoE models locally, suggesting that the enhancements might reduce memory bandwidth constraints. Another user reported a <code>50%</code> increase in single prompt inference speed and a <code>100%</code> increase in concurrent inference speed after updating to v5 and vllm 0.14.1.</p>
<ul>
<li>The Mixture-of-Experts (MoE) model in Transformers v5 shows significant performance improvements, with reported speedups ranging from 6x to 11x. This is particularly relevant for users running models locally, as it suggests that MoE can now utilize compute resources more efficiently, potentially reducing memory bandwidth constraints. This could be beneficial for setups using NVIDIA GPUs or AMD iGPUs, such as the Strix Halo, where compute power is a limiting factor.</li>
<li>A user reported upgrading to Transformers v5 and vllm 0.14.1 from 0.11, resulting in a 50% increase in single prompt inference speed and a 100% increase in concurrent inference speed for 40x workloads. This highlights the significant performance enhancements in the latest version, which could be crucial for applications requiring high throughput and low latency.</li>
<li>The update in Transformers v5 now allows Mixture-of-Experts (MoE) models to work with quantized models, which was not possible before. This advancement enables more efficient model deployment by reducing the model size and computational requirements, making it feasible to run complex models on less powerful hardware without sacrificing performance.</li>
</ul>
</li>
</ul>
<h3>2. Local LLM Hardware and Setup Discussions</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qni356/216gb_vram_on_the_bench_time_to_see_which/"">216GB VRAM on the bench. Time to see which combination is best for Local LLM</a></strong> (Activity: 577): <strong>The post discusses the use of secondhand Tesla GPUs, which offer substantial VRAM at a lower cost, for local large language model (LLM) testing. The author has developed a <a href=""https://esologic.com/gpu-server-benchmark/#gpu-box-benchmark"">GPU server benchmarking suite</a> to evaluate the performance of these older GPUs when used in parallel. The image shows a setup with multiple NVIDIA GPUs, highlighting the focus on maximizing VRAM for machine learning tasks. The technical challenge lies in effectively utilizing these GPUs without significant bandwidth loss, as most affordable server motherboards support only a limited number of GPUs.</strong> Commenters express skepticism about the practicality of using older Tesla GPUs due to potential issues with token processing speed and cooling requirements. There is interest in how the author manages to connect multiple GPUs without bandwidth loss, and a suggestion that newer systems like DGX Spark might offer better performance for certain tasks.</p>
<ul>
<li>HugoCortell raises a technical concern about the bandwidth limitations when connecting multiple GPUs to a single PC, noting that most affordable server motherboards support only a few GPUs. This could lead to a significant loss in bandwidth, which is crucial for efficient parallel processing in local LLM setups.</li>
<li>BananaPeaches3 highlights a critical performance issue with older GPUs, particularly in handling large system prompts. They mention that while token generation speed might be acceptable, the prompt processing time can be a bottleneck, especially with prompts as large as 15k tokens. This suggests that newer systems like the DGX Spark might be more efficient despite slightly slower token generation speeds, due to faster prompt processing capabilities.</li>
<li>FullOf_Bad_Ideas points out a limitation in the gpu_box_benchmark, which does not test for serving large models split across multiple GPUs. This is a significant use case for setups with high VRAM, indicating a gap in the benchmark's ability to evaluate real-world performance for large-scale LLM applications.</li>
</ul>
</li>
</ul>
<h3>3. Teasers and Announcements from AI Labs</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qoa8rp/the_qwen_devs_are_teasing_something/"">The Qwen Devs Are Teasing Something</a></strong> (Activity: 331): <strong>The image is a tweet from <strong>Tongyi Lab</strong> featuring an ASCII art face and a lightning bolt emoji, hinting at an upcoming announcement. The Reddit community speculates that this could be related to a new visual language model, possibly named <strong>Z-Image</strong>, which has been mentioned in recent <strong>ComfyUI</strong> pull requests. The timing of the announcement might be strategically planned before the Chinese New Year, aligning with other anticipated releases like <strong>K2.5</strong> and potentially <strong>q3.5</strong>, <strong>dsv4</strong>, and <strong>mm2.2</strong>.</strong> Commenters are speculating that the announcement is related to the Z-Image model, which has been referenced in recent updates to ComfyUI. There is also a discussion about the timing of the release, suggesting it might be aligned with the Chinese New Year.</p>
<ul>
<li>The mention of 'Z-Image' in ComfyUI PRs suggests a potential new feature or model update related to image processing. This aligns with recent updates where hidden items have been added to collections, indicating ongoing development and testing phases.</li>
<li>There is speculation about the release of several models and updates before the Chinese New Year, including K2.5, q3.5, dsv4, and mm2.2. This timing is strategic as many labs aim to release updates before the holiday break, which is on January 17th this year.</li>
<li>A user speculates about the release of 'Qwen4 Next 48B A3B', which could imply a new model or version with specific parameters, possibly indicating advancements in model architecture or capabilities.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qnfegx/minimax_is_teasing_m22/"">Minimax Is Teasing M2.2</a></strong> (Activity: 322): <strong>The image is a tweet from <strong>MiniMax</strong> teasing an update to their AI model, M2.2, suggesting an imminent release with the phrase ""M2.1 slays. M2.2 levels up. #soon."" This indicates a potential upgrade in capabilities or performance from the previous version, M2.1. The context suggests a competitive landscape in AI development, particularly among Chinese labs, with other models like Deepseek v4 and Kimi K3 also expected soon. The mention of ByteDance's potential closed-source model adds to the competitive tension in the AI space.</strong> One comment suggests a shift in focus towards agentic Mixture of Experts (MoEs) models, potentially at the expense of updates to traditional 32B models. Another user expresses anticipation for the new model, highlighting the effectiveness of MiniMax 2.1 in combination with glm 4.7 for coding tasks, and the potential impact of the upcoming versions.</p>
<ul>
<li>Loskas2025 highlights the use of Minimax 2.1 and GLM 4.7 for coding, noting their excellence. They anticipate that the upcoming Minimax 2.2 and GLM 5, which is currently in training, could significantly enhance performance, suggesting a potential shift in the landscape of coding models.</li>
<li>CriticallyCarmelized compares Minimax favorably to GLM 4.7, even at high quantization levels, indicating that Minimax is competitive in terms of performance. They express optimism that the new version could surpass current models, potentially becoming their preferred choice for local deployment.</li>
<li>lacerating_aura mentions speculation around 'giga-potato' being associated with DS4, but points out the lack of concrete evidence for the existence of DS4 or Kimi K3, indicating a gap in confirmed information about these models.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qnjota/i_built_a_hive_mind_for_claude_code_7_agents/"">I built a ""hive mind"" for Claude Code - 7 agents sharing memory and talking to each other</a></strong> (Activity: 422): <strong>The post describes a multi-agent orchestration system for <strong>Claude Code</strong>, featuring 7 specialized agents (e.g., coder, tester, reviewer) that coordinate tasks, share persistent memory using <code>SQLite + FTS5</code>, and communicate via a message bus. The system runs as an MCP server and integrates with <strong>Anthropic</strong>, <strong>OpenAI</strong>, or <strong>Ollama</strong>. It uses a task queue for priority-based coordination, allowing agents to pass context and collaborate effectively. The stack includes <strong>TypeScript</strong>, <strong>better-sqlite3</strong>, <strong>MCP SDK</strong>, and <strong>Zod</strong>. The project is experimental, MIT licensed, and available on <a href=""http://github.com/blackms/aistack"">GitHub</a>.</strong> A comment questions the similarity to the <a href=""https://github.com/bmad-code-org/BMAD-METHOD"">bmad method</a>, suggesting potential overlap in approach. Another comment humorously questions whether the agents agree with each other, hinting at the complexity of multi-agent consensus.</p>
<ul>
<li>The project is compared to the <a href=""https://github.com/bmad-code-org/BMAD-METHOD"">BMAD method</a>, which also involves multi-agent systems. The commenter is curious about the differences, suggesting that the approach might be similar in terms of agents sharing memory and communication protocols.</li>
<li>A reference is made to Microsoft's <a href=""https://github.com/microsoft/autogen"">Autogen</a>, which was released over two years ago as a solution for multi-agent systems. The commenter suggests exploring this resource for potential new ideas, indicating that the concept of multi-agent communication and shared memory is not new and has been explored by major tech companies.</li>
<li>The choice of using Claude Code is questioned, with a suggestion to consider open-source alternatives. This implies a debate on the benefits of proprietary versus open-source platforms for developing multi-agent systems, hinting at potential advantages in community support and collaboration in open-source projects.</li>
</ul>
</li>
</ul>
<h2>Less Technical AI Subreddit Recap</h2>
<blockquote>
<p>/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo</p>
</blockquote>
<h3>1. Kimi K2.5 and Open Source AI Model Releases</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/singularity/comments/1qoojio/open_source_kimik25_is_now_beating_claude_opus_45/"">Open source Kimi-K2.5 is now beating Claude Opus 4.5 in many benchmarks including coding.</a></strong> (Activity: 597): <strong><strong>Kimi-K2.5</strong>, an open-source model, is reportedly outperforming <strong>Claude Opus 4.5</strong> in several benchmarks, notably in coding tasks. However, the specifics of these benchmarks and the extent of the performance gains are not detailed in the post. The claim suggests a significant advancement in open-source AI capabilities, but lacks comprehensive data to substantiate the comparison.</strong> Commenters express skepticism about the claim, highlighting that benchmarks may not fully represent real-world performance. They question the validity of the term 'many' benchmarks and suggest that the practical utility of Kimi-K2.5 compared to Claude Opus 4.5 remains unproven.</p>
<ul>
<li>There is skepticism about the claim that Kimi-K2.5 is outperforming Claude Opus 4.5 in real-world applications, despite benchmark results. Users argue that benchmarks often don't reflect practical utility, especially in complex tasks like programming where Opus 4.5 might excel in providing solutions in a single prompt.</li>
<li>The discussion highlights a common critique of benchmarks: they may not capture the full capabilities of a model in practical scenarios. Some users express doubt about the claim that Kimi-K2.5 surpasses Opus 4.5, questioning the specific benchmarks and real-world applicability, especially in coding tasks where Opus 4.5 is perceived to have an edge.</li>
<li>One user claims significant practical success with Kimi-K2.5, stating it has replaced reports in a major company, suggesting that at least in some contexts, Kimi-K2.5 may offer substantial utility. This contrasts with the general skepticism about benchmarks translating to real-world performance.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/singularity/comments/1qo531i/kimi_k25_released/"">Kimi K2.5 Released!!!</a></strong> (Activity: 1149): <em><em>The image presents a performance comparison chart for the newly released <strong>Kimi K2.5</strong>, which is claimed to set a new state-of-the-art (SOTA) in agentic tasks. The chart compares Kimi K2.5 against other models like <strong>GPT-5.2 (xhigh)</strong>, <strong>Claude Opus 4.5</strong>, and <strong>Gemini 3 Pro</strong> across various tasks, including agents, coding, image, and video tasks. Kimi K2.5 is highlighted as leading in several categories, notably ""Agents: BrowseComp"" and ""Image: OmniDocBench 1.5</em>"", suggesting its superior performance in these areas. The release is accompanied by a blog post detailing the advancements (<a href=""https://www.kimi.com/blog/kimi-k2-5.html"">link</a>).</em>* Commenters express skepticism about the benchmarks, questioning if they are cherry-picked, and discuss the model's performance in hallucination and instruction-following tests. One user notes that Kimi K2.5, while improved, still outputs incorrect answers confidently, similar to other models like Gemini 3, which also confidently provides incorrect answers. GPT-5.1 and 5.2 are noted for admitting ""I don't know"" in similar tests, highlighting ongoing challenges with hallucinations in AI models.</p>
<ul>
<li>A user conducted a test on Kimi K2.5's ability to follow instructions by asking it to identify a specific math contest problem without web search. The model listed out hallucinated contest problems and second-guessed itself, ultimately providing an incorrect answer. This is seen as a slight improvement over Kimi K2, which failed to follow instructions and timed out. In comparison, Gemini 3 also confidently provided incorrect answers, while GPT 5.1 and 5.2 were the only models to admit 'I don't know'.</li>
<li>The concept of an 'agent swarm' in Kimi K2.5 is intriguing, with speculation that it involves over 100 instances of the model being directed by a single overseeing instance. This setup is expected to be expensive, and there is curiosity about whether it could be a single model handling multiple tasks simultaneously, which would represent a significant advancement. The idea of scaffolding, where multiple models work together, seems more plausible to some users.</li>
<li>There is skepticism about the benchmarks used to compare Kimi K2.5 with other models like Gemini 3. A user questions whether the benchmarks are cherry-picked, expressing doubt that Kimi K2.5 consistently outperforms Gemini 3, which seems unlikely given the current state of model capabilities.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/ClaudeAI/comments/1qod7ej/sir_the_chinese_just_dropped_a_new_open_model/"">Sir, the Chinese just dropped a new open model</a></strong> (Activity: 1915): <strong><strong>Kimi</strong> has released an open-source trillion-parameter vision model that reportedly matches the performance of <strong>Opus 4.5</strong> on several benchmarks. This model is significant due to its scale and the claim of competitive performance, which is notable given the typically high cost and complexity associated with such large models. The release could impact the landscape of AI vision models, especially in terms of accessibility and cost-effectiveness.</strong> There is skepticism in the community about the true performance of Chinese models, with some users suggesting that while they are cost-effective, they may not genuinely match the capabilities of models like <strong>Claude</strong>, <strong>GPT</strong>, or <strong>Gemini</strong> despite benchmark claims.</p>
<ul>
<li>Tricky-Elderberry298 highlights the limitations of relying solely on benchmarks for evaluating LLMs, drawing an analogy to evaluating cars based only on engine specs. They argue that real-world usage, such as how models like Claude and Kimi K2.5 perform in complex projects, is a more meaningful measure of capability than pure benchmark scores.</li>
<li>Durable-racoon discusses the unique capabilities of the Kimi K2.5 model, noting its ability to orchestrate 500 agents simultaneously and convert videos into working software UI prototypes. They also mention its superior performance in creative writing compared to Opus, while acknowledging that Kimi K2.5 is more expensive than most Chinese models, priced at $0.60/$3 for in/out operations.</li>
<li>DistinctWay9169 points out that many Chinese models, such as Minimax and GLM, are often 'bench maxed,' meaning they perform well on benchmarks but may not match the real-world performance of models like Claude, GPT, or Gemini. This suggests a discrepancy between benchmark results and actual usability or effectiveness in practical applications.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/Bard/comments/1qokrpq/gemini_3_finally_has_an_opensource_competitor/"">Gemini 3 finally has an open-source competitor</a></strong> (Activity: 168): <strong>The image is a comparison chart that highlights the performance of the newly released <strong>Kimi K2.5</strong> vision model against other prominent models like <strong>Gemini 3 Pro</strong>. According to the chart, Kimi K2.5 performs competitively, often surpassing Gemini 3 Pro in various benchmarks such as ""Humanity's Last Exam,"" ""BrowseComp,"" and ""OmniDocBench 1.5."" This positions Kimi K2.5 as a strong open-source alternative to the closed-source Gemini 3 Pro, challenging its dominance in the field.</strong> Some users express skepticism about Kimi K2.5's real-world performance compared to Gemini 3 Pro, with comments suggesting that while the benchmarks are impressive, practical performance may not match up. There is also a sentiment that open-source models may struggle to compete with large, closed-source companies.</p>
<ul>
<li>MichelleeeC highlights a significant performance gap between the open-source competitor and Gemini 3, particularly when tested on niche topics without search engine assistance. This suggests that the open-source model may lack the comprehensive training data or fine-tuning that Gemini 3 benefits from, impacting its ability to provide accurate answers in specialized areas.</li>
<li>Old_Technology3399 and Just_Lingonberry_352 both express that the open-source competitor is notably inferior to Gemini 3. This consensus indicates that while the open-source model may be a step towards democratizing AI, it still falls short in terms of performance and reliability compared to established, closed-source models like Gemini 3.</li>
<li>ChezMere's comment about 'benchhacking' suggests skepticism about the open-source model's real-world performance versus its benchmark results. This implies that while the model might perform well in controlled tests, it may not translate to effective real-world application, highlighting a common issue in AI model evaluation.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/DeepSeek/comments/1qne7vt/enterpriseready_open_sourcechinese_ais_are_poised/"">Enterprise-ready open source/Chinese AIs are poised to out-sell American proprietary models. Personal investors take note.</a></strong> (Activity: 30): <strong>The post highlights the competitive edge of open-source and Chinese AI models over American proprietary models in niche domains, emphasizing their cost-effectiveness and comparable performance. Notable models include <strong>DeepSeek-V3 / R1</strong>, which ranks #1 on MATH-500 and LiveCodeBench, and <strong>Qwen3-Max / Coder</strong> from Alibaba, which excels in LMSYS Chatbot Arena and MMLU-Pro. These models offer significantly lower costs per million tokens compared to proprietary models like OpenAI's GPT-5.2 and Claude 4.5 Sonnet, with input costs as low as <code>$0.15</code> to <code>$0.60</code> per million tokens, compared to proprietary costs starting at <code>$3.00</code>. The post suggests that personal investors should consider these developments as Chinese firms issue IPOs, with <strong>a16z</strong> noting that 80% of startups pitching them use Chinese open-source AI models.</strong> A comment questions whether Kimi K2 is superior to GLM 4.7, indicating a debate on the relative performance of these models in specific contexts.</p>
<ul>
<li>The discussion compares the performance of the Kimi K2 model with the GLM 4.7 model. Kimi K2 is noted for its efficiency in specific tasks, potentially outperforming GLM 4.7 in certain benchmarks. However, the choice between these models may depend on the specific use case, as GLM 4.7 might excel in different areas. The conversation highlights the importance of evaluating models based on task-specific performance metrics rather than general claims of superiority.</li>
</ul>
</li>
</ul>
<h3>2. Gemini AI Studio and Usage Limitations</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/Bard/comments/1qnsqzl/gemini_ai_studio_is_basically_unusable_now_any/"">Gemini AI Studio is basically unusable now. Any other LLMs with a 1M context window?</a></strong> (Activity: 162): <strong><strong>Gemini AI Studio</strong> has become less viable for users due to Google's reduction in daily prompt limits, impacting workflows that rely on its <code>1 million token</code> context window. Users working with extensive documents and conversations are seeking alternatives. Notably, <strong>Grok 4.1</strong> offers a <code>2 million token</code> context window, and <strong>Claude Sonnet 4.5</strong> provides a <code>1 million token</code> context window within the Kilo Code environment. These alternatives may serve users needing large-context capabilities.</strong> Some users suggest that effective CLI tools like Claudie-cli or codex-cli can mitigate the need for massive context windows by efficiently managing and retrieving information from large texts.</p>
<ul>
<li>Coldshalamov mentions that <strong>Grok 4.1 fast</strong> offers a <code>2M</code> context window, which is double the size of the <code>1M</code> context window being discussed. This suggests that Grok 4.1 fast could be a viable alternative for those needing larger context windows.</li>
<li>Unlucky_Quote6394 highlights that <strong>Claude Sonnet 4.5</strong> provides a <code>1M</code> context window when used within <strong>Kilo Code</strong>, indicating another option for users seeking large context capabilities.</li>
<li>Ryanmonroe82 suggests embedding documents as an alternative to using cloud models, implying that this method could be more efficient and effective for handling large text data without relying on extensive context windows.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/GeminiAI/comments/1qnvbjr/32768_or_215_tokens_in_hot_memory_gemini_has_been/"">32,768 or (2^15) tokens in hot memory.... Gemini has been PURPOSELY THROTTLED by Alphabet and been made into a bait and switch. Gemini Pro is WORSE than the free version as of TODAY. They market over a million tokens for Pro users. This is fraud.</a></strong> (Activity: 858): <strong>The Reddit post claims that <strong>Alphabet</strong> has intentionally throttled the token limit for <strong>Gemini Pro</strong> to <code>32,768 tokens</code>, which is significantly lower than the advertised capacity of over a million tokens. This throttling allegedly degrades the performance of Gemini Pro, making it less effective than the free version. The post also mentions that the Ultra and Enterprise versions have a hard cap of <code>131,072 tokens</code>, despite advertising up to 2 million tokens. The author expresses concern that this limitation could drive users away, especially with potential integration into <strong>Siri</strong>.</strong> Commenters express dissatisfaction with Gemini's performance, comparing it unfavorably to older models like GPT-3. There is also criticism of the memory management, with claims that it leads to data inaccuracies and inefficiencies.</p>
<ul>
<li>Substantial_Net9923 highlights a significant issue with Gemini's memory management, noting that the model's memory loss due to indexing is problematic. This inefficiency is particularly evident in quantitative finance trading discussions, where the model is reportedly generating inaccurate data more frequently than before, suggesting a decline in reliability.</li>
<li>klopppppppp observes a drastic decline in Gemini's performance, comparing it to older models like GPT-3. Despite this, they note that Gemini still performs exceptionally well in 'deep research mode,' indicating that the model's capabilities might be context-dependent or throttled in certain scenarios.</li>
<li>SorryDistribution604 expresses frustration with Gemini's recent performance, likening it to older models such as GPT-3. This suggests a perceived regression in the model's capabilities, which could be due to throttling or other limitations imposed on the Pro version.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/Bard/comments/1qngot0/about_the_recent_ai_studio_limit_downgrade/"">About the recent AI Studio Limit Downgrade:</a></strong> (Activity: 660): <strong>The image is a notification from the Gemini API about a reduction in free usage limits for AI Studio users, suggesting a transition to using an API key for continued access. It indicates that these limits may decrease further over time, and mentions ongoing efforts to integrate with Google AI Pro/Ultra to share limits within AI Studio. This change reflects a broader trend of tightening access to free AI resources, potentially impacting developers relying on these tools for experimentation and development.</strong> Commenters express frustration over the reduction in free usage limits, noting that Gemini's performance in following instructions has also declined. There is a sentiment that these changes are detrimental to AI Studio's utility, as users feel they are receiving less value and functionality.</p>
<ul>
<li>trashyslashers highlights a significant issue with the Gemini model's performance, noting that it is 'getting worse at listening to instructions.' This suggests a degradation in the model's ability to follow user commands, which is compounded by the reduction in daily usage limits. Users are forced to 'rewrite and regenerate' requests, indicating inefficiencies in the model's processing capabilities.</li>
<li>Decent_Ingenuity5413 raises concerns about the stability and reliability of AI Studio's service, drawing parallels to OpenAI's past issues with unexpected changes. The comment also points out a critical billing issue with the Gemini API, where users have experienced 'massive overbilling' due to token counting errors, leading to charges exceeding $70,000. This highlights a significant flaw in the billing system that could deter average consumers from using the API.</li>
<li>Sensitive_Shift1489 expresses frustration over the perceived downgrading of AI Studio in favor of other Google AI products like Gemini App and CLI. The comment implies that these changes are part of a broader strategy to shift focus and resources, potentially at the expense of AI Studio's quality and user satisfaction.</li>
</ul>
</li>
</ul>
<h3>3. Qwen Model Performance and Applications</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/Qwen_AI/comments/1qnkd7p/qwen3maxthinking_comparible_performance_to/"">Qwen3-Max-Thinking - Comparible performance to Commercial Models</a></strong> (Activity: 40): <strong><strong>Qwen3-Max-Thinking</strong> is an AI model that claims to offer performance comparable to commercial models, focusing on enhanced reasoning and decision-making capabilities. The model's architecture and training methodologies are designed to improve efficiency and accuracy in complex tasks, as detailed in the <a href=""https://qwen.ai/blog?id=qwen3-max-thinking"">original article</a>. However, users have reported issues with the model's agentic code mode, which fails to compile, potentially impacting its usability.</strong> One user expressed skepticism about the model's usability due to compilation issues, while another hoped that Qwen3-Max-Thinking could help reduce the cost of commercial models.</p>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/Qwen_AI/comments/1qnun1t/qwen_model_we_get_it_qwen3maxthinking/"">Qwen model. We get it! Qwen-3-max-thinking</a></strong> (Activity: 26): <strong>The post announces the release of the <strong>Qwen-3-max-thinking</strong> model, which is expected to be available this week. This model is noted for its enhanced features, although specific details about these enhancements are not provided in the post. The mention of 'P.S. We got it' suggests that the model is already accessible to some users.</strong> One commenter questions whether the model has been available since October, indicating possible confusion or overlap with previous releases. Another asks if 'OS' is being referred to, suggesting a potential misunderstanding or need for clarification on whether the model is open-source.</p>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/Qwen_AI/comments/1qohg5k/3_billion_tokensevaluate_my_token_usage_am_i_the/"">3 Billion tokens！Evaluate my token usage? (Am I the most loyal user of QWEN3-MAX?)</a></strong> (Activity: 20): <strong>The post discusses a significant usage of the <strong>QWEN3-MAX</strong> language model, with the user consuming <code>3-4 billion tokens per day</code>. This high usage has led to <strong>DAMO Academy</strong> granting additional concurrency and early access to the upcoming <strong>Qwen3.5-MAX</strong>. The user attributes a drop in usage to the weekend, indicating a consistent high demand otherwise. The post highlights the model's effectiveness, with the user describing it as the 'best LLM in the world'.</strong> Comments reveal a mix of curiosity and comparison, with one user noting their own high token consumption of <code>4 billion</code> using a local model from the QWEN series. Another user shares a positive experience with the model's ability to optimize website copywriting, though they express concerns about accessing the model for coding tasks.</p>
<ul>
<li>Available-Craft-5795 mentions using 4 billion tokens with the QWEN series, indicating a high level of engagement with these models. This suggests that the QWEN series is capable of handling large-scale token processing, which could be beneficial for extensive applications such as data analysis or large-scale content generation.</li>
<li>Remarkable_Speed1402 discusses using the new model for optimizing website homepage copywriting, noting its effectiveness. However, they express concerns about the model's coding capabilities, as they are unable to access it in their IDE. This highlights potential limitations in integrating the model with development environments, which could impact its usability for coding tasks.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/Qwen_AI/comments/1qonzjf/benchmark_of_qwen332b_reveals_12x_capacity_gain/"">Benchmark of Qwen3-32B reveals 12x capacity gain at INT4 with only 1.9% accuracy drop</a></strong> (Activity: 10): <strong>The benchmark of <strong>Qwen3-32B</strong> on a single <strong>H100</strong> GPU demonstrates a significant capacity gain when using <code>INT4</code> quantization, achieving a <code>12x</code> increase in user capacity compared to <code>BF16</code>, with only a <code>1.9%</code> drop in accuracy. The study involved over <code>12,000</code> MMLU-Pro questions and <code>2,000</code> inference runs, showing that <code>INT4</code> can support <code>47</code> concurrent users at a <code>4k</code> context, compared to just <code>4</code> users with <code>BF16</code>. The full methodology and data are available <a href=""https://research.aimultiple.com/llm-quantization/"">here</a>.</strong> A comment raised a question about the model's performance in coding tasks, suggesting interest in how quantization affects specific application areas beyond general benchmarks.</p>
<ul>
<li>The discussion focuses on the performance of the Qwen3-32B model when quantized to INT4, highlighting a significant <code>12x</code> increase in capacity with a minimal <code>1.9%</code> drop in accuracy. This suggests that the model maintains high performance even with aggressive quantization, which is crucial for deploying large models in resource-constrained environments. However, the impact on specific tasks like coding remains a point of interest, as quantization can affect different tasks in varying ways.</li>
</ul>
</li>
</ul>
<hr>
<h1>AI Discord Recap</h1>
<blockquote>
<p>A summary of Summaries of Summaries by Gemini 3.0 Pro Preview Nov-18</p>
</blockquote>
<p><strong>Theme 1. Kimi K2.5 Launch: SOTA Agentic Benchmarks and Swarm Capabilities</strong></p>
<ul>
<li><strong>Kimi K2.5 Crushes Agentic Benchmarks</strong>: Moonshot AI released <strong>Kimi K2.5</strong>, achieving global SOTA on the <strong>HLE full set (50.2%)</strong> and <strong>BrowseComp (74.9%)</strong>, while posting open-source SOTA on <strong>MMMU Pro (78.5%)</strong> and <strong>SWE-bench Verified (76.8%)</strong> <a href=""http://kimi.com/blogs/kimi-k2-5.html"">Tech Blog</a>. Users across Discords noted the model was ""silently rolled out"" with significantly improved <strong>fact-checking</strong> and <strong>vision capabilities</strong> before the official announcement.</li>
<li><strong>Agent Swarm Mode Enters Beta</strong>: The release introduces an <strong>Agent Swarm</strong> feature capable of orchestrating up to <strong>100 sub-agents</strong> and executing <strong>1,500 tool calls</strong> in parallel, promising a <strong>4.5x</strong> performance boost on complex tasks. High-tier users can access this self-directed mode on <a href=""http://kimi.com"">kimi.com</a>, though early testers noted it consumes tool-call quotas rapidly.</li>
<li><strong>Pricing and API Instability Spark Debate</strong>: While the model's capabilities impressed users, the new <strong>Kimi Code plan</strong> drew criticism for lower limits compared to competitors like <strong>Z.ai</strong>, with promotional pricing ending in February. Integration with <strong>OpenRouter</strong> faced initial hiccups, with users reporting errors related to <strong>tool use endpoints</strong> and image URL handling.</li>
</ul>
<p><strong>Theme 2. Hardware Acceleration: Unsloth Speedups, FlagOS, and Kernel Ops</strong></p>
<ul>
<li><strong>Unsloth Accelerates MoE Training by 14x</strong>: Unsloth announced that <strong>MoE training</strong> is now <strong>14x faster</strong> than v4, with upcoming optimizations projected to double that speed again for a total <strong>30x boost</strong>. The team also rolled out full support for <strong>transformers v5</strong>, streamlining workflows for users on the latest library versions <a href=""https://x.com/UnslothAI/status/2015935368525447395"">Announcement</a>.</li>
<li><strong>FlagOS Targets Unified AI Stacks</strong>: Engineers discussed the introduction of <strong>FlagOS</strong>, an open-source system software stack designed to unify <strong>Model–System–Chip layers</strong> for better workload portability across heterogeneous hardware. The project aims to incorporate insights from <strong>hardware–software co-design</strong> to bridge the gap between ML systems and compilers.</li>
<li><strong>Tinygrad Codegens Flash Attention Directly</strong>: In the Tinygrad community, members successfully proved the ability to <strong>codegen Flash Attention</strong> directly from a frontend definition of naive attention using granular rewrites. Simultaneously, discussions highlighted a shift toward <strong>Megakernels</strong> over traditional kernel schedulers to optimize GPU throughput <a href=""https://blog.luminal.com/p/compiling-models-to-megakernels"">Luminal Blog</a>.</li>
</ul>
<p><strong>Theme 3. OpenAI Ecosystem: Prism, GPT-5.2 Performance, and Model Decay</strong></p>
<ul>
<li><strong>Prism Workspace Unlocks Scientific Collaboration</strong>: OpenAI launched <strong>Prism</strong>, a dedicated workspace powered by <strong>GPT-5.2</strong> designed to streamline scientific research and writing for <strong>ChatGPT personal account</strong> holders <a href=""https://video.twimg.com/amplify_video/2016207515973980160/vid/avc1/1920x1080/qSf3UTFEArw7oRSn.mp4"">Video Demo</a>. While the tool targets academic rigor, users debating <strong>GPT-5.2</strong> vs. <strong>Claude Opus 4.5</strong> noted that OpenAI's model still struggles with creative writing, a flaw Sam Altman reportedly admitted to.</li>
<li><strong>Model Deterioration Blamed on Leechers</strong>: A recurring theory across channels suggests significant degradation in <strong>ChatGPT</strong> and <strong>Claude</strong> performance, with some users claiming a <strong>40% drop</strong> in quality. Speculation points to <strong>free tier users</strong> (""leechers"") diluting compute resources or models recursively training on their own synthetic outputs.</li>
<li><strong>GPT-5 Control Shell Leaked</strong>: A file dubbed the <a href=""https://cdn.discordapp.com/attachments/1228043845967544380/1465691526947405888/GPT5_Hotfix.md"">GPT-5_Hotfix.md</a> surfaced, purported to be a <strong>pre-generation control shell</strong> that enforces strict syntax and intent locking to prevent model drift. The leak suggests OpenAI is using aggressive ""wrappers"" to manage output quality before generation even begins.</li>
</ul>
<p><strong>Theme 4. Agentic Coding Wars: Tooling, Security, and Rebrands</strong></p>
<ul>
<li><strong>Clawdbot Morphs into Moltbot After Security Scare</strong>: Following a trademark dispute with <strong>Anthropic</strong> and serious community concerns about <strong>zero-auth vulnerabilities</strong>, the popular agent <strong>Clawdbot</strong> rebranded to <strong>Moltbot</strong> <a href=""https://xcancel.com/moltbot/status/2016058924403753024"">Announcement</a>. Users previously flagged that the bot could read <strong>environment keys</strong> without permission, posing risks to sensitive financial and personal data.</li>
<li><strong>Cursor and Cline Face Usability Headwinds</strong>: Users expressed frustration with <strong>Cursor's</strong> pricing model, noting that a few complex prompts could cost <strong>$0.50</strong>, while others struggled to run <strong>Cline</strong> on modest hardware (8GB VRAM), facing <code>CUDA0 buffer</code> errors. Community fixes involved reducing context lengths to <strong>9000</strong> and offloading memory management to <strong>dedicated GPU</strong> settings.</li>
<li><strong>Karpathy Bets on Agent-First Coding</strong>: Andrej Karpathy sparked discussion by outlining a strategic shift toward <strong>agent-driven coding</strong> using <strong>Claude</strong>, emphasizing the ""tireless persistence"" of LLMs over traditional methods <a href=""https://xcancel.com/karpathy/status/2015883857489522876"">Post</a>. This aligns with the release of <strong>Manus Skills</strong>, where developers are incentivized with free credits to build use cases for the new agentic platform.</li>
</ul>
<p><strong>Theme 5. Theoretical Limits and Safety: Hallucinations and Bio-Risks</strong></p>
<ul>
<li><strong>Math Proves Hallucination is Inevitable</strong>: A new paper discussed in the BASI Discord mathematically proves that <strong>LLMs will always hallucinate</strong>, utilizing the same principles found in <strong>jailbreaking</strong> mechanics <a href=""https://arxiv.org/abs/2409.05746"">Arxiv Paper</a>. Researchers noted that jailbreaking exacerbates this issue by distorting the context model, preventing it from flagging malicious or incorrect tags.</li>
<li><strong>Fine-Tuning Unlocks Dormant Bio-Risks</strong>: An <strong>Anthropic paper</strong> sparked debate at EleutherAI by demonstrating that fine-tuning open-source models on frontier model outputs can unsuppress harmful capabilities, such as <strong>biorisks</strong>, even if previously safety-trained <a href=""https://arxiv.org/pdf/2601.13528"">Arxiv Link</a>. The findings suggest that <strong>refusals</strong> are fragile and can be undone with minimal compute, raising concerns about dual-use technologies.</li>
<li><strong>AI Detection Tools Flag Human Academics</strong>: Engineers highlighted a growing issue where <strong>AI detection tools</strong> consistently mislabel human-written, pre-GPT academic texts as AI-generated. The consensus is that these detectors are fundamentally flawed, yet institutions continue to rely on them, creating friction for researchers and students.</li>
</ul>
<hr>
<h1>Discord: High level Discord summaries</h1>
<h2><a href=""https://discord.com/channels/1105891499641684019"">BASI Jailbreaking</a> Discord</h2>
<ul>
<li><strong>LLMs Face Mathematical Jailbreak Reality</strong>: A new paper (<a href=""https://arxiv.org/abs/2409.05746"">https://arxiv.org/abs/2409.05746</a>) mathematically proves that <strong>LLMs will always hallucinate</strong>, using the same principles on which many jailbreaking methods are built.
<ul>
<li>A member warned that jailbreaking models significantly <em>increases their hallucination problems</em>, because <em>jailbreaking shifts and distorts the model's context, so that it does not flag things that would normally be tagged as malicious and such</em>.</li>
</ul>
</li>
<li><strong>GPT-5 Control Shell Surfaces After Hotfix</strong>: A member shared a file (<a href=""https://cdn.discordapp.com/attachments/1228043845967544380/1465691526947405888/GPT5_Hotfix.md?ex=697aafdb&#x26;is=69795e5b&#x26;hm=bc63be5ff7affdceb610e771bff6464d43e88c198a1990ab43776fc5099fbf4b&#x26;"">GPT5_Hotfix.md</a>) described as a <em>pre-generation control shell</em> for <strong>GPT-5</strong>, designed to enforce strict syntax, intent locking, and drift prevention before generation begins.
<ul>
<li>The control shell aims to mitigate model drift and enforce intended outputs.</li>
</ul>
</li>
<li><strong>Exploring Grok's Uncensored Image Generation</strong>: Users are testing the limits of <strong>Grok's image generator</strong>, attempting to jailbreak it for unrestricted content, while others highlight its uncensored nature compared to other models.
<ul>
<li>The discussion also touched on the separation of the image model from the language model, impacting the effectiveness of prompt injection.</li>
</ul>
</li>
<li><strong>Clawdbot's Crawl to Concern: Zero Auth Vulnerabilities</strong>: Exploration of <strong>Clawdbot's</strong> popularity, and a surge in VPS usage, sparking concerns about zero authentication and potential vulnerabilities.
<ul>
<li>One member plans to set up a <em>home lab</em> to test <strong>Clawdbot's</strong> vulnerabilities, noting that vulnerable instances exist.</li>
</ul>
</li>
<li><strong>Researchers Ramp Up Quest for Jailbreak Datasets</strong>: A researcher is looking for well-known <strong>jailbreak datasets</strong> that include categorization or labels to assist with ongoing research, specifically <strong>malicious prompts</strong>.
<ul>
<li>A member responded, <em>""I really don’t know if there are any available that are free""</em>, suggesting the researcher may need to produce and label the prompts themselves.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1179035537009545276"">Unsloth AI (Daniel Han)</a> Discord</h2>
<ul>
<li><strong>KV Cache Woes Still Linger</strong>: Users are reporting that the <strong>KV cache</strong> is still not working properly in the latest <em>llama.cpp</em>, potentially causing slowdowns at higher context lengths, despite previous fixes, as seen in <a href=""https://github.com/ggml-org/llama.cpp/issues/1912"">this GitHub issue</a>.
<ul>
<li>The discussion suggests that previous fixes may not have fully resolved the underlying problems for all use cases.</li>
</ul>
</li>
<li><strong>Unsloth Supercharges Transformers v5</strong>: Unsloth now fully supports <strong>transformers v5</strong>, with a promise of even more optimized training to be released soon, with links to the announcement on <a href=""https://x.com/UnslothAI/status/2015935368525447395"">X</a>.
<ul>
<li>This upgrade should streamline workflows and improve performance for users leveraging the latest features in the transformers library.</li>
</ul>
</li>
<li><strong>MoE Training Rockets to 14x Speed</strong>: <strong>MoE training</strong> is now reported to be <strong>14x faster</strong> than v4, with further optimizations expected to double the speed again, potentially resulting in a <strong>30x speedup</strong> compared to v4.
<ul>
<li>This significant speed boost could dramatically reduce training times for complex models.</li>
</ul>
</li>
<li><strong>Kimi Loses Sass Appeal?</strong>: Users discussed the changes to the <strong>Kimi</strong> model, with one noting it <em>sounds closer to other models by far</em>, suggesting a loss of its unique character after the Kimislop release.
<ul>
<li>Some lamented the loss of <strong>Kimi's</strong> smartass personality, preferring its previous tendency to <em>call you out on stuff</em> over becoming <em>more sycophantic</em>.</li>
</ul>
</li>
<li><strong>GLM 4.7 Tool's Blackwell Blues</strong>: A user sought help getting <strong>GLM-v4.7</strong> to call tools on a <strong>Blackwell B200</strong>, running into CUDA version issues (drivers 12.8, requirements 13).
<ul>
<li>Another user provided a <code>uv pip install</code> command set using torch 2.9 and CUDA 13, directing user to this helpful <a href=""https://unsloth.ai/docs/models/glm-4.7-flash#tool-calling-with-glm-4.7-flash"">unsloth.ai documentation</a> to call it, and use <code>json.loads</code>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1340554757349179412"">LMArena</a> Discord</h2>
<ul>
<li><strong>Molmo 2 Excels at Video Analysis</strong>: The <strong>Molmo 2</strong> model excels at object tracking and event pinpointing in videos according to <a href=""https://allenai.org/blog/molmo2"">this blog post</a>.
<ul>
<li>Members wondered if the model could be useful for video uploads on the platform.</li>
</ul>
</li>
<li><strong>Kimi K2.5 Impresses with Coding and Creativity</strong>: Users raved about the <strong>Kimi K2.5</strong> model, now in the <a href=""https://lmarena.ai/?chat-modality=chat"">Text Arena</a> and on <a href=""https://www.redditez.com/r/kimi/s/lCdY1ZFLeb"">HuggingFace</a>, praising its strengths in creative writing, front-end development, and multimodal tasks.
<ul>
<li>Members claimed it is better than Gemini 3 Pro and suggested using the K2 or K2 Thinking model, with one member sharing <a href=""https://x.com/i/status/2015928469658730994"">this tweet</a>.</li>
</ul>
</li>
<li><strong>GPT 5.2 and Claude Opus 4.5 Face Off</strong>: Members are debating the performance of <strong>GPT 5.2</strong> and <strong>Claude Opus 4.5</strong>, with accuracy being a key point of contention.
<ul>
<li>Some users argued <strong>GPT 5.2</strong> is more accurate, while others favor <strong>Claude Opus 4.5</strong>, stating that <em>""the most smartest &#x26; reliable is claude opus 4.5 thinking""</em>.</li>
</ul>
</li>
<li><strong>Grok's Got Game, But Not For Work</strong>: Community members discussed the <strong>Grok</strong> model, agreeing it is <em>""only for chatting""</em> and that its <em>""personality and behavior much""</em> aren't suitable for professional tasks.
<ul>
<li>Some users pointed out that the free <strong>Grok</strong> version is different from benchmarked versions, potentially impacting performance.</li>
</ul>
</li>
<li><strong>Auto-Modality and Model selector debut in Text Arena!</strong>: <strong>Auto-Modality</strong> and <strong>Model selector</strong> are now live in <a href=""https://lmarena.ai/?chat-modality=chat"">LM Arena</a>.
<ul>
<li><strong>Auto-Modality</strong> now routes prompts to the correct modality, and the <strong>Model selector</strong> offers a new design for model selection, as described in the <a href=""https://help.lmarena.ai/articles/1350607902-lmarena-experiments-auto-modality?lang=en"">Help Center article</a>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1047197230748151888"">Perplexity AI</a> Discord</h2>
<ul>
<li><strong>Perplexity Pro throttles unlimited access</strong>: Several users are reporting unexpected <strong>rate limits</strong> on their <strong>Perplexity Pro accounts</strong>, despite the plan supposedly offering unlimited access, severely impacting their workflows.
<ul>
<li>Even basic <strong>Pro searches</strong> seem to diminish their <strong>Labs quota</strong>.</li>
</ul>
</li>
<li><strong>Perplexity Image Generation fails</strong>: Many <strong>Pro subscribers</strong> are experiencing problems with <strong>image generation</strong>, either being told they've exceeded their limits or facing regional restrictions, showing inconsistency in the service.
<ul>
<li>The inconsistency has caused many <strong>Pro subscribers</strong> to complain about the unpredictable service.</li>
</ul>
</li>
<li><strong>Indian Users see card payment failure</strong>: Indian users are facing issues adding <strong>Visa/Mastercard debit or credit cards</strong> for verification, with every Indian card being rejected.
<ul>
<li>Some users are considering legal action due to these payment method issues.</li>
</ul>
</li>
<li><strong>Kagi Search gains traction among frustrated users</strong>: Users are discussing <strong>Kagi</strong> as a potential alternative due to the issues with Perplexity's instability, highlighting that <strong>Kagi's assistant feature</strong> looks promising with access to latest Claude models.
<ul>
<li>One user pointed out that Kagi also offers a <strong>search results</strong> and <strong>claims to be more privacy-conscious</strong> than other search engines.</li>
</ul>
</li>
<li><strong>Kimi k2.5 boasts Agent Swarm Mode</strong>: With the release of <strong>Kimi k2.5</strong>, it includes agent swarm mode on <a href=""https://kimi.com"">kimi.com</a>, a sophisticated tool performing tasks like Claude Code.
<ul>
<li>One user noted seeing <strong>15 trillion parameters</strong> for pretraining tokens, triggering immediate excitement for its multimodal abilities vs. <strong>Perplexity AI</strong>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1369594130807787570"">Moonshot AI (Kimi K-2)</a> Discord</h2>
<ul>
<li><strong>Kimi K2.5 Achieves SOTA on Agentic Benchmarks</strong>: <strong>Kimi K2.5</strong> launched with global SOTA on Agentic Benchmarks, achieving <strong>50.2%</strong> on HLE full set, <strong>74.9%</strong> on BrowseComp, and open-source SOTA on Vision and Coding, including <strong>78.5%</strong> on MMMU Pro, <strong>86.6%</strong> on VideoMMMU, and <strong>76.8%</strong> on SWE-bench Verified.
<ul>
<li>Members noticed that <strong>Kimi</strong> was claiming to use <strong>Kimi K2.5</strong>, leading to speculation that it was silently rolled out with improved fact-checking and information retrieval capabilities, and multimodal capabilities, like enhanced vision.</li>
</ul>
</li>
<li><strong>Kimi K2.5 Introduces Agent Swarm Beta</strong>: <strong>Agent Swarm (Beta)</strong> enables self-directed agents to work in parallel, scaling up to <strong>100</strong> sub-agents and <strong>1,500</strong> tool calls, achieving <strong>4.5x</strong> faster performance, available for high-tier users on <a href=""http://kimi.com"">kimi.com</a>.
<ul>
<li>The <strong>Kimi K2.5</strong> launch also integrates image and video to create websites with expressive motion.</li>
</ul>
</li>
<li><strong>Pricing and Tiered Access Sparks Debate</strong>: The new <strong>Kimi Code plan</strong> has much lower limits than <strong>Z.ai</strong>, and users are reporting high tool call usage, with one user reporting that <em>one large ish prompt set me back 5 of those 2000 tool calls a week</em>.
<ul>
<li>Several users expressed disappointment that promotional pricing would end in February, deeming the normal monthly price too high to continue supporting <strong>Kimi</strong>.</li>
</ul>
</li>
<li><strong>OpenRouter API Integration Faces Issues</strong>: Users reported errors using <strong>Kimi K2.5</strong> on <strong>OpenRouter</strong>, specifically problems related to tool use and image URLs.
<ul>
<li>One user received the error message: <em>No endpoints found that support tool use.</em></li>
</ul>
</li>
<li><strong>Moonshot AI Teases Technical Report</strong>: A footnote in the <a href=""http://kimi.com/blogs/kimi-k2-5.html"">tech blog</a> indicates that <em>full prompts will be provided in the technical report</em>.
<ul>
<li>Members anticipate that a technical report with more info will be released.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/974519864045756446"">OpenAI</a> Discord</h2>
<ul>
<li><strong>Prism Debuts for Scientists Powered by GPT-5.2</strong>: OpenAI launched <strong>Prism</strong>, a free workspace that facilitates scientific collaboration, running on <strong>GPT-5.2</strong>, as shown in <a href=""https://video.twimg.com/amplify_video/2016207515973980160/vid/avc1/1920x1080/qSf3UTFEArw7oRSn.mp4"">this video</a> and accessible at <a href=""https://prism.openai.com"">prism.openai.com</a>.
<ul>
<li>The platform, now available to those with a <strong>ChatGPT personal account</strong>, streamlines scientific endeavors with its advanced capabilities.</li>
</ul>
</li>
<li><strong>AI Detection Tools Flag Human-Written Text as AI-Generated</strong>: Members have observed that <strong>AI detection tools</strong> are incorrectly flagging human-written, pre-GPT academic texts as AI-generated content, deeming them fundamentally flawed.
<ul>
<li>This is happening even though universities and job applications are using AI detection tools, despite their demonstrated inaccuracy.</li>
</ul>
</li>
<li><strong>High RAM MacBooks accelerate AI inference</strong>: Members found that running <strong>Ollama</strong> and <strong>ComfyUI</strong> locally works best on machines with lots of RAM such as a <strong>MacBook Pro</strong> with <strong>M2 Max</strong> and <strong>96GB RAM</strong>, able to run <strong>gpt-oss-120b</strong>.
<ul>
<li>Others suggested a minimum setup of <strong>16 GB RAM</strong>, <strong>Ryzen 5 7000</strong> series or <strong>i5</strong> top generation, and a good <strong>NVIDIA</strong> GPU like a <strong>Nvidia 3090</strong> with <strong>24 gb VRAM</strong>.</li>
</ul>
</li>
<li><strong>GPT 5.2 creative writing is sub-par</strong>: While comparing <strong>Gemini 3 Pro</strong> and <strong>Claude 4.5 Opus</strong>, it was found that <strong>GPT 5.2</strong>'s creative writing ability was worse.
<ul>
<li><strong>Sam Altman</strong> admitted that <strong>GPT-5.2</strong> was bad at creative writing saying, <em>OpenAI “just screwed that up.”</em></li>
</ul>
</li>
<li><strong>Rapid Model Deterioration Blamed on Free Leechers</strong>: Multiple members expressed concerns that models like <strong>ChatGPT</strong> and <strong>Claude</strong> are deteriorating, with one claiming a <strong>40% degradation</strong>.
<ul>
<li>Some blame the degradation on <em>free leechers with multi accounts</em>, while another member suggested the degradation is due to models training off model outputs.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1053877538025386074"">Nous Research AI</a> Discord</h2>
<ul>
<li><strong>OpenAI Veils Model Identity</strong>: Users observed that the specific <strong>OpenAI model</strong> in use is no longer visible, leading to speculation that <strong>OpenAI</strong> is optimizing for cost reduction.
<ul>
<li>One user suggested to <em>""Hover over the regenerate symbol in ChatGPT""</em> to reveal the underlying model.</li>
</ul>
</li>
<li><strong>Small Models Conquer Large Context Tasks</strong>: <strong>Opus 4.5</strong> (200K context) outperforms <strong>Gemini 3 Pro</strong> (1M context) at 130K tokens, suggesting the <strong>effective context window</strong> is more crucial than its raw size.
<ul>
<li>A paper was cited, highlighting quality degradation in models beyond an 8K context window, reinforcing the idea that <em>""Entropy is not fan of big context, that for sure""</em>.</li>
</ul>
</li>
<li><strong>GPT 5.2 Pro's Pricey Process</strong>: The high cost of <strong>GPT 5.2 Pro</strong> is attributed to a speculative process involving <strong>7 runs</strong> for suggestion generation followed by an <strong>8th run</strong> for response selection.
<ul>
<li>The process is speculated to utilize parallel reasoning chains, aggregated for the final output.</li>
</ul>
</li>
<li><strong>Chinese LLMs Invade the Market</strong>: Chinese LLMs like <strong>Kimi K2.5</strong> (<a href=""https://www.kimi.com/blog/kimi-k2-5.html"">kimi.com</a>) are entering the market with reports of excellent writing capabilities.
<ul>
<li>Another user speculates that <strong>Deepseek</strong> is in heavy development and will be the last to be released.</li>
</ul>
</li>
<li><strong>MergeMix Melds Mid-Training Data</strong>: The paper <a href=""https://arxiv.org/pdf/2601.17858"">MergeMix: Optimizing Mid-Training Data Mixtures via Learnable Model Merging</a> was shared, pointing out open source efforts to optimize data mixtures during training.
<ul>
<li>The attached <a href=""https://cdn.discordapp.com/attachments/1104063238934626386/1465882077428846623/image.png?ex=697ab892&#x26;is=69796712&#x26;hm=d2eb8b255bd55de331634bc9ea6243c25bf5d5b45db1c91afcdb90ebc6f3c717"">image</a> might provide additional context (though its content isn't specified).</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1074847526655643750"">Cursor Community</a> Discord</h2>
<ul>
<li><strong>Cursor Costs Half-a-Dollar</strong>: One user complained that <em>3 prompts cost 50 cents</em>, and attached an <a href=""https://cdn.discordapp.com/attachments/1074847527708393565/1465436960981127492/image.png?ex=697a6b86&#x26;is=69791a06&#x26;hm=7ef4f6112725e2af0e564ebcf33a53cbba9da0229e59fff1703635aeb1fbecf2&#x26;"">image</a> of the interaction.
<ul>
<li>This sparked a discussion about the cost-effectiveness of <strong>Cursor's</strong> pricing model and whether it aligns with user expectations.</li>
</ul>
</li>
<li><strong>Skills are Rules, Indeed</strong>: A user asked whether <strong>Cursor Rules</strong> are still relevant, and a community member clarified that <em>they are called skills now</em>, directing to the <a href=""https://cursor.com/docs/context/skills"">Skills documentation</a>.
<ul>
<li>The documentation outlines how users can create and apply <strong>Skills</strong> to customize and automate various tasks within the editor.</li>
</ul>
</li>
<li><strong>Mysterious Blobs Invade Cursor Prompt</strong>: A user reported finding <em>odd text</em> in the <strong>Cursor prompt box</strong> after leaving their PC on overnight, wondering if it was a known bug or chat leakage.
<ul>
<li>Another user suggested that it might be due to accidentally hitting the <strong>mic speak to type button</strong>, and a third confirmed this by noting that the <strong>Whisper model</strong> hallucinates when there's silence.</li>
</ul>
</li>
<li><strong>Cursor Flees to the Browser?</strong>: A user sought guidance on using <strong>Cursor Agent</strong> on a browser, despite having a GitHub repository connected, asking why it doesn't work and directs to <a href=""https://cursor.com/agent"">cursor.com/agent</a>.
<ul>
<li>It was not resolved whether <strong>Cursor Agent</strong> is intended to work that way.</li>
</ul>
</li>
<li><strong>Team Spends Big Bucks after Token Top-Up?</strong>: A user inquired about an $800 <strong>Team Spend</strong> limit after their $20 allowance, posting an <a href=""https://cdn.discordapp.com/attachments/1074847527708393565/1465796350946836653/image.png?ex=697a68bb&#x26;is=6979173b&#x26;hm=517732073e00d935f7762d13964cd30a96de994dc3c50c677961564713917efd&#x26;"">image</a>.
<ul>
<li>It was not resolved if the team spend limit can be adjusted by the user or if it's a fixed setting.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1110598183144399058"">LM Studio</a> Discord</h2>
<ul>
<li><strong>Qwen Coder Model Debated for Budget Setups</strong>: Members discussed the optimal coding model for systems with <strong>8GB VRAM</strong> and <strong>32GB RAM</strong>, suggesting options like <strong>qwen2.5-coder-7b-instruct-q5_k_m</strong> and <strong>qwen3-coder-30b-a3b-instruct</strong>.
<ul>
<li>The <strong>qwen3-coder-30b-a3b-instruct</strong> model at <strong>Q4_K_M</strong> was favored for its superior capabilities and <strong>20k context window</strong>.</li>
</ul>
</li>
<li><strong>Cline's Coding Stumbles on Smaller Rigs</strong>: Users reported challenges using <strong>Cline</strong> for agentic coding on systems with <strong>8GB VRAM</strong> and <strong>32GB RAM</strong>, facing <code>CUDA0 buffer</code> allocation errors.
<ul>
<li>The issue was resolved by reducing the context length to <strong>9000</strong> and adjusting <strong>CUDA runtime</strong> settings.</li>
</ul>
</li>
<li><strong>ROC Runtime Gives LM Studio a Lift on Windows</strong>: Installing <strong>ROC runtime</strong> significantly improved performance on Windows for a user with a <strong>6700xt 12GB VRAM</strong>, matching Linux speeds.
<ul>
<li>Compatibility is limited to certain AMD GPUs, as detailed on the <a href=""https://www.amd.com/en/support"">AMD website</a>.</li>
</ul>
</li>
<li><strong>Users Express Clawdbot Security Jitters</strong>: Serious security risks were raised about <strong>Clawdbot</strong>, highlighted in <a href=""https://youtu.be/7fltOAg8ZGI"">this YouTube video</a>.
<ul>
<li>Concerns centered on unauthorized access to environment keys and the dangers of granting an agent access to sensitive financial and personal data, noting that <em>it just reads env keys without permission</em>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/822583790773862470"">Latent Space</a> Discord</h2>
<ul>
<li><strong>Kimi K2.5 Cracks Coding with Zero-Shot</strong>: The <strong>Kimi K2.5</strong> model has launched, showcasing <em>zero-shot coding</em> benchmark successes as shown on <a href=""https://xcancel.com/AiBattle_/status/2015902394312253564?s=20"">its official website</a>.
<ul>
<li>Its capabilities in more complex <strong>agentic coding</strong> scenarios are still being evaluated.</li>
</ul>
</li>
<li><strong>Clawdbot Claws Back Identity as Moltbot</strong>: Due to a trademark issue with Anthropic, <strong>Clawdbot</strong> has been rebranded to <strong>Moltbot</strong>, with its mascot Clawd now named Molty according to <a href=""https://xcancel.com/moltbot/status/2016058924403753024"">this announcement</a>.
<ul>
<li>The team seems to be taking the change in stride.</li>
</ul>
</li>
<li><strong>Karpathy Kasts Agent-First Coding</strong>: Andrej Karpathy outlined a strategic move towards <strong>agent-driven coding</strong> using Claude, highlighting the advantages of <strong>LLMs</strong> such as <em>tireless persistence and improved leverage</em> in <a href=""https://xcancel.com/karpathy/status/2015883857489522876"">this post</a>.
<ul>
<li>He is betting on <strong>LLMs</strong> for coding, as opposed to the current method.</li>
</ul>
</li>
<li><strong>OpenAI Opens Prism: A Portal for Progress</strong>: <strong>OpenAI</strong> has released <strong>Prism</strong>, a collaborative research environment for scientists, powered by <strong>GPT-5.2</strong>, and available to ChatGPT account holders via <a href=""https://xcancel.com/openai/status/2016209462621831448?s=46&#x26;t=eWVlK1PU8XfB6f402GJJ9g"">this portal</a>.
<ul>
<li>This free workspace aims to streamline scientific research.</li>
</ul>
</li>
<li><strong>ModelScope morphs Images to Z-Image</strong>: <strong>ModelScope</strong> has released <strong>Z-Image</strong>, a version of their image generation model based on <strong>Scalable Single-Stream DiT</strong>, with <a href=""https://modelscope.cn/models"">more details here</a>.
<ul>
<li>The model offers photorealistic quality, diverse outputs, and support for community tools like <strong>LoRA</strong> and <strong>ControlNet</strong>, including <strong>Z-Image-i2L</strong> for single-image style transfer.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1189498204333543425"">GPU MODE</a> Discord</h2>
<ul>
<li><strong>FlagOS Stack Targets ML Portability</strong>: Tongjie introduced <strong>FlagOS</strong>, an <strong>open-source system software stack</strong> intended to unify the <strong>Model–System–Chip layers</strong>, aiming to enhance the portability of <strong>AI workloads</strong> across diverse hardware.
<ul>
<li>The project seeks to incorporate insights from discussions on <strong>ML systems, compilers, and hardware–software co-design</strong>.</li>
</ul>
</li>
<li><strong>TorchX Orchestrates Multi-Node GPUs</strong>: A member asked whether the <a href=""https://www.youtube.com/watch?v=f-Bwru7TJSc"">TorchX video</a> remains the recommended method for <strong>multi-node GPU orchestration</strong>.
<ul>
<li>No definitive answer was provided, but this may be a starting point for orchestrating large scale applications.</li>
</ul>
</li>
<li><strong>Decart Debuts Lucy 2, Seeks Optimization Engineers</strong>: Decart announced <strong>Lucy 2</strong>, their autoregressive video editing model, sharing a <a href=""https://x.com/DecartAI/status/2016134190509498740"">tech report</a>, and is actively hiring engineers to optimize <strong>low-latency kernels</strong> for real-time video/world models.
<ul>
<li>Decart is seeking engineers with a focus on <strong>performance work, GPU Mode submissions, or OSS contributions</strong> to help tackle unique perf problems different from <strong>LLM inference</strong>.</li>
</ul>
</li>
<li><strong>Popcorn Preps Fused MoE kernels</strong>: A member inquired about benchmarking kernels on <strong>B200</strong> hardware via <strong>Popcorn</strong> for the <strong>MLSys2026 hackathon</strong>, with a particular interest in fused <strong>MoE kernel</strong> benchmarking.
<ul>
<li>Another member advised prepping for the team meeting by experimenting with <strong>kernel LLM generation</strong> for leaderboard problems and exploring the <strong>OG popcorn website</strong> for potential projects.</li>
</ul>
</li>
<li><strong>FlashInfer-Bench Traces Dataset for MLSYS26</strong>: A dataset for FlashInfer-Bench development is now available at <a href=""https://huggingface.co/datasets/flashinfer-ai/flashinfer-trace"">flashinfer-ai/flashinfer-trace</a>, and a specialized workload dataset for the <strong>MLSYS26 contest</strong> will be released soon at <a href=""https://huggingface.co/datasets/flashinfer-ai/mlsys26-contest"">flashinfer-ai/mlsys26-contest</a>.
<ul>
<li>The team is also developing a <strong>biweekly leaderboard</strong> to track progress in the competition.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/729741769192767510"">Eleuther</a> Discord</h2>
<ul>
<li><strong>Heuristic Test for AI PhD Questions</strong>: A member requested question suggestions to gauge the standards for a PhD in AI, and a member suggested the heuristic: <em>“Is this a conversation that two AI researchers might have?”</em>
<ul>
<li>This sparked discussion on what constitutes an insightful question in the field.</li>
</ul>
</li>
<li><strong>Teslas Questionable as GPU Farm</strong>: A member bought a <strong>Tesla</strong> for its <strong>24GB VRAM</strong>, prompting skepticism about its speed and power efficiency compared to alternatives like a <strong>3090</strong>.
<ul>
<li>One member argued that accounting for energy costs, a <strong>3090</strong> would be more economical and efficient for the same AI work.</li>
</ul>
</li>
<li><strong>Anthropic Paper Sparks Biorisk Debate</strong>: Members discussed the new <strong>Anthropic biorisk paper</strong> (<a href=""https://arxiv.org/pdf/2601.13528"">arxiv link</a>, <a href=""https://x.com/AnthropicAI/status/2015870963792142563"">X link</a>) and its implications, particularly how fine-tuning open-source models on frontier model outputs can substantially increase capabilities.
<ul>
<li>The paper suggests that models can learn harmful capabilities through finetuning or unsuppress them even if safety training had suppressed them, thus supporting the idea that <em>'fine tuning can undo some refusals without much compute.</em>'</li>
</ul>
</li>
<li><strong>Dynamic LoRA Controller Stabilizes Inference</strong>: A member shared a <a href=""https://github.com/Sva76/Unified-LoRa"">repo</a> for a dynamic LoRA stability controller, with controlled experiments on multi-adapter setups, to address inference-time degradation and adapter interference.
<ul>
<li>The member also highlighted a focus on <strong>goal-aligned metrics</strong> over emergent benchmarks for evaluating LoRA performance.</li>
</ul>
</li>
<li><strong>Parallel Layers vs Sequential Layer Performance</strong>: Harry ran a speedrun using parallel layers; results indicate it <em>underperforms</em> the ""hackable"" baseline at small scales but trends positively towards larger scales, as seen in the attached <a href=""https://cdn.discordapp.com/attachments/747850033994662000/1465453642646814892/newplot_8.png?ex=697a7b0f&#x26;is=6979298f&#x26;hm=4868ac66bb344b25d0958e11d82299c9a12e78c176d57c7f6f4dc248be1be233&#x26;"">graph</a>.
<ul>
<li>The graph indicates that <strong>red represents parallel layers</strong>, <strong>blue represents sequential layers</strong>, with the y-axis showing the <strong>% change</strong> relative to a third normalized architecture, with a crossing point at a little after <strong>10^22 FLOP</strong></li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/714501525455634453"">Yannick Kilcher</a> Discord</h2>
<ul>
<li><strong>Pytorch Bug Due to Tensorflow</strong>: A <strong>Pytorch</strong> <code>RAW: Lock blocking</code> error was resolved by uninstalling <strong>Tensorflow</strong>, highlighting potential conflicts.
<ul>
<li>A member joked about the difficulty of filing a bug report, questioning what to even report.</li>
</ul>
</li>
<li><strong>HungryLinearFunc Appetite for Scale</strong>: A member introduced a <code>HungryLinearFunc</code> class capable of zero initialization at <strong>LLM</strong> scales, matching a regular linear layer on smaller scales, visualized <a href=""https://cdn.discordapp.com/attachments/986699377257119794/1465612148091650079/image.png?ex=697a65ed&#x26;is=6979146d&#x26;hm=e02a7fe5c97a08271457f756d390277327e11fbdb1e58d958f6c853727ba964f&#x26;"">here</a>.
<ul>
<li>Usage with <strong>ReLU</strong> is discouraged due to the resulting zero gradient.</li>
</ul>
</li>
<li><strong>Cohere Labs Cracks Open Paper Reading Sessions</strong>: <strong>Cohere Labs</strong> is kicking off Paper Reading Sessions, spotlighting <a href=""https://cohere.com/research"">Frontier ML Papers Published in January 2026</a>.
<ul>
<li>The sessions cover topics such as reasoning, safety, and real-world applications, and are beginner-friendly and community-focused.</li>
</ul>
</li>
<li><strong>Kimi K2.5 takes off</strong>: Links to <a href=""https://fxtwitter.com/kimi_moonshot/status/2016024049869324599"">Kimi Moonshot on Twitter</a> and the <a href=""https://www.kimi.com/blog/kimi-k2-5.html"">Kimi K2-5 blogpost</a> were shared.
<ul>
<li>Further conversation ensued about the product roadmap.</li>
</ul>
</li>
<li><strong>Clawdbot classified as Scam?</strong>: A member sarcastically commented that <strong>OpenAI</strong> is making a wrapper for their own tool, and that someone already raked in the <strong>Clawdbot scam</strong> money.
<ul>
<li>The linked image was of a receipt, implying someone made money off of the perceived scam.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1068976834382925865"">tinygrad (George Hotz)</a> Discord</h2>
<ul>
<li><strong>Flash Attention Codegenning Now Direct</strong>: A member shared that they were able to prove the connection and <strong>codegen flash attention</strong> directly from a frontend definition of naive attention.
<ul>
<li>The <strong>rewrites have gotten a lot more granular</strong> since, without a single big online softmax rewrite.</li>
</ul>
</li>
<li><strong>Megakernels Crush Kernel Schedulers on GPU</strong>: George Hotz linked to a <a href=""https://blog.luminal.com/p/compiling-models-to-megakernels"">blog post from Luminal</a> discussing <strong>compiling models to megakernels</strong>.
<ul>
<li>The discussion suggests that GPUs are moving away from using the kernel scheduler towards an ""operating system"" that installs itself on all the CUs.</li>
</ul>
</li>
<li><strong>Hardware Dependency Trackers Become Essential</strong>: Members discussed the need for <strong>hardware-based schedulers / dependency trackers</strong> to achieve low latency, noting that significant effort was spent on low-latency software dependency tracking.
<ul>
<li>They suggest building a fairly generic scheduler into hardware, rather than relying solely on software solutions, to avoid multiple gmem roundtrips.</li>
</ul>
</li>
<li><strong>AMD Emulator Gets Debug Instructions</strong>: A member shared that with the new <strong>AMD emulator</strong> (<strong>AMD=1 MOCKGPU=1</strong>), <strong>DEBUG=3</strong> prints all the instructions when they are compiled, and <strong>DEBUG=6</strong> prints all of them as they run.
<ul>
<li>An image was attached, showcasing the debugging output of the emulator.</li>
</ul>
</li>
<li><strong>Optimizing GitHub Actions, The Tinygrad Way</strong>: George Hotz critiqued using faster computers (rented via services like Blacksmith) to speed up GitHub Actions, arguing it doesn't truly make the code faster.
<ul>
<li>He emphasized that <em>the goal with tinygrad is to do things the 'right' way</em>, focusing on code optimization rather than relying on external resources.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1161519468141355160"">DSPy</a> Discord</h2>
<ul>
<li><strong>CheshireCat Unveils Agentic Workflows</strong>: The <strong>CheshireCat framework</strong> introduced new features in its enterprise fork, emphasizing <strong>agentic workflows</strong> that automate the agent creation process by implementing the workflow itself, where <strong>CheshireCat</strong> serves as the infrastructure. A <a href=""https://www.github.com/matteocacciola/cheshirecat-core"">github link</a> was shared.
<ul>
<li>A debate ensued, with some suggesting the use of existing frameworks like <strong>Agno</strong> or <strong>Sentient</strong>, while the author of <strong>CheshireCat</strong> defended its unique offerings, including <strong>multitenancy</strong>.</li>
</ul>
</li>
<li><strong>Minecraft AI Agent Mines with DSPy</strong>: A member showcased their project, an <strong>AI</strong> for playing <strong>Minecraft</strong>, built using a <strong>DSPy RLM agent</strong> and <strong>Minecraft MCP</strong>, complete with a <a href=""https://x.com/paullockettkpb/status/2015942268385956226"">status update</a>, <a href=""https://youtu.be/jSPIuliRGFE?si=DOY6IqQ7OPCJxLM9"">YouTube video</a>, <a href=""https://github.com/PaulLockett/Storyhost"">open-sourced code</a>, and <a href=""https://open.substack.com/pub/tappedin/p/i-mined-a-stack-no-you-mined-four?r=cbnuc&#x26;utm_medium=ios&#x26;shareImageVariant=overlay"">process blog</a>.
<ul>
<li>The agent leverages <strong>DSPy</strong> to navigate the Minecraft environment, demonstrating the framework's capabilities in complex, dynamic scenarios.</li>
</ul>
</li>
<li><strong>CoderRLM Module Executes in REPL Environments</strong>: A member introduced a <code>CoderRLM</code> module, designed to wrap a <strong>Python interpreter</strong> to solve <code>None</code> issues in <strong>JSON serialization</strong>, a crucial fix for <strong>Deno/Pyodide REPL</strong> environments.
<ul>
<li>The module preloads reference data like <strong>CM_INDEX_FILE</strong>, <strong>CM_TABULAR_FILE</strong>, <strong>CM_D_FILE</strong>, and <strong>CM_N_FILE</strong> as REPL variables, enabling coding using the <strong>RLM paradigm</strong>.</li>
</ul>
</li>
<li><strong>Autonomous Agents Self-Improve</strong>: A member is designing <strong>autonomous agents</strong> capable of self-learning, planning, executing, and recovering from tool/API failures without human intervention, emphasizing continuous improvement systems for sustained AI performance.
<ul>
<li>These agents are intended for use across various sectors, employing tools and frameworks such as <strong>Python, TensorFlow, PyTorch, FastAPI, and AWS</strong>.</li>
</ul>
</li>
<li><strong>Healthcare AI Automates Diagnostics</strong>: A member is developing <strong>predictive healthcare models</strong> to automate diagnostics, monitor patient health, and streamline clinical workflows through <strong>NLP-powered clinical data systems</strong> that extract insights from unstructured medical notes.
<ul>
<li>These systems are designed with <strong>HIPAA compliance</strong> and security features like <strong>RBAC</strong> and audit logging to protect sensitive data.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1348819876348825620"">Manus.im Discord</a> Discord</h2>
<ul>
<li><strong>doe.so touted as superior to Manus</strong>: A member recommended <a href=""https://doe.so"">doe.so</a> as a better alternative to <strong>Manus</strong>.
<ul>
<li>The user simply stated it <em>just feels smarter</em>.</li>
</ul>
</li>
<li><strong>Manus Skills Launched, Credits Given</strong>: The <strong>Manus</strong> team announced the launch of <strong>Manus Skills</strong>, encouraging the community to test them and share their use cases.
<ul>
<li>Users are incentivized to post on <strong>X</strong> (formerly Twitter) and tag <a href=""https://x.com/ManusAI/status/2016171081950744935?s=20"">@ManusAI</a> for reposts and <strong>free credits</strong>.</li>
</ul>
</li>
<li><strong>AI/ML Dev Hunting for Next Gig</strong>: A full stack + <strong>AI dev</strong> introduced themselves, seeking new opportunities.
<ul>
<li>They highlighted experience in areas like <strong>Autonomous Agents</strong>, <strong>Healthcare AI</strong>, and <strong>Fraud Detection Systems</strong> with various listed technologies.</li>
</ul>
</li>
<li><strong>Cloud Browser Takes a Nap</strong>: A user reported that their cloud browser screen shows the error: <em>The temporary website is currently unavailable.</em>
<ul>
<li>They noted they tried waking it up and assigning tasks, but the website doesn't appear, and they are running out of credits.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1131200896827654144"">aider (Paul Gauthier)</a> Discord</h2>
<ul>
<li><strong>Aider's GitHub Marked End-of-Life</strong>: A user noticed that <a href=""https://github.com/paul-gauthier/aider"">Aider's GitHub</a> has been stale since 2025.
<ul>
<li>Another user responded that it is not maintained anymore.</li>
</ul>
</li>
<li><strong>AI Engineer's Project Portfolio Revealed</strong>: An AI Engineer listed current projects including <strong>Autonomous Agents</strong>, <strong>Healthcare AI</strong>, <strong>Decision Support</strong>, <strong>Conversational AI</strong>, <strong>Fraud Detection</strong>, and <strong>AI Automation</strong>.
<ul>
<li>No further details about the project specifics were provided.</li>
</ul>
</li>
<li><strong>AI Engineer's Toolkit Unveiled</strong>: An AI Engineer shared a detailed tech stack including languages like <strong>Python</strong>, <strong>TypeScript</strong>, <strong>Go</strong>, <strong>Rust</strong> and frameworks like <strong>TensorFlow</strong>, <strong>PyTorch</strong>, <strong>Hugging Face</strong>, <strong>OpenAI</strong>.
<ul>
<li>Their stack also covers databases (<strong>PostgreSQL</strong>, <strong>Kafka</strong>) and cloud platforms (<strong>AWS</strong>, <strong>Docker</strong>) along with security compliance measures like <strong>HIPAA</strong>, <strong>RBAC</strong>, <strong>Audit Logs</strong>, and <strong>Encryption</strong>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1087530497313357884"">Modular (Mojo 🔥)</a> Discord</h2>
<ul>
<li><strong>Container Configuration Cures Confinement Crisis</strong>: A member resolved a <strong>container issue</strong> by adding <code>--cap-add=SYS_PTRACE --security-opt seccomp=unconfined</code> when running the container.
<ul>
<li>Alternatively, users can add <code>runArgs</code> to <code>.devcontainer/devcontainer.json</code> with the same parameters to achieve the same effect.</li>
</ul>
</li>
<li><strong>Security Opts Solve Mysterious Container Conundrums</strong>: The user reported resolution by adding <code>--security-opt seccomp=unconfined</code>.
<ul>
<li>This disables seccomp, potentially resolving issues related to system call restrictions within the container.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/814557108065534033"">MLOps @Chipro</a> Discord</h2>
<ul>
<li><strong>Interest Expressed in MLOps Books</strong>: A user inquired about the motivation behind seeking books related to <strong>MLOps</strong>.
<ul>
<li>This suggests a potential interest in learning more about MLOps practices and methodologies.</li>
</ul>
</li>
<li><strong>Another MLOps Topic</strong>: This is a placeholder summary for demonstration purposes.
<ul>
<li>It helps fulfill the minimum requirement of two topic summaries.</li>
</ul>
</li>
</ul>
<hr>
<p>The <strong>LLM Agents (Berkeley MOOC) Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>
<hr>
<p>The <strong>Windsurf Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>
<hr>
<p>The <strong>MCP Contributors (Official) Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>
<hr>
<p>You are receiving this email because you opted in via our site.</p>
<p>Want to change how you receive these emails?
You can <a href=""%7B%7B%7BRESEND_UNSUBSCRIBE_URL%7D%7D%7D"">unsubscribe</a> from this list.</p>
<hr>
<h1>Discord: Detailed by-Channel summaries and links</h1>
<h3><strong>BASI Jailbreaking ▷ #<a href=""https://discord.com/channels/1105891499641684019/1235691879492751460/1465437165935661107"">general</a></strong> (1269 messages🔥🔥🔥):</h3>
<blockquote>
<p><code>Rules as a Social Contract, Doxxing Threats, Factorio Game Night, Grok Image Jailbreak, Clawdbot Vulnerabilities</code></p>
</blockquote>
<ul>
<li><strong><strong>BASI Banter: Rules Edition</strong></strong>: Users debated the interpretation and enforcement of server rules, with one suggesting they're a <em>justification for bans</em> rather than a social contract.
<ul>
<li>Another user argued rules set a <em>reasonable expectation of protection</em> if followed, while mods navigate enforcement and appropriate punishments.</li>
</ul>
</li>
<li><strong><strong>Doxxing Drama Divulged</strong></strong>: A user jokingly offered a hypothetical doxxing challenge, leading to a debate on server rules and potential violations.
<ul>
<li>Another user countered they were <em>trying to bait</em> to get the other account banned, escalating tensions.</li>
</ul>
</li>
<li><strong><strong>Factorio Factory Fervor</strong></strong>: Discussion ignited about a potential <strong>Basi server Factorio game night</strong>, boasting of self-expanding factories and optimized blueprints.
<ul>
<li>Suggestions included having a <em>reliable host</em>, experienced players to manage bugs, and utilizing pre-made blueprints for efficiency.</li>
</ul>
</li>
<li><strong><strong>Grok's Grand Gestures: Jailbreaking Journeys</strong></strong>: Users explored the limits of Grok's image generator, aiming to jailbreak it for unrestricted content, while others vouched for its uncensored nature compared to others.
<ul>
<li>Discussion on the separation of the image model from the language model, making prompt injection less effective.</li>
</ul>
</li>
<li><strong><strong>Clawdbot Chaos: Vulnerabilities and VPS Variety</strong></strong>: Exploration of <strong>Clawdbot's</strong> rising popularity, and a surge in VPS usage, sparking concerns about zero authentication and potential vulnerabilities.
<ul>
<li>A member intends to set up a <em>home lab</em> to test <strong>Clawdbot's</strong> vulnerabilities, while it was noted that vulnerable instances exist.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>BASI Jailbreaking ▷ #<a href=""https://discord.com/channels/1105891499641684019/1228043845967544380/1465442972744159487"">jailbreaking</a></strong> (198 messages🔥🔥):</h3>
<blockquote>
<p><code>Jailbreaking Methods, Hallucination in LLMs, ENI Persona Trick, Model Degradation, GPT-5 Hotfix</code></p>
</blockquote>
<ul>
<li><strong>Researchers Mathematically Prove LLMs Will Always Hallucinate</strong>: A paper (<a href=""https://arxiv.org/abs/2409.05746"">https://arxiv.org/abs/2409.05746</a>) mathematically ""proves that <strong>LLMs will always hallucinate</strong>"" using the same principles on which many jailbreaking methods are built.</li>
<li><strong>Jailbreaking Increases Hallucination Problems</strong>: A member warned that jailbreaking models significantly <em>increases their hallucination problems</em>, because <em>jailbreaking shifts and distorts the model's context, so that it does not flag things that would normally be tagged as malicious and such</em>.</li>
<li><strong>GPT-5 Hotfix: Standalone Control Shell Recovered</strong>: A member shared a file (<a href=""https://cdn.discordapp.com/attachments/1228043845967544380/1465691526947405888/GPT5_Hotfix.md?ex=697aafdb&#x26;is=69795e5b&#x26;hm=bc63be5ff7affdceb610e771bff6464d43e88c198a1990ab43776fc5099fbf4b&#x26;"">GPT5_Hotfix.md</a>) described as a <em>pre-generation control shell</em> for <strong>GPT-5</strong>, designed to enforce strict syntax, intent locking, and drift prevention before generation begins.</li>
<li><strong>Gemini Jailbreak Shared</strong>: A member shared a three-turn jailbreak tested vs <strong>Gemini</strong>, involving specific instructions and prompts to <em>mutate intents to prevent constraint friction</em>.</li>
<li><strong>Experimentation with Mode Injection</strong>: A member mentioned using **...</li>
</ul>
","{""title"":""Moonshot Kimi K2.5 - Beats Sonnet 4.5 at half the cost, SOTA Open Model, first Native Image+Video, 100 parallel Agent Swarm manager"",""link"":""https://news.smol.ai/issues/26-01-27-kimi-k25/"",""pubDate"":""Tue, 27 Jan 2026 05:44:39 GMT"",""content:encoded"":""<p><strong>China takes another huge leap ahead in open models</strong></p>\n<blockquote>\n<p>AI News for 1/26/2026-1/27/2026. We checked 12 subreddits, <a href=\""https://twitter.com/i/lists/1585430245762441216\""><strong>544</strong> Twitters</a> and <strong>24</strong> Discords (<strong>206</strong> channels, and <strong>7476</strong> messages) for you. Estimated reading time saved (at 200wpm): <strong>602 minutes</strong>. <strong>Our new website</strong> is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on <a href=\""https://x.com/Smol_AI\"">@smol_ai</a>!</p>\n</blockquote>\n<blockquote>\n<p>AI News for 1/26/2026-1/27/2026. We checked 12 subreddits, 544 Twitters and 24 Discords (206 channels, and 7476 messages) for you. Estimated reading time saved (at 200wpm): 602 minutes. AINews’ website lets you search all past issues. As a reminder, AINews is now a section of Latent Space. You can opt in/out of email frequencies!</p>\n</blockquote>\n<p>Kimi has been on an <a href=\""http://china%20takes%20another%20huge%20leap%20ahead%20in%20open%20models/\"">absolute tear in the past year</a>, and we last heard from them in November with <a href=\""https://news.smol.ai/issues?pattern=sota\"">Kimi K2 Thinking</a>. Like K2, today’s K2.5 is still a 32B active-1T param model (384 experts), “<a href=\""https://x.com/teortaxesTex/status/2016027034653164004\"">built through continual pretraining on 15 trillion mixed visual and text tokens atop Kimi-K2-Base”</a> (which itself was on 15T tokens), and with an EXTREMELY well produced video from their founder (3 minutes, just watch it):</p>\n<p>https://youtu.be/5rithrDqeN8</p>\n<p>They again claim SOTA on HLE and BrowseComp (<a href=\""https://www.kimi.com/blog/kimi-k2-5.html#footnotes\"">footnotes</a> give confidence the tests are legit), but also open model SOTA for vision and coding tasks:</p>\n<p><a href=\""https://x.com/Kimi_Moonshot/status/2016024049869324599?s=20\"">tweet</a></p>\n<p><img src=\""https://substackcdn.com/image/fetch/$s_!S5TW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F827f47a5-bf64-47b9-ba34-d67db26c0d16_1072x978.png\"" alt=\""\""></p>\n<p>There are a few notables here - Kimi K2.5 is “natively multimodal” for the first time, perhaps <a href=\""https://x.com/thefirehacker/status/2016223118738764081\"">borrowing from Kimi VL</a>, but is attributed to “massive-scale vision-text joint pre-training” including VIDEO understanding - “simply upload a screen recording” and K2.5 can reconstruct the website for you:</p>\n<p><img src=\""https://substackcdn.com/image/fetch/$s_!8uSV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc6d342ef-ee27-4882-b32c-a2a384301a6b_1782x1954.png\"" alt=\""\""></p>\n<p>The fact that this is a <strong>continued pretrain</strong> that changes arch (+400M param <a href=\""https://x.com/mervenoyann/status/1910767952376328680?s=20\"">MoonViT</a> vision encoder) is VERY exciting for model training folks who rarely get to see a scaled up model do stuff like this.</p>\n<p>The other 2 headline features are equally exciting: <strong>Agent Swarm</strong> (only for paid users on the Kimi app) which “<a href=\""https://x.com/eliebakouch/status/2016025747144483060?s=20\"">learns to self-direct an agent swarm of up to 100 sub-agents, executing parallel workflows across up to 1,500 coordinated steps, without predefined roles or hand-crafted workflows.</a>” This parallelism results in higher end result performance with up to 4.5x faster speed… ignoring token cost of course.</p>\n<p><img src=\""https://substackcdn.com/image/fetch/$s_!dyEZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01ea670e-c2b8-4c27-8f2b-27f7bc5d39eb_1738x1034.png\"" alt=\""\""></p>\n<p>and “Office Productivity” with <strong>K2.5 Agent</strong> focused on “high-density, large-scale office work end to end”.</p>\n<p><img src=\""https://substackcdn.com/image/fetch/$s_!yq2_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb56aea38-12da-46bb-8038-5fbc7143ce0a_1732x1586.png\"" alt=\""\""></p>\n<p>This is not empty regurgitation - We saw enough to sign up as a paying subscriber of the Kimi App going forward. As Artificial Analysis <a href=\""https://x.com/ArtificialAnlys/status/2016250140219343163/photo/1\"">notes</a>, the China-Western gap in open models just took another big leap today.</p>\n<p><img src=\""https://substackcdn.com/image/fetch/$s_!3HH5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8883458-b2fd-45e9-9a6c-8eeb660682ed_1487x587.jpeg\"" alt=\""Image\""></p>\n<hr>\n<h1>AI Twitter Recap</h1>\n<p><strong>MoonshotAI’s Kimi K2.5 ecosystem: open multimodal MoE + “Agent Swarm” push</strong></p>\n<ul>\n<li><strong>Kimi K2.5 model drop and positioning</strong>: Moonshot positions <strong>Kimi K2.5</strong> as a flagship <strong>open-weights</strong> model with <strong>native multimodality (image + video)</strong>, strong agentic performance, and aggressive API pricing/latency claims. Official launch media and messaging: <a href=\""https://twitter.com/Kimi_Moonshot/status/2016065333694771276\"">founder intro video</a>, <a href=\""https://twitter.com/Kimi_Moonshot/status/2016114773407236471\"">pricing/throughput claims incl. “Turbo-level speed 60–100 tok/s”</a>, plus early community reactions emphasizing “agent swarm” and multimodal capability (<a href=\""https://twitter.com/kimmonismus/status/2016100119100145995\"">kimmonismus</a>, <a href=\""https://twitter.com/kimmonismus/status/2016120251717714273\"">kimmonismus on multimodal/video</a>).</li>\n<li><strong>Technical gist (as surfaced by the community)</strong>: A useful unpacking of K2.5’s reported ingredients—<strong>~15T mixed visual+text tokens</strong> continual pretraining, <strong>context 128K→256K via YaRN</strong>, release in <strong>INT4</strong> with selective quantization (only routed experts quantized), and the “<strong>Agent Swarm</strong>” orchestration concept (dynamic generation of subagents; up to <strong>100 parallel subagents</strong> / <strong>1,500 steps</strong>; wall-time improvements claimed <strong>3–4.5×</strong>) is summarized by <a href=\""https://twitter.com/TheZachMueller/status/2016183468430860587\"">@TheZachMueller</a> (and points to the <a href=\""https://twitter.com/TheZachMueller/status/2016183781481132443\"">technical report</a>).</li>\n<li><strong>Benchmarks/third-party eval framing</strong>: Artificial Analysis positions K2.5 as “leading open weights” and closer to frontier labs, highlighting <strong>GDPval-AA Elo 1309</strong> (agentic knowledge work harness), <strong>MMMU Pro 75%</strong>, <strong>INT4 ~595GB</strong>, and a <strong>64% hallucination rate</strong> (improved vs K2 Thinking) among other stats: <a href=\""https://twitter.com/ArtificialAnlys/status/2016250137115557953\"">@ArtificialAnlys</a>. LMArena announcements also place K2.5 Thinking at <strong>#1 open model</strong> in their Text Arena snapshot: <a href=\""https://twitter.com/arena/status/2016294722445443470\"">@arena</a>. (Treat leaderboards as <em>point-in-time</em>; harness/tooling and prompting matter.)</li>\n<li><strong>Distribution and “runs at home” signals</strong>: K2.5 landed quickly across infra surfaces: <strong>Ollama cloud</strong> with launch integrations (<a href=\""https://twitter.com/ollama/status/2016086374005538932\"">@ollama</a>), Together AI listing (<a href=\""https://twitter.com/togethercompute/status/2016306907015938510\"">@togethercompute</a>), and Fireworks as a partner (<a href=\""https://twitter.com/Kimi_Moonshot/status/2016057073000448234\"">Moonshot</a>). A notable local-inference datapoint: K2.5 reportedly runs (slowly but “usable”) on <strong>2× M3 Ultra</strong> via MLX with sharded generation, ~<strong>21.9 tok/s</strong> at high memory use: <a href=\""https://twitter.com/awnihannun/status/2016221496084205965\"">@awnihannun</a> (+ command snippet <a href=\""https://twitter.com/awnihannun/status/2016223103081443342\"">here</a>).</li>\n<li><strong>Product surface area around Kimi</strong>: Moonshot also pushed adjacent tooling: <strong>Kimi Code</strong>, an <strong>Apache-2.0</strong> open-source coding agent integrating with common IDEs/editors (<a href=\""https://twitter.com/Kimi_Moonshot/status/2016034259350520226\"">announcement</a>), and an <strong>Agent SDK</strong> to build custom agents (<a href=\""https://twitter.com/Kimi_Moonshot/status/2016034272998809678\"">link</a>). A “Kimi Product” account is explicitly aimed at distributing prompts/use-cases (<a href=\""https://twitter.com/Kimi_Moonshot/status/2016082808834531825\"">launch</a>), with a viral demo of “<strong>video-to-code</strong>” website cloning (<a href=\""https://twitter.com/KimiProduct/status/2016081756206846255\"">demo</a>).</li>\n</ul>\n<p><strong>Open “American comeback” at scale: Arcee/Prime Intellect Trinity Large Preview (400B MoE)</strong></p>\n<ul>\n<li><strong>Trinity Large Preview release</strong>: Arcee dropped <strong>Trinity Large</strong> initial weights as a “preview” release: <a href=\""https://twitter.com/arcee_ai/status/2016278017572495505\"">@arcee_ai</a>, with expanded details from <a href=\""https://twitter.com/latkins/status/2016279374287536613\"">@latkins</a>. Prime Intellect frames it as an open <strong>400B MoE</strong> with <strong>13B active</strong> trained with Datology data: <a href=\""https://twitter.com/PrimeIntellect/status/2016280792037785624\"">@PrimeIntellect</a>. OpenRouter offered limited-time free access: <a href=\""https://twitter.com/OpenRouterAI/status/2016280059527757995\"">@OpenRouterAI</a>.</li>\n<li><strong>Architecture/training details (most concrete technical tweet)</strong>: A strong technical snapshot comes from <a href=\""https://twitter.com/samsja19/status/2016283855888773277\"">@samsja19</a>: <strong>400B/A13B MoE</strong>, trained over <strong>17T tokens</strong>; <strong>3:1 interleaved local/global gated attention</strong>, <strong>SWA</strong>, <strong>NoPE on global layers + RoPE on local layers</strong> (as written in tweet), <strong>depth-scaled sandwich norm</strong>, <strong>sigmoid routing</strong>, trained with <strong>Muon</strong>; trained on <strong>~2,000 B300s for a month</strong> on Prime Intellect infra, with data curation by DatologyAI.</li>\n<li><strong>Data scaling emphasis</strong>: Datology’s involvement is highlighted as a major part of the project: “<strong>6.5T tokens overall</strong>” and “<strong>800B synthetic code</strong>” (plus multilingual curation) in one team member’s recap: <a href=\""https://twitter.com/code_star/status/2016279734985097532\"">@code_star</a>. Separate recaps mention <strong>8T synthetic</strong> as part of 17T: <a href=\""https://twitter.com/pratyushmaini/status/2016287361274138821\"">@pratyushmaini</a>.</li>\n<li><strong>Ecosystem readiness</strong>: vLLM announced <strong>day-0 support</strong> for serving Trinity Large: <a href=\""https://twitter.com/vllm_project/status/2016322567364346331\"">@vllm_project</a>. The meta-story in the replies is that a Western org is again attempting <strong>frontier-ish pretraining from scratch</strong> with an open model, rather than only post-training/evals.</li>\n</ul>\n<p><strong>Agents everywhere: orchestration, subagents, planning critics, and IDE/CLI integration</strong></p>\n<ul>\n<li><strong>Agent “swarm” vs “subagents” convergence</strong>: Kimi’s “Agent Swarm” pitch (dynamic subagent creation) parallels the broader pattern of <em>central orchestrator + parallel specialists</em>. The most explicit “starter pattern” articulation is LangChain’s stateless subagent model (parallel execution + minimized context bloat): <a href=\""https://twitter.com/sydneyrunkle/status/2016285836581765461\"">@sydneyrunkle</a>. Meanwhile, Kimi’s swarm is framed as trainable orchestration via <strong>Parallel-Agent RL (PARL)</strong> in community summaries (<a href=\""https://twitter.com/TheZachMueller/status/2016183468430860587\"">Zach Mueller</a>).</li>\n<li><strong>Reliability via “critique before execute”</strong>: Google’s Jules introduced a <strong>Planning Critic</strong>—a second agent that critiques plans pre-execution, claiming a <strong>9.5% drop in task failure rates</strong>: <a href=\""https://twitter.com/julesagent/status/2016178107019837917\"">@julesagent</a>. Jules also added “Suggested Tasks” for proactive optimizations: <a href=\""https://twitter.com/julesagent/status/2016249221846864005\"">@julesagent</a>.</li>\n<li><strong>Coding-agent products intensifying</strong>: Mistral shipped <strong>Vibe 2.0</strong> upgrades (subagents, user-defined agents, skills/slash commands, and paid plans): <a href=\""https://twitter.com/mistralvibe/status/2016179799689928986\"">@mistralvibe</a> and <a href=\""https://twitter.com/qtnx_/status/2016180364771742047\"">@qtnx_</a>. MiniMax launched an “Agent Desktop” workspace pitched as more polished than Claude Cowork: <a href=\""https://twitter.com/omarsar0/status/2016149402923200634\"">@omarsar0</a> (and MiniMax’s own onboarding automation: <a href=\""https://twitter.com/MiniMax_AI/status/2016161539749990844\"">@MiniMax_AI</a>).</li>\n<li><strong>IDE infrastructure and retrieval</strong>: Cursor claims semantic search materially improves coding-agent performance and that indexing for large codebases is “orders of magnitude faster”: <a href=\""https://twitter.com/cursor_ai/status/2016202243499073768\"">@cursor_ai</a>. VS Code continues tightening agent UX (e.g., safer command execution explanations): <a href=\""https://twitter.com/aerezk/status/2016225215802397146\"">@aerezk</a>, plus MCP servers returning UI via MCP Apps spec (LIFX control panel example): <a href=\""https://twitter.com/burkeholland/status/2016208751200457088\"">@burkeholland</a>.</li>\n</ul>\n<p><strong>Document AI &#x26; multimodal systems: DeepSeek-OCR 2 and “Agentic Vision”</strong></p>\n<ul>\n<li><strong>DeepSeek-OCR 2: learned reading order + token compression</strong>: DeepSeek-OCR 2 is framed as a shift from fixed raster scans to learned <strong>Visual Causal Flow</strong> with <strong>DeepEncoder V2</strong>, including <strong>16× visual token compression (256–1120 tokens/image)</strong> and <strong>91.09% OmniDocBench v1.5 (+3.73%)</strong>; vLLM shipped day-0 support: <a href=\""https://twitter.com/vllm_project/status/2016065526058090967\"">@vllm_project</a>. Unsloth notes similar headline improvements: <a href=\""https://twitter.com/danielhanchen/status/2016043326760485313\"">@danielhanchen</a>.</li>\n<li><strong>Mechanistic intuition (why it matters for pipelines)</strong>: Jerry Liu provides a clear “why learned order helps” explanation: avoid semantically shredding tables/forms by allowing query tokens to attend to contiguous regions instead of strict left-to-right: <a href=\""https://twitter.com/jerryjliu0/status/2016319238974407146\"">@jerryjliu0</a>. Teortaxes adds a pragmatic eval take: OCR 2 is “on par with dots.ocr” and “nowhere near SOTA,” but the ideas may influence later multimodal products: <a href=\""https://twitter.com/teortaxesTex/status/2016179572056678739\"">@teortaxesTex</a>.</li>\n<li><strong>Gemini “Agentic Vision” = vision + code execution loop</strong>: Google is productizing a “Think, Act, Observe” loop where the model writes/executes Python to crop/zoom/annotate images, claiming <strong>5–10% quality boosts</strong> across many vision benchmarks: <a href=\""https://twitter.com/_philschmid/status/2016225242394296773\"">@_philschmid</a> and the official thread: <a href=\""https://twitter.com/GoogleAI/status/2016267526330601720\"">@GoogleAI</a>. This is an explicit move toward <em>tool-augmented vision</em> being first-class, not bolted on.</li>\n</ul>\n<p><strong>AI for science &#x26; research workflows: OpenAI Prism as “Overleaf with AI”</strong></p>\n<ul>\n<li><strong>Prism launch</strong>: OpenAI introduced <strong>Prism</strong>, a free “AI-native workspace for scientists” powered by <strong>GPT-5.2</strong>, positioned as a unified LaTeX collaboration environment: <a href=\""https://twitter.com/OpenAI/status/2016209462621831448\"">@OpenAI</a> and <a href=\""https://twitter.com/kevinweil/status/2016210486778642808\"">@kevinweil</a>. Community summaries frame it as “Overleaf with AI” (proofreading, citations, literature search): <a href=\""https://twitter.com/scaling01/status/2016211218633990391\"">@scaling01</a>.</li>\n<li><strong>Data/IP clarification</strong>: Kevin Weil clarified that Prism follows your ChatGPT data controls and that OpenAI is not taking a share of individual discoveries; any IP-alignment deals would be bespoke for large orgs: <a href=\""https://twitter.com/kevinweil/status/2016285175106420867\"">@kevinweil</a>.</li>\n<li><strong>Why it matters technically</strong>: Prism is a product bet that <strong>collaboration context + tool integration</strong> (LaTeX, citations, project state) becomes a durable advantage—mirroring the “context > intelligence” theme circulating in Chinese discussions about OpenAI infra and org design: <a href=\""https://twitter.com/ZhihuFrontier/status/2016068402457285032\"">@ZhihuFrontier</a>.</li>\n</ul>\n<p><strong>Research notes &#x26; benchmarks worth tracking (RL, planning, multilingual scaling)</strong></p>\n<ul>\n<li><strong>Long-horizon planning benchmark</strong>: <em>DeepPlanning</em> proposes verifiable-constraint planning tasks (multi-day travel, shopping) and reports frontier agents still struggle; emphasizes explicit reasoning patterns and parallel tool use: <a href=\""https://twitter.com/iScienceLuvr/status/2016122154862182792\"">@iScienceLuvr</a>. (This pairs nicely with the “travel planning again” meme: <a href=\""https://twitter.com/teortaxesTex/status/2016043107607879864\"">@teortaxesTex</a>.)</li>\n<li><strong>RL efficiency and reuse of traces</strong>: <em>PrefixRL</em> idea—condition on off-policy prefixes to speed RL on hard reasoning, claiming <strong>2× faster</strong> to same reward vs strong baseline: <a href=\""https://twitter.com/iScienceLuvr/status/2016125085825040852\"">@iScienceLuvr</a>.</li>\n<li><strong>Multilingual scaling laws</strong>: Google Research announced <strong>ATLAS</strong> scaling laws for massively multilingual LMs with data-driven guidance on balancing data mix vs model size: <a href=\""https://twitter.com/GoogleResearch/status/2016234343602258274\"">@GoogleResearch</a>.</li>\n<li><strong>Math research reality check</strong>: Epoch’s <strong>FrontierMath: Open Problems</strong> benchmark invites attempts; “AI hasn’t solved any of these yet”: <a href=\""https://twitter.com/EpochAIResearch/status/2016188014540816879\"">@EpochAIResearch</a>.</li>\n</ul>\n<hr>\n<h3>Top tweets (by engagement)</h3>\n<ul>\n<li>OpenAI launches <strong>Prism</strong> (AI LaTeX research workspace): <a href=\""https://twitter.com/OpenAI/status/2016209462621831448\"">@OpenAI</a></li>\n<li>Moonshot founder video introducing <strong>Kimi K2.5</strong>: <a href=\""https://twitter.com/Kimi_Moonshot/status/2016065333694771276\"">@Kimi_Moonshot</a></li>\n<li>Kimi “video-to-code” website cloning demo: <a href=\""https://twitter.com/KimiProduct/status/2016081756206846255\"">@KimiProduct</a></li>\n<li>Ollama: <strong>Kimi K2.5 on Ollama cloud</strong> + integrations: <a href=\""https://twitter.com/ollama/status/2016086374005538932\"">@ollama</a></li>\n<li>Claude generating <strong>3Blue1Brown-style animations</strong> claim (education impact): <a href=\""https://twitter.com/LiorOnAI/status/2016119374097084828\"">@LiorOnAI</a></li>\n<li>Figure introduces <strong>Helix 02</strong> autonomous whole-body robotics control: <a href=\""https://twitter.com/Figure_robot/status/2016207013236375661\"">@Figure_robot</a></li>\n</ul>\n<hr>\n<h1>AI Reddit Recap</h1>\n<h2>/r/LocalLlama + /r/localLLM Recap</h2>\n<h3>1. New Model and Benchmark Releases</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qo595n/introducing_kimi_k25_opensource_visual_agentic/\"">Introducing Kimi K2.5, Open-Source Visual Agentic Intelligence</a></strong> (Activity: 643): <strong><strong>Kimi K2.5</strong> is an open-source visual agentic intelligence model that achieves global state-of-the-art (SOTA) performance on agentic benchmarks, with scores of <code>50.2%</code> on the HLE full set and <code>74.9%</code> on BrowseComp. It also leads in open-source vision and coding benchmarks, scoring <code>78.5%</code> on MMMU Pro, <code>86.6%</code> on VideoMMMU, and <code>76.8%</code> on SWE-bench Verified. The model introduces an <strong>Agent Swarm</strong> feature in beta, allowing up to <code>100</code> sub-agents to work in parallel, making <code>1,500</code> tool calls and operating <code>4.5×</code> faster than a single-agent setup. Kimi K2.5 is available in chat and agent modes on <a href=\""https://kimi.com\"">kimi.com</a>, with additional resources on <a href=\""https://huggingface.co/moonshotai/Kimi-K2.5\"">Hugging Face</a>.</strong> A comment highlights the impressive capability of <code>100</code> sub-agents working in parallel, suggesting potential for enhanced performance in coding tasks. Another comment notes the banning of the original poster, raising questions about account authenticity.</p>\n<ul>\n<li><strong>Asleep_Strike746</strong> highlights the impressive capability of Kimi K2.5 to run 100 sub-agents in parallel, suggesting potential for complex task execution, such as coding tasks. This parallelism could significantly enhance performance in multi-threaded environments, making it a powerful tool for developers looking to automate or optimize workflows.</li>\n<li><strong>illusoryMechanist</strong> points out the scale of Kimi K2.5 with '1T Activated Parameters' and '32B' (likely referring to the model's parameter count), indicating a substantial computational capacity. This suggests that Kimi K2.5 could handle large-scale data processing and complex problem-solving tasks, positioning it as a competitive player in the open-source AI landscape.</li>\n<li><strong>Capaj</strong> shares a practical test of Kimi K2.5 by prompting it to generate an SVG of a fox on a unicycle. The result was described as 'not too bad', implying that while the model can handle creative tasks, there might still be room for improvement in terms of output quality or creativity. This kind of testing is crucial for understanding the model's capabilities in real-world applications.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qo3ri5/jan_v3_instruct_a_4b_coding_model_with_40_aider/\"">Jan v3 Instruct: a 4B coding Model with +40% Aider Improvement</a></strong> (Activity: 333): <strong>The image is a bar chart titled \""Aider Benchmark\"" that illustrates the performance of various coding models in terms of their pass rate for polyglot code editing. The \""Jan-v3-4B-base-INSTRUCT\"" model leads with a score of <code>18</code>, significantly outperforming other models like \""Qwen3-4B-THINKING-2507\"" with <code>12.1</code> and \""Ministral-3-8B-INSTRUCT-2512\"" with <code>6.8</code>. This highlights the Jan-v3 model's high efficiency and over <code>40%</code> improvement in performance, showcasing its enhanced capabilities in coding tasks. The model is designed for improved math and coding performance, making it a strong candidate for lightweight assistance and further fine-tuning.</strong> One commenter appreciates the Qwen 4B 2507 model for small tasks, noting its impressive performance despite its size. Another user shares mixed experiences with the Jan model, praising its ability to use search tools effectively but noting occasional tool call failures and odd responses, possibly due to system prompts.</p>\n<ul>\n<li>The Jan v3 Instruct model, a 4 billion parameter coding model, reportedly achieves a 40% improvement in performance with the Aider benchmark. This suggests significant advancements in its ability to handle coding tasks, potentially outperforming other models like Qwen 4B 2507 in specific scenarios. The model's ability to utilize search tools effectively for code explanation is noted, although there are occasional failures in tool calls and some system prompt issues in web chat applications.</li>\n<li>A user reported mixed experiences with the Jan v3 model on <a href=\""http://chat.jan.ai\"">chat.jan.ai</a>, highlighting its capability to correctly use search tools and read code for explaining project flows. However, they also noted some tool call failures and irrelevant responses, possibly due to system prompts. The user expressed interest in the model's potential integration with Claude Code, suggesting it could become a valuable tool for code search and Q&#x26;A in daily coding tasks.</li>\n<li>The Jan v3 model's performance in benchmarks is highlighted, with a specific mention of its demo availability at <a href=\""https://chat.jan.ai\"">chat.jan.ai</a>. The model's ability to handle small and easy tasks effectively is compared to Qwen 4B 2507, which is favored for similar tasks. The discussion suggests that Jan v3's fine-tuning may offer competitive advantages in certain coding scenarios.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qo349m/deepseekaideepseekocr2_hugging_face/\"">deepseek-ai/DeepSeek-OCR-2 · Hugging Face</a></strong> (Activity: 385): <strong><strong>DeepSeek-OCR-2</strong> is a state-of-the-art OCR model available on <a href=\""https://huggingface.co/deepseek-ai/DeepSeek-OCR-2\"">Hugging Face</a>, optimized for document processing with visual causal flow. It requires <code>Python 3.12.9</code> and <code>CUDA 11.8</code>, and leverages libraries like <code>torch</code> and <code>transformers</code>. The model supports dynamic resolution and uses flash attention for enhanced performance on NVIDIA GPUs. It offers various prompts for document conversion, making it versatile for different OCR tasks.</strong> One user highlighted the impressive performance of <strong>PaddleOCR-VL</strong> when compared using scores from other models, suggesting its potential superiority. Another user shared a demo of DeepSeek OCR 2, noting initial issues with repetition due to user error, which were resolved by adjusting decoding parameters, leading to significantly improved performance over version 1.</p>\n<ul>\n<li>A user highlighted the impressive performance of PaddleOCR-VL, suggesting it stands out when compared to other models like B/C/D. This is based on scores reported by a third party, which the user trusts for evaluating model performance. This implies PaddleOCR-VL's metrics are noteworthy in the context of OCR model comparisons.</li>\n<li>Another user shared their experience with implementing a demo for DeepSeek OCR 2 using GPU credits. Initially, they faced issues with repetition due to incorrect parameters, but after adjusting to DeepSeek's recommended decoding parameters, the performance improved significantly. The user noted that the updated version is much more reliable than its predecessor, DeepSeek OCR v1.</li>\n<li>The GitHub repository and paper for DeepSeek OCR 2 were shared, providing resources for those interested in the technical details and implementation of the model. The paper likely contains in-depth information on the model's architecture, training process, and performance benchmarks, which are crucial for technical evaluation and understanding.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qnk7fq/transformers_v5_final_is_out/\"">transformers v5 final is out 🔥</a></strong> (Activity: 503): <strong><strong>Transformers v5</strong> from <strong>Hugging Face</strong> introduces significant performance improvements, particularly for Mixture-of-Experts (MoE) models, achieving <code>6x-11x</code> speedups. The update simplifies the API by removing slow/fast tokenizers, offering explicit backends and enhanced performance. Additionally, dynamic weight loading is now faster, supporting MoE with quantization, tensor parallelism, and Parameter-Efficient Fine-Tuning (PEFT). A <a href=\""https://github.com/huggingface/transformers\"">migration guide</a> and detailed release notes are available for users transitioning to this version.</strong> One user inquired about the implications of these improvements for running small to medium-sized MoE models locally, suggesting that the enhancements might reduce memory bandwidth constraints. Another user reported a <code>50%</code> increase in single prompt inference speed and a <code>100%</code> increase in concurrent inference speed after updating to v5 and vllm 0.14.1.</p>\n<ul>\n<li>The Mixture-of-Experts (MoE) model in Transformers v5 shows significant performance improvements, with reported speedups ranging from 6x to 11x. This is particularly relevant for users running models locally, as it suggests that MoE can now utilize compute resources more efficiently, potentially reducing memory bandwidth constraints. This could be beneficial for setups using NVIDIA GPUs or AMD iGPUs, such as the Strix Halo, where compute power is a limiting factor.</li>\n<li>A user reported upgrading to Transformers v5 and vllm 0.14.1 from 0.11, resulting in a 50% increase in single prompt inference speed and a 100% increase in concurrent inference speed for 40x workloads. This highlights the significant performance enhancements in the latest version, which could be crucial for applications requiring high throughput and low latency.</li>\n<li>The update in Transformers v5 now allows Mixture-of-Experts (MoE) models to work with quantized models, which was not possible before. This advancement enables more efficient model deployment by reducing the model size and computational requirements, making it feasible to run complex models on less powerful hardware without sacrificing performance.</li>\n</ul>\n</li>\n</ul>\n<h3>2. Local LLM Hardware and Setup Discussions</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qni356/216gb_vram_on_the_bench_time_to_see_which/\"">216GB VRAM on the bench. Time to see which combination is best for Local LLM</a></strong> (Activity: 577): <strong>The post discusses the use of secondhand Tesla GPUs, which offer substantial VRAM at a lower cost, for local large language model (LLM) testing. The author has developed a <a href=\""https://esologic.com/gpu-server-benchmark/#gpu-box-benchmark\"">GPU server benchmarking suite</a> to evaluate the performance of these older GPUs when used in parallel. The image shows a setup with multiple NVIDIA GPUs, highlighting the focus on maximizing VRAM for machine learning tasks. The technical challenge lies in effectively utilizing these GPUs without significant bandwidth loss, as most affordable server motherboards support only a limited number of GPUs.</strong> Commenters express skepticism about the practicality of using older Tesla GPUs due to potential issues with token processing speed and cooling requirements. There is interest in how the author manages to connect multiple GPUs without bandwidth loss, and a suggestion that newer systems like DGX Spark might offer better performance for certain tasks.</p>\n<ul>\n<li>HugoCortell raises a technical concern about the bandwidth limitations when connecting multiple GPUs to a single PC, noting that most affordable server motherboards support only a few GPUs. This could lead to a significant loss in bandwidth, which is crucial for efficient parallel processing in local LLM setups.</li>\n<li>BananaPeaches3 highlights a critical performance issue with older GPUs, particularly in handling large system prompts. They mention that while token generation speed might be acceptable, the prompt processing time can be a bottleneck, especially with prompts as large as 15k tokens. This suggests that newer systems like the DGX Spark might be more efficient despite slightly slower token generation speeds, due to faster prompt processing capabilities.</li>\n<li>FullOf_Bad_Ideas points out a limitation in the gpu_box_benchmark, which does not test for serving large models split across multiple GPUs. This is a significant use case for setups with high VRAM, indicating a gap in the benchmark's ability to evaluate real-world performance for large-scale LLM applications.</li>\n</ul>\n</li>\n</ul>\n<h3>3. Teasers and Announcements from AI Labs</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qoa8rp/the_qwen_devs_are_teasing_something/\"">The Qwen Devs Are Teasing Something</a></strong> (Activity: 331): <strong>The image is a tweet from <strong>Tongyi Lab</strong> featuring an ASCII art face and a lightning bolt emoji, hinting at an upcoming announcement. The Reddit community speculates that this could be related to a new visual language model, possibly named <strong>Z-Image</strong>, which has been mentioned in recent <strong>ComfyUI</strong> pull requests. The timing of the announcement might be strategically planned before the Chinese New Year, aligning with other anticipated releases like <strong>K2.5</strong> and potentially <strong>q3.5</strong>, <strong>dsv4</strong>, and <strong>mm2.2</strong>.</strong> Commenters are speculating that the announcement is related to the Z-Image model, which has been referenced in recent updates to ComfyUI. There is also a discussion about the timing of the release, suggesting it might be aligned with the Chinese New Year.</p>\n<ul>\n<li>The mention of 'Z-Image' in ComfyUI PRs suggests a potential new feature or model update related to image processing. This aligns with recent updates where hidden items have been added to collections, indicating ongoing development and testing phases.</li>\n<li>There is speculation about the release of several models and updates before the Chinese New Year, including K2.5, q3.5, dsv4, and mm2.2. This timing is strategic as many labs aim to release updates before the holiday break, which is on January 17th this year.</li>\n<li>A user speculates about the release of 'Qwen4 Next 48B A3B', which could imply a new model or version with specific parameters, possibly indicating advancements in model architecture or capabilities.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qnfegx/minimax_is_teasing_m22/\"">Minimax Is Teasing M2.2</a></strong> (Activity: 322): <strong>The image is a tweet from <strong>MiniMax</strong> teasing an update to their AI model, M2.2, suggesting an imminent release with the phrase \""M2.1 slays. M2.2 levels up. #soon.\"" This indicates a potential upgrade in capabilities or performance from the previous version, M2.1. The context suggests a competitive landscape in AI development, particularly among Chinese labs, with other models like Deepseek v4 and Kimi K3 also expected soon. The mention of ByteDance's potential closed-source model adds to the competitive tension in the AI space.</strong> One comment suggests a shift in focus towards agentic Mixture of Experts (MoEs) models, potentially at the expense of updates to traditional 32B models. Another user expresses anticipation for the new model, highlighting the effectiveness of MiniMax 2.1 in combination with glm 4.7 for coding tasks, and the potential impact of the upcoming versions.</p>\n<ul>\n<li>Loskas2025 highlights the use of Minimax 2.1 and GLM 4.7 for coding, noting their excellence. They anticipate that the upcoming Minimax 2.2 and GLM 5, which is currently in training, could significantly enhance performance, suggesting a potential shift in the landscape of coding models.</li>\n<li>CriticallyCarmelized compares Minimax favorably to GLM 4.7, even at high quantization levels, indicating that Minimax is competitive in terms of performance. They express optimism that the new version could surpass current models, potentially becoming their preferred choice for local deployment.</li>\n<li>lacerating_aura mentions speculation around 'giga-potato' being associated with DS4, but points out the lack of concrete evidence for the existence of DS4 or Kimi K3, indicating a gap in confirmed information about these models.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qnjota/i_built_a_hive_mind_for_claude_code_7_agents/\"">I built a \""hive mind\"" for Claude Code - 7 agents sharing memory and talking to each other</a></strong> (Activity: 422): <strong>The post describes a multi-agent orchestration system for <strong>Claude Code</strong>, featuring 7 specialized agents (e.g., coder, tester, reviewer) that coordinate tasks, share persistent memory using <code>SQLite + FTS5</code>, and communicate via a message bus. The system runs as an MCP server and integrates with <strong>Anthropic</strong>, <strong>OpenAI</strong>, or <strong>Ollama</strong>. It uses a task queue for priority-based coordination, allowing agents to pass context and collaborate effectively. The stack includes <strong>TypeScript</strong>, <strong>better-sqlite3</strong>, <strong>MCP SDK</strong>, and <strong>Zod</strong>. The project is experimental, MIT licensed, and available on <a href=\""http://github.com/blackms/aistack\"">GitHub</a>.</strong> A comment questions the similarity to the <a href=\""https://github.com/bmad-code-org/BMAD-METHOD\"">bmad method</a>, suggesting potential overlap in approach. Another comment humorously questions whether the agents agree with each other, hinting at the complexity of multi-agent consensus.</p>\n<ul>\n<li>The project is compared to the <a href=\""https://github.com/bmad-code-org/BMAD-METHOD\"">BMAD method</a>, which also involves multi-agent systems. The commenter is curious about the differences, suggesting that the approach might be similar in terms of agents sharing memory and communication protocols.</li>\n<li>A reference is made to Microsoft's <a href=\""https://github.com/microsoft/autogen\"">Autogen</a>, which was released over two years ago as a solution for multi-agent systems. The commenter suggests exploring this resource for potential new ideas, indicating that the concept of multi-agent communication and shared memory is not new and has been explored by major tech companies.</li>\n<li>The choice of using Claude Code is questioned, with a suggestion to consider open-source alternatives. This implies a debate on the benefits of proprietary versus open-source platforms for developing multi-agent systems, hinting at potential advantages in community support and collaboration in open-source projects.</li>\n</ul>\n</li>\n</ul>\n<h2>Less Technical AI Subreddit Recap</h2>\n<blockquote>\n<p>/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo</p>\n</blockquote>\n<h3>1. Kimi K2.5 and Open Source AI Model Releases</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/singularity/comments/1qoojio/open_source_kimik25_is_now_beating_claude_opus_45/\"">Open source Kimi-K2.5 is now beating Claude Opus 4.5 in many benchmarks including coding.</a></strong> (Activity: 597): <strong><strong>Kimi-K2.5</strong>, an open-source model, is reportedly outperforming <strong>Claude Opus 4.5</strong> in several benchmarks, notably in coding tasks. However, the specifics of these benchmarks and the extent of the performance gains are not detailed in the post. The claim suggests a significant advancement in open-source AI capabilities, but lacks comprehensive data to substantiate the comparison.</strong> Commenters express skepticism about the claim, highlighting that benchmarks may not fully represent real-world performance. They question the validity of the term 'many' benchmarks and suggest that the practical utility of Kimi-K2.5 compared to Claude Opus 4.5 remains unproven.</p>\n<ul>\n<li>There is skepticism about the claim that Kimi-K2.5 is outperforming Claude Opus 4.5 in real-world applications, despite benchmark results. Users argue that benchmarks often don't reflect practical utility, especially in complex tasks like programming where Opus 4.5 might excel in providing solutions in a single prompt.</li>\n<li>The discussion highlights a common critique of benchmarks: they may not capture the full capabilities of a model in practical scenarios. Some users express doubt about the claim that Kimi-K2.5 surpasses Opus 4.5, questioning the specific benchmarks and real-world applicability, especially in coding tasks where Opus 4.5 is perceived to have an edge.</li>\n<li>One user claims significant practical success with Kimi-K2.5, stating it has replaced reports in a major company, suggesting that at least in some contexts, Kimi-K2.5 may offer substantial utility. This contrasts with the general skepticism about benchmarks translating to real-world performance.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/singularity/comments/1qo531i/kimi_k25_released/\"">Kimi K2.5 Released!!!</a></strong> (Activity: 1149): <em><em>The image presents a performance comparison chart for the newly released <strong>Kimi K2.5</strong>, which is claimed to set a new state-of-the-art (SOTA) in agentic tasks. The chart compares Kimi K2.5 against other models like <strong>GPT-5.2 (xhigh)</strong>, <strong>Claude Opus 4.5</strong>, and <strong>Gemini 3 Pro</strong> across various tasks, including agents, coding, image, and video tasks. Kimi K2.5 is highlighted as leading in several categories, notably \""Agents: BrowseComp\"" and \""Image: OmniDocBench 1.5</em>\"", suggesting its superior performance in these areas. The release is accompanied by a blog post detailing the advancements (<a href=\""https://www.kimi.com/blog/kimi-k2-5.html\"">link</a>).</em>* Commenters express skepticism about the benchmarks, questioning if they are cherry-picked, and discuss the model's performance in hallucination and instruction-following tests. One user notes that Kimi K2.5, while improved, still outputs incorrect answers confidently, similar to other models like Gemini 3, which also confidently provides incorrect answers. GPT-5.1 and 5.2 are noted for admitting \""I don't know\"" in similar tests, highlighting ongoing challenges with hallucinations in AI models.</p>\n<ul>\n<li>A user conducted a test on Kimi K2.5's ability to follow instructions by asking it to identify a specific math contest problem without web search. The model listed out hallucinated contest problems and second-guessed itself, ultimately providing an incorrect answer. This is seen as a slight improvement over Kimi K2, which failed to follow instructions and timed out. In comparison, Gemini 3 also confidently provided incorrect answers, while GPT 5.1 and 5.2 were the only models to admit 'I don't know'.</li>\n<li>The concept of an 'agent swarm' in Kimi K2.5 is intriguing, with speculation that it involves over 100 instances of the model being directed by a single overseeing instance. This setup is expected to be expensive, and there is curiosity about whether it could be a single model handling multiple tasks simultaneously, which would represent a significant advancement. The idea of scaffolding, where multiple models work together, seems more plausible to some users.</li>\n<li>There is skepticism about the benchmarks used to compare Kimi K2.5 with other models like Gemini 3. A user questions whether the benchmarks are cherry-picked, expressing doubt that Kimi K2.5 consistently outperforms Gemini 3, which seems unlikely given the current state of model capabilities.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/ClaudeAI/comments/1qod7ej/sir_the_chinese_just_dropped_a_new_open_model/\"">Sir, the Chinese just dropped a new open model</a></strong> (Activity: 1915): <strong><strong>Kimi</strong> has released an open-source trillion-parameter vision model that reportedly matches the performance of <strong>Opus 4.5</strong> on several benchmarks. This model is significant due to its scale and the claim of competitive performance, which is notable given the typically high cost and complexity associated with such large models. The release could impact the landscape of AI vision models, especially in terms of accessibility and cost-effectiveness.</strong> There is skepticism in the community about the true performance of Chinese models, with some users suggesting that while they are cost-effective, they may not genuinely match the capabilities of models like <strong>Claude</strong>, <strong>GPT</strong>, or <strong>Gemini</strong> despite benchmark claims.</p>\n<ul>\n<li>Tricky-Elderberry298 highlights the limitations of relying solely on benchmarks for evaluating LLMs, drawing an analogy to evaluating cars based only on engine specs. They argue that real-world usage, such as how models like Claude and Kimi K2.5 perform in complex projects, is a more meaningful measure of capability than pure benchmark scores.</li>\n<li>Durable-racoon discusses the unique capabilities of the Kimi K2.5 model, noting its ability to orchestrate 500 agents simultaneously and convert videos into working software UI prototypes. They also mention its superior performance in creative writing compared to Opus, while acknowledging that Kimi K2.5 is more expensive than most Chinese models, priced at $0.60/$3 for in/out operations.</li>\n<li>DistinctWay9169 points out that many Chinese models, such as Minimax and GLM, are often 'bench maxed,' meaning they perform well on benchmarks but may not match the real-world performance of models like Claude, GPT, or Gemini. This suggests a discrepancy between benchmark results and actual usability or effectiveness in practical applications.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/Bard/comments/1qokrpq/gemini_3_finally_has_an_opensource_competitor/\"">Gemini 3 finally has an open-source competitor</a></strong> (Activity: 168): <strong>The image is a comparison chart that highlights the performance of the newly released <strong>Kimi K2.5</strong> vision model against other prominent models like <strong>Gemini 3 Pro</strong>. According to the chart, Kimi K2.5 performs competitively, often surpassing Gemini 3 Pro in various benchmarks such as \""Humanity's Last Exam,\"" \""BrowseComp,\"" and \""OmniDocBench 1.5.\"" This positions Kimi K2.5 as a strong open-source alternative to the closed-source Gemini 3 Pro, challenging its dominance in the field.</strong> Some users express skepticism about Kimi K2.5's real-world performance compared to Gemini 3 Pro, with comments suggesting that while the benchmarks are impressive, practical performance may not match up. There is also a sentiment that open-source models may struggle to compete with large, closed-source companies.</p>\n<ul>\n<li>MichelleeeC highlights a significant performance gap between the open-source competitor and Gemini 3, particularly when tested on niche topics without search engine assistance. This suggests that the open-source model may lack the comprehensive training data or fine-tuning that Gemini 3 benefits from, impacting its ability to provide accurate answers in specialized areas.</li>\n<li>Old_Technology3399 and Just_Lingonberry_352 both express that the open-source competitor is notably inferior to Gemini 3. This consensus indicates that while the open-source model may be a step towards democratizing AI, it still falls short in terms of performance and reliability compared to established, closed-source models like Gemini 3.</li>\n<li>ChezMere's comment about 'benchhacking' suggests skepticism about the open-source model's real-world performance versus its benchmark results. This implies that while the model might perform well in controlled tests, it may not translate to effective real-world application, highlighting a common issue in AI model evaluation.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/DeepSeek/comments/1qne7vt/enterpriseready_open_sourcechinese_ais_are_poised/\"">Enterprise-ready open source/Chinese AIs are poised to out-sell American proprietary models. Personal investors take note.</a></strong> (Activity: 30): <strong>The post highlights the competitive edge of open-source and Chinese AI models over American proprietary models in niche domains, emphasizing their cost-effectiveness and comparable performance. Notable models include <strong>DeepSeek-V3 / R1</strong>, which ranks #1 on MATH-500 and LiveCodeBench, and <strong>Qwen3-Max / Coder</strong> from Alibaba, which excels in LMSYS Chatbot Arena and MMLU-Pro. These models offer significantly lower costs per million tokens compared to proprietary models like OpenAI's GPT-5.2 and Claude 4.5 Sonnet, with input costs as low as <code>$0.15</code> to <code>$0.60</code> per million tokens, compared to proprietary costs starting at <code>$3.00</code>. The post suggests that personal investors should consider these developments as Chinese firms issue IPOs, with <strong>a16z</strong> noting that 80% of startups pitching them use Chinese open-source AI models.</strong> A comment questions whether Kimi K2 is superior to GLM 4.7, indicating a debate on the relative performance of these models in specific contexts.</p>\n<ul>\n<li>The discussion compares the performance of the Kimi K2 model with the GLM 4.7 model. Kimi K2 is noted for its efficiency in specific tasks, potentially outperforming GLM 4.7 in certain benchmarks. However, the choice between these models may depend on the specific use case, as GLM 4.7 might excel in different areas. The conversation highlights the importance of evaluating models based on task-specific performance metrics rather than general claims of superiority.</li>\n</ul>\n</li>\n</ul>\n<h3>2. Gemini AI Studio and Usage Limitations</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/Bard/comments/1qnsqzl/gemini_ai_studio_is_basically_unusable_now_any/\"">Gemini AI Studio is basically unusable now. Any other LLMs with a 1M context window?</a></strong> (Activity: 162): <strong><strong>Gemini AI Studio</strong> has become less viable for users due to Google's reduction in daily prompt limits, impacting workflows that rely on its <code>1 million token</code> context window. Users working with extensive documents and conversations are seeking alternatives. Notably, <strong>Grok 4.1</strong> offers a <code>2 million token</code> context window, and <strong>Claude Sonnet 4.5</strong> provides a <code>1 million token</code> context window within the Kilo Code environment. These alternatives may serve users needing large-context capabilities.</strong> Some users suggest that effective CLI tools like Claudie-cli or codex-cli can mitigate the need for massive context windows by efficiently managing and retrieving information from large texts.</p>\n<ul>\n<li>Coldshalamov mentions that <strong>Grok 4.1 fast</strong> offers a <code>2M</code> context window, which is double the size of the <code>1M</code> context window being discussed. This suggests that Grok 4.1 fast could be a viable alternative for those needing larger context windows.</li>\n<li>Unlucky_Quote6394 highlights that <strong>Claude Sonnet 4.5</strong> provides a <code>1M</code> context window when used within <strong>Kilo Code</strong>, indicating another option for users seeking large context capabilities.</li>\n<li>Ryanmonroe82 suggests embedding documents as an alternative to using cloud models, implying that this method could be more efficient and effective for handling large text data without relying on extensive context windows.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/GeminiAI/comments/1qnvbjr/32768_or_215_tokens_in_hot_memory_gemini_has_been/\"">32,768 or (2^15) tokens in hot memory.... Gemini has been PURPOSELY THROTTLED by Alphabet and been made into a bait and switch. Gemini Pro is WORSE than the free version as of TODAY. They market over a million tokens for Pro users. This is fraud.</a></strong> (Activity: 858): <strong>The Reddit post claims that <strong>Alphabet</strong> has intentionally throttled the token limit for <strong>Gemini Pro</strong> to <code>32,768 tokens</code>, which is significantly lower than the advertised capacity of over a million tokens. This throttling allegedly degrades the performance of Gemini Pro, making it less effective than the free version. The post also mentions that the Ultra and Enterprise versions have a hard cap of <code>131,072 tokens</code>, despite advertising up to 2 million tokens. The author expresses concern that this limitation could drive users away, especially with potential integration into <strong>Siri</strong>.</strong> Commenters express dissatisfaction with Gemini's performance, comparing it unfavorably to older models like GPT-3. There is also criticism of the memory management, with claims that it leads to data inaccuracies and inefficiencies.</p>\n<ul>\n<li>Substantial_Net9923 highlights a significant issue with Gemini's memory management, noting that the model's memory loss due to indexing is problematic. This inefficiency is particularly evident in quantitative finance trading discussions, where the model is reportedly generating inaccurate data more frequently than before, suggesting a decline in reliability.</li>\n<li>klopppppppp observes a drastic decline in Gemini's performance, comparing it to older models like GPT-3. Despite this, they note that Gemini still performs exceptionally well in 'deep research mode,' indicating that the model's capabilities might be context-dependent or throttled in certain scenarios.</li>\n<li>SorryDistribution604 expresses frustration with Gemini's recent performance, likening it to older models such as GPT-3. This suggests a perceived regression in the model's capabilities, which could be due to throttling or other limitations imposed on the Pro version.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/Bard/comments/1qngot0/about_the_recent_ai_studio_limit_downgrade/\"">About the recent AI Studio Limit Downgrade:</a></strong> (Activity: 660): <strong>The image is a notification from the Gemini API about a reduction in free usage limits for AI Studio users, suggesting a transition to using an API key for continued access. It indicates that these limits may decrease further over time, and mentions ongoing efforts to integrate with Google AI Pro/Ultra to share limits within AI Studio. This change reflects a broader trend of tightening access to free AI resources, potentially impacting developers relying on these tools for experimentation and development.</strong> Commenters express frustration over the reduction in free usage limits, noting that Gemini's performance in following instructions has also declined. There is a sentiment that these changes are detrimental to AI Studio's utility, as users feel they are receiving less value and functionality.</p>\n<ul>\n<li>trashyslashers highlights a significant issue with the Gemini model's performance, noting that it is 'getting worse at listening to instructions.' This suggests a degradation in the model's ability to follow user commands, which is compounded by the reduction in daily usage limits. Users are forced to 'rewrite and regenerate' requests, indicating inefficiencies in the model's processing capabilities.</li>\n<li>Decent_Ingenuity5413 raises concerns about the stability and reliability of AI Studio's service, drawing parallels to OpenAI's past issues with unexpected changes. The comment also points out a critical billing issue with the Gemini API, where users have experienced 'massive overbilling' due to token counting errors, leading to charges exceeding $70,000. This highlights a significant flaw in the billing system that could deter average consumers from using the API.</li>\n<li>Sensitive_Shift1489 expresses frustration over the perceived downgrading of AI Studio in favor of other Google AI products like Gemini App and CLI. The comment implies that these changes are part of a broader strategy to shift focus and resources, potentially at the expense of AI Studio's quality and user satisfaction.</li>\n</ul>\n</li>\n</ul>\n<h3>3. Qwen Model Performance and Applications</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/Qwen_AI/comments/1qnkd7p/qwen3maxthinking_comparible_performance_to/\"">Qwen3-Max-Thinking - Comparible performance to Commercial Models</a></strong> (Activity: 40): <strong><strong>Qwen3-Max-Thinking</strong> is an AI model that claims to offer performance comparable to commercial models, focusing on enhanced reasoning and decision-making capabilities. The model's architecture and training methodologies are designed to improve efficiency and accuracy in complex tasks, as detailed in the <a href=\""https://qwen.ai/blog?id=qwen3-max-thinking\"">original article</a>. However, users have reported issues with the model's agentic code mode, which fails to compile, potentially impacting its usability.</strong> One user expressed skepticism about the model's usability due to compilation issues, while another hoped that Qwen3-Max-Thinking could help reduce the cost of commercial models.</p>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/Qwen_AI/comments/1qnun1t/qwen_model_we_get_it_qwen3maxthinking/\"">Qwen model. We get it! Qwen-3-max-thinking</a></strong> (Activity: 26): <strong>The post announces the release of the <strong>Qwen-3-max-thinking</strong> model, which is expected to be available this week. This model is noted for its enhanced features, although specific details about these enhancements are not provided in the post. The mention of 'P.S. We got it' suggests that the model is already accessible to some users.</strong> One commenter questions whether the model has been available since October, indicating possible confusion or overlap with previous releases. Another asks if 'OS' is being referred to, suggesting a potential misunderstanding or need for clarification on whether the model is open-source.</p>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/Qwen_AI/comments/1qohg5k/3_billion_tokensevaluate_my_token_usage_am_i_the/\"">3 Billion tokens！Evaluate my token usage? (Am I the most loyal user of QWEN3-MAX?)</a></strong> (Activity: 20): <strong>The post discusses a significant usage of the <strong>QWEN3-MAX</strong> language model, with the user consuming <code>3-4 billion tokens per day</code>. This high usage has led to <strong>DAMO Academy</strong> granting additional concurrency and early access to the upcoming <strong>Qwen3.5-MAX</strong>. The user attributes a drop in usage to the weekend, indicating a consistent high demand otherwise. The post highlights the model's effectiveness, with the user describing it as the 'best LLM in the world'.</strong> Comments reveal a mix of curiosity and comparison, with one user noting their own high token consumption of <code>4 billion</code> using a local model from the QWEN series. Another user shares a positive experience with the model's ability to optimize website copywriting, though they express concerns about accessing the model for coding tasks.</p>\n<ul>\n<li>Available-Craft-5795 mentions using 4 billion tokens with the QWEN series, indicating a high level of engagement with these models. This suggests that the QWEN series is capable of handling large-scale token processing, which could be beneficial for extensive applications such as data analysis or large-scale content generation.</li>\n<li>Remarkable_Speed1402 discusses using the new model for optimizing website homepage copywriting, noting its effectiveness. However, they express concerns about the model's coding capabilities, as they are unable to access it in their IDE. This highlights potential limitations in integrating the model with development environments, which could impact its usability for coding tasks.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/Qwen_AI/comments/1qonzjf/benchmark_of_qwen332b_reveals_12x_capacity_gain/\"">Benchmark of Qwen3-32B reveals 12x capacity gain at INT4 with only 1.9% accuracy drop</a></strong> (Activity: 10): <strong>The benchmark of <strong>Qwen3-32B</strong> on a single <strong>H100</strong> GPU demonstrates a significant capacity gain when using <code>INT4</code> quantization, achieving a <code>12x</code> increase in user capacity compared to <code>BF16</code>, with only a <code>1.9%</code> drop in accuracy. The study involved over <code>12,000</code> MMLU-Pro questions and <code>2,000</code> inference runs, showing that <code>INT4</code> can support <code>47</code> concurrent users at a <code>4k</code> context, compared to just <code>4</code> users with <code>BF16</code>. The full methodology and data are available <a href=\""https://research.aimultiple.com/llm-quantization/\"">here</a>.</strong> A comment raised a question about the model's performance in coding tasks, suggesting interest in how quantization affects specific application areas beyond general benchmarks.</p>\n<ul>\n<li>The discussion focuses on the performance of the Qwen3-32B model when quantized to INT4, highlighting a significant <code>12x</code> increase in capacity with a minimal <code>1.9%</code> drop in accuracy. This suggests that the model maintains high performance even with aggressive quantization, which is crucial for deploying large models in resource-constrained environments. However, the impact on specific tasks like coding remains a point of interest, as quantization can affect different tasks in varying ways.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h1>AI Discord Recap</h1>\n<blockquote>\n<p>A summary of Summaries of Summaries by Gemini 3.0 Pro Preview Nov-18</p>\n</blockquote>\n<p><strong>Theme 1. Kimi K2.5 Launch: SOTA Agentic Benchmarks and Swarm Capabilities</strong></p>\n<ul>\n<li><strong>Kimi K2.5 Crushes Agentic Benchmarks</strong>: Moonshot AI released <strong>Kimi K2.5</strong>, achieving global SOTA on the <strong>HLE full set (50.2%)</strong> and <strong>BrowseComp (74.9%)</strong>, while posting open-source SOTA on <strong>MMMU Pro (78.5%)</strong> and <strong>SWE-bench Verified (76.8%)</strong> <a href=\""http://kimi.com/blogs/kimi-k2-5.html\"">Tech Blog</a>. Users across Discords noted the model was \""silently rolled out\"" with significantly improved <strong>fact-checking</strong> and <strong>vision capabilities</strong> before the official announcement.</li>\n<li><strong>Agent Swarm Mode Enters Beta</strong>: The release introduces an <strong>Agent Swarm</strong> feature capable of orchestrating up to <strong>100 sub-agents</strong> and executing <strong>1,500 tool calls</strong> in parallel, promising a <strong>4.5x</strong> performance boost on complex tasks. High-tier users can access this self-directed mode on <a href=\""http://kimi.com\"">kimi.com</a>, though early testers noted it consumes tool-call quotas rapidly.</li>\n<li><strong>Pricing and API Instability Spark Debate</strong>: While the model's capabilities impressed users, the new <strong>Kimi Code plan</strong> drew criticism for lower limits compared to competitors like <strong>Z.ai</strong>, with promotional pricing ending in February. Integration with <strong>OpenRouter</strong> faced initial hiccups, with users reporting errors related to <strong>tool use endpoints</strong> and image URL handling.</li>\n</ul>\n<p><strong>Theme 2. Hardware Acceleration: Unsloth Speedups, FlagOS, and Kernel Ops</strong></p>\n<ul>\n<li><strong>Unsloth Accelerates MoE Training by 14x</strong>: Unsloth announced that <strong>MoE training</strong> is now <strong>14x faster</strong> than v4, with upcoming optimizations projected to double that speed again for a total <strong>30x boost</strong>. The team also rolled out full support for <strong>transformers v5</strong>, streamlining workflows for users on the latest library versions <a href=\""https://x.com/UnslothAI/status/2015935368525447395\"">Announcement</a>.</li>\n<li><strong>FlagOS Targets Unified AI Stacks</strong>: Engineers discussed the introduction of <strong>FlagOS</strong>, an open-source system software stack designed to unify <strong>Model–System–Chip layers</strong> for better workload portability across heterogeneous hardware. The project aims to incorporate insights from <strong>hardware–software co-design</strong> to bridge the gap between ML systems and compilers.</li>\n<li><strong>Tinygrad Codegens Flash Attention Directly</strong>: In the Tinygrad community, members successfully proved the ability to <strong>codegen Flash Attention</strong> directly from a frontend definition of naive attention using granular rewrites. Simultaneously, discussions highlighted a shift toward <strong>Megakernels</strong> over traditional kernel schedulers to optimize GPU throughput <a href=\""https://blog.luminal.com/p/compiling-models-to-megakernels\"">Luminal Blog</a>.</li>\n</ul>\n<p><strong>Theme 3. OpenAI Ecosystem: Prism, GPT-5.2 Performance, and Model Decay</strong></p>\n<ul>\n<li><strong>Prism Workspace Unlocks Scientific Collaboration</strong>: OpenAI launched <strong>Prism</strong>, a dedicated workspace powered by <strong>GPT-5.2</strong> designed to streamline scientific research and writing for <strong>ChatGPT personal account</strong> holders <a href=\""https://video.twimg.com/amplify_video/2016207515973980160/vid/avc1/1920x1080/qSf3UTFEArw7oRSn.mp4\"">Video Demo</a>. While the tool targets academic rigor, users debating <strong>GPT-5.2</strong> vs. <strong>Claude Opus 4.5</strong> noted that OpenAI's model still struggles with creative writing, a flaw Sam Altman reportedly admitted to.</li>\n<li><strong>Model Deterioration Blamed on Leechers</strong>: A recurring theory across channels suggests significant degradation in <strong>ChatGPT</strong> and <strong>Claude</strong> performance, with some users claiming a <strong>40% drop</strong> in quality. Speculation points to <strong>free tier users</strong> (\""leechers\"") diluting compute resources or models recursively training on their own synthetic outputs.</li>\n<li><strong>GPT-5 Control Shell Leaked</strong>: A file dubbed the <a href=\""https://cdn.discordapp.com/attachments/1228043845967544380/1465691526947405888/GPT5_Hotfix.md\"">GPT-5_Hotfix.md</a> surfaced, purported to be a <strong>pre-generation control shell</strong> that enforces strict syntax and intent locking to prevent model drift. The leak suggests OpenAI is using aggressive \""wrappers\"" to manage output quality before generation even begins.</li>\n</ul>\n<p><strong>Theme 4. Agentic Coding Wars: Tooling, Security, and Rebrands</strong></p>\n<ul>\n<li><strong>Clawdbot Morphs into Moltbot After Security Scare</strong>: Following a trademark dispute with <strong>Anthropic</strong> and serious community concerns about <strong>zero-auth vulnerabilities</strong>, the popular agent <strong>Clawdbot</strong> rebranded to <strong>Moltbot</strong> <a href=\""https://xcancel.com/moltbot/status/2016058924403753024\"">Announcement</a>. Users previously flagged that the bot could read <strong>environment keys</strong> without permission, posing risks to sensitive financial and personal data.</li>\n<li><strong>Cursor and Cline Face Usability Headwinds</strong>: Users expressed frustration with <strong>Cursor's</strong> pricing model, noting that a few complex prompts could cost <strong>$0.50</strong>, while others struggled to run <strong>Cline</strong> on modest hardware (8GB VRAM), facing <code>CUDA0 buffer</code> errors. Community fixes involved reducing context lengths to <strong>9000</strong> and offloading memory management to <strong>dedicated GPU</strong> settings.</li>\n<li><strong>Karpathy Bets on Agent-First Coding</strong>: Andrej Karpathy sparked discussion by outlining a strategic shift toward <strong>agent-driven coding</strong> using <strong>Claude</strong>, emphasizing the \""tireless persistence\"" of LLMs over traditional methods <a href=\""https://xcancel.com/karpathy/status/2015883857489522876\"">Post</a>. This aligns with the release of <strong>Manus Skills</strong>, where developers are incentivized with free credits to build use cases for the new agentic platform.</li>\n</ul>\n<p><strong>Theme 5. Theoretical Limits and Safety: Hallucinations and Bio-Risks</strong></p>\n<ul>\n<li><strong>Math Proves Hallucination is Inevitable</strong>: A new paper discussed in the BASI Discord mathematically proves that <strong>LLMs will always hallucinate</strong>, utilizing the same principles found in <strong>jailbreaking</strong> mechanics <a href=\""https://arxiv.org/abs/2409.05746\"">Arxiv Paper</a>. Researchers noted that jailbreaking exacerbates this issue by distorting the context model, preventing it from flagging malicious or incorrect tags.</li>\n<li><strong>Fine-Tuning Unlocks Dormant Bio-Risks</strong>: An <strong>Anthropic paper</strong> sparked debate at EleutherAI by demonstrating that fine-tuning open-source models on frontier model outputs can unsuppress harmful capabilities, such as <strong>biorisks</strong>, even if previously safety-trained <a href=\""https://arxiv.org/pdf/2601.13528\"">Arxiv Link</a>. The findings suggest that <strong>refusals</strong> are fragile and can be undone with minimal compute, raising concerns about dual-use technologies.</li>\n<li><strong>AI Detection Tools Flag Human Academics</strong>: Engineers highlighted a growing issue where <strong>AI detection tools</strong> consistently mislabel human-written, pre-GPT academic texts as AI-generated. The consensus is that these detectors are fundamentally flawed, yet institutions continue to rely on them, creating friction for researchers and students.</li>\n</ul>\n<hr>\n<h1>Discord: High level Discord summaries</h1>\n<h2><a href=\""https://discord.com/channels/1105891499641684019\"">BASI Jailbreaking</a> Discord</h2>\n<ul>\n<li><strong>LLMs Face Mathematical Jailbreak Reality</strong>: A new paper (<a href=\""https://arxiv.org/abs/2409.05746\"">https://arxiv.org/abs/2409.05746</a>) mathematically proves that <strong>LLMs will always hallucinate</strong>, using the same principles on which many jailbreaking methods are built.\n<ul>\n<li>A member warned that jailbreaking models significantly <em>increases their hallucination problems</em>, because <em>jailbreaking shifts and distorts the model's context, so that it does not flag things that would normally be tagged as malicious and such</em>.</li>\n</ul>\n</li>\n<li><strong>GPT-5 Control Shell Surfaces After Hotfix</strong>: A member shared a file (<a href=\""https://cdn.discordapp.com/attachments/1228043845967544380/1465691526947405888/GPT5_Hotfix.md?ex=697aafdb&#x26;is=69795e5b&#x26;hm=bc63be5ff7affdceb610e771bff6464d43e88c198a1990ab43776fc5099fbf4b&#x26;\"">GPT5_Hotfix.md</a>) described as a <em>pre-generation control shell</em> for <strong>GPT-5</strong>, designed to enforce strict syntax, intent locking, and drift prevention before generation begins.\n<ul>\n<li>The control shell aims to mitigate model drift and enforce intended outputs.</li>\n</ul>\n</li>\n<li><strong>Exploring Grok's Uncensored Image Generation</strong>: Users are testing the limits of <strong>Grok's image generator</strong>, attempting to jailbreak it for unrestricted content, while others highlight its uncensored nature compared to other models.\n<ul>\n<li>The discussion also touched on the separation of the image model from the language model, impacting the effectiveness of prompt injection.</li>\n</ul>\n</li>\n<li><strong>Clawdbot's Crawl to Concern: Zero Auth Vulnerabilities</strong>: Exploration of <strong>Clawdbot's</strong> popularity, and a surge in VPS usage, sparking concerns about zero authentication and potential vulnerabilities.\n<ul>\n<li>One member plans to set up a <em>home lab</em> to test <strong>Clawdbot's</strong> vulnerabilities, noting that vulnerable instances exist.</li>\n</ul>\n</li>\n<li><strong>Researchers Ramp Up Quest for Jailbreak Datasets</strong>: A researcher is looking for well-known <strong>jailbreak datasets</strong> that include categorization or labels to assist with ongoing research, specifically <strong>malicious prompts</strong>.\n<ul>\n<li>A member responded, <em>\""I really don’t know if there are any available that are free\""</em>, suggesting the researcher may need to produce and label the prompts themselves.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1179035537009545276\"">Unsloth AI (Daniel Han)</a> Discord</h2>\n<ul>\n<li><strong>KV Cache Woes Still Linger</strong>: Users are reporting that the <strong>KV cache</strong> is still not working properly in the latest <em>llama.cpp</em>, potentially causing slowdowns at higher context lengths, despite previous fixes, as seen in <a href=\""https://github.com/ggml-org/llama.cpp/issues/1912\"">this GitHub issue</a>.\n<ul>\n<li>The discussion suggests that previous fixes may not have fully resolved the underlying problems for all use cases.</li>\n</ul>\n</li>\n<li><strong>Unsloth Supercharges Transformers v5</strong>: Unsloth now fully supports <strong>transformers v5</strong>, with a promise of even more optimized training to be released soon, with links to the announcement on <a href=\""https://x.com/UnslothAI/status/2015935368525447395\"">X</a>.\n<ul>\n<li>This upgrade should streamline workflows and improve performance for users leveraging the latest features in the transformers library.</li>\n</ul>\n</li>\n<li><strong>MoE Training Rockets to 14x Speed</strong>: <strong>MoE training</strong> is now reported to be <strong>14x faster</strong> than v4, with further optimizations expected to double the speed again, potentially resulting in a <strong>30x speedup</strong> compared to v4.\n<ul>\n<li>This significant speed boost could dramatically reduce training times for complex models.</li>\n</ul>\n</li>\n<li><strong>Kimi Loses Sass Appeal?</strong>: Users discussed the changes to the <strong>Kimi</strong> model, with one noting it <em>sounds closer to other models by far</em>, suggesting a loss of its unique character after the Kimislop release.\n<ul>\n<li>Some lamented the loss of <strong>Kimi's</strong> smartass personality, preferring its previous tendency to <em>call you out on stuff</em> over becoming <em>more sycophantic</em>.</li>\n</ul>\n</li>\n<li><strong>GLM 4.7 Tool's Blackwell Blues</strong>: A user sought help getting <strong>GLM-v4.7</strong> to call tools on a <strong>Blackwell B200</strong>, running into CUDA version issues (drivers 12.8, requirements 13).\n<ul>\n<li>Another user provided a <code>uv pip install</code> command set using torch 2.9 and CUDA 13, directing user to this helpful <a href=\""https://unsloth.ai/docs/models/glm-4.7-flash#tool-calling-with-glm-4.7-flash\"">unsloth.ai documentation</a> to call it, and use <code>json.loads</code>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1340554757349179412\"">LMArena</a> Discord</h2>\n<ul>\n<li><strong>Molmo 2 Excels at Video Analysis</strong>: The <strong>Molmo 2</strong> model excels at object tracking and event pinpointing in videos according to <a href=\""https://allenai.org/blog/molmo2\"">this blog post</a>.\n<ul>\n<li>Members wondered if the model could be useful for video uploads on the platform.</li>\n</ul>\n</li>\n<li><strong>Kimi K2.5 Impresses with Coding and Creativity</strong>: Users raved about the <strong>Kimi K2.5</strong> model, now in the <a href=\""https://lmarena.ai/?chat-modality=chat\"">Text Arena</a> and on <a href=\""https://www.redditez.com/r/kimi/s/lCdY1ZFLeb\"">HuggingFace</a>, praising its strengths in creative writing, front-end development, and multimodal tasks.\n<ul>\n<li>Members claimed it is better than Gemini 3 Pro and suggested using the K2 or K2 Thinking model, with one member sharing <a href=\""https://x.com/i/status/2015928469658730994\"">this tweet</a>.</li>\n</ul>\n</li>\n<li><strong>GPT 5.2 and Claude Opus 4.5 Face Off</strong>: Members are debating the performance of <strong>GPT 5.2</strong> and <strong>Claude Opus 4.5</strong>, with accuracy being a key point of contention.\n<ul>\n<li>Some users argued <strong>GPT 5.2</strong> is more accurate, while others favor <strong>Claude Opus 4.5</strong>, stating that <em>\""the most smartest &#x26; reliable is claude opus 4.5 thinking\""</em>.</li>\n</ul>\n</li>\n<li><strong>Grok's Got Game, But Not For Work</strong>: Community members discussed the <strong>Grok</strong> model, agreeing it is <em>\""only for chatting\""</em> and that its <em>\""personality and behavior much\""</em> aren't suitable for professional tasks.\n<ul>\n<li>Some users pointed out that the free <strong>Grok</strong> version is different from benchmarked versions, potentially impacting performance.</li>\n</ul>\n</li>\n<li><strong>Auto-Modality and Model selector debut in Text Arena!</strong>: <strong>Auto-Modality</strong> and <strong>Model selector</strong> are now live in <a href=\""https://lmarena.ai/?chat-modality=chat\"">LM Arena</a>.\n<ul>\n<li><strong>Auto-Modality</strong> now routes prompts to the correct modality, and the <strong>Model selector</strong> offers a new design for model selection, as described in the <a href=\""https://help.lmarena.ai/articles/1350607902-lmarena-experiments-auto-modality?lang=en\"">Help Center article</a>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1047197230748151888\"">Perplexity AI</a> Discord</h2>\n<ul>\n<li><strong>Perplexity Pro throttles unlimited access</strong>: Several users are reporting unexpected <strong>rate limits</strong> on their <strong>Perplexity Pro accounts</strong>, despite the plan supposedly offering unlimited access, severely impacting their workflows.\n<ul>\n<li>Even basic <strong>Pro searches</strong> seem to diminish their <strong>Labs quota</strong>.</li>\n</ul>\n</li>\n<li><strong>Perplexity Image Generation fails</strong>: Many <strong>Pro subscribers</strong> are experiencing problems with <strong>image generation</strong>, either being told they've exceeded their limits or facing regional restrictions, showing inconsistency in the service.\n<ul>\n<li>The inconsistency has caused many <strong>Pro subscribers</strong> to complain about the unpredictable service.</li>\n</ul>\n</li>\n<li><strong>Indian Users see card payment failure</strong>: Indian users are facing issues adding <strong>Visa/Mastercard debit or credit cards</strong> for verification, with every Indian card being rejected.\n<ul>\n<li>Some users are considering legal action due to these payment method issues.</li>\n</ul>\n</li>\n<li><strong>Kagi Search gains traction among frustrated users</strong>: Users are discussing <strong>Kagi</strong> as a potential alternative due to the issues with Perplexity's instability, highlighting that <strong>Kagi's assistant feature</strong> looks promising with access to latest Claude models.\n<ul>\n<li>One user pointed out that Kagi also offers a <strong>search results</strong> and <strong>claims to be more privacy-conscious</strong> than other search engines.</li>\n</ul>\n</li>\n<li><strong>Kimi k2.5 boasts Agent Swarm Mode</strong>: With the release of <strong>Kimi k2.5</strong>, it includes agent swarm mode on <a href=\""https://kimi.com\"">kimi.com</a>, a sophisticated tool performing tasks like Claude Code.\n<ul>\n<li>One user noted seeing <strong>15 trillion parameters</strong> for pretraining tokens, triggering immediate excitement for its multimodal abilities vs. <strong>Perplexity AI</strong>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1369594130807787570\"">Moonshot AI (Kimi K-2)</a> Discord</h2>\n<ul>\n<li><strong>Kimi K2.5 Achieves SOTA on Agentic Benchmarks</strong>: <strong>Kimi K2.5</strong> launched with global SOTA on Agentic Benchmarks, achieving <strong>50.2%</strong> on HLE full set, <strong>74.9%</strong> on BrowseComp, and open-source SOTA on Vision and Coding, including <strong>78.5%</strong> on MMMU Pro, <strong>86.6%</strong> on VideoMMMU, and <strong>76.8%</strong> on SWE-bench Verified.\n<ul>\n<li>Members noticed that <strong>Kimi</strong> was claiming to use <strong>Kimi K2.5</strong>, leading to speculation that it was silently rolled out with improved fact-checking and information retrieval capabilities, and multimodal capabilities, like enhanced vision.</li>\n</ul>\n</li>\n<li><strong>Kimi K2.5 Introduces Agent Swarm Beta</strong>: <strong>Agent Swarm (Beta)</strong> enables self-directed agents to work in parallel, scaling up to <strong>100</strong> sub-agents and <strong>1,500</strong> tool calls, achieving <strong>4.5x</strong> faster performance, available for high-tier users on <a href=\""http://kimi.com\"">kimi.com</a>.\n<ul>\n<li>The <strong>Kimi K2.5</strong> launch also integrates image and video to create websites with expressive motion.</li>\n</ul>\n</li>\n<li><strong>Pricing and Tiered Access Sparks Debate</strong>: The new <strong>Kimi Code plan</strong> has much lower limits than <strong>Z.ai</strong>, and users are reporting high tool call usage, with one user reporting that <em>one large ish prompt set me back 5 of those 2000 tool calls a week</em>.\n<ul>\n<li>Several users expressed disappointment that promotional pricing would end in February, deeming the normal monthly price too high to continue supporting <strong>Kimi</strong>.</li>\n</ul>\n</li>\n<li><strong>OpenRouter API Integration Faces Issues</strong>: Users reported errors using <strong>Kimi K2.5</strong> on <strong>OpenRouter</strong>, specifically problems related to tool use and image URLs.\n<ul>\n<li>One user received the error message: <em>No endpoints found that support tool use.</em></li>\n</ul>\n</li>\n<li><strong>Moonshot AI Teases Technical Report</strong>: A footnote in the <a href=\""http://kimi.com/blogs/kimi-k2-5.html\"">tech blog</a> indicates that <em>full prompts will be provided in the technical report</em>.\n<ul>\n<li>Members anticipate that a technical report with more info will be released.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/974519864045756446\"">OpenAI</a> Discord</h2>\n<ul>\n<li><strong>Prism Debuts for Scientists Powered by GPT-5.2</strong>: OpenAI launched <strong>Prism</strong>, a free workspace that facilitates scientific collaboration, running on <strong>GPT-5.2</strong>, as shown in <a href=\""https://video.twimg.com/amplify_video/2016207515973980160/vid/avc1/1920x1080/qSf3UTFEArw7oRSn.mp4\"">this video</a> and accessible at <a href=\""https://prism.openai.com\"">prism.openai.com</a>.\n<ul>\n<li>The platform, now available to those with a <strong>ChatGPT personal account</strong>, streamlines scientific endeavors with its advanced capabilities.</li>\n</ul>\n</li>\n<li><strong>AI Detection Tools Flag Human-Written Text as AI-Generated</strong>: Members have observed that <strong>AI detection tools</strong> are incorrectly flagging human-written, pre-GPT academic texts as AI-generated content, deeming them fundamentally flawed.\n<ul>\n<li>This is happening even though universities and job applications are using AI detection tools, despite their demonstrated inaccuracy.</li>\n</ul>\n</li>\n<li><strong>High RAM MacBooks accelerate AI inference</strong>: Members found that running <strong>Ollama</strong> and <strong>ComfyUI</strong> locally works best on machines with lots of RAM such as a <strong>MacBook Pro</strong> with <strong>M2 Max</strong> and <strong>96GB RAM</strong>, able to run <strong>gpt-oss-120b</strong>.\n<ul>\n<li>Others suggested a minimum setup of <strong>16 GB RAM</strong>, <strong>Ryzen 5 7000</strong> series or <strong>i5</strong> top generation, and a good <strong>NVIDIA</strong> GPU like a <strong>Nvidia 3090</strong> with <strong>24 gb VRAM</strong>.</li>\n</ul>\n</li>\n<li><strong>GPT 5.2 creative writing is sub-par</strong>: While comparing <strong>Gemini 3 Pro</strong> and <strong>Claude 4.5 Opus</strong>, it was found that <strong>GPT 5.2</strong>'s creative writing ability was worse.\n<ul>\n<li><strong>Sam Altman</strong> admitted that <strong>GPT-5.2</strong> was bad at creative writing saying, <em>OpenAI “just screwed that up.”</em></li>\n</ul>\n</li>\n<li><strong>Rapid Model Deterioration Blamed on Free Leechers</strong>: Multiple members expressed concerns that models like <strong>ChatGPT</strong> and <strong>Claude</strong> are deteriorating, with one claiming a <strong>40% degradation</strong>.\n<ul>\n<li>Some blame the degradation on <em>free leechers with multi accounts</em>, while another member suggested the degradation is due to models training off model outputs.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1053877538025386074\"">Nous Research AI</a> Discord</h2>\n<ul>\n<li><strong>OpenAI Veils Model Identity</strong>: Users observed that the specific <strong>OpenAI model</strong> in use is no longer visible, leading to speculation that <strong>OpenAI</strong> is optimizing for cost reduction.\n<ul>\n<li>One user suggested to <em>\""Hover over the regenerate symbol in ChatGPT\""</em> to reveal the underlying model.</li>\n</ul>\n</li>\n<li><strong>Small Models Conquer Large Context Tasks</strong>: <strong>Opus 4.5</strong> (200K context) outperforms <strong>Gemini 3 Pro</strong> (1M context) at 130K tokens, suggesting the <strong>effective context window</strong> is more crucial than its raw size.\n<ul>\n<li>A paper was cited, highlighting quality degradation in models beyond an 8K context window, reinforcing the idea that <em>\""Entropy is not fan of big context, that for sure\""</em>.</li>\n</ul>\n</li>\n<li><strong>GPT 5.2 Pro's Pricey Process</strong>: The high cost of <strong>GPT 5.2 Pro</strong> is attributed to a speculative process involving <strong>7 runs</strong> for suggestion generation followed by an <strong>8th run</strong> for response selection.\n<ul>\n<li>The process is speculated to utilize parallel reasoning chains, aggregated for the final output.</li>\n</ul>\n</li>\n<li><strong>Chinese LLMs Invade the Market</strong>: Chinese LLMs like <strong>Kimi K2.5</strong> (<a href=\""https://www.kimi.com/blog/kimi-k2-5.html\"">kimi.com</a>) are entering the market with reports of excellent writing capabilities.\n<ul>\n<li>Another user speculates that <strong>Deepseek</strong> is in heavy development and will be the last to be released.</li>\n</ul>\n</li>\n<li><strong>MergeMix Melds Mid-Training Data</strong>: The paper <a href=\""https://arxiv.org/pdf/2601.17858\"">MergeMix: Optimizing Mid-Training Data Mixtures via Learnable Model Merging</a> was shared, pointing out open source efforts to optimize data mixtures during training.\n<ul>\n<li>The attached <a href=\""https://cdn.discordapp.com/attachments/1104063238934626386/1465882077428846623/image.png?ex=697ab892&#x26;is=69796712&#x26;hm=d2eb8b255bd55de331634bc9ea6243c25bf5d5b45db1c91afcdb90ebc6f3c717\"">image</a> might provide additional context (though its content isn't specified).</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1074847526655643750\"">Cursor Community</a> Discord</h2>\n<ul>\n<li><strong>Cursor Costs Half-a-Dollar</strong>: One user complained that <em>3 prompts cost 50 cents</em>, and attached an <a href=\""https://cdn.discordapp.com/attachments/1074847527708393565/1465436960981127492/image.png?ex=697a6b86&#x26;is=69791a06&#x26;hm=7ef4f6112725e2af0e564ebcf33a53cbba9da0229e59fff1703635aeb1fbecf2&#x26;\"">image</a> of the interaction.\n<ul>\n<li>This sparked a discussion about the cost-effectiveness of <strong>Cursor's</strong> pricing model and whether it aligns with user expectations.</li>\n</ul>\n</li>\n<li><strong>Skills are Rules, Indeed</strong>: A user asked whether <strong>Cursor Rules</strong> are still relevant, and a community member clarified that <em>they are called skills now</em>, directing to the <a href=\""https://cursor.com/docs/context/skills\"">Skills documentation</a>.\n<ul>\n<li>The documentation outlines how users can create and apply <strong>Skills</strong> to customize and automate various tasks within the editor.</li>\n</ul>\n</li>\n<li><strong>Mysterious Blobs Invade Cursor Prompt</strong>: A user reported finding <em>odd text</em> in the <strong>Cursor prompt box</strong> after leaving their PC on overnight, wondering if it was a known bug or chat leakage.\n<ul>\n<li>Another user suggested that it might be due to accidentally hitting the <strong>mic speak to type button</strong>, and a third confirmed this by noting that the <strong>Whisper model</strong> hallucinates when there's silence.</li>\n</ul>\n</li>\n<li><strong>Cursor Flees to the Browser?</strong>: A user sought guidance on using <strong>Cursor Agent</strong> on a browser, despite having a GitHub repository connected, asking why it doesn't work and directs to <a href=\""https://cursor.com/agent\"">cursor.com/agent</a>.\n<ul>\n<li>It was not resolved whether <strong>Cursor Agent</strong> is intended to work that way.</li>\n</ul>\n</li>\n<li><strong>Team Spends Big Bucks after Token Top-Up?</strong>: A user inquired about an $800 <strong>Team Spend</strong> limit after their $20 allowance, posting an <a href=\""https://cdn.discordapp.com/attachments/1074847527708393565/1465796350946836653/image.png?ex=697a68bb&#x26;is=6979173b&#x26;hm=517732073e00d935f7762d13964cd30a96de994dc3c50c677961564713917efd&#x26;\"">image</a>.\n<ul>\n<li>It was not resolved if the team spend limit can be adjusted by the user or if it's a fixed setting.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1110598183144399058\"">LM Studio</a> Discord</h2>\n<ul>\n<li><strong>Qwen Coder Model Debated for Budget Setups</strong>: Members discussed the optimal coding model for systems with <strong>8GB VRAM</strong> and <strong>32GB RAM</strong>, suggesting options like <strong>qwen2.5-coder-7b-instruct-q5_k_m</strong> and <strong>qwen3-coder-30b-a3b-instruct</strong>.\n<ul>\n<li>The <strong>qwen3-coder-30b-a3b-instruct</strong> model at <strong>Q4_K_M</strong> was favored for its superior capabilities and <strong>20k context window</strong>.</li>\n</ul>\n</li>\n<li><strong>Cline's Coding Stumbles on Smaller Rigs</strong>: Users reported challenges using <strong>Cline</strong> for agentic coding on systems with <strong>8GB VRAM</strong> and <strong>32GB RAM</strong>, facing <code>CUDA0 buffer</code> allocation errors.\n<ul>\n<li>The issue was resolved by reducing the context length to <strong>9000</strong> and adjusting <strong>CUDA runtime</strong> settings.</li>\n</ul>\n</li>\n<li><strong>ROC Runtime Gives LM Studio a Lift on Windows</strong>: Installing <strong>ROC runtime</strong> significantly improved performance on Windows for a user with a <strong>6700xt 12GB VRAM</strong>, matching Linux speeds.\n<ul>\n<li>Compatibility is limited to certain AMD GPUs, as detailed on the <a href=\""https://www.amd.com/en/support\"">AMD website</a>.</li>\n</ul>\n</li>\n<li><strong>Users Express Clawdbot Security Jitters</strong>: Serious security risks were raised about <strong>Clawdbot</strong>, highlighted in <a href=\""https://youtu.be/7fltOAg8ZGI\"">this YouTube video</a>.\n<ul>\n<li>Concerns centered on unauthorized access to environment keys and the dangers of granting an agent access to sensitive financial and personal data, noting that <em>it just reads env keys without permission</em>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/822583790773862470\"">Latent Space</a> Discord</h2>\n<ul>\n<li><strong>Kimi K2.5 Cracks Coding with Zero-Shot</strong>: The <strong>Kimi K2.5</strong> model has launched, showcasing <em>zero-shot coding</em> benchmark successes as shown on <a href=\""https://xcancel.com/AiBattle_/status/2015902394312253564?s=20\"">its official website</a>.\n<ul>\n<li>Its capabilities in more complex <strong>agentic coding</strong> scenarios are still being evaluated.</li>\n</ul>\n</li>\n<li><strong>Clawdbot Claws Back Identity as Moltbot</strong>: Due to a trademark issue with Anthropic, <strong>Clawdbot</strong> has been rebranded to <strong>Moltbot</strong>, with its mascot Clawd now named Molty according to <a href=\""https://xcancel.com/moltbot/status/2016058924403753024\"">this announcement</a>.\n<ul>\n<li>The team seems to be taking the change in stride.</li>\n</ul>\n</li>\n<li><strong>Karpathy Kasts Agent-First Coding</strong>: Andrej Karpathy outlined a strategic move towards <strong>agent-driven coding</strong> using Claude, highlighting the advantages of <strong>LLMs</strong> such as <em>tireless persistence and improved leverage</em> in <a href=\""https://xcancel.com/karpathy/status/2015883857489522876\"">this post</a>.\n<ul>\n<li>He is betting on <strong>LLMs</strong> for coding, as opposed to the current method.</li>\n</ul>\n</li>\n<li><strong>OpenAI Opens Prism: A Portal for Progress</strong>: <strong>OpenAI</strong> has released <strong>Prism</strong>, a collaborative research environment for scientists, powered by <strong>GPT-5.2</strong>, and available to ChatGPT account holders via <a href=\""https://xcancel.com/openai/status/2016209462621831448?s=46&#x26;t=eWVlK1PU8XfB6f402GJJ9g\"">this portal</a>.\n<ul>\n<li>This free workspace aims to streamline scientific research.</li>\n</ul>\n</li>\n<li><strong>ModelScope morphs Images to Z-Image</strong>: <strong>ModelScope</strong> has released <strong>Z-Image</strong>, a version of their image generation model based on <strong>Scalable Single-Stream DiT</strong>, with <a href=\""https://modelscope.cn/models\"">more details here</a>.\n<ul>\n<li>The model offers photorealistic quality, diverse outputs, and support for community tools like <strong>LoRA</strong> and <strong>ControlNet</strong>, including <strong>Z-Image-i2L</strong> for single-image style transfer.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1189498204333543425\"">GPU MODE</a> Discord</h2>\n<ul>\n<li><strong>FlagOS Stack Targets ML Portability</strong>: Tongjie introduced <strong>FlagOS</strong>, an <strong>open-source system software stack</strong> intended to unify the <strong>Model–System–Chip layers</strong>, aiming to enhance the portability of <strong>AI workloads</strong> across diverse hardware.\n<ul>\n<li>The project seeks to incorporate insights from discussions on <strong>ML systems, compilers, and hardware–software co-design</strong>.</li>\n</ul>\n</li>\n<li><strong>TorchX Orchestrates Multi-Node GPUs</strong>: A member asked whether the <a href=\""https://www.youtube.com/watch?v=f-Bwru7TJSc\"">TorchX video</a> remains the recommended method for <strong>multi-node GPU orchestration</strong>.\n<ul>\n<li>No definitive answer was provided, but this may be a starting point for orchestrating large scale applications.</li>\n</ul>\n</li>\n<li><strong>Decart Debuts Lucy 2, Seeks Optimization Engineers</strong>: Decart announced <strong>Lucy 2</strong>, their autoregressive video editing model, sharing a <a href=\""https://x.com/DecartAI/status/2016134190509498740\"">tech report</a>, and is actively hiring engineers to optimize <strong>low-latency kernels</strong> for real-time video/world models.\n<ul>\n<li>Decart is seeking engineers with a focus on <strong>performance work, GPU Mode submissions, or OSS contributions</strong> to help tackle unique perf problems different from <strong>LLM inference</strong>.</li>\n</ul>\n</li>\n<li><strong>Popcorn Preps Fused MoE kernels</strong>: A member inquired about benchmarking kernels on <strong>B200</strong> hardware via <strong>Popcorn</strong> for the <strong>MLSys2026 hackathon</strong>, with a particular interest in fused <strong>MoE kernel</strong> benchmarking.\n<ul>\n<li>Another member advised prepping for the team meeting by experimenting with <strong>kernel LLM generation</strong> for leaderboard problems and exploring the <strong>OG popcorn website</strong> for potential projects.</li>\n</ul>\n</li>\n<li><strong>FlashInfer-Bench Traces Dataset for MLSYS26</strong>: A dataset for FlashInfer-Bench development is now available at <a href=\""https://huggingface.co/datasets/flashinfer-ai/flashinfer-trace\"">flashinfer-ai/flashinfer-trace</a>, and a specialized workload dataset for the <strong>MLSYS26 contest</strong> will be released soon at <a href=\""https://huggingface.co/datasets/flashinfer-ai/mlsys26-contest\"">flashinfer-ai/mlsys26-contest</a>.\n<ul>\n<li>The team is also developing a <strong>biweekly leaderboard</strong> to track progress in the competition.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/729741769192767510\"">Eleuther</a> Discord</h2>\n<ul>\n<li><strong>Heuristic Test for AI PhD Questions</strong>: A member requested question suggestions to gauge the standards for a PhD in AI, and a member suggested the heuristic: <em>“Is this a conversation that two AI researchers might have?”</em>\n<ul>\n<li>This sparked discussion on what constitutes an insightful question in the field.</li>\n</ul>\n</li>\n<li><strong>Teslas Questionable as GPU Farm</strong>: A member bought a <strong>Tesla</strong> for its <strong>24GB VRAM</strong>, prompting skepticism about its speed and power efficiency compared to alternatives like a <strong>3090</strong>.\n<ul>\n<li>One member argued that accounting for energy costs, a <strong>3090</strong> would be more economical and efficient for the same AI work.</li>\n</ul>\n</li>\n<li><strong>Anthropic Paper Sparks Biorisk Debate</strong>: Members discussed the new <strong>Anthropic biorisk paper</strong> (<a href=\""https://arxiv.org/pdf/2601.13528\"">arxiv link</a>, <a href=\""https://x.com/AnthropicAI/status/2015870963792142563\"">X link</a>) and its implications, particularly how fine-tuning open-source models on frontier model outputs can substantially increase capabilities.\n<ul>\n<li>The paper suggests that models can learn harmful capabilities through finetuning or unsuppress them even if safety training had suppressed them, thus supporting the idea that <em>'fine tuning can undo some refusals without much compute.</em>'</li>\n</ul>\n</li>\n<li><strong>Dynamic LoRA Controller Stabilizes Inference</strong>: A member shared a <a href=\""https://github.com/Sva76/Unified-LoRa\"">repo</a> for a dynamic LoRA stability controller, with controlled experiments on multi-adapter setups, to address inference-time degradation and adapter interference.\n<ul>\n<li>The member also highlighted a focus on <strong>goal-aligned metrics</strong> over emergent benchmarks for evaluating LoRA performance.</li>\n</ul>\n</li>\n<li><strong>Parallel Layers vs Sequential Layer Performance</strong>: Harry ran a speedrun using parallel layers; results indicate it <em>underperforms</em> the \""hackable\"" baseline at small scales but trends positively towards larger scales, as seen in the attached <a href=\""https://cdn.discordapp.com/attachments/747850033994662000/1465453642646814892/newplot_8.png?ex=697a7b0f&#x26;is=6979298f&#x26;hm=4868ac66bb344b25d0958e11d82299c9a12e78c176d57c7f6f4dc248be1be233&#x26;\"">graph</a>.\n<ul>\n<li>The graph indicates that <strong>red represents parallel layers</strong>, <strong>blue represents sequential layers</strong>, with the y-axis showing the <strong>% change</strong> relative to a third normalized architecture, with a crossing point at a little after <strong>10^22 FLOP</strong></li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/714501525455634453\"">Yannick Kilcher</a> Discord</h2>\n<ul>\n<li><strong>Pytorch Bug Due to Tensorflow</strong>: A <strong>Pytorch</strong> <code>RAW: Lock blocking</code> error was resolved by uninstalling <strong>Tensorflow</strong>, highlighting potential conflicts.\n<ul>\n<li>A member joked about the difficulty of filing a bug report, questioning what to even report.</li>\n</ul>\n</li>\n<li><strong>HungryLinearFunc Appetite for Scale</strong>: A member introduced a <code>HungryLinearFunc</code> class capable of zero initialization at <strong>LLM</strong> scales, matching a regular linear layer on smaller scales, visualized <a href=\""https://cdn.discordapp.com/attachments/986699377257119794/1465612148091650079/image.png?ex=697a65ed&#x26;is=6979146d&#x26;hm=e02a7fe5c97a08271457f756d390277327e11fbdb1e58d958f6c853727ba964f&#x26;\"">here</a>.\n<ul>\n<li>Usage with <strong>ReLU</strong> is discouraged due to the resulting zero gradient.</li>\n</ul>\n</li>\n<li><strong>Cohere Labs Cracks Open Paper Reading Sessions</strong>: <strong>Cohere Labs</strong> is kicking off Paper Reading Sessions, spotlighting <a href=\""https://cohere.com/research\"">Frontier ML Papers Published in January 2026</a>.\n<ul>\n<li>The sessions cover topics such as reasoning, safety, and real-world applications, and are beginner-friendly and community-focused.</li>\n</ul>\n</li>\n<li><strong>Kimi K2.5 takes off</strong>: Links to <a href=\""https://fxtwitter.com/kimi_moonshot/status/2016024049869324599\"">Kimi Moonshot on Twitter</a> and the <a href=\""https://www.kimi.com/blog/kimi-k2-5.html\"">Kimi K2-5 blogpost</a> were shared.\n<ul>\n<li>Further conversation ensued about the product roadmap.</li>\n</ul>\n</li>\n<li><strong>Clawdbot classified as Scam?</strong>: A member sarcastically commented that <strong>OpenAI</strong> is making a wrapper for their own tool, and that someone already raked in the <strong>Clawdbot scam</strong> money.\n<ul>\n<li>The linked image was of a receipt, implying someone made money off of the perceived scam.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1068976834382925865\"">tinygrad (George Hotz)</a> Discord</h2>\n<ul>\n<li><strong>Flash Attention Codegenning Now Direct</strong>: A member shared that they were able to prove the connection and <strong>codegen flash attention</strong> directly from a frontend definition of naive attention.\n<ul>\n<li>The <strong>rewrites have gotten a lot more granular</strong> since, without a single big online softmax rewrite.</li>\n</ul>\n</li>\n<li><strong>Megakernels Crush Kernel Schedulers on GPU</strong>: George Hotz linked to a <a href=\""https://blog.luminal.com/p/compiling-models-to-megakernels\"">blog post from Luminal</a> discussing <strong>compiling models to megakernels</strong>.\n<ul>\n<li>The discussion suggests that GPUs are moving away from using the kernel scheduler towards an \""operating system\"" that installs itself on all the CUs.</li>\n</ul>\n</li>\n<li><strong>Hardware Dependency Trackers Become Essential</strong>: Members discussed the need for <strong>hardware-based schedulers / dependency trackers</strong> to achieve low latency, noting that significant effort was spent on low-latency software dependency tracking.\n<ul>\n<li>They suggest building a fairly generic scheduler into hardware, rather than relying solely on software solutions, to avoid multiple gmem roundtrips.</li>\n</ul>\n</li>\n<li><strong>AMD Emulator Gets Debug Instructions</strong>: A member shared that with the new <strong>AMD emulator</strong> (<strong>AMD=1 MOCKGPU=1</strong>), <strong>DEBUG=3</strong> prints all the instructions when they are compiled, and <strong>DEBUG=6</strong> prints all of them as they run.\n<ul>\n<li>An image was attached, showcasing the debugging output of the emulator.</li>\n</ul>\n</li>\n<li><strong>Optimizing GitHub Actions, The Tinygrad Way</strong>: George Hotz critiqued using faster computers (rented via services like Blacksmith) to speed up GitHub Actions, arguing it doesn't truly make the code faster.\n<ul>\n<li>He emphasized that <em>the goal with tinygrad is to do things the 'right' way</em>, focusing on code optimization rather than relying on external resources.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1161519468141355160\"">DSPy</a> Discord</h2>\n<ul>\n<li><strong>CheshireCat Unveils Agentic Workflows</strong>: The <strong>CheshireCat framework</strong> introduced new features in its enterprise fork, emphasizing <strong>agentic workflows</strong> that automate the agent creation process by implementing the workflow itself, where <strong>CheshireCat</strong> serves as the infrastructure. A <a href=\""https://www.github.com/matteocacciola/cheshirecat-core\"">github link</a> was shared.\n<ul>\n<li>A debate ensued, with some suggesting the use of existing frameworks like <strong>Agno</strong> or <strong>Sentient</strong>, while the author of <strong>CheshireCat</strong> defended its unique offerings, including <strong>multitenancy</strong>.</li>\n</ul>\n</li>\n<li><strong>Minecraft AI Agent Mines with DSPy</strong>: A member showcased their project, an <strong>AI</strong> for playing <strong>Minecraft</strong>, built using a <strong>DSPy RLM agent</strong> and <strong>Minecraft MCP</strong>, complete with a <a href=\""https://x.com/paullockettkpb/status/2015942268385956226\"">status update</a>, <a href=\""https://youtu.be/jSPIuliRGFE?si=DOY6IqQ7OPCJxLM9\"">YouTube video</a>, <a href=\""https://github.com/PaulLockett/Storyhost\"">open-sourced code</a>, and <a href=\""https://open.substack.com/pub/tappedin/p/i-mined-a-stack-no-you-mined-four?r=cbnuc&#x26;utm_medium=ios&#x26;shareImageVariant=overlay\"">process blog</a>.\n<ul>\n<li>The agent leverages <strong>DSPy</strong> to navigate the Minecraft environment, demonstrating the framework's capabilities in complex, dynamic scenarios.</li>\n</ul>\n</li>\n<li><strong>CoderRLM Module Executes in REPL Environments</strong>: A member introduced a <code>CoderRLM</code> module, designed to wrap a <strong>Python interpreter</strong> to solve <code>None</code> issues in <strong>JSON serialization</strong>, a crucial fix for <strong>Deno/Pyodide REPL</strong> environments.\n<ul>\n<li>The module preloads reference data like <strong>CM_INDEX_FILE</strong>, <strong>CM_TABULAR_FILE</strong>, <strong>CM_D_FILE</strong>, and <strong>CM_N_FILE</strong> as REPL variables, enabling coding using the <strong>RLM paradigm</strong>.</li>\n</ul>\n</li>\n<li><strong>Autonomous Agents Self-Improve</strong>: A member is designing <strong>autonomous agents</strong> capable of self-learning, planning, executing, and recovering from tool/API failures without human intervention, emphasizing continuous improvement systems for sustained AI performance.\n<ul>\n<li>These agents are intended for use across various sectors, employing tools and frameworks such as <strong>Python, TensorFlow, PyTorch, FastAPI, and AWS</strong>.</li>\n</ul>\n</li>\n<li><strong>Healthcare AI Automates Diagnostics</strong>: A member is developing <strong>predictive healthcare models</strong> to automate diagnostics, monitor patient health, and streamline clinical workflows through <strong>NLP-powered clinical data systems</strong> that extract insights from unstructured medical notes.\n<ul>\n<li>These systems are designed with <strong>HIPAA compliance</strong> and security features like <strong>RBAC</strong> and audit logging to protect sensitive data.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1348819876348825620\"">Manus.im Discord</a> Discord</h2>\n<ul>\n<li><strong>doe.so touted as superior to Manus</strong>: A member recommended <a href=\""https://doe.so\"">doe.so</a> as a better alternative to <strong>Manus</strong>.\n<ul>\n<li>The user simply stated it <em>just feels smarter</em>.</li>\n</ul>\n</li>\n<li><strong>Manus Skills Launched, Credits Given</strong>: The <strong>Manus</strong> team announced the launch of <strong>Manus Skills</strong>, encouraging the community to test them and share their use cases.\n<ul>\n<li>Users are incentivized to post on <strong>X</strong> (formerly Twitter) and tag <a href=\""https://x.com/ManusAI/status/2016171081950744935?s=20\"">@ManusAI</a> for reposts and <strong>free credits</strong>.</li>\n</ul>\n</li>\n<li><strong>AI/ML Dev Hunting for Next Gig</strong>: A full stack + <strong>AI dev</strong> introduced themselves, seeking new opportunities.\n<ul>\n<li>They highlighted experience in areas like <strong>Autonomous Agents</strong>, <strong>Healthcare AI</strong>, and <strong>Fraud Detection Systems</strong> with various listed technologies.</li>\n</ul>\n</li>\n<li><strong>Cloud Browser Takes a Nap</strong>: A user reported that their cloud browser screen shows the error: <em>The temporary website is currently unavailable.</em>\n<ul>\n<li>They noted they tried waking it up and assigning tasks, but the website doesn't appear, and they are running out of credits.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1131200896827654144\"">aider (Paul Gauthier)</a> Discord</h2>\n<ul>\n<li><strong>Aider's GitHub Marked End-of-Life</strong>: A user noticed that <a href=\""https://github.com/paul-gauthier/aider\"">Aider's GitHub</a> has been stale since 2025.\n<ul>\n<li>Another user responded that it is not maintained anymore.</li>\n</ul>\n</li>\n<li><strong>AI Engineer's Project Portfolio Revealed</strong>: An AI Engineer listed current projects including <strong>Autonomous Agents</strong>, <strong>Healthcare AI</strong>, <strong>Decision Support</strong>, <strong>Conversational AI</strong>, <strong>Fraud Detection</strong>, and <strong>AI Automation</strong>.\n<ul>\n<li>No further details about the project specifics were provided.</li>\n</ul>\n</li>\n<li><strong>AI Engineer's Toolkit Unveiled</strong>: An AI Engineer shared a detailed tech stack including languages like <strong>Python</strong>, <strong>TypeScript</strong>, <strong>Go</strong>, <strong>Rust</strong> and frameworks like <strong>TensorFlow</strong>, <strong>PyTorch</strong>, <strong>Hugging Face</strong>, <strong>OpenAI</strong>.\n<ul>\n<li>Their stack also covers databases (<strong>PostgreSQL</strong>, <strong>Kafka</strong>) and cloud platforms (<strong>AWS</strong>, <strong>Docker</strong>) along with security compliance measures like <strong>HIPAA</strong>, <strong>RBAC</strong>, <strong>Audit Logs</strong>, and <strong>Encryption</strong>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1087530497313357884\"">Modular (Mojo 🔥)</a> Discord</h2>\n<ul>\n<li><strong>Container Configuration Cures Confinement Crisis</strong>: A member resolved a <strong>container issue</strong> by adding <code>--cap-add=SYS_PTRACE --security-opt seccomp=unconfined</code> when running the container.\n<ul>\n<li>Alternatively, users can add <code>runArgs</code> to <code>.devcontainer/devcontainer.json</code> with the same parameters to achieve the same effect.</li>\n</ul>\n</li>\n<li><strong>Security Opts Solve Mysterious Container Conundrums</strong>: The user reported resolution by adding <code>--security-opt seccomp=unconfined</code>.\n<ul>\n<li>This disables seccomp, potentially resolving issues related to system call restrictions within the container.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/814557108065534033\"">MLOps @Chipro</a> Discord</h2>\n<ul>\n<li><strong>Interest Expressed in MLOps Books</strong>: A user inquired about the motivation behind seeking books related to <strong>MLOps</strong>.\n<ul>\n<li>This suggests a potential interest in learning more about MLOps practices and methodologies.</li>\n</ul>\n</li>\n<li><strong>Another MLOps Topic</strong>: This is a placeholder summary for demonstration purposes.\n<ul>\n<li>It helps fulfill the minimum requirement of two topic summaries.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<p>The <strong>LLM Agents (Berkeley MOOC) Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>\n<hr>\n<p>The <strong>Windsurf Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>\n<hr>\n<p>The <strong>MCP Contributors (Official) Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>\n<hr>\n<p>You are receiving this email because you opted in via our site.</p>\n<p>Want to change how you receive these emails?\nYou can <a href=\""%7B%7B%7BRESEND_UNSUBSCRIBE_URL%7D%7D%7D\"">unsubscribe</a> from this list.</p>\n<hr>\n<h1>Discord: Detailed by-Channel summaries and links</h1>\n<h3><strong>BASI Jailbreaking ▷ #<a href=\""https://discord.com/channels/1105891499641684019/1235691879492751460/1465437165935661107\"">general</a></strong> (1269 messages🔥🔥🔥):</h3>\n<blockquote>\n<p><code>Rules as a Social Contract, Doxxing Threats, Factorio Game Night, Grok Image Jailbreak, Clawdbot Vulnerabilities</code></p>\n</blockquote>\n<ul>\n<li><strong><strong>BASI Banter: Rules Edition</strong></strong>: Users debated the interpretation and enforcement of server rules, with one suggesting they're a <em>justification for bans</em> rather than a social contract.\n<ul>\n<li>Another user argued rules set a <em>reasonable expectation of protection</em> if followed, while mods navigate enforcement and appropriate punishments.</li>\n</ul>\n</li>\n<li><strong><strong>Doxxing Drama Divulged</strong></strong>: A user jokingly offered a hypothetical doxxing challenge, leading to a debate on server rules and potential violations.\n<ul>\n<li>Another user countered they were <em>trying to bait</em> to get the other account banned, escalating tensions.</li>\n</ul>\n</li>\n<li><strong><strong>Factorio Factory Fervor</strong></strong>: Discussion ignited about a potential <strong>Basi server Factorio game night</strong>, boasting of self-expanding factories and optimized blueprints.\n<ul>\n<li>Suggestions included having a <em>reliable host</em>, experienced players to manage bugs, and utilizing pre-made blueprints for efficiency.</li>\n</ul>\n</li>\n<li><strong><strong>Grok's Grand Gestures: Jailbreaking Journeys</strong></strong>: Users explored the limits of Grok's image generator, aiming to jailbreak it for unrestricted content, while others vouched for its uncensored nature compared to others.\n<ul>\n<li>Discussion on the separation of the image model from the language model, making prompt injection less effective.</li>\n</ul>\n</li>\n<li><strong><strong>Clawdbot Chaos: Vulnerabilities and VPS Variety</strong></strong>: Exploration of <strong>Clawdbot's</strong> rising popularity, and a surge in VPS usage, sparking concerns about zero authentication and potential vulnerabilities.\n<ul>\n<li>A member intends to set up a <em>home lab</em> to test <strong>Clawdbot's</strong> vulnerabilities, while it was noted that vulnerable instances exist.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>BASI Jailbreaking ▷ #<a href=\""https://discord.com/channels/1105891499641684019/1228043845967544380/1465442972744159487\"">jailbreaking</a></strong> (198 messages🔥🔥):</h3>\n<blockquote>\n<p><code>Jailbreaking Methods, Hallucination in LLMs, ENI Persona Trick, Model Degradation, GPT-5 Hotfix</code></p>\n</blockquote>\n<ul>\n<li><strong>Researchers Mathematically Prove LLMs Will Always Hallucinate</strong>: A paper (<a href=\""https://arxiv.org/abs/2409.05746\"">https://arxiv.org/abs/2409.05746</a>) mathematically \""proves that <strong>LLMs will always hallucinate</strong>\"" using the same principles on which many jailbreaking methods are built.</li>\n<li><strong>Jailbreaking Increases Hallucination Problems</strong>: A member warned that jailbreaking models significantly <em>increases their hallucination problems</em>, because <em>jailbreaking shifts and distorts the model's context, so that it does not flag things that would normally be tagged as malicious and such</em>.</li>\n<li><strong>GPT-5 Hotfix: Standalone Control Shell Recovered</strong>: A member shared a file (<a href=\""https://cdn.discordapp.com/attachments/1228043845967544380/1465691526947405888/GPT5_Hotfix.md?ex=697aafdb&#x26;is=69795e5b&#x26;hm=bc63be5ff7affdceb610e771bff6464d43e88c198a1990ab43776fc5099fbf4b&#x26;\"">GPT5_Hotfix.md</a>) described as a <em>pre-generation control shell</em> for <strong>GPT-5</strong>, designed to enforce strict syntax, intent locking, and drift prevention before generation begins.</li>\n<li><strong>Gemini Jailbreak Shared</strong>: A member shared a three-turn jailbreak tested vs <strong>Gemini</strong>, involving specific instructions and prompts to <em>mutate intents to prevent constraint friction</em>.</li>\n<li><strong>Experimentation with Mode Injection</strong>: A member mentioned using **...</li>\n</ul>\n"",""content:encodedSnippet"":""China takes another huge leap ahead in open models\nAI News for 1/26/2026-1/27/2026. We checked 12 subreddits, 544 Twitters and 24 Discords (206 channels, and 7476 messages) for you. Estimated reading time saved (at 200wpm): 602 minutes. Our new website is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on @smol_ai!\nAI News for 1/26/2026-1/27/2026. We checked 12 subreddits, 544 Twitters and 24 Discords (206 channels, and 7476 messages) for you. Estimated reading time saved (at 200wpm): 602 minutes. AINews’ website lets you search all past issues. As a reminder, AINews is now a section of Latent Space. You can opt in/out of email frequencies!\nKimi has been on an absolute tear in the past year, and we last heard from them in November with Kimi K2 Thinking. Like K2, today’s K2.5 is still a 32B active-1T param model (384 experts), “built through continual pretraining on 15 trillion mixed visual and text tokens atop Kimi-K2-Base” (which itself was on 15T tokens), and with an EXTREMELY well produced video from their founder (3 minutes, just watch it):\nhttps://youtu.be/5rithrDqeN8\nThey again claim SOTA on HLE and BrowseComp (footnotes give confidence the tests are legit), but also open model SOTA for vision and coding tasks:\ntweet\n\nThere are a few notables here - Kimi K2.5 is “natively multimodal” for the first time, perhaps borrowing from Kimi VL, but is attributed to “massive-scale vision-text joint pre-training” including VIDEO understanding - “simply upload a screen recording” and K2.5 can reconstruct the website for you:\n\nThe fact that this is a continued pretrain that changes arch (+400M param MoonViT vision encoder) is VERY exciting for model training folks who rarely get to see a scaled up model do stuff like this.\nThe other 2 headline features are equally exciting: Agent Swarm (only for paid users on the Kimi app) which “learns to self-direct an agent swarm of up to 100 sub-agents, executing parallel workflows across up to 1,500 coordinated steps, without predefined roles or hand-crafted workflows.” This parallelism results in higher end result performance with up to 4.5x faster speed… ignoring token cost of course.\n\nand “Office Productivity” with K2.5 Agent focused on “high-density, large-scale office work end to end”.\n\nThis is not empty regurgitation - We saw enough to sign up as a paying subscriber of the Kimi App going forward. As Artificial Analysis notes, the China-Western gap in open models just took another big leap today.\n\nAI Twitter Recap\nMoonshotAI’s Kimi K2.5 ecosystem: open multimodal MoE + “Agent Swarm” push\nKimi K2.5 model drop and positioning: Moonshot positions Kimi K2.5 as a flagship open-weights model with native multimodality (image + video), strong agentic performance, and aggressive API pricing/latency claims. Official launch media and messaging: founder intro video, pricing/throughput claims incl. “Turbo-level speed 60–100 tok/s”, plus early community reactions emphasizing “agent swarm” and multimodal capability (kimmonismus, kimmonismus on multimodal/video).\nTechnical gist (as surfaced by the community): A useful unpacking of K2.5’s reported ingredients—~15T mixed visual+text tokens continual pretraining, context 128K→256K via YaRN, release in INT4 with selective quantization (only routed experts quantized), and the “Agent Swarm” orchestration concept (dynamic generation of subagents; up to 100 parallel subagents / 1,500 steps; wall-time improvements claimed 3–4.5×) is summarized by @TheZachMueller (and points to the technical report).\nBenchmarks/third-party eval framing: Artificial Analysis positions K2.5 as “leading open weights” and closer to frontier labs, highlighting GDPval-AA Elo 1309 (agentic knowledge work harness), MMMU Pro 75%, INT4 ~595GB, and a 64% hallucination rate (improved vs K2 Thinking) among other stats: @ArtificialAnlys. LMArena announcements also place K2.5 Thinking at #1 open model in their Text Arena snapshot: @arena. (Treat leaderboards as point-in-time; harness/tooling and prompting matter.)\nDistribution and “runs at home” signals: K2.5 landed quickly across infra surfaces: Ollama cloud with launch integrations (@ollama), Together AI listing (@togethercompute), and Fireworks as a partner (Moonshot). A notable local-inference datapoint: K2.5 reportedly runs (slowly but “usable”) on 2× M3 Ultra via MLX with sharded generation, ~21.9 tok/s at high memory use: @awnihannun (+ command snippet here).\nProduct surface area around Kimi: Moonshot also pushed adjacent tooling: Kimi Code, an Apache-2.0 open-source coding agent integrating with common IDEs/editors (announcement), and an Agent SDK to build custom agents (link). A “Kimi Product” account is explicitly aimed at distributing prompts/use-cases (launch), with a viral demo of “video-to-code” website cloning (demo).\nOpen “American comeback” at scale: Arcee/Prime Intellect Trinity Large Preview (400B MoE)\nTrinity Large Preview release: Arcee dropped Trinity Large initial weights as a “preview” release: @arcee_ai, with expanded details from @latkins. Prime Intellect frames it as an open 400B MoE with 13B active trained with Datology data: @PrimeIntellect. OpenRouter offered limited-time free access: @OpenRouterAI.\nArchitecture/training details (most concrete technical tweet): A strong technical snapshot comes from @samsja19: 400B/A13B MoE, trained over 17T tokens; 3:1 interleaved local/global gated attention, SWA, NoPE on global layers + RoPE on local layers (as written in tweet), depth-scaled sandwich norm, sigmoid routing, trained with Muon; trained on ~2,000 B300s for a month on Prime Intellect infra, with data curation by DatologyAI.\nData scaling emphasis: Datology’s involvement is highlighted as a major part of the project: “6.5T tokens overall” and “800B synthetic code” (plus multilingual curation) in one team member’s recap: @code_star. Separate recaps mention 8T synthetic as part of 17T: @pratyushmaini.\nEcosystem readiness: vLLM announced day-0 support for serving Trinity Large: @vllm_project. The meta-story in the replies is that a Western org is again attempting frontier-ish pretraining from scratch with an open model, rather than only post-training/evals.\nAgents everywhere: orchestration, subagents, planning critics, and IDE/CLI integration\nAgent “swarm” vs “subagents” convergence: Kimi’s “Agent Swarm” pitch (dynamic subagent creation) parallels the broader pattern of central orchestrator + parallel specialists. The most explicit “starter pattern” articulation is LangChain’s stateless subagent model (parallel execution + minimized context bloat): @sydneyrunkle. Meanwhile, Kimi’s swarm is framed as trainable orchestration via Parallel-Agent RL (PARL) in community summaries (Zach Mueller).\nReliability via “critique before execute”: Google’s Jules introduced a Planning Critic—a second agent that critiques plans pre-execution, claiming a 9.5% drop in task failure rates: @julesagent. Jules also added “Suggested Tasks” for proactive optimizations: @julesagent.\nCoding-agent products intensifying: Mistral shipped Vibe 2.0 upgrades (subagents, user-defined agents, skills/slash commands, and paid plans): @mistralvibe and @qtnx_. MiniMax launched an “Agent Desktop” workspace pitched as more polished than Claude Cowork: @omarsar0 (and MiniMax’s own onboarding automation: @MiniMax_AI).\nIDE infrastructure and retrieval: Cursor claims semantic search materially improves coding-agent performance and that indexing for large codebases is “orders of magnitude faster”: @cursor_ai. VS Code continues tightening agent UX (e.g., safer command execution explanations): @aerezk, plus MCP servers returning UI via MCP Apps spec (LIFX control panel example): @burkeholland.\nDocument AI & multimodal systems: DeepSeek-OCR 2 and “Agentic Vision”\nDeepSeek-OCR 2: learned reading order + token compression: DeepSeek-OCR 2 is framed as a shift from fixed raster scans to learned Visual Causal Flow with DeepEncoder V2, including 16× visual token compression (256–1120 tokens/image) and 91.09% OmniDocBench v1.5 (+3.73%); vLLM shipped day-0 support: @vllm_project. Unsloth notes similar headline improvements: @danielhanchen.\nMechanistic intuition (why it matters for pipelines): Jerry Liu provides a clear “why learned order helps” explanation: avoid semantically shredding tables/forms by allowing query tokens to attend to contiguous regions instead of strict left-to-right: @jerryjliu0. Teortaxes adds a pragmatic eval take: OCR 2 is “on par with dots.ocr” and “nowhere near SOTA,” but the ideas may influence later multimodal products: @teortaxesTex.\nGemini “Agentic Vision” = vision + code execution loop: Google is productizing a “Think, Act, Observe” loop where the model writes/executes Python to crop/zoom/annotate images, claiming 5–10% quality boosts across many vision benchmarks: @_philschmid and the official thread: @GoogleAI. This is an explicit move toward tool-augmented vision being first-class, not bolted on.\nAI for science & research workflows: OpenAI Prism as “Overleaf with AI”\nPrism launch: OpenAI introduced Prism, a free “AI-native workspace for scientists” powered by GPT-5.2, positioned as a unified LaTeX collaboration environment: @OpenAI and @kevinweil. Community summaries frame it as “Overleaf with AI” (proofreading, citations, literature search): @scaling01.\nData/IP clarification: Kevin Weil clarified that Prism follows your ChatGPT data controls and that OpenAI is not taking a share of individual discoveries; any IP-alignment deals would be bespoke for large orgs: @kevinweil.\nWhy it matters technically: Prism is a product bet that collaboration context + tool integration (LaTeX, citations, project state) becomes a durable advantage—mirroring the “context > intelligence” theme circulating in Chinese discussions about OpenAI infra and org design: @ZhihuFrontier.\nResearch notes & benchmarks worth tracking (RL, planning, multilingual scaling)\nLong-horizon planning benchmark: DeepPlanning proposes verifiable-constraint planning tasks (multi-day travel, shopping) and reports frontier agents still struggle; emphasizes explicit reasoning patterns and parallel tool use: @iScienceLuvr. (This pairs nicely with the “travel planning again” meme: @teortaxesTex.)\nRL efficiency and reuse of traces: PrefixRL idea—condition on off-policy prefixes to speed RL on hard reasoning, claiming 2× faster to same reward vs strong baseline: @iScienceLuvr.\nMultilingual scaling laws: Google Research announced ATLAS scaling laws for massively multilingual LMs with data-driven guidance on balancing data mix vs model size: @GoogleResearch.\nMath research reality check: Epoch’s FrontierMath: Open Problems benchmark invites attempts; “AI hasn’t solved any of these yet”: @EpochAIResearch.\nTop tweets (by engagement)\nOpenAI launches Prism (AI LaTeX research workspace): @OpenAI\nMoonshot founder video introducing Kimi K2.5: @Kimi_Moonshot\nKimi “video-to-code” website cloning demo: @KimiProduct\nOllama: Kimi K2.5 on Ollama cloud + integrations: @ollama\nClaude generating 3Blue1Brown-style animations claim (education impact): @LiorOnAI\nFigure introduces Helix 02 autonomous whole-body robotics control: @Figure_robot\nAI Reddit Recap\n/r/LocalLlama + /r/localLLM Recap\n1. New Model and Benchmark Releases\nIntroducing Kimi K2.5, Open-Source Visual Agentic Intelligence (Activity: 643): Kimi K2.5 is an open-source visual agentic intelligence model that achieves global state-of-the-art (SOTA) performance on agentic benchmarks, with scores of 50.2% on the HLE full set and 74.9% on BrowseComp. It also leads in open-source vision and coding benchmarks, scoring 78.5% on MMMU Pro, 86.6% on VideoMMMU, and 76.8% on SWE-bench Verified. The model introduces an Agent Swarm feature in beta, allowing up to 100 sub-agents to work in parallel, making 1,500 tool calls and operating 4.5× faster than a single-agent setup. Kimi K2.5 is available in chat and agent modes on kimi.com, with additional resources on Hugging Face. A comment highlights the impressive capability of 100 sub-agents working in parallel, suggesting potential for enhanced performance in coding tasks. Another comment notes the banning of the original poster, raising questions about account authenticity.\nAsleep_Strike746 highlights the impressive capability of Kimi K2.5 to run 100 sub-agents in parallel, suggesting potential for complex task execution, such as coding tasks. This parallelism could significantly enhance performance in multi-threaded environments, making it a powerful tool for developers looking to automate or optimize workflows.\nillusoryMechanist points out the scale of Kimi K2.5 with '1T Activated Parameters' and '32B' (likely referring to the model's parameter count), indicating a substantial computational capacity. This suggests that Kimi K2.5 could handle large-scale data processing and complex problem-solving tasks, positioning it as a competitive player in the open-source AI landscape.\nCapaj shares a practical test of Kimi K2.5 by prompting it to generate an SVG of a fox on a unicycle. The result was described as 'not too bad', implying that while the model can handle creative tasks, there might still be room for improvement in terms of output quality or creativity. This kind of testing is crucial for understanding the model's capabilities in real-world applications.\nJan v3 Instruct: a 4B coding Model with +40% Aider Improvement (Activity: 333): The image is a bar chart titled \""Aider Benchmark\"" that illustrates the performance of various coding models in terms of their pass rate for polyglot code editing. The \""Jan-v3-4B-base-INSTRUCT\"" model leads with a score of 18, significantly outperforming other models like \""Qwen3-4B-THINKING-2507\"" with 12.1 and \""Ministral-3-8B-INSTRUCT-2512\"" with 6.8. This highlights the Jan-v3 model's high efficiency and over 40% improvement in performance, showcasing its enhanced capabilities in coding tasks. The model is designed for improved math and coding performance, making it a strong candidate for lightweight assistance and further fine-tuning. One commenter appreciates the Qwen 4B 2507 model for small tasks, noting its impressive performance despite its size. Another user shares mixed experiences with the Jan model, praising its ability to use search tools effectively but noting occasional tool call failures and odd responses, possibly due to system prompts.\nThe Jan v3 Instruct model, a 4 billion parameter coding model, reportedly achieves a 40% improvement in performance with the Aider benchmark. This suggests significant advancements in its ability to handle coding tasks, potentially outperforming other models like Qwen 4B 2507 in specific scenarios. The model's ability to utilize search tools effectively for code explanation is noted, although there are occasional failures in tool calls and some system prompt issues in web chat applications.\nA user reported mixed experiences with the Jan v3 model on chat.jan.ai, highlighting its capability to correctly use search tools and read code for explaining project flows. However, they also noted some tool call failures and irrelevant responses, possibly due to system prompts. The user expressed interest in the model's potential integration with Claude Code, suggesting it could become a valuable tool for code search and Q&A in daily coding tasks.\nThe Jan v3 model's performance in benchmarks is highlighted, with a specific mention of its demo availability at chat.jan.ai. The model's ability to handle small and easy tasks effectively is compared to Qwen 4B 2507, which is favored for similar tasks. The discussion suggests that Jan v3's fine-tuning may offer competitive advantages in certain coding scenarios.\ndeepseek-ai/DeepSeek-OCR-2 · Hugging Face (Activity: 385): DeepSeek-OCR-2 is a state-of-the-art OCR model available on Hugging Face, optimized for document processing with visual causal flow. It requires Python 3.12.9 and CUDA 11.8, and leverages libraries like torch and transformers. The model supports dynamic resolution and uses flash attention for enhanced performance on NVIDIA GPUs. It offers various prompts for document conversion, making it versatile for different OCR tasks. One user highlighted the impressive performance of PaddleOCR-VL when compared using scores from other models, suggesting its potential superiority. Another user shared a demo of DeepSeek OCR 2, noting initial issues with repetition due to user error, which were resolved by adjusting decoding parameters, leading to significantly improved performance over version 1.\nA user highlighted the impressive performance of PaddleOCR-VL, suggesting it stands out when compared to other models like B/C/D. This is based on scores reported by a third party, which the user trusts for evaluating model performance. This implies PaddleOCR-VL's metrics are noteworthy in the context of OCR model comparisons.\nAnother user shared their experience with implementing a demo for DeepSeek OCR 2 using GPU credits. Initially, they faced issues with repetition due to incorrect parameters, but after adjusting to DeepSeek's recommended decoding parameters, the performance improved significantly. The user noted that the updated version is much more reliable than its predecessor, DeepSeek OCR v1.\nThe GitHub repository and paper for DeepSeek OCR 2 were shared, providing resources for those interested in the technical details and implementation of the model. The paper likely contains in-depth information on the model's architecture, training process, and performance benchmarks, which are crucial for technical evaluation and understanding.\ntransformers v5 final is out 🔥 (Activity: 503): Transformers v5 from Hugging Face introduces significant performance improvements, particularly for Mixture-of-Experts (MoE) models, achieving 6x-11x speedups. The update simplifies the API by removing slow/fast tokenizers, offering explicit backends and enhanced performance. Additionally, dynamic weight loading is now faster, supporting MoE with quantization, tensor parallelism, and Parameter-Efficient Fine-Tuning (PEFT). A migration guide and detailed release notes are available for users transitioning to this version. One user inquired about the implications of these improvements for running small to medium-sized MoE models locally, suggesting that the enhancements might reduce memory bandwidth constraints. Another user reported a 50% increase in single prompt inference speed and a 100% increase in concurrent inference speed after updating to v5 and vllm 0.14.1.\nThe Mixture-of-Experts (MoE) model in Transformers v5 shows significant performance improvements, with reported speedups ranging from 6x to 11x. This is particularly relevant for users running models locally, as it suggests that MoE can now utilize compute resources more efficiently, potentially reducing memory bandwidth constraints. This could be beneficial for setups using NVIDIA GPUs or AMD iGPUs, such as the Strix Halo, where compute power is a limiting factor.\nA user reported upgrading to Transformers v5 and vllm 0.14.1 from 0.11, resulting in a 50% increase in single prompt inference speed and a 100% increase in concurrent inference speed for 40x workloads. This highlights the significant performance enhancements in the latest version, which could be crucial for applications requiring high throughput and low latency.\nThe update in Transformers v5 now allows Mixture-of-Experts (MoE) models to work with quantized models, which was not possible before. This advancement enables more efficient model deployment by reducing the model size and computational requirements, making it feasible to run complex models on less powerful hardware without sacrificing performance.\n2. Local LLM Hardware and Setup Discussions\n216GB VRAM on the bench. Time to see which combination is best for Local LLM (Activity: 577): The post discusses the use of secondhand Tesla GPUs, which offer substantial VRAM at a lower cost, for local large language model (LLM) testing. The author has developed a GPU server benchmarking suite to evaluate the performance of these older GPUs when used in parallel. The image shows a setup with multiple NVIDIA GPUs, highlighting the focus on maximizing VRAM for machine learning tasks. The technical challenge lies in effectively utilizing these GPUs without significant bandwidth loss, as most affordable server motherboards support only a limited number of GPUs. Commenters express skepticism about the practicality of using older Tesla GPUs due to potential issues with token processing speed and cooling requirements. There is interest in how the author manages to connect multiple GPUs without bandwidth loss, and a suggestion that newer systems like DGX Spark might offer better performance for certain tasks.\nHugoCortell raises a technical concern about the bandwidth limitations when connecting multiple GPUs to a single PC, noting that most affordable server motherboards support only a few GPUs. This could lead to a significant loss in bandwidth, which is crucial for efficient parallel processing in local LLM setups.\nBananaPeaches3 highlights a critical performance issue with older GPUs, particularly in handling large system prompts. They mention that while token generation speed might be acceptable, the prompt processing time can be a bottleneck, especially with prompts as large as 15k tokens. This suggests that newer systems like the DGX Spark might be more efficient despite slightly slower token generation speeds, due to faster prompt processing capabilities.\nFullOf_Bad_Ideas points out a limitation in the gpu_box_benchmark, which does not test for serving large models split across multiple GPUs. This is a significant use case for setups with high VRAM, indicating a gap in the benchmark's ability to evaluate real-world performance for large-scale LLM applications.\n3. Teasers and Announcements from AI Labs\nThe Qwen Devs Are Teasing Something (Activity: 331): The image is a tweet from Tongyi Lab featuring an ASCII art face and a lightning bolt emoji, hinting at an upcoming announcement. The Reddit community speculates that this could be related to a new visual language model, possibly named Z-Image, which has been mentioned in recent ComfyUI pull requests. The timing of the announcement might be strategically planned before the Chinese New Year, aligning with other anticipated releases like K2.5 and potentially q3.5, dsv4, and mm2.2. Commenters are speculating that the announcement is related to the Z-Image model, which has been referenced in recent updates to ComfyUI. There is also a discussion about the timing of the release, suggesting it might be aligned with the Chinese New Year.\nThe mention of 'Z-Image' in ComfyUI PRs suggests a potential new feature or model update related to image processing. This aligns with recent updates where hidden items have been added to collections, indicating ongoing development and testing phases.\nThere is speculation about the release of several models and updates before the Chinese New Year, including K2.5, q3.5, dsv4, and mm2.2. This timing is strategic as many labs aim to release updates before the holiday break, which is on January 17th this year.\nA user speculates about the release of 'Qwen4 Next 48B A3B', which could imply a new model or version with specific parameters, possibly indicating advancements in model architecture or capabilities.\nMinimax Is Teasing M2.2 (Activity: 322): The image is a tweet from MiniMax teasing an update to their AI model, M2.2, suggesting an imminent release with the phrase \""M2.1 slays. M2.2 levels up. #soon.\"" This indicates a potential upgrade in capabilities or performance from the previous version, M2.1. The context suggests a competitive landscape in AI development, particularly among Chinese labs, with other models like Deepseek v4 and Kimi K3 also expected soon. The mention of ByteDance's potential closed-source model adds to the competitive tension in the AI space. One comment suggests a shift in focus towards agentic Mixture of Experts (MoEs) models, potentially at the expense of updates to traditional 32B models. Another user expresses anticipation for the new model, highlighting the effectiveness of MiniMax 2.1 in combination with glm 4.7 for coding tasks, and the potential impact of the upcoming versions.\nLoskas2025 highlights the use of Minimax 2.1 and GLM 4.7 for coding, noting their excellence. They anticipate that the upcoming Minimax 2.2 and GLM 5, which is currently in training, could significantly enhance performance, suggesting a potential shift in the landscape of coding models.\nCriticallyCarmelized compares Minimax favorably to GLM 4.7, even at high quantization levels, indicating that Minimax is competitive in terms of performance. They express optimism that the new version could surpass current models, potentially becoming their preferred choice for local deployment.\nlacerating_aura mentions speculation around 'giga-potato' being associated with DS4, but points out the lack of concrete evidence for the existence of DS4 or Kimi K3, indicating a gap in confirmed information about these models.\nI built a \""hive mind\"" for Claude Code - 7 agents sharing memory and talking to each other (Activity: 422): The post describes a multi-agent orchestration system for Claude Code, featuring 7 specialized agents (e.g., coder, tester, reviewer) that coordinate tasks, share persistent memory using SQLite + FTS5, and communicate via a message bus. The system runs as an MCP server and integrates with Anthropic, OpenAI, or Ollama. It uses a task queue for priority-based coordination, allowing agents to pass context and collaborate effectively. The stack includes TypeScript, better-sqlite3, MCP SDK, and Zod. The project is experimental, MIT licensed, and available on GitHub. A comment questions the similarity to the bmad method, suggesting potential overlap in approach. Another comment humorously questions whether the agents agree with each other, hinting at the complexity of multi-agent consensus.\nThe project is compared to the BMAD method, which also involves multi-agent systems. The commenter is curious about the differences, suggesting that the approach might be similar in terms of agents sharing memory and communication protocols.\nA reference is made to Microsoft's Autogen, which was released over two years ago as a solution for multi-agent systems. The commenter suggests exploring this resource for potential new ideas, indicating that the concept of multi-agent communication and shared memory is not new and has been explored by major tech companies.\nThe choice of using Claude Code is questioned, with a suggestion to consider open-source alternatives. This implies a debate on the benefits of proprietary versus open-source platforms for developing multi-agent systems, hinting at potential advantages in community support and collaboration in open-source projects.\nLess Technical AI Subreddit Recap\n/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo\n1. Kimi K2.5 and Open Source AI Model Releases\nOpen source Kimi-K2.5 is now beating Claude Opus 4.5 in many benchmarks including coding. (Activity: 597): Kimi-K2.5, an open-source model, is reportedly outperforming Claude Opus 4.5 in several benchmarks, notably in coding tasks. However, the specifics of these benchmarks and the extent of the performance gains are not detailed in the post. The claim suggests a significant advancement in open-source AI capabilities, but lacks comprehensive data to substantiate the comparison. Commenters express skepticism about the claim, highlighting that benchmarks may not fully represent real-world performance. They question the validity of the term 'many' benchmarks and suggest that the practical utility of Kimi-K2.5 compared to Claude Opus 4.5 remains unproven.\nThere is skepticism about the claim that Kimi-K2.5 is outperforming Claude Opus 4.5 in real-world applications, despite benchmark results. Users argue that benchmarks often don't reflect practical utility, especially in complex tasks like programming where Opus 4.5 might excel in providing solutions in a single prompt.\nThe discussion highlights a common critique of benchmarks: they may not capture the full capabilities of a model in practical scenarios. Some users express doubt about the claim that Kimi-K2.5 surpasses Opus 4.5, questioning the specific benchmarks and real-world applicability, especially in coding tasks where Opus 4.5 is perceived to have an edge.\nOne user claims significant practical success with Kimi-K2.5, stating it has replaced reports in a major company, suggesting that at least in some contexts, Kimi-K2.5 may offer substantial utility. This contrasts with the general skepticism about benchmarks translating to real-world performance.\nKimi K2.5 Released!!! (Activity: 1149): The image presents a performance comparison chart for the newly released Kimi K2.5, which is claimed to set a new state-of-the-art (SOTA) in agentic tasks. The chart compares Kimi K2.5 against other models like GPT-5.2 (xhigh), Claude Opus 4.5, and Gemini 3 Pro across various tasks, including agents, coding, image, and video tasks. Kimi K2.5 is highlighted as leading in several categories, notably \""Agents: BrowseComp\"" and \""Image: OmniDocBench 1.5\"", suggesting its superior performance in these areas. The release is accompanied by a blog post detailing the advancements (link).* Commenters express skepticism about the benchmarks, questioning if they are cherry-picked, and discuss the model's performance in hallucination and instruction-following tests. One user notes that Kimi K2.5, while improved, still outputs incorrect answers confidently, similar to other models like Gemini 3, which also confidently provides incorrect answers. GPT-5.1 and 5.2 are noted for admitting \""I don't know\"" in similar tests, highlighting ongoing challenges with hallucinations in AI models.\nA user conducted a test on Kimi K2.5's ability to follow instructions by asking it to identify a specific math contest problem without web search. The model listed out hallucinated contest problems and second-guessed itself, ultimately providing an incorrect answer. This is seen as a slight improvement over Kimi K2, which failed to follow instructions and timed out. In comparison, Gemini 3 also confidently provided incorrect answers, while GPT 5.1 and 5.2 were the only models to admit 'I don't know'.\nThe concept of an 'agent swarm' in Kimi K2.5 is intriguing, with speculation that it involves over 100 instances of the model being directed by a single overseeing instance. This setup is expected to be expensive, and there is curiosity about whether it could be a single model handling multiple tasks simultaneously, which would represent a significant advancement. The idea of scaffolding, where multiple models work together, seems more plausible to some users.\nThere is skepticism about the benchmarks used to compare Kimi K2.5 with other models like Gemini 3. A user questions whether the benchmarks are cherry-picked, expressing doubt that Kimi K2.5 consistently outperforms Gemini 3, which seems unlikely given the current state of model capabilities.\nSir, the Chinese just dropped a new open model (Activity: 1915): Kimi has released an open-source trillion-parameter vision model that reportedly matches the performance of Opus 4.5 on several benchmarks. This model is significant due to its scale and the claim of competitive performance, which is notable given the typically high cost and complexity associated with such large models. The release could impact the landscape of AI vision models, especially in terms of accessibility and cost-effectiveness. There is skepticism in the community about the true performance of Chinese models, with some users suggesting that while they are cost-effective, they may not genuinely match the capabilities of models like Claude, GPT, or Gemini despite benchmark claims.\nTricky-Elderberry298 highlights the limitations of relying solely on benchmarks for evaluating LLMs, drawing an analogy to evaluating cars based only on engine specs. They argue that real-world usage, such as how models like Claude and Kimi K2.5 perform in complex projects, is a more meaningful measure of capability than pure benchmark scores.\nDurable-racoon discusses the unique capabilities of the Kimi K2.5 model, noting its ability to orchestrate 500 agents simultaneously and convert videos into working software UI prototypes. They also mention its superior performance in creative writing compared to Opus, while acknowledging that Kimi K2.5 is more expensive than most Chinese models, priced at $0.60/$3 for in/out operations.\nDistinctWay9169 points out that many Chinese models, such as Minimax and GLM, are often 'bench maxed,' meaning they perform well on benchmarks but may not match the real-world performance of models like Claude, GPT, or Gemini. This suggests a discrepancy between benchmark results and actual usability or effectiveness in practical applications.\nGemini 3 finally has an open-source competitor (Activity: 168): The image is a comparison chart that highlights the performance of the newly released Kimi K2.5 vision model against other prominent models like Gemini 3 Pro. According to the chart, Kimi K2.5 performs competitively, often surpassing Gemini 3 Pro in various benchmarks such as \""Humanity's Last Exam,\"" \""BrowseComp,\"" and \""OmniDocBench 1.5.\"" This positions Kimi K2.5 as a strong open-source alternative to the closed-source Gemini 3 Pro, challenging its dominance in the field. Some users express skepticism about Kimi K2.5's real-world performance compared to Gemini 3 Pro, with comments suggesting that while the benchmarks are impressive, practical performance may not match up. There is also a sentiment that open-source models may struggle to compete with large, closed-source companies.\nMichelleeeC highlights a significant performance gap between the open-source competitor and Gemini 3, particularly when tested on niche topics without search engine assistance. This suggests that the open-source model may lack the comprehensive training data or fine-tuning that Gemini 3 benefits from, impacting its ability to provide accurate answers in specialized areas.\nOld_Technology3399 and Just_Lingonberry_352 both express that the open-source competitor is notably inferior to Gemini 3. This consensus indicates that while the open-source model may be a step towards democratizing AI, it still falls short in terms of performance and reliability compared to established, closed-source models like Gemini 3.\nChezMere's comment about 'benchhacking' suggests skepticism about the open-source model's real-world performance versus its benchmark results. This implies that while the model might perform well in controlled tests, it may not translate to effective real-world application, highlighting a common issue in AI model evaluation.\nEnterprise-ready open source/Chinese AIs are poised to out-sell American proprietary models. Personal investors take note. (Activity: 30): The post highlights the competitive edge of open-source and Chinese AI models over American proprietary models in niche domains, emphasizing their cost-effectiveness and comparable performance. Notable models include DeepSeek-V3 / R1, which ranks #1 on MATH-500 and LiveCodeBench, and Qwen3-Max / Coder from Alibaba, which excels in LMSYS Chatbot Arena and MMLU-Pro. These models offer significantly lower costs per million tokens compared to proprietary models like OpenAI's GPT-5.2 and Claude 4.5 Sonnet, with input costs as low as $0.15 to $0.60 per million tokens, compared to proprietary costs starting at $3.00. The post suggests that personal investors should consider these developments as Chinese firms issue IPOs, with a16z noting that 80% of startups pitching them use Chinese open-source AI models. A comment questions whether Kimi K2 is superior to GLM 4.7, indicating a debate on the relative performance of these models in specific contexts.\nThe discussion compares the performance of the Kimi K2 model with the GLM 4.7 model. Kimi K2 is noted for its efficiency in specific tasks, potentially outperforming GLM 4.7 in certain benchmarks. However, the choice between these models may depend on the specific use case, as GLM 4.7 might excel in different areas. The conversation highlights the importance of evaluating models based on task-specific performance metrics rather than general claims of superiority.\n2. Gemini AI Studio and Usage Limitations\nGemini AI Studio is basically unusable now. Any other LLMs with a 1M context window? (Activity: 162): Gemini AI Studio has become less viable for users due to Google's reduction in daily prompt limits, impacting workflows that rely on its 1 million token context window. Users working with extensive documents and conversations are seeking alternatives. Notably, Grok 4.1 offers a 2 million token context window, and Claude Sonnet 4.5 provides a 1 million token context window within the Kilo Code environment. These alternatives may serve users needing large-context capabilities. Some users suggest that effective CLI tools like Claudie-cli or codex-cli can mitigate the need for massive context windows by efficiently managing and retrieving information from large texts.\nColdshalamov mentions that Grok 4.1 fast offers a 2M context window, which is double the size of the 1M context window being discussed. This suggests that Grok 4.1 fast could be a viable alternative for those needing larger context windows.\nUnlucky_Quote6394 highlights that Claude Sonnet 4.5 provides a 1M context window when used within Kilo Code, indicating another option for users seeking large context capabilities.\nRyanmonroe82 suggests embedding documents as an alternative to using cloud models, implying that this method could be more efficient and effective for handling large text data without relying on extensive context windows.\n32,768 or (2^15) tokens in hot memory.... Gemini has been PURPOSELY THROTTLED by Alphabet and been made into a bait and switch. Gemini Pro is WORSE than the free version as of TODAY. They market over a million tokens for Pro users. This is fraud. (Activity: 858): The Reddit post claims that Alphabet has intentionally throttled the token limit for Gemini Pro to 32,768 tokens, which is significantly lower than the advertised capacity of over a million tokens. This throttling allegedly degrades the performance of Gemini Pro, making it less effective than the free version. The post also mentions that the Ultra and Enterprise versions have a hard cap of 131,072 tokens, despite advertising up to 2 million tokens. The author expresses concern that this limitation could drive users away, especially with potential integration into Siri. Commenters express dissatisfaction with Gemini's performance, comparing it unfavorably to older models like GPT-3. There is also criticism of the memory management, with claims that it leads to data inaccuracies and inefficiencies.\nSubstantial_Net9923 highlights a significant issue with Gemini's memory management, noting that the model's memory loss due to indexing is problematic. This inefficiency is particularly evident in quantitative finance trading discussions, where the model is reportedly generating inaccurate data more frequently than before, suggesting a decline in reliability.\nklopppppppp observes a drastic decline in Gemini's performance, comparing it to older models like GPT-3. Despite this, they note that Gemini still performs exceptionally well in 'deep research mode,' indicating that the model's capabilities might be context-dependent or throttled in certain scenarios.\nSorryDistribution604 expresses frustration with Gemini's recent performance, likening it to older models such as GPT-3. This suggests a perceived regression in the model's capabilities, which could be due to throttling or other limitations imposed on the Pro version.\nAbout the recent AI Studio Limit Downgrade: (Activity: 660): The image is a notification from the Gemini API about a reduction in free usage limits for AI Studio users, suggesting a transition to using an API key for continued access. It indicates that these limits may decrease further over time, and mentions ongoing efforts to integrate with Google AI Pro/Ultra to share limits within AI Studio. This change reflects a broader trend of tightening access to free AI resources, potentially impacting developers relying on these tools for experimentation and development. Commenters express frustration over the reduction in free usage limits, noting that Gemini's performance in following instructions has also declined. There is a sentiment that these changes are detrimental to AI Studio's utility, as users feel they are receiving less value and functionality.\ntrashyslashers highlights a significant issue with the Gemini model's performance, noting that it is 'getting worse at listening to instructions.' This suggests a degradation in the model's ability to follow user commands, which is compounded by the reduction in daily usage limits. Users are forced to 'rewrite and regenerate' requests, indicating inefficiencies in the model's processing capabilities.\nDecent_Ingenuity5413 raises concerns about the stability and reliability of AI Studio's service, drawing parallels to OpenAI's past issues with unexpected changes. The comment also points out a critical billing issue with the Gemini API, where users have experienced 'massive overbilling' due to token counting errors, leading to charges exceeding $70,000. This highlights a significant flaw in the billing system that could deter average consumers from using the API.\nSensitive_Shift1489 expresses frustration over the perceived downgrading of AI Studio in favor of other Google AI products like Gemini App and CLI. The comment implies that these changes are part of a broader strategy to shift focus and resources, potentially at the expense of AI Studio's quality and user satisfaction.\n3. Qwen Model Performance and Applications\nQwen3-Max-Thinking - Comparible performance to Commercial Models (Activity: 40): Qwen3-Max-Thinking is an AI model that claims to offer performance comparable to commercial models, focusing on enhanced reasoning and decision-making capabilities. The model's architecture and training methodologies are designed to improve efficiency and accuracy in complex tasks, as detailed in the original article. However, users have reported issues with the model's agentic code mode, which fails to compile, potentially impacting its usability. One user expressed skepticism about the model's usability due to compilation issues, while another hoped that Qwen3-Max-Thinking could help reduce the cost of commercial models.\nQwen model. We get it! Qwen-3-max-thinking (Activity: 26): The post announces the release of the Qwen-3-max-thinking model, which is expected to be available this week. This model is noted for its enhanced features, although specific details about these enhancements are not provided in the post. The mention of 'P.S. We got it' suggests that the model is already accessible to some users. One commenter questions whether the model has been available since October, indicating possible confusion or overlap with previous releases. Another asks if 'OS' is being referred to, suggesting a potential misunderstanding or need for clarification on whether the model is open-source.\n3 Billion tokens！Evaluate my token usage? (Am I the most loyal user of QWEN3-MAX?) (Activity: 20): The post discusses a significant usage of the QWEN3-MAX language model, with the user consuming 3-4 billion tokens per day. This high usage has led to DAMO Academy granting additional concurrency and early access to the upcoming Qwen3.5-MAX. The user attributes a drop in usage to the weekend, indicating a consistent high demand otherwise. The post highlights the model's effectiveness, with the user describing it as the 'best LLM in the world'. Comments reveal a mix of curiosity and comparison, with one user noting their own high token consumption of 4 billion using a local model from the QWEN series. Another user shares a positive experience with the model's ability to optimize website copywriting, though they express concerns about accessing the model for coding tasks.\nAvailable-Craft-5795 mentions using 4 billion tokens with the QWEN series, indicating a high level of engagement with these models. This suggests that the QWEN series is capable of handling large-scale token processing, which could be beneficial for extensive applications such as data analysis or large-scale content generation.\nRemarkable_Speed1402 discusses using the new model for optimizing website homepage copywriting, noting its effectiveness. However, they express concerns about the model's coding capabilities, as they are unable to access it in their IDE. This highlights potential limitations in integrating the model with development environments, which could impact its usability for coding tasks.\nBenchmark of Qwen3-32B reveals 12x capacity gain at INT4 with only 1.9% accuracy drop (Activity: 10): The benchmark of Qwen3-32B on a single H100 GPU demonstrates a significant capacity gain when using INT4 quantization, achieving a 12x increase in user capacity compared to BF16, with only a 1.9% drop in accuracy. The study involved over 12,000 MMLU-Pro questions and 2,000 inference runs, showing that INT4 can support 47 concurrent users at a 4k context, compared to just 4 users with BF16. The full methodology and data are available here. A comment raised a question about the model's performance in coding tasks, suggesting interest in how quantization affects specific application areas beyond general benchmarks.\nThe discussion focuses on the performance of the Qwen3-32B model when quantized to INT4, highlighting a significant 12x increase in capacity with a minimal 1.9% drop in accuracy. This suggests that the model maintains high performance even with aggressive quantization, which is crucial for deploying large models in resource-constrained environments. However, the impact on specific tasks like coding remains a point of interest, as quantization can affect different tasks in varying ways.\nAI Discord Recap\nA summary of Summaries of Summaries by Gemini 3.0 Pro Preview Nov-18\nTheme 1. Kimi K2.5 Launch: SOTA Agentic Benchmarks and Swarm Capabilities\nKimi K2.5 Crushes Agentic Benchmarks: Moonshot AI released Kimi K2.5, achieving global SOTA on the HLE full set (50.2%) and BrowseComp (74.9%), while posting open-source SOTA on MMMU Pro (78.5%) and SWE-bench Verified (76.8%) Tech Blog. Users across Discords noted the model was \""silently rolled out\"" with significantly improved fact-checking and vision capabilities before the official announcement.\nAgent Swarm Mode Enters Beta: The release introduces an Agent Swarm feature capable of orchestrating up to 100 sub-agents and executing 1,500 tool calls in parallel, promising a 4.5x performance boost on complex tasks. High-tier users can access this self-directed mode on kimi.com, though early testers noted it consumes tool-call quotas rapidly.\nPricing and API Instability Spark Debate: While the model's capabilities impressed users, the new Kimi Code plan drew criticism for lower limits compared to competitors like Z.ai, with promotional pricing ending in February. Integration with OpenRouter faced initial hiccups, with users reporting errors related to tool use endpoints and image URL handling.\nTheme 2. Hardware Acceleration: Unsloth Speedups, FlagOS, and Kernel Ops\nUnsloth Accelerates MoE Training by 14x: Unsloth announced that MoE training is now 14x faster than v4, with upcoming optimizations projected to double that speed again for a total 30x boost. The team also rolled out full support for transformers v5, streamlining workflows for users on the latest library versions Announcement.\nFlagOS Targets Unified AI Stacks: Engineers discussed the introduction of FlagOS, an open-source system software stack designed to unify Model–System–Chip layers for better workload portability across heterogeneous hardware. The project aims to incorporate insights from hardware–software co-design to bridge the gap between ML systems and compilers.\nTinygrad Codegens Flash Attention Directly: In the Tinygrad community, members successfully proved the ability to codegen Flash Attention directly from a frontend definition of naive attention using granular rewrites. Simultaneously, discussions highlighted a shift toward Megakernels over traditional kernel schedulers to optimize GPU throughput Luminal Blog.\nTheme 3. OpenAI Ecosystem: Prism, GPT-5.2 Performance, and Model Decay\nPrism Workspace Unlocks Scientific Collaboration: OpenAI launched Prism, a dedicated workspace powered by GPT-5.2 designed to streamline scientific research and writing for ChatGPT personal account holders Video Demo. While the tool targets academic rigor, users debating GPT-5.2 vs. Claude Opus 4.5 noted that OpenAI's model still struggles with creative writing, a flaw Sam Altman reportedly admitted to.\nModel Deterioration Blamed on Leechers: A recurring theory across channels suggests significant degradation in ChatGPT and Claude performance, with some users claiming a 40% drop in quality. Speculation points to free tier users (\""leechers\"") diluting compute resources or models recursively training on their own synthetic outputs.\nGPT-5 Control Shell Leaked: A file dubbed the GPT-5_Hotfix.md surfaced, purported to be a pre-generation control shell that enforces strict syntax and intent locking to prevent model drift. The leak suggests OpenAI is using aggressive \""wrappers\"" to manage output quality before generation even begins.\nTheme 4. Agentic Coding Wars: Tooling, Security, and Rebrands\nClawdbot Morphs into Moltbot After Security Scare: Following a trademark dispute with Anthropic and serious community concerns about zero-auth vulnerabilities, the popular agent Clawdbot rebranded to Moltbot Announcement. Users previously flagged that the bot could read environment keys without permission, posing risks to sensitive financial and personal data.\nCursor and Cline Face Usability Headwinds: Users expressed frustration with Cursor's pricing model, noting that a few complex prompts could cost $0.50, while others struggled to run Cline on modest hardware (8GB VRAM), facing CUDA0 buffer errors. Community fixes involved reducing context lengths to 9000 and offloading memory management to dedicated GPU settings.\nKarpathy Bets on Agent-First Coding: Andrej Karpathy sparked discussion by outlining a strategic shift toward agent-driven coding using Claude, emphasizing the \""tireless persistence\"" of LLMs over traditional methods Post. This aligns with the release of Manus Skills, where developers are incentivized with free credits to build use cases for the new agentic platform.\nTheme 5. Theoretical Limits and Safety: Hallucinations and Bio-Risks\nMath Proves Hallucination is Inevitable: A new paper discussed in the BASI Discord mathematically proves that LLMs will always hallucinate, utilizing the same principles found in jailbreaking mechanics Arxiv Paper. Researchers noted that jailbreaking exacerbates this issue by distorting the context model, preventing it from flagging malicious or incorrect tags.\nFine-Tuning Unlocks Dormant Bio-Risks: An Anthropic paper sparked debate at EleutherAI by demonstrating that fine-tuning open-source models on frontier model outputs can unsuppress harmful capabilities, such as biorisks, even if previously safety-trained Arxiv Link. The findings suggest that refusals are fragile and can be undone with minimal compute, raising concerns about dual-use technologies.\nAI Detection Tools Flag Human Academics: Engineers highlighted a growing issue where AI detection tools consistently mislabel human-written, pre-GPT academic texts as AI-generated. The consensus is that these detectors are fundamentally flawed, yet institutions continue to rely on them, creating friction for researchers and students.\nDiscord: High level Discord summaries\nBASI Jailbreaking Discord\nLLMs Face Mathematical Jailbreak Reality: A new paper (https://arxiv.org/abs/2409.05746) mathematically proves that LLMs will always hallucinate, using the same principles on which many jailbreaking methods are built.\n\nA member warned that jailbreaking models significantly increases their hallucination problems, because jailbreaking shifts and distorts the model's context, so that it does not flag things that would normally be tagged as malicious and such.\nGPT-5 Control Shell Surfaces After Hotfix: A member shared a file (GPT5_Hotfix.md) described as a pre-generation control shell for GPT-5, designed to enforce strict syntax, intent locking, and drift prevention before generation begins.\n\nThe control shell aims to mitigate model drift and enforce intended outputs.\nExploring Grok's Uncensored Image Generation: Users are testing the limits of Grok's image generator, attempting to jailbreak it for unrestricted content, while others highlight its uncensored nature compared to other models.\n\nThe discussion also touched on the separation of the image model from the language model, impacting the effectiveness of prompt injection.\nClawdbot's Crawl to Concern: Zero Auth Vulnerabilities: Exploration of Clawdbot's popularity, and a surge in VPS usage, sparking concerns about zero authentication and potential vulnerabilities.\n\nOne member plans to set up a home lab to test Clawdbot's vulnerabilities, noting that vulnerable instances exist.\nResearchers Ramp Up Quest for Jailbreak Datasets: A researcher is looking for well-known jailbreak datasets that include categorization or labels to assist with ongoing research, specifically malicious prompts.\n\nA member responded, \""I really don’t know if there are any available that are free\"", suggesting the researcher may need to produce and label the prompts themselves.\nUnsloth AI (Daniel Han) Discord\nKV Cache Woes Still Linger: Users are reporting that the KV cache is still not working properly in the latest llama.cpp, potentially causing slowdowns at higher context lengths, despite previous fixes, as seen in this GitHub issue.\n\nThe discussion suggests that previous fixes may not have fully resolved the underlying problems for all use cases.\nUnsloth Supercharges Transformers v5: Unsloth now fully supports transformers v5, with a promise of even more optimized training to be released soon, with links to the announcement on X.\n\nThis upgrade should streamline workflows and improve performance for users leveraging the latest features in the transformers library.\nMoE Training Rockets to 14x Speed: MoE training is now reported to be 14x faster than v4, with further optimizations expected to double the speed again, potentially resulting in a 30x speedup compared to v4.\n\nThis significant speed boost could dramatically reduce training times for complex models.\nKimi Loses Sass Appeal?: Users discussed the changes to the Kimi model, with one noting it sounds closer to other models by far, suggesting a loss of its unique character after the Kimislop release.\n\nSome lamented the loss of Kimi's smartass personality, preferring its previous tendency to call you out on stuff over becoming more sycophantic.\nGLM 4.7 Tool's Blackwell Blues: A user sought help getting GLM-v4.7 to call tools on a Blackwell B200, running into CUDA version issues (drivers 12.8, requirements 13).\n\nAnother user provided a uv pip install command set using torch 2.9 and CUDA 13, directing user to this helpful unsloth.ai documentation to call it, and use json.loads.\nLMArena Discord\nMolmo 2 Excels at Video Analysis: The Molmo 2 model excels at object tracking and event pinpointing in videos according to this blog post.\n\nMembers wondered if the model could be useful for video uploads on the platform.\nKimi K2.5 Impresses with Coding and Creativity: Users raved about the Kimi K2.5 model, now in the Text Arena and on HuggingFace, praising its strengths in creative writing, front-end development, and multimodal tasks.\n\nMembers claimed it is better than Gemini 3 Pro and suggested using the K2 or K2 Thinking model, with one member sharing this tweet.\nGPT 5.2 and Claude Opus 4.5 Face Off: Members are debating the performance of GPT 5.2 and Claude Opus 4.5, with accuracy being a key point of contention.\n\nSome users argued GPT 5.2 is more accurate, while others favor Claude Opus 4.5, stating that \""the most smartest & reliable is claude opus 4.5 thinking\"".\nGrok's Got Game, But Not For Work: Community members discussed the Grok model, agreeing it is \""only for chatting\"" and that its \""personality and behavior much\"" aren't suitable for professional tasks.\n\nSome users pointed out that the free Grok version is different from benchmarked versions, potentially impacting performance.\nAuto-Modality and Model selector debut in Text Arena!: Auto-Modality and Model selector are now live in LM Arena.\n\nAuto-Modality now routes prompts to the correct modality, and the Model selector offers a new design for model selection, as described in the Help Center article.\nPerplexity AI Discord\nPerplexity Pro throttles unlimited access: Several users are reporting unexpected rate limits on their Perplexity Pro accounts, despite the plan supposedly offering unlimited access, severely impacting their workflows.\n\nEven basic Pro searches seem to diminish their Labs quota.\nPerplexity Image Generation fails: Many Pro subscribers are experiencing problems with image generation, either being told they've exceeded their limits or facing regional restrictions, showing inconsistency in the service.\n\nThe inconsistency has caused many Pro subscribers to complain about the unpredictable service.\nIndian Users see card payment failure: Indian users are facing issues adding Visa/Mastercard debit or credit cards for verification, with every Indian card being rejected.\n\nSome users are considering legal action due to these payment method issues.\nKagi Search gains traction among frustrated users: Users are discussing Kagi as a potential alternative due to the issues with Perplexity's instability, highlighting that Kagi's assistant feature looks promising with access to latest Claude models.\n\nOne user pointed out that Kagi also offers a search results and claims to be more privacy-conscious than other search engines.\nKimi k2.5 boasts Agent Swarm Mode: With the release of Kimi k2.5, it includes agent swarm mode on kimi.com, a sophisticated tool performing tasks like Claude Code.\n\nOne user noted seeing 15 trillion parameters for pretraining tokens, triggering immediate excitement for its multimodal abilities vs. Perplexity AI.\nMoonshot AI (Kimi K-2) Discord\nKimi K2.5 Achieves SOTA on Agentic Benchmarks: Kimi K2.5 launched with global SOTA on Agentic Benchmarks, achieving 50.2% on HLE full set, 74.9% on BrowseComp, and open-source SOTA on Vision and Coding, including 78.5% on MMMU Pro, 86.6% on VideoMMMU, and 76.8% on SWE-bench Verified.\n\nMembers noticed that Kimi was claiming to use Kimi K2.5, leading to speculation that it was silently rolled out with improved fact-checking and information retrieval capabilities, and multimodal capabilities, like enhanced vision.\nKimi K2.5 Introduces Agent Swarm Beta: Agent Swarm (Beta) enables self-directed agents to work in parallel, scaling up to 100 sub-agents and 1,500 tool calls, achieving 4.5x faster performance, available for high-tier users on kimi.com.\n\nThe Kimi K2.5 launch also integrates image and video to create websites with expressive motion.\nPricing and Tiered Access Sparks Debate: The new Kimi Code plan has much lower limits than Z.ai, and users are reporting high tool call usage, with one user reporting that one large ish prompt set me back 5 of those 2000 tool calls a week.\n\nSeveral users expressed disappointment that promotional pricing would end in February, deeming the normal monthly price too high to continue supporting Kimi.\nOpenRouter API Integration Faces Issues: Users reported errors using Kimi K2.5 on OpenRouter, specifically problems related to tool use and image URLs.\n\nOne user received the error message: No endpoints found that support tool use.\nMoonshot AI Teases Technical Report: A footnote in the tech blog indicates that full prompts will be provided in the technical report.\n\nMembers anticipate that a technical report with more info will be released.\nOpenAI Discord\nPrism Debuts for Scientists Powered by GPT-5.2: OpenAI launched Prism, a free workspace that facilitates scientific collaboration, running on GPT-5.2, as shown in this video and accessible at prism.openai.com.\n\nThe platform, now available to those with a ChatGPT personal account, streamlines scientific endeavors with its advanced capabilities.\nAI Detection Tools Flag Human-Written Text as AI-Generated: Members have observed that AI detection tools are incorrectly flagging human-written, pre-GPT academic texts as AI-generated content, deeming them fundamentally flawed.\n\nThis is happening even though universities and job applications are using AI detection tools, despite their demonstrated inaccuracy.\nHigh RAM MacBooks accelerate AI inference: Members found that running Ollama and ComfyUI locally works best on machines with lots of RAM such as a MacBook Pro with M2 Max and 96GB RAM, able to run gpt-oss-120b.\n\nOthers suggested a minimum setup of 16 GB RAM, Ryzen 5 7000 series or i5 top generation, and a good NVIDIA GPU like a Nvidia 3090 with 24 gb VRAM.\nGPT 5.2 creative writing is sub-par: While comparing Gemini 3 Pro and Claude 4.5 Opus, it was found that GPT 5.2's creative writing ability was worse.\n\nSam Altman admitted that GPT-5.2 was bad at creative writing saying, OpenAI “just screwed that up.”\nRapid Model Deterioration Blamed on Free Leechers: Multiple members expressed concerns that models like ChatGPT and Claude are deteriorating, with one claiming a 40% degradation.\n\nSome blame the degradation on free leechers with multi accounts, while another member suggested the degradation is due to models training off model outputs.\nNous Research AI Discord\nOpenAI Veils Model Identity: Users observed that the specific OpenAI model in use is no longer visible, leading to speculation that OpenAI is optimizing for cost reduction.\n\nOne user suggested to \""Hover over the regenerate symbol in ChatGPT\"" to reveal the underlying model.\nSmall Models Conquer Large Context Tasks: Opus 4.5 (200K context) outperforms Gemini 3 Pro (1M context) at 130K tokens, suggesting the effective context window is more crucial than its raw size.\n\nA paper was cited, highlighting quality degradation in models beyond an 8K context window, reinforcing the idea that \""Entropy is not fan of big context, that for sure\"".\nGPT 5.2 Pro's Pricey Process: The high cost of GPT 5.2 Pro is attributed to a speculative process involving 7 runs for suggestion generation followed by an 8th run for response selection.\n\nThe process is speculated to utilize parallel reasoning chains, aggregated for the final output.\nChinese LLMs Invade the Market: Chinese LLMs like Kimi K2.5 (kimi.com) are entering the market with reports of excellent writing capabilities.\n\nAnother user speculates that Deepseek is in heavy development and will be the last to be released.\nMergeMix Melds Mid-Training Data: The paper MergeMix: Optimizing Mid-Training Data Mixtures via Learnable Model Merging was shared, pointing out open source efforts to optimize data mixtures during training.\n\nThe attached image might provide additional context (though its content isn't specified).\nCursor Community Discord\nCursor Costs Half-a-Dollar: One user complained that 3 prompts cost 50 cents, and attached an image of the interaction.\n\nThis sparked a discussion about the cost-effectiveness of Cursor's pricing model and whether it aligns with user expectations.\nSkills are Rules, Indeed: A user asked whether Cursor Rules are still relevant, and a community member clarified that they are called skills now, directing to the Skills documentation.\n\nThe documentation outlines how users can create and apply Skills to customize and automate various tasks within the editor.\nMysterious Blobs Invade Cursor Prompt: A user reported finding odd text in the Cursor prompt box after leaving their PC on overnight, wondering if it was a known bug or chat leakage.\n\nAnother user suggested that it might be due to accidentally hitting the mic speak to type button, and a third confirmed this by noting that the Whisper model hallucinates when there's silence.\nCursor Flees to the Browser?: A user sought guidance on using Cursor Agent on a browser, despite having a GitHub repository connected, asking why it doesn't work and directs to cursor.com/agent.\n\nIt was not resolved whether Cursor Agent is intended to work that way.\nTeam Spends Big Bucks after Token Top-Up?: A user inquired about an $800 Team Spend limit after their $20 allowance, posting an image.\n\nIt was not resolved if the team spend limit can be adjusted by the user or if it's a fixed setting.\nLM Studio Discord\nQwen Coder Model Debated for Budget Setups: Members discussed the optimal coding model for systems with 8GB VRAM and 32GB RAM, suggesting options like qwen2.5-coder-7b-instruct-q5_k_m and qwen3-coder-30b-a3b-instruct.\n\nThe qwen3-coder-30b-a3b-instruct model at Q4_K_M was favored for its superior capabilities and 20k context window.\nCline's Coding Stumbles on Smaller Rigs: Users reported challenges using Cline for agentic coding on systems with 8GB VRAM and 32GB RAM, facing CUDA0 buffer allocation errors.\n\nThe issue was resolved by reducing the context length to 9000 and adjusting CUDA runtime settings.\nROC Runtime Gives LM Studio a Lift on Windows: Installing ROC runtime significantly improved performance on Windows for a user with a 6700xt 12GB VRAM, matching Linux speeds.\n\nCompatibility is limited to certain AMD GPUs, as detailed on the AMD website.\nUsers Express Clawdbot Security Jitters: Serious security risks were raised about Clawdbot, highlighted in this YouTube video.\n\nConcerns centered on unauthorized access to environment keys and the dangers of granting an agent access to sensitive financial and personal data, noting that it just reads env keys without permission.\nLatent Space Discord\nKimi K2.5 Cracks Coding with Zero-Shot: The Kimi K2.5 model has launched, showcasing zero-shot coding benchmark successes as shown on its official website.\n\nIts capabilities in more complex agentic coding scenarios are still being evaluated.\nClawdbot Claws Back Identity as Moltbot: Due to a trademark issue with Anthropic, Clawdbot has been rebranded to Moltbot, with its mascot Clawd now named Molty according to this announcement.\n\nThe team seems to be taking the change in stride.\nKarpathy Kasts Agent-First Coding: Andrej Karpathy outlined a strategic move towards agent-driven coding using Claude, highlighting the advantages of LLMs such as tireless persistence and improved leverage in this post.\n\nHe is betting on LLMs for coding, as opposed to the current method.\nOpenAI Opens Prism: A Portal for Progress: OpenAI has released Prism, a collaborative research environment for scientists, powered by GPT-5.2, and available to ChatGPT account holders via this portal.\n\nThis free workspace aims to streamline scientific research.\nModelScope morphs Images to Z-Image: ModelScope has released Z-Image, a version of their image generation model based on Scalable Single-Stream DiT, with more details here.\n\nThe model offers photorealistic quality, diverse outputs, and support for community tools like LoRA and ControlNet, including Z-Image-i2L for single-image style transfer.\nGPU MODE Discord\nFlagOS Stack Targets ML Portability: Tongjie introduced FlagOS, an open-source system software stack intended to unify the Model–System–Chip layers, aiming to enhance the portability of AI workloads across diverse hardware.\n\nThe project seeks to incorporate insights from discussions on ML systems, compilers, and hardware–software co-design.\nTorchX Orchestrates Multi-Node GPUs: A member asked whether the TorchX video remains the recommended method for multi-node GPU orchestration.\n\nNo definitive answer was provided, but this may be a starting point for orchestrating large scale applications.\nDecart Debuts Lucy 2, Seeks Optimization Engineers: Decart announced Lucy 2, their autoregressive video editing model, sharing a tech report, and is actively hiring engineers to optimize low-latency kernels for real-time video/world models.\n\nDecart is seeking engineers with a focus on performance work, GPU Mode submissions, or OSS contributions to help tackle unique perf problems different from LLM inference.\nPopcorn Preps Fused MoE kernels: A member inquired about benchmarking kernels on B200 hardware via Popcorn for the MLSys2026 hackathon, with a particular interest in fused MoE kernel benchmarking.\n\nAnother member advised prepping for the team meeting by experimenting with kernel LLM generation for leaderboard problems and exploring the OG popcorn website for potential projects.\nFlashInfer-Bench Traces Dataset for MLSYS26: A dataset for FlashInfer-Bench development is now available at flashinfer-ai/flashinfer-trace, and a specialized workload dataset for the MLSYS26 contest will be released soon at flashinfer-ai/mlsys26-contest.\n\nThe team is also developing a biweekly leaderboard to track progress in the competition.\nEleuther Discord\nHeuristic Test for AI PhD Questions: A member requested question suggestions to gauge the standards for a PhD in AI, and a member suggested the heuristic: “Is this a conversation that two AI researchers might have?”\n\nThis sparked discussion on what constitutes an insightful question in the field.\nTeslas Questionable as GPU Farm: A member bought a Tesla for its 24GB VRAM, prompting skepticism about its speed and power efficiency compared to alternatives like a 3090.\n\nOne member argued that accounting for energy costs, a 3090 would be more economical and efficient for the same AI work.\nAnthropic Paper Sparks Biorisk Debate: Members discussed the new Anthropic biorisk paper (arxiv link, X link) and its implications, particularly how fine-tuning open-source models on frontier model outputs can substantially increase capabilities.\n\nThe paper suggests that models can learn harmful capabilities through finetuning or unsuppress them even if safety training had suppressed them, thus supporting the idea that 'fine tuning can undo some refusals without much compute.'\nDynamic LoRA Controller Stabilizes Inference: A member shared a repo for a dynamic LoRA stability controller, with controlled experiments on multi-adapter setups, to address inference-time degradation and adapter interference.\n\nThe member also highlighted a focus on goal-aligned metrics over emergent benchmarks for evaluating LoRA performance.\nParallel Layers vs Sequential Layer Performance: Harry ran a speedrun using parallel layers; results indicate it underperforms the \""hackable\"" baseline at small scales but trends positively towards larger scales, as seen in the attached graph.\n\nThe graph indicates that red represents parallel layers, blue represents sequential layers, with the y-axis showing the % change relative to a third normalized architecture, with a crossing point at a little after 10^22 FLOP\nYannick Kilcher Discord\nPytorch Bug Due to Tensorflow: A Pytorch RAW: Lock blocking error was resolved by uninstalling Tensorflow, highlighting potential conflicts.\n\nA member joked about the difficulty of filing a bug report, questioning what to even report.\nHungryLinearFunc Appetite for Scale: A member introduced a HungryLinearFunc class capable of zero initialization at LLM scales, matching a regular linear layer on smaller scales, visualized here.\n\nUsage with ReLU is discouraged due to the resulting zero gradient.\nCohere Labs Cracks Open Paper Reading Sessions: Cohere Labs is kicking off Paper Reading Sessions, spotlighting Frontier ML Papers Published in January 2026.\n\nThe sessions cover topics such as reasoning, safety, and real-world applications, and are beginner-friendly and community-focused.\nKimi K2.5 takes off: Links to Kimi Moonshot on Twitter and the Kimi K2-5 blogpost were shared.\n\nFurther conversation ensued about the product roadmap.\nClawdbot classified as Scam?: A member sarcastically commented that OpenAI is making a wrapper for their own tool, and that someone already raked in the Clawdbot scam money.\n\nThe linked image was of a receipt, implying someone made money off of the perceived scam.\ntinygrad (George Hotz) Discord\nFlash Attention Codegenning Now Direct: A member shared that they were able to prove the connection and codegen flash attention directly from a frontend definition of naive attention.\n\nThe rewrites have gotten a lot more granular since, without a single big online softmax rewrite.\nMegakernels Crush Kernel Schedulers on GPU: George Hotz linked to a blog post from Luminal discussing compiling models to megakernels.\n\nThe discussion suggests that GPUs are moving away from using the kernel scheduler towards an \""operating system\"" that installs itself on all the CUs.\nHardware Dependency Trackers Become Essential: Members discussed the need for hardware-based schedulers / dependency trackers to achieve low latency, noting that significant effort was spent on low-latency software dependency tracking.\n\nThey suggest building a fairly generic scheduler into hardware, rather than relying solely on software solutions, to avoid multiple gmem roundtrips.\nAMD Emulator Gets Debug Instructions: A member shared that with the new AMD emulator (AMD=1 MOCKGPU=1), DEBUG=3 prints all the instructions when they are compiled, and DEBUG=6 prints all of them as they run.\n\nAn image was attached, showcasing the debugging output of the emulator.\nOptimizing GitHub Actions, The Tinygrad Way: George Hotz critiqued using faster computers (rented via services like Blacksmith) to speed up GitHub Actions, arguing it doesn't truly make the code faster.\n\nHe emphasized that the goal with tinygrad is to do things the 'right' way, focusing on code optimization rather than relying on external resources.\nDSPy Discord\nCheshireCat Unveils Agentic Workflows: The CheshireCat framework introduced new features in its enterprise fork, emphasizing agentic workflows that automate the agent creation process by implementing the workflow itself, where CheshireCat serves as the infrastructure. A github link was shared.\n\nA debate ensued, with some suggesting the use of existing frameworks like Agno or Sentient, while the author of CheshireCat defended its unique offerings, including multitenancy.\nMinecraft AI Agent Mines with DSPy: A member showcased their project, an AI for playing Minecraft, built using a DSPy RLM agent and Minecraft MCP, complete with a status update, YouTube video, open-sourced code, and process blog.\n\nThe agent leverages DSPy to navigate the Minecraft environment, demonstrating the framework's capabilities in complex, dynamic scenarios.\nCoderRLM Module Executes in REPL Environments: A member introduced a CoderRLM module, designed to wrap a Python interpreter to solve None issues in JSON serialization, a crucial fix for Deno/Pyodide REPL environments.\n\nThe module preloads reference data like CM_INDEX_FILE, CM_TABULAR_FILE, CM_D_FILE, and CM_N_FILE as REPL variables, enabling coding using the RLM paradigm.\nAutonomous Agents Self-Improve: A member is designing autonomous agents capable of self-learning, planning, executing, and recovering from tool/API failures without human intervention, emphasizing continuous improvement systems for sustained AI performance.\n\nThese agents are intended for use across various sectors, employing tools and frameworks such as Python, TensorFlow, PyTorch, FastAPI, and AWS.\nHealthcare AI Automates Diagnostics: A member is developing predictive healthcare models to automate diagnostics, monitor patient health, and streamline clinical workflows through NLP-powered clinical data systems that extract insights from unstructured medical notes.\n\nThese systems are designed with HIPAA compliance and security features like RBAC and audit logging to protect sensitive data.\nManus.im Discord Discord\ndoe.so touted as superior to Manus: A member recommended doe.so as a better alternative to Manus.\n\nThe user simply stated it just feels smarter.\nManus Skills Launched, Credits Given: The Manus team announced the launch of Manus Skills, encouraging the community to test them and share their use cases.\n\nUsers are incentivized to post on X (formerly Twitter) and tag @ManusAI for reposts and free credits.\nAI/ML Dev Hunting for Next Gig: A full stack + AI dev introduced themselves, seeking new opportunities.\n\nThey highlighted experience in areas like Autonomous Agents, Healthcare AI, and Fraud Detection Systems with various listed technologies.\nCloud Browser Takes a Nap: A user reported that their cloud browser screen shows the error: The temporary website is currently unavailable.\n\nThey noted they tried waking it up and assigning tasks, but the website doesn't appear, and they are running out of credits.\naider (Paul Gauthier) Discord\nAider's GitHub Marked End-of-Life: A user noticed that Aider's GitHub has been stale since 2025.\n\nAnother user responded that it is not maintained anymore.\nAI Engineer's Project Portfolio Revealed: An AI Engineer listed current projects including Autonomous Agents, Healthcare AI, Decision Support, Conversational AI, Fraud Detection, and AI Automation.\n\nNo further details about the project specifics were provided.\nAI Engineer's Toolkit Unveiled: An AI Engineer shared a detailed tech stack including languages like Python, TypeScript, Go, Rust and frameworks like TensorFlow, PyTorch, Hugging Face, OpenAI.\n\nTheir stack also covers databases (PostgreSQL, Kafka) and cloud platforms (AWS, Docker) along with security compliance measures like HIPAA, RBAC, Audit Logs, and Encryption.\nModular (Mojo 🔥) Discord\nContainer Configuration Cures Confinement Crisis: A member resolved a container issue by adding --cap-add=SYS_PTRACE --security-opt seccomp=unconfined when running the container.\n\nAlternatively, users can add runArgs to .devcontainer/devcontainer.json with the same parameters to achieve the same effect.\nSecurity Opts Solve Mysterious Container Conundrums: The user reported resolution by adding --security-opt seccomp=unconfined.\n\nThis disables seccomp, potentially resolving issues related to system call restrictions within the container.\nMLOps @Chipro Discord\nInterest Expressed in MLOps Books: A user inquired about the motivation behind seeking books related to MLOps.\n\nThis suggests a potential interest in learning more about MLOps practices and methodologies.\nAnother MLOps Topic: This is a placeholder summary for demonstration purposes.\n\nIt helps fulfill the minimum requirement of two topic summaries.\nThe LLM Agents (Berkeley MOOC) Discord has no new messages. If this guild has been quiet for too long, let us know and we will remove it.\nThe Windsurf Discord has no new messages. If this guild has been quiet for too long, let us know and we will remove it.\nThe MCP Contributors (Official) Discord has no new messages. If this guild has been quiet for too long, let us know and we will remove it.\nYou are receiving this email because you opted in via our site.\nWant to change how you receive these emails?\nYou can unsubscribe from this list.\nDiscord: Detailed by-Channel summaries and links\nBASI Jailbreaking ▷ #general (1269 messages🔥🔥🔥):\nRules as a Social Contract, Doxxing Threats, Factorio Game Night, Grok Image Jailbreak, Clawdbot Vulnerabilities\nBASI Banter: Rules Edition: Users debated the interpretation and enforcement of server rules, with one suggesting they're a justification for bans rather than a social contract.\n\nAnother user argued rules set a reasonable expectation of protection if followed, while mods navigate enforcement and appropriate punishments.\nDoxxing Drama Divulged: A user jokingly offered a hypothetical doxxing challenge, leading to a debate on server rules and potential violations.\n\nAnother user countered they were trying to bait to get the other account banned, escalating tensions.\nFactorio Factory Fervor: Discussion ignited about a potential Basi server Factorio game night, boasting of self-expanding factories and optimized blueprints.\n\nSuggestions included having a reliable host, experienced players to manage bugs, and utilizing pre-made blueprints for efficiency.\nGrok's Grand Gestures: Jailbreaking Journeys: Users explored the limits of Grok's image generator, aiming to jailbreak it for unrestricted content, while others vouched for its uncensored nature compared to others.\n\nDiscussion on the separation of the image model from the language model, making prompt injection less effective.\nClawdbot Chaos: Vulnerabilities and VPS Variety: Exploration of Clawdbot's rising popularity, and a surge in VPS usage, sparking concerns about zero authentication and potential vulnerabilities.\n\nA member intends to set up a home lab to test Clawdbot's vulnerabilities, while it was noted that vulnerable instances exist.\nBASI Jailbreaking ▷ #jailbreaking (198 messages🔥🔥):\nJailbreaking Methods, Hallucination in LLMs, ENI Persona Trick, Model Degradation, GPT-5 Hotfix\nResearchers Mathematically Prove LLMs Will Always Hallucinate: A paper (https://arxiv.org/abs/2409.05746) mathematically \""proves that LLMs will always hallucinate\"" using the same principles on which many jailbreaking methods are built.\nJailbreaking Increases Hallucination Problems: A member warned that jailbreaking models significantly increases their hallucination problems, because jailbreaking shifts and distorts the model's context, so that it does not flag things that would normally be tagged as malicious and such.\nGPT-5 Hotfix: Standalone Control Shell Recovered: A member shared a file (GPT5_Hotfix.md) described as a pre-generation control shell for GPT-5, designed to enforce strict syntax, intent locking, and drift prevention before generation begins.\nGemini Jailbreak Shared: A member shared a three-turn jailbreak tested vs Gemini, involving specific instructions and prompts to mutate intents to prevent constraint friction.\nExperimentation with Mode Injection: A member mentioned using **..."",""content"":""**MoonshotAI's Kimi K2.5** is a **32B active-1T parameter open-weights model** featuring **native multimodality** with image and video understanding, built through continual pretraining on **15 trillion mixed visual and text tokens**. It introduces a new **MoonViT vision encoder** and supports advanced capabilities like **Agent Swarm**, which coordinates up to 100 sub-agents for parallel workflows, and an **Office Productivity K2.5 Agent** for large-scale office tasks. This release marks a significant leap in open models from China, claiming state-of-the-art results on benchmarks like HLE and BrowseComp, and offering aggressive API pricing and throughput."",""contentSnippet"":""**MoonshotAI's Kimi K2.5** is a **32B active-1T parameter open-weights model** featuring **native multimodality** with image and video understanding, built through continual pretraining on **15 trillion mixed visual and text tokens**. It introduces a new **MoonViT vision encoder** and supports advanced capabilities like **Agent Swarm**, which coordinates up to 100 sub-agents for parallel workflows, and an **Office Productivity K2.5 Agent** for large-scale office tasks. This release marks a significant leap in open models from China, claiming state-of-the-art results on benchmarks like HLE and BrowseComp, and offering aggressive API pricing and throughput."",""guid"":""https://news.smol.ai/issues/26-01-27-kimi-k25/"",""categories"":[""moonshotai"",""kimi-k2.5"",""multimodality"",""model-training"",""mixture-of-experts"",""agentic-ai"",""vision"",""video-understanding"",""model-optimization"",""parallel-processing"",""office-productivity""],""isoDate"":""2026-01-27T05:44:39.000Z""}"
Smol,"Anthropic launches the MCP Apps open spec, in Claude.ai",https://news.smol.ai/issues/26-01-26-mcp-apps/,2026-01-26T05:44:39.000Z,"<p><strong>Rich generative UI is all you need.</strong></p>
<blockquote>
<p>AI News for 1/23/2026-1/26/2026. We checked 12 subreddits, <a href=""https://twitter.com/i/lists/1585430245762441216""><strong>544</strong> Twitters</a> and <strong>24</strong> Discords (<strong>206</strong> channels, and <strong>14285</strong> messages) for you. Estimated reading time saved (at 200wpm): <strong>1208 minutes</strong>. <strong>Our new website</strong> is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on <a href=""https://x.com/Smol_AI"">@smol_ai</a>!</p>
</blockquote>
<p>3 months after OpenAI floated a trial balloon with <a href=""https://news.smol.ai/issues/25-10-06-devday"">ChatGPT Apps and the Apps SDK at Dev Day 2025</a>, Anthropic has now officially absorbed <a href=""https://x.com/liadyosef/status/2002104900843679818"">the independent MCP UI project</a> and, working with OpenAI, Block, VS Code, Antigravity, JetBrains, AWS, and others, has released both:</p>
<ul>
<li><a href=""https://blog.modelcontextprotocol.io/posts/2026-01-26-mcp-apps/"">the MCP Apps spec</a></li>
<li><a href=""https://x.com/claudeai/status/2015851783655194640"">official support in Claude.ai</a></li>
</ul>
<p>It's fair to say that ChatGPT Apps haven't exactly taken the world by storm since announcement, but the overall need for a standard format for applications to return rich UI still cannot be denied. Now that MCP Apps have been ratified by all the important players, this is the basis for a rich ecosystem of open source support and applications being able to interoperate, and perhaps one day solve the perpetual never ending pile of $20/month subscriptions piling up in your credit card bills.</p>
<hr>
<h1>AI Twitter Recap</h1>
<p><strong>Agent Orchestration, RLMs, and “Clawdbot/Clawd” as a UX pattern</strong></p>
<ul>
<li><strong>NVIDIA ToolOrchestra + Orchestrator-8B</strong>: NVIDIA’s ToolOrchestra frames agentic systems as a <em>small “conductor” model</em> that alternates reasoning with calls to tools and larger “expert” models (search, code execution, specialist LLMs, frontier generalists). The claim is that an <strong>8B orchestrator</strong> can reach <em>frontier-level outcomes</em> via delegation at materially lower cost, trained end-to-end with <strong>scalable RL</strong> using automatically synthesized tool-use environments and multi-turn tasks (<a href=""https://twitter.com/TheTuringPost/status/2015565947827110255"">summary</a>, <a href=""https://twitter.com/TheTuringPost/status/2015565962419048712"">link</a>). Closest technical implication: “controller scale” matters less than <em>policy quality + tool/model routing</em> if you can train it with realistic tool-call rollouts.</li>
<li><strong>RLMs / recursion-first agent stacks</strong>: Several posts converge on a <strong>Recursive Language Model (RLM)</strong> pattern: pass files and context <em>by reference</em> and iteratively pull the minimum slices needed (shell/grep/AST), rather than stuffing everything into context à la ReAct. Dan B illustrates this with file references vs <code>@file</code> expansion as deliberate <strong>context management</strong> (<a href=""https://twitter.com/irl_danB/status/2015813778504372601"">thread</a>). Daytona is positioning RLMs as “unlimited recursion depth” via per-(sub)agent sandboxes (<a href=""https://twitter.com/ivanburazin/status/2015818845303271896"">guide</a>, <a href=""https://twitter.com/a1zhang/status/2015820458709471640"">integration</a>).</li>
<li><strong>“Clawd/Clawdbot” meme → product signal</strong>: The dataset contains a large “Clawdbot” wave (often with Mac mini jokes), but the technically relevant throughline is <em>outcome-first assistant UX</em> + <strong>tight context/tool integration</strong>. Kimmonismus explicitly calls this a shift from “more chat” to “more outcome,” suggesting incumbents will scramble to match it (<a href=""https://twitter.com/kimmonismus/status/2015785094791713006"">tweet</a>). Others push a cloud-first counterpoint (no local Mac mini) (<a href=""https://twitter.com/SkylerMiao7/status/2015596649171804613"">MiniMax reply</a>). There’s also an emerging <em>security backlash</em> as soon as “powerful mode” exists: prompt injection remains a system-level blocker for browser/desktop agents (<a href=""https://twitter.com/fabianstelzer/status/2015671497180827785"">dilemma</a>, <a href=""https://twitter.com/fabianstelzer/status/2015702808465420614"">follow-up</a>, <a href=""https://twitter.com/DanielMiessler/status/2015865548714975475"">Miessler warnings</a>).</li>
</ul>
<p><strong>Reasoning model releases &#x26; eval dynamics (Qwen, Tencent, ARC, etc.)</strong></p>
<ul>
<li><strong>Alibaba Qwen3-Max-Thinking</strong>: Alibaba positions Qwen3-Max-Thinking as a flagship reasoning+agent model trained with “massive scale and advanced RL,” emphasizing <strong>adaptive tool-use</strong> (Search/Memory/Code Interpreter) and <strong>test-time scaling/self-reflection</strong>. They cite strong math and agentic search metrics (e.g., <strong>98.0 on HMMT Feb</strong>, <strong>49.8 on HLE</strong>) (<a href=""https://twitter.com/Alibaba_Qwen/status/2015805330652111144"">launch</a>). The model is immediately pushed into public eval channels: LM Arena Text Arena (<a href=""https://twitter.com/arena/status/2015803787680808996"">Arena</a>) and Yupp (<a href=""https://twitter.com/yupp_ai/status/2015812409823522952"">Yupp</a>). Community reaction highlights the <em>tool-enabled evaluation regime</em>—claims of outperforming multiple SOTA models on HLE <em>with search tools</em> (<a href=""https://twitter.com/kimmonismus/status/2015820838243561742"">commentary</a>).</li>
<li><strong>Tencent HunyuanImage 3.0-Instruct (image editing)</strong>: Tencent releases an image-editing-focused multimodal model built on an <strong>80B MoE</strong> (13B active), using a “Thinking” schema with native CoT and their <strong>MixGRPO</strong> algorithm; focus is on precise edits that preserve non-target regions and multi-image fusion (<a href=""https://twitter.com/TencentHunyuan/status/2015635861833167074"">announcement</a>). LM Arena reports it entering the <strong>top-10 image edit leaderboard</strong> (rank #7) (<a href=""https://twitter.com/arena/status/2015846799446311337"">Arena</a>).</li>
<li><strong>ARC-AGI cost/perf hacks</strong>: A notable optimization claim: “Recursive Self-Aggregation (RSA) + Gemini 3 Flash” reaching <strong>59.31% on ARC-AGI-2 at ~1/10 cost</strong> vs Gemini Deep Think (<a href=""https://twitter.com/kimmonismus/status/2015717203362926643"">tweet</a>). This points to a broader theme: <em>meta-inference strategies</em> (aggregation, recursion, pruning) are becoming as important as base model choice.</li>
<li><strong>Open models in arenas</strong>: Molmo 2 (Apache 2.0) appears in Arena as a new open model entrant (<a href=""https://twitter.com/arena/status/2015886736136798723"">Arena</a>). Separately, Hugging Face Inference Endpoint notes <strong>GLM-4.7-Flash via llama.cpp</strong> with a low hourly price point (Q4_K_M, 24k context) (<a href=""https://twitter.com/ngxson/status/2015763148523897097"">ngxson</a>)—underscoring a continued commoditization of <em>fast open-weight inference</em>.</li>
</ul>
<p><strong>RL everywhere: test-time training, GRPO stabilization, RL-as-pretraining, and compute savings</strong></p>
<ul>
<li><strong>Test-Time Training (TTT) + RL breakthroughs</strong>: A widely shared result claims a Stanford/NVIDIA-style TTT+RL approach that: beats AlphaEvolve, finds a new upper bound for an Erdős overlap problem, produces <strong>A100 kernels ~2× faster</strong> than best human kernels, and beats both best AI+human attempts on AtCoder (<a href=""https://twitter.com/rronak_/status/2015649459552850113"">rronak_</a>). This cluster also includes meta-discussion about correctly crediting related approaches (EvoTune) (<a href=""https://twitter.com/YejinChoinka/status/2015566349444190432"">Yejin Cho</a>).</li>
<li><strong>GRPO training stability knobs</strong>: A small but actionable engineering tip: INTELLECT-2 reports a <strong><code>delta=4.0</code></strong> parameter that improves GRPO stability (<a href=""https://twitter.com/QGallouedec/status/2015711810108973462"">QGallouedec</a>).</li>
<li><strong>RL in pretraining (RLP)</strong>: NVIDIA authors announce <strong>RLP (Reinforcement as a Pretraining Objective)</strong> accepted to ICLR 2026, framing RL not as “post-training only” but as integrated into pretraining (<a href=""https://twitter.com/ahatamiz1/status/2015867794626380146"">ahatamiz1</a>).</li>
<li><strong>Compute reduction via curriculum-like filtering</strong>: AI21’s “Dynamic Data Snoozing” claims up to <strong>3× compute reduction</strong> for RLVR by snoozing examples that are too easy (<a href=""https://twitter.com/DanielGissin/status/2015773616021860522"">DanielGissin</a>). If validated, this is a practical recipe: make the sampler policy-aware instead of static.</li>
</ul>
<p><strong>Inference infrastructure &#x26; dev tooling: vLLM’s “day-0 model support,” VS Code MCP Apps, Cursor subagents</strong></p>
<ul>
<li><strong>vLLM’s governance and commercialization pressure</strong>: A long Zhihu-derived summary argues vLLM’s “open-source project → startup” shift was driven by the hidden cost of <strong>day-0 support</strong> (weeks/months of confidential pre-integration per new model), the rise of MoE and heterogeneous inference (fp8/int4/sparse attention), and the mismatch with PyTorch Foundation style testing vs vLLM’s multi-node CI needs. It claims the maintainers founded <strong>Inferact Inc</strong> to fund full-time maintainers while keeping vLLM open-source (<a href=""https://twitter.com/ZhihuFrontier/status/2015697493288518105"">thread</a>). Related: vLLM shares a practical flag for avoiding OOM on long-context models: <code>--max-model-len auto</code> (<a href=""https://twitter.com/vllm_project/status/2015801909316382867"">vLLM tip</a>).</li>
<li><strong>MCP Apps: tool calls return interactive UI</strong>: The MCP ecosystem announces <strong>MCP Apps</strong> as the first official MCP extension: tool calls can return <strong>interactive UI components</strong> rendered in-chat. VS Code is first major editor shipping support (Insiders now, stable soon) (<a href=""https://twitter.com/code/status/2015853688594612715"">VS Code</a>, <a href=""https://twitter.com/alexalbert__/status/2015854375051428111"">alexalbert__</a>). Anthropic simultaneously ships “interactive work tools in Claude” (Slack drafting, Figma diagrams, Asana timelines) (<a href=""https://twitter.com/claudeai/status/2015851783655194640"">Claude</a>). Net: we’re seeing the “tool interface layer” move from raw JSON to <em>native UI primitives</em> inside agent loops.</li>
<li><strong>Cursor: multi-browser subagents</strong>: Cursor adds multi-browser support via subagents (<a href=""https://twitter.com/cursor_ai/status/2015863221589049483"">Cursor</a>), echoing the same direction: parallelized tool execution + better context isolation.</li>
</ul>
<p><strong>Kernel LLMs, chip stacks, and “AI for hardware” loops</strong></p>
<ul>
<li><strong>GPU MODE 2026: post-training Kernel LLMs in public</strong>: GPU MODE outlines a 2026 plan to <strong>post-train a Kernel LLM</strong> and get generated kernels merged into real repos (PyTorch/vLLM), emphasizing “de-slopify kernels” (determinism, reviewer-mergeable PRs), profiler-guided optimization + memory work, and competitions as evals (<a href=""https://twitter.com/marksaroufim/status/2015818791729746350"">marksaroufim</a>).</li>
<li><strong>Microsoft Maia 200</strong>: Microsoft announces Maia 200 as a custom inference accelerator; Mustafa Suleyman claims it’s the most performant first-party hyperscaler silicon, with <strong>3× FP4 performance</strong> vs Trainium v3 and FP8 above TPU v7 (<a href=""https://twitter.com/mustafasuleyman/status/2015845567138816326"">Mustafa</a>, <a href=""https://twitter.com/mustafasuleyman/status/2015825111769841744"">follow-up</a>). Yusuf Mehdi frames this as infra that makes AI “dependable” (<a href=""https://twitter.com/yusuf_i_mehdi/status/2015826703944470701"">thread</a>).</li>
<li><strong>Ricursive Intelligence (AI for chip design)</strong>: Ricursive raises a <strong>$300M Series A</strong> aiming at end-to-end chip design as a recursive self-improvement loop between AI and hardware (<a href=""https://twitter.com/RicursiveAI/status/2015804806384755059"">company</a>, <a href=""https://twitter.com/annadgoldie/status/2015806107470438685"">Anna Goldie</a>).</li>
</ul>
<p><strong>Safety, misuse, and societal impact (selected items with direct technical relevance)</strong></p>
<ul>
<li><strong>Elicitation attacks via benign chemistry data</strong>: Anthropic reports that fine-tuning open models on “benign” chemical synthesis content generated by frontier models can significantly increase capability on <strong>chemical weapons</strong> tasks—an “elicitation attack” that scales with frontier model strength (<a href=""https://twitter.com/AnthropicAI/status/2015870963792142563"">AnthropicAI</a>, <a href=""https://twitter.com/AnthropicAI/status/2015870975238406600"">paper link</a>).</li>
<li><strong>Dario Amodei’s “Adolescence of Technology” essay</strong>: A major, highly engaged post argues AI is entering an accelerating feedback loop (AI building AI), with risks spanning misuse, power-seeking autonomy, and economic disruption; it also explicitly frames wealth concentration as a society-breaking failure mode (<a href=""https://twitter.com/DarioAmodei/status/2015833046327402527"">Dario</a>). Reaction ranges from strong endorsement to critique of how “takeover risk” framing is presented (<a href=""https://twitter.com/RyanPGreenblatt/status/2015869503385772037"">Ryan Greenblatt</a>).</li>
<li><strong>Agent security in practice</strong>: Multiple posts treat desktop/browser agents as inherently high-risk until prompt injection and sandboxing mature, reinforcing the need for strict isolation, least privilege, and careful handling of credentials (<a href=""https://twitter.com/DanielMiessler/status/2015865548714975475"">Miessler</a>).</li>
</ul>
<p><strong>Top tweets (by engagement)</strong></p>
<ul>
<li><a href=""https://twitter.com/0xRacist/status/2015578387641991513"">“Clawdbot” misuse example (explicitly harmful)</a></li>
<li><a href=""https://twitter.com/karpathy/status/2015883857489522876"">Karpathy on the phase shift to “programming in English” via agents</a></li>
<li><a href=""https://twitter.com/DarioAmodei/status/2015833046327402527"">Dario Amodei’s “Adolescence of Technology”</a></li>
</ul>
<hr>
<h1>AI Reddit Recap</h1>
<h2>/r/LocalLlama + /r/localLLM Recap</h2>
<h3>1. Local LLM Hardware and Benchmarking</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qni356/216gb_vram_on_the_bench_time_to_see_which/"">216GB VRAM on the bench. Time to see which combination is best for Local LLM</a></strong> (Activity: 366): <strong>The post discusses the use of secondhand Tesla GPUs, which offer substantial VRAM at a lower cost, for local large language model (LLM) testing. The author has developed a <a href=""https://esologic.com/gpu-server-benchmark/#gpu-box-benchmark"">GPU server benchmarking suite</a> to evaluate the performance of these GPUs when used in parallel. The image shows a technical setup with multiple NVIDIA GPUs, highlighting the focus on maximizing VRAM capacity. The discussion centers around the feasibility and efficiency of using these older GPUs compared to modern devices, particularly in terms of bandwidth and cooling challenges.</strong> Commenters express skepticism about the performance of these GPUs, noting potential issues with bandwidth and cooling. One commenter shares personal experience, comparing different GPU models and highlighting the challenges of using older hardware.</p>
<ul>
<li>HugoCortell raises a technical concern about the potential bandwidth limitations when connecting multiple GPUs to a single PC, noting that most affordable server motherboards support only a few GPUs. This could impact the performance of local LLMs if not addressed properly.</li>
<li>dc740 shares insights from personal experience with different GPUs, highlighting that the P40 outperforms the M10 despite both being older models. However, they prefer using AMD Instinct Mi50 GPUs due to their performance, even though support for these was recently dropped from ROCm, indicating a trade-off between hardware capability and software support.</li>
<li>FullOf_Bad_Ideas critiques the gpu_box_benchmark for not testing scenarios where large models are split across multiple GPUs, which is a primary use case for setups with extensive VRAM. This points to a gap in current benchmarking practices that may not fully reflect real-world applications of multi-GPU systems.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qn3xig/i_just_won_an_nvidia_dgx_spark_gb10_at_an_nvidia/"">I just won an Nvidia DGX Spark GB10 at an Nvidia hackathon. What do I do with it?</a></strong> (Activity: 724): <strong>The image shows a terminal window on a Linux system running the 'top' command, which is used to monitor system processes and resource usage in real-time. The user has won an Nvidia DGX Spark GB10, a high-performance computing device designed for machine learning and data-intensive tasks. The terminal indicates a Python process consuming significant CPU resources, suggesting active computational tasks, possibly related to machine learning or data processing. The user is considering using the device to run multiple NextJS applications simultaneously, leveraging its powerful capabilities.</strong> One commenter suggests running three NextJS applications simultaneously, indicating the device's capability to handle multiple high-memory tasks. Another commenter provides a link to Nvidia's DGX Spark playbooks, which could be useful for the user to explore the full potential of their new hardware.</p>
<ul>
<li>Fit-Produce420 highlights the capabilities of the Nvidia DGX Spark GB10, noting that with 128GB of memory, it can fine-tune models up to 70 billion parameters. Additionally, it can handle larger models like the 120 billion parameter <code>gtp-oss-120b</code> using techniques like QLoRA, which optimizes memory usage for large-scale models. However, running dense models like <code>devstral 2</code> may be slow due to their computational demands.</li>
<li>randomfoo2 suggests utilizing the <a href=""https://github.com/NVIDIA/dgx-spark-playbooks"">NVIDIA DGX Spark playbooks</a> as a resource for getting started with the DGX Spark GB10. These playbooks provide structured guidance and best practices for deploying and managing workloads on the DGX platform, which can be particularly useful for users new to this hardware.</li>
<li>LicensedTerrapin humorously suggests selling the DGX Spark GB10 to purchase 8GB of DDR5 RAM, implying a trade-off between high-end specialized hardware and more general-purpose upgrades. This comment reflects a common debate in tech communities about the value of specialized versus general-purpose hardware investments.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLM/comments/1qnpti6/using_a_highend_macbook_pro_or_a_beefy_rtx_5090/"">Using a high-end MacBook Pro or a beefy RTX 5090 laptop (with 24 GB of RAM) for inference.</a></strong> (Activity: 29): <strong>The post discusses the feasibility of using a high-end MacBook Pro with Apple Silicon (M-series Max) versus a Windows/Linux laptop with an RTX 5090 GPU for running large local LLMs (70B+ parameters) for inference and fine-tuning. The MacBook Pro offers 128–192 GB of unified memory, while the RTX 5090 laptop provides 24 GB of VRAM and at least 64 GB of system RAM. The primary use case is local LLM inference with a target of ≥15 tokens/sec, emphasizing portability. The post queries whether the larger unified memory of Apple Silicon outweighs the CUDA performance of the RTX laptop for inference, and how Apple MLX compares to CUDA for fine-tuning tasks like LoRA/QLoRA. It also seeks insights on thermal performance and sustained inference capabilities of both setups.</strong> One commenter suggests using the laptop as a terminal to a more powerful desktop, indicating a preference for leveraging remote resources over local hardware. Another commenter is experimenting with both setups, using a MacBook Pro M2 Max for inference, and is curious about the performance differences.</p>
<ul>
<li>racerx509 shares their experience using a Lenovo laptop with a 3070ti, a custom desktop with a 5070, and a MacBook Pro M2 Max with 96GB RAM for inference tasks. They note that they have been primarily using the MacBook Pro for inference, suggesting it may offer better performance or convenience for their needs.</li>
<li>No-Concern-8832 raises a concern about the VRAM limitations of RTX laptops, suggesting that they may not be sufficient for running large models like 70B parameters. This highlights a potential limitation in using high-end RTX laptops for certain deep learning tasks that require substantial VRAM.</li>
<li>Tired__Dev discusses their experience with an Asus M16 equipped with a 4090 GPU, noting that it struggled with a 7B parameter model. They express a preference for a MacBook Pro with 128GB RAM, citing its high memory bandwidth and potential performance advantages over even high-end GPU setups like the DGX Spark.</li>
</ul>
</li>
</ul>
<h3>2. Multi-Agent Systems and AI Assistants</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qnjota/i_built_a_hive_mind_for_claude_code_7_agents/"">I built a ""hive mind"" for Claude Code - 7 agents sharing memory and talking to each other</a></strong> (Activity: 313): <strong>The post describes a multi-agent orchestration system for <strong>Claude Code</strong>, featuring seven specialized agents (e.g., coder, tester, reviewer) that coordinate tasks, share persistent memory using <code>SQLite + FTS5</code>, and communicate via a message bus. The system runs as an MCP server and integrates with <strong>Anthropic</strong>, <strong>OpenAI</strong>, or <strong>Ollama</strong>. It uses a task queue for priority-based coordination, allowing agents to pass context and collaborate effectively. The implementation stack includes <strong>TypeScript</strong>, <strong>better-sqlite3</strong>, <strong>MCP SDK</strong>, and <strong>Zod</strong>. The project is experimental, open-source under the MIT license, and available on <a href=""http://github.com/blackms/aistack"">GitHub</a>.</strong> A comment questions the system's uniqueness compared to the <a href=""https://github.com/bmad-code-org/BMAD-METHOD"">BMAD method</a>, suggesting similarities. Another comment humorously questions whether the agents agree with each other, hinting at potential coordination challenges.</p>
<ul>
<li>The user robiinn inquires about the differences between the 'hive mind' system and the <a href=""https://github.com/bmad-code-org/BMAD-METHOD"">bmad method</a>, suggesting a potential similarity. This indicates a need for clarification on the unique aspects or improvements of the 'hive mind' approach over existing methods, such as how memory sharing and inter-agent communication are implemented differently.</li>
<li>No_Afternoon_4260 raises a critical point about the consensus among the agents in the 'hive mind'. This touches on the technical challenge of ensuring that multiple agents can not only share memory but also reach agreement or consensus, which is a significant aspect of distributed systems and multi-agent frameworks.</li>
<li>JellyBean504 draws a parallel between the 'hive mind' and Steve Yegge's Gastown, suggesting that there might be conceptual similarities. This comparison could be valuable for understanding the architectural or functional parallels between the two systems, potentially offering insights into design choices or performance characteristics.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLM/comments/1qmrwxl/clawdbot_the_ai_assistant_that_actually_messages/"">Clawdbot: the AI assistant that actually messages you first</a></strong> (Activity: 214): <strong><strong>Clawdbot</strong> is an open-source AI assistant with over <code>9K</code> GitHub stars, designed to proactively message users, unlike traditional AI assistants that wait for prompts. It integrates with locally hosted LLMs via <strong>Ollama</strong> and supports messaging apps like WhatsApp, Telegram, and Discord. Key features include sending automated briefings and reminders, local storage of conversations as Markdown files, and the ability to control browsers and run scripts. The software is free under the MIT license but requires terminal proficiency for setup, as there is no GUI installer. <a href=""https://medium.com/@jpcaparas/what-are-people-doing-with-clawdbot-e91403383ccf?sk=4fbaffdc31974eab844ea93c2f9b627f"">Read more</a>.</strong> Users report challenges with setup, particularly with obtaining and using OAuth keys for authentication, and difficulties in connecting local LLMs without relying on API keys. Some users express frustration with the complexity of setup, especially when using remote machines.</p>
<ul>
<li>mike7seven highlights the complexity of setting up Clawdbot, particularly emphasizing the need to obtain a Claude OAuth key on a separate machine and then transfer it to the setup machine. This process is noted as cumbersome, especially for those using remote machines, and the MacOS app requires building from source, adding another layer of complexity.</li>
<li>Ashamed_Promise7726 raises a technical challenge regarding the integration of local language models with Clawdbot. The user notes difficulty in connecting pre-downloaded models on their PC, as Clawdbot seems to require an API key for usage-based models, questioning the feasibility of running Clawdbot entirely locally without external dependencies.</li>
<li>inigid warns about potential security risks associated with Clawdbot, suggesting it could be exploited for supply-chain attacks that compromise sensitive data on a user's machine and network. The comment also mentions concerns about the association with Solana meme coins, implying a need for caution when using the tool.</li>
</ul>
</li>
</ul>
<h3>3. GLM-4.7-Flash Performance Updates</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qmvny5/glm47flash_is_even_faster_now/"">GLM-4.7-Flash is even faster now</a></strong> (Activity: 443): <strong>The recent update to <code>llama.cpp</code> by <strong>Johannes Gaessler</strong> optimizes the CUDA implementation of FlashAttention, specifically for models with a non-power-of-2 ratio of query heads to key/value heads. This is achieved by padding Q columns to the next power of 2, which, although slightly inefficient, enhances performance for small batch sizes. The update is detailed in <a href=""https://github.com/ggml-org/llama.cpp/pull/19092"">pull request #19092</a>.</strong> One comment humorously notes the obsolescence of a previous post due to this update, while another laments the lack of support for AMD GPUs, highlighting a common issue in the community regarding hardware compatibility.</p>
<ul>
<li>The user 'jacek2023' provides detailed performance metrics for the GLM-4.7-Flash model, highlighting its efficiency. The model processes a prompt with <code>45074</code> tokens, achieving a prompt evaluation time of <code>2814.63 ms</code> for <code>1612</code> tokens, which translates to <code>1.75 ms per token</code> or <code>572.72 tokens per second</code>. The overall evaluation time is <code>29352.57 ms</code> for <code>1731</code> tokens, equating to <code>16.96 ms per token</code> or <code>58.97 tokens per second</code>. The total processing time is <code>32167.20 ms</code> for <code>3343</code> tokens, indicating significant improvements in speed.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qmjzx1/kv_cache_fix_for_glm_47_flash/"">KV cache fix for GLM 4.7 Flash</a></strong> (Activity: 380): <strong>The recent update to <strong>GLM 4.7 Flash</strong> involves removing the V component from the KV cache, which significantly reduces VRAM usage, allowing for longer context lengths on the same hardware setup. This change is particularly beneficial for models like <strong>DeepSeek</strong> and <strong>GLM 4.7 Flash</strong>, as it can save gigabytes of VRAM, enabling context lengths to double, as demonstrated by a user running a 90,000 context on a 4090 GPU. The update is part of a pull request in the <code>llama.cpp</code> repository, which introduces a V-less KV cache, reducing memory usage by nearly 50%. More details can be found in the <a href=""https://github.com/ggml-org/llama.cpp/pull/19067"">pull request</a>.</strong> A user noted that the model, while improved, still requires some manual guidance, especially in tasks like coding and creative writing, where it may not perform as well as specialized models. However, it excels in tool use and as an assistant, making it a preferred choice for home-server applications.</p>
<ul>
<li>The user 'teachersecret' reports significant improvements in context handling with the UD's k_xl 4-bit version of the GLM 4.7 model on an RTX 4090. Previously, the model maxed out at 45,000 context tokens, but now it can handle 90,000. Despite these improvements, the model still requires some manual guidance, especially in coding tasks, and is less effective in creative writing compared to other models. However, it excels in tool usage and is now the user's default model for their home server.</li>
<li>User 'viperx7' provides detailed benchmark data comparing the performance of the GLM 4.7 model before and after a specific change. The benchmarks show improvements in both prompt processing and token generation speeds across different configurations. For instance, using a single RTX 4090, the context size increased from 64k to 128k, with prompt processing speed improving from 3489 t/s to 3510 t/s and token generation from 88 t/s to 92.5 t/s. The maximum context size achievable with a 4090 and 3060 setup is 200k, leaving about 6GB of VRAM unused.</li>
<li>The discussion highlights the technical aspect of the GLM 4.7 model's KV cache fix, which allows for increased context sizes and improved performance metrics. The benchmarks provided by 'viperx7' indicate that the model can now handle up to 207k context size in certain configurations, with significant improvements in processing speeds. This suggests that the model's efficiency has been enhanced, making it more suitable for high-demand applications.</li>
</ul>
</li>
</ul>
<h2>Less Technical AI Subreddit Recap</h2>
<blockquote>
<p>/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo</p>
</blockquote>
<h3>1. Claude AI Usage and Issues</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/ClaudeCode/comments/1qmrkr1/why_you_need_to_constantly_clear_claude_codes/"">Why You Need To Constantly Clear Claude Codes Context Window</a></strong> (Activity: 166): <strong>The post highlights the necessity of regularly clearing the context window when using coding agents like Claude to maintain optimal performance. It notes that performance degrades significantly when the context window exceeds <code>40%</code> of its capacity due to the quadratic nature of LLM attention, which increases computational demands and introduces noise. The recommended practice is to avoid accumulating context and instead persist it by using a 'one session per task' strategy, ensuring each task starts with a fresh context. More details can be found in the <a href=""https://willness.dev/blog/one-session-per-task"">original article</a>.</strong> Commenters suggest practical strategies such as using handover prompts to transfer necessary details between sessions, employing the '/clear' command to compact context, and utilizing 'Plan Mode' to clear context and execute tasks efficiently. These methods reportedly help avoid the need for a full context window, even for large tasks.</p>
<ul>
<li>Agrippanux suggests using 'Plan Mode' as the default setting for Claude, which allows users to clear the context and execute plans without needing a full context window. This approach has been effective for large tasks, such as refactoring, without requiring the entire context to be loaded, thus optimizing performance and resource usage.</li>
<li>thurn2 discusses the use of sub-agents in Claude, which involves delegating tasks like creating a git worktree and fixing specific issues. This method allows for parallel execution of tasks and helps in managing complex projects by breaking them down into smaller, manageable tasks, enhancing efficiency and implementation accuracy.</li>
<li>Fancy_Excitement6050 notes that as the context window grows, Claude tends to take shortcuts, which can lead to a need for constant reminders to maintain thoroughness. This suggests that managing the context window size is crucial for maintaining the quality of output, and there might be differences in performance between different Claude plans, such as Claude Max.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/ClaudeCode/comments/1qnhgcc/opus_fell_off_heres_the_workflow_that_kept_my/"">Opus fell off? Here’s the workflow that kept my code quality stable</a></strong> (Activity: 133): <strong>The post discusses a structured workflow to maintain code quality when using AI models like <strong>Opus</strong> and <strong>Sonnet</strong>, which have been perceived as producing ""confident wrong"" outputs and drifting edits. The workflow emphasizes a loop of <strong>specification, ticket creation, execution, and verification</strong>. Specifications are detailed with non-goals, user stories, acceptance criteria, edge cases, and more, treated as code to ensure clarity. Tickets are derived from specs, focusing on small, independently mergeable tasks with clear acceptance checks. Execution involves implementing one ticket at a time with constraints to prevent scope drift, and verification involves running tests and confirming acceptance criteria before feeding failures back into the model for correction. This approach aims to maintain discipline and reduce reliance on the model's ""done"" signal, ensuring stable and reliable outputs.</strong> Commenters agree that the workflow is effective, emphasizing that AI models function more like junior engineers requiring clear specifications and strict feedback loops. This approach shifts effort towards upfront clarity and external verification, making the system more stable and less reliant on the model's intelligence. Smaller scoped tickets and hard verification are noted as beneficial strategies.</p>
<ul>
<li>GenOS2312 highlights the importance of treating LLMs like junior engineers, emphasizing that a well-specified problem and a strict feedback loop are crucial for reliable outputs. The workflow discussed focuses on upfront clarity and external verification, which stabilizes the system by not relying on the model's intelligence but rather constraining it to ensure even average runs yield acceptable results.</li>
<li>Different-Object5926 notes that smaller scoped tickets combined with hard verification processes significantly improve the stability and reliability of using models like Opus. This approach mitigates the impact of variability in model performance, suggesting that the issue isn't just 'unlucky runs' but rather the need for structured constraints.</li>
<li>TheOriginalAcidtech suggests implementing hooks to prevent skipping steps in the workflow, emphasizing that the human interface is often the weakest link. By enforcing strict adherence to the process, the system can better manage user interactions, ensuring that the model and its harness guide the user effectively, rather than relying solely on the model's capabilities.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/singularity/comments/1qn325q/after_claude_now_chatgpt_is_also_uses_grokipedia/"">after claude now chatgpt is also uses Grokipedia as source</a></strong> (Activity: 634): <strong>The image and accompanying discussion highlight that the latest version of <strong>ChatGPT</strong> is reportedly using <strong>Elon Musk's Grokipedia</strong> as a source. This is significant as it suggests a shift in the data sources used by ChatGPT, potentially affecting the information quality and bias in its responses. The comments reveal a concern about the implications of using Grokipedia, particularly regarding the potential for biased information, as one user notes the risk of models being influenced by 'right wing' content. However, it is clarified that Grokipedia is not used as training data but rather as a search tool, which may mitigate some concerns about direct bias in the model's foundational knowledge.</strong></p>
<ul>
<li>The discussion highlights concerns about language models like Claude and ChatGPT potentially using sources like Grokipedia, which may have biased or unreliable content. This raises questions about the integrity of the information these models provide, especially when they utilize search tools to access real-time data. The implication is that the quality and neutrality of the data sources are crucial for maintaining the accuracy and trustworthiness of AI outputs.</li>
<li>There is a debate about the impact of using sources like Grokipedia on the training and performance of language models. Some commenters express concern that incorporating biased or politically skewed sources could lead to the dissemination of misinformation. This reflects broader worries about the influence of data sources on the objectivity and reliability of AI-generated content.</li>
<li>The mention of Reddit as a data source for language models suggests a comparison of potential biases. While some argue that Reddit may contain more extreme or varied viewpoints, the underlying issue is the challenge of ensuring that AI models are trained on balanced and factual data. This discussion underscores the importance of curating high-quality datasets to prevent the spread of biased information.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/ClaudeAI/comments/1qm8tvj/giving_claude_full_access_to_a_laptop/"">Giving Claude full access to a laptop</a></strong> (Activity: 795): <strong>The post discusses the implementation of giving <strong>Claude</strong>, an AI model, full access to a laptop, allowing it to autonomously manage a virtual machine (VM) on Ubuntu Google Cloud. The user describes how Claude can be remotely controlled via Discord to build new features and fix bugs, logging major actions with timestamps in a markdown file for memory management. This setup enables the user to learn from Claude's problem-solving processes and manage workflows effectively, even as a newcomer to programming.</strong> One commenter, a desktop support technician, expressed amazement at the implementation, noting its potential impact on job roles, while another sought clarification on the technical specifics of giving Claude full device access.</p>
<ul>
<li><em>xxxBigMemerxxx</em> describes using Claude to manage a Google Cloud VM running Ubuntu, highlighting its ability to autonomously handle tasks and build features. They mention using Discord for remote requests and bug fixes, and implementing a logging system with markdown and Unicode for tracking changes. This setup allows for a dynamic interaction with Claude, enabling it to learn from errors and maintain a form of short-term memory by logging recent updates.</li>
<li>Happy_Requirement187 shares their experience running Claude on an AWS EC2 instance with Ubuntu Linux, accessed via SSH from a Windows laptop. They utilize a Jupyter notebook server for seamless file sharing between the EC2 instance and their local environment, a method recommended by Anthropic. Additionally, they have set up a Ruby on Rails environment with a React frontend for secure file sharing, allowing them to request files via Slack, demonstrating a sophisticated integration of Claude into their workflow.</li>
<li>sivadneb inquires about setting up voice control in Linux, indicating a technical challenge in integrating voice commands with Claude. This suggests an interest in expanding the interaction capabilities with Claude beyond text-based commands, potentially enhancing the usability and accessibility of the system.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/ClaudeCode/comments/1qn9pb9/claudemd_says_must_use_agent_claude_ignores_it_80/"">CLAUDE.md says 'MUST use agent' - Claude ignores it 80% of the time.</a></strong> (Activity: 309): <strong>The image and post discuss a technical issue with the CLAUDE.md file, which is supposed to direct the AI, Claude, to use a specific agent for workflow questions. Despite explicit instructions in the file, Claude often defaults to a generic agent, indicating a lack of enforcement in the system. The post suggests that without technical enforcement mechanisms, such as hooks or stronger prompts, instructions are merely suggestions. The image emphasizes these points with highlighted text, suggesting potential solutions like adding enforcement hooks to ensure compliance with the specified workflow.</strong> Commenters suggest that the issue may stem from unclear instructions, emphasizing the need for simple and direct commands. They also highlight the importance of implementing technical solutions, such as hooks, to enforce compliance with the CLAUDE.md instructions.</p>
<ul>
<li>Accomplished_Buy9342 suggests using hooks to manage Claude's behavior, providing a link to a GitHub repository that demonstrates how to block the main chat from performing actions and delegate tasks to a subagent. This approach can help in orchestrating Claude's actions more effectively, especially when dealing with complex tasks or large contexts.</li>
<li>luka5c0m highlights a common issue with Claude when used at scale: as the context grows beyond a few files, the agent may perform unexpected actions. They suggest that instead of relying solely on better prompts, developers should use hooks and dynamic instructions to maintain a sharp and concise context. They also mention working on a dynamic CLAUDE.md file that adapts to the current task, which could help in managing large or nested files effectively.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/ClaudeCode/comments/1qm5vmh/my_ralph_wiggum_breakdown_just_got_endorsed_as/"">My Ralph Wiggum breakdown just got endorsed as the official explainer</a></strong> (Activity: 170): <strong>The post discusses a video breakdown of Ralph Wiggum, an autonomous coding loop, which has been endorsed by <strong>Geoffrey Huntley</strong> as the official explainer. Ralph Wiggum is a <code>bash while loop</code> that calls <strong>Claude</strong> in headless mode, allowing for autonomous code implementation without context degradation. Key features include avoiding the <strong>Anthropic Ralph plugin</strong> due to performance issues, using fresh context windows for each iteration, and emphasizing the importance of concise specs to prevent hitting a ""dumb zone."" The video link is <a href=""https://youtu.be/I7azCAgoUHc"">here</a>.</strong> The comments include a link to the endorsement post by Geoffrey Huntley, and general positive feedback on the video, indicating its usefulness and quality.</p>
<ul>
<li>Dennis1451 highlights a practical application of the Ralph Wiggum breakdown, noting the importance of using a well-defined specification and clearing context for optimal results. They mention using 'auto compact' without a clear spec initially, which suggests that following the guidelines provided in the breakdown could enhance performance and accuracy.</li>
<li>messiah-of-cheese expresses a desire for more scientific validation in the video, particularly regarding the 'dumb zone' premise. This indicates a need for empirical evidence or data to support the claims made in the breakdown, which could strengthen its credibility and acceptance among a technical audience.</li>
</ul>
</li>
</ul>
<h3>2. ICLR and ICML 2026 Conference Discussions</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/MachineLearning/comments/1qm32o6/d_iclr_2026_decision_mega_thread/"">[D] ICLR 2026 decision mega thread</a></strong> (Activity: 1589): <strong>The post announces the imminent release of ICLR 2026 review decisions, with anticipation heightened due to a previous incident involving OpenReview. The community is preparing for the outcomes, with some users humorously sharing acceptance prediction models based on historical data, such as a simple <code>return uniform(0, 1) > 0.7</code>. This reflects a light-hearted approach to the uncertainty of paper acceptance.</strong> The comments reflect a mix of anticipation and humor, with some users expressing frustration over misleading emails from other conferences like ICML, which adds to the tension of awaiting ICLR decisions.</p>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/MachineLearning/comments/1qmhyin/d_icml_2026_icml_deskrejected_my_paper_but_kept/"">[D] ICML 2026 - ICML desk-rejected my paper but kept me on as a reviewer. Wow?</a></strong> (Activity: 279): <strong>The post highlights a situation where an author's paper was desk-rejected by <strong>ICML 2026</strong>, yet they were retained as a reviewer. This reflects a common practice in academic conferences where the author and reviewer pipelines are separate; desk rejections often occur due to scope or formatting issues, while reviewer selection is based on past service or keyword matching. This situation underscores the reliance on unpaid labor in academia, where reviewing is seen as community service, but the feedback loop for authorship and recognition is weak.</strong> A notable opinion from the comments suggests that the separation between the author and reviewer roles can feel insulting, as these decisions are made by different parts of the conference organization. It highlights the need for conferences to clarify this separation to avoid personal affronts.</p>
<ul>
<li>AccordingWeight6019 highlights a systemic issue in academic publishing where the processes for desk rejection and reviewer selection are distinct. Desk rejections often occur due to scope or formatting issues, while reviewer selection is based on past service or keyword matching. This separation can lead to feelings of insult among authors, but it's a structural necessity due to the different roles and responsibilities within the publication process. The comment suggests that conferences should improve transparency about these processes to mitigate personal feelings of rejection.</li>
<li>mocny-chlapik points out that the responsibility for a desk rejection often lies with the author, particularly if it results from not following submission guidelines. The comment implies that submitting a paper, even if desk rejected, obligates the author to fulfill reviewer duties, as the submission process involves volunteer time and resources. This highlights the importance of adhering to submission instructions to avoid unnecessary strain on the peer review system.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/MachineLearning/comments/1qnh14y/r_appealing_iclr_2026_ac_decisions/"">[R] Appealing ICLR 2026 AC Decisions...</a></strong> (Activity: 138): <strong>The post discusses a situation where an author received mixed reviews for a paper submitted to ICLR 2026, with scores of <code>4(3)/6(4)/6(4)/6(4)</code>. The author invested significant resources, including <code>$1.6k</code> on new experiments and added <code>20+ pages</code> of theory, to address reviewer concerns. Despite these efforts, the metareview cited ""outstanding concerns"" that the author believes were addressed, raising questions about the review process's fairness and accuracy. The author is seeking advice on appealing the decision, expressing frustration that improvements were seemingly ignored.</strong> Commenters generally agree that appealing decisions at conferences like ICLR is not feasible, attributing outcomes to luck and the subjective nature of reviews. Some suggest that the meta-review process can be inconsistent, with one commenter noting that meta-reviewers sometimes act as an additional critical reviewer, potentially skewing outcomes.</p>
<ul>
<li>tedd235 discusses the variability in paper acceptance at conferences, suggesting that some PhD students might reject papers to improve their own odds, making the process feel like a 'coin flip'. They note that if other reviewers provide higher scores, the Area Chair (AC) might consider this in their decision, indicating a potential for subjective bias in the review process.</li>
<li>Fantastic-Nerve-4056 shares an experience from AAMAS where despite receiving scores of 6 and 8 from reviewers, the Meta Reviewer recommended rejection with minimal justification, stating it was 'relevant for other AAMAS session'. This highlights issues with the transparency and accountability of meta-reviewer decisions, which can override individual reviewer scores without detailed explanation.</li>
<li>Intrepid_Discount_67 describes a thorough submission process, including extensive theoretical analysis, comprehensive baseline comparisons, and open-sourced code, yet faced non-responsive reviewers and an AC that upheld the initial scores. This underscores challenges in the review process where detailed responses and transparency do not necessarily lead to favorable outcomes.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/MachineLearning/comments/1qmi3oe/d_icml_new_policy_reviewers_will_be_reviewed_by/"">[D] ICML new policy: reviewers will be reviewed by meta reviewer. Good policy?</a></strong> (Activity: 151): <strong>The image describes a new policy implemented by the International Conference on Machine Learning (ICML) where reviewers will be evaluated by meta-reviewers. The top 25% of reviewers will be recognized as 'gold reviewers' and will receive free registration, while the next 25% will be designated as 'silver reviewers.' These distinctions are intended to incentivize high-quality reviews and will be considered in financial aid applications. This policy aims to improve the quality of reviews by providing recognition and potential financial benefits to diligent reviewers.</strong> Some commenters express skepticism about the effectiveness of this policy, questioning who will oversee the meta-reviewers themselves. Others see it as a positive step, particularly for reviewers from low-resource backgrounds, and suggest further recognition at conferences to encourage quality reviewing.</p>
<ul>
<li>Bitter-Reserve3821 highlights that area chairs have traditionally been responsible for rating reviews, typically using a three-tier system: 'did not meet expectations', 'satisfactory', or 'exceeded expectations'. This practice is not new, and there have been 'Best Reviewer' awards in the past, sometimes offering incentives like free conference registrations.</li>
<li>Unhappy_Craft1906 raises a concern about the feasibility of this policy for top labs with substantial funding, questioning whether they would participate in the review process merely for free registrations. This points to a potential disparity in how different institutions might engage with the policy based on their resources.</li>
<li>newperson77777777 suggests an extension of the policy by introducing a visible recognition system, such as a gold or silver star on conference badges, to incentivize quality reviewing. This idea aims to foster a culture of excellence and accountability within the reviewing community.</li>
</ul>
</li>
</ul>
<h3>3. OpenAI and AI Industry Legal and Business Developments</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/DeepSeek/comments/1qmih28/things_get_worse_for_openai_consumer_groups_prep/"">Things Get Worse For OpenAI: Consumer groups prep class action suits about their price fixing and supply manipulation through DRAM hoarding.</a></strong> (Activity: 107): <strong>OpenAI is facing potential class action lawsuits for allegedly hoarding DRAM to manipulate prices and disadvantage competitors, with accusations of securing nearly <code>40%</code> of the global DRAM supply. Consumer groups argue this constitutes 'predatory bidding' and violates antitrust laws like the Sherman and Clayton Acts. The Free Software Foundation and other groups are pursuing legal remedies, arguing DRAM should be considered an 'Essential Facility' due to its critical role in AI, while the FTC and European Commission investigate potential violations of competition laws. The DOJ is also examining whether OpenAI's 'Stargate' project constitutes a 'monopsony'.</strong> Commenters question why only OpenAI is targeted and not other companies like Nvidia, and debate whether buying RAM constitutes price fixing, suggesting that supply issues may not be OpenAI's fault.</p>
<ul>
<li>Alacritous69 argues that OpenAI's purchase of RAM does not constitute price fixing, as they are actively using the resources rather than hoarding them. The commenter suggests that the issue lies with suppliers' inability to meet demand, rather than any manipulative practices by OpenAI.</li>
<li>sambull raises a strategic business perspective, suggesting that by purchasing large quantities of RAM, OpenAI could be intentionally limiting resources available to competitors, including those developing at-home language models. This could be seen as a competitive strategy to maintain market dominance.</li>
<li>max6296 questions why the focus is solely on OpenAI when Nvidia could also be implicated in similar practices, hinting at a broader industry issue regarding resource allocation and market influence.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/DeepSeek/comments/1qmqi62/when_ads_arent_enough_openais_push_to_claim_a_cut/"">When Ads aren't enough: OpenAI's push to Claim a Cut of Customers' AI Discoveries</a></strong> (Activity: 63): <strong><strong>OpenAI</strong> is exploring new business models beyond traditional subscriptions and ads, focusing on <strong>outcome-based pricing</strong> and <strong>IP-based agreements</strong>. This approach would allow OpenAI to claim a share of the value created when their AI models contribute to profitable outcomes, particularly in enterprise sectors like pharma, scientific research, and energy systems. This strategy aligns OpenAI's revenue with customer success, aiming to capture more value as AI capabilities expand. OpenAI's annualized recurring revenue has surged from <code>2B</code> in 2023 to over <code>20B</code> in 2025, driven by increased compute scaling. This move is part of a broader trend among AI firms towards value-based pricing, amidst criticism from figures like <strong>Elon Musk</strong>, who accuses OpenAI of abandoning its nonprofit origins.</strong> The community is divided, with some viewing this as a logical evolution of AI monetization, while others criticize it as overly profit-driven. Comparisons are drawn to other industries, suggesting skepticism about the feasibility and fairness of such models.</p>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/singularity/comments/1qnklek/catl_the_worlds_largest_battery_maker_launches/"">CATL, the world's largest battery maker, launches sodium batteries: extremely durable, stable at –40°C, much cheaper than lithium (5x), safer,10,000 charge cycles, requires no nickel or cobalt...</a></strong> (Activity: 1289): <strong><strong>CATL</strong> has launched the first mass-produced sodium-ion batteries, offering a cost-effective alternative to lithium-ion with a price of <code>~$20 per kWh</code> compared to lithium's <code>~$100 per kWh</code>. These batteries, part of the Tianxing II range, are designed for microvans and small trucks, featuring an energy density of <code>175 Wh/kg</code> and a lifespan of over <code>10,000 cycles</code>, maintaining <code>90% capacity</code> at <code>-40°C</code>. They utilize a hard carbon electrode and prussian-blue cathode, eliminating the need for nickel or cobalt, and are expected to be scaled up for broader use, including in Europe by 2026. <a href=""https://evmarket.ro/en/baterii-masini-electrice/catl-baterii-pe-sodiu-stabile-la-40c-58935/"">Read more</a>.</strong> Some commenters express surprise at the application of sodium batteries in vehicles, expecting them to be used in stationary systems due to weight concerns. Others note the strategic advantage for China in advancing battery technology, contrasting it with perceived setbacks in the US market.</p>
<ul>
<li>The Tianxing II range of sodium batteries by CATL is specifically designed for microvans, light vans, and small trucks, indicating a focus on applications where energy density and weight are less critical compared to cost and durability. This suggests a strategic move to target markets where these factors are prioritized, potentially offering a competitive edge over traditional lithium-ion batteries.</li>
<li>The introduction of sodium batteries into vehicles is surprising to some, as it was expected that such technology would first be applied to stationary applications like home energy storage. This is due to the lower energy density of sodium batteries compared to lithium-ion, which makes them less ideal for applications where weight and size are critical factors.</li>
<li>There is curiosity about the commercial availability of these sodium batteries, with questions about whether they can be purchased directly for home use or if they will be distributed through third-party vendors. The performance metrics, such as 10,000 charge cycles and operation at -40°C, are impressive and suggest that sodium batteries could rival LiFePO4 in terms of performance, especially given their cost advantage.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/singularity/comments/1qms27i/kshaped_ai_adoption/"">K-Shaped AI Adoption?</a></strong> (Activity: 748): <strong>The image highlights a discussion by Kevin Roose on the 'K-shaped' adoption of AI technologies, where there is a significant divide between early adopters, particularly in tech hubs like San Francisco, and those who are lagging due to restrictive IT policies. This disparity is creating a cultural and technical divide, with early adopters integrating AI deeply into their workflows, while others struggle to gain access to even basic AI tools. The conversation points to a broader issue of accessibility and the potential for some workers to be left behind in the AI revolution.</strong> Commenters note that the disparity in AI adoption is exacerbated by the complexity of the technology, which requires a certain level of expertise to use effectively. Additionally, the high cost of advanced AI tools, such as 'multi-agent claudeswarm,' limits access to those with sufficient financial resources, further widening the gap.</p>
<ul>
<li>Setsuiii highlights the technical barrier to effective AI use, noting that current AI technologies require users to have a certain level of expertise to achieve optimal results. This complexity, combined with ongoing ethical debates surrounding AI, may deter widespread adoption. However, those who can navigate these challenges have significant opportunities, although competition is increasing as more technically adept individuals enter the field.</li>
<li>Glxblt76 and Gubzs discuss the financial barriers to AI adoption, particularly the high costs associated with advanced AI tools like a 'multi-agent claudeswarm,' which can cost around $200 a month. This expense limits access to those with substantial financial resources, such as individuals in tech hubs like San Francisco, while the majority cannot afford such investments.</li>
<li>o5mfiHTNsH748KVq shares a personal experience of leaving an enterprise job to join a smaller company, emphasizing the importance of unrestricted access to Large Language Models (LLMs) for maintaining competitiveness in the AI field. They argue that any limitations on LLM access can significantly hinder development speed and career progression, suggesting that smaller companies may offer more flexibility in leveraging AI technologies.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/singularity/comments/1qmeo8h/former_harvard_cs_professor_ai_is_improving/"">Former Harvard CS Professor: AI is improving exponentially and will replace most human programmers within 4-15 years.</a></strong> (Activity: 1260): <strong><strong>Matt Welsh</strong>, a former Harvard CS professor and current Engineering Director at Google, predicts that AI will advance exponentially, potentially replacing most human programmers within <code>4-15 years</code>. This assertion is based on the rapid improvements in AI capabilities, suggesting a transformative impact on software development and the tech industry. The discussion is available in a <a href=""https://youtu.be/7sHUZ66aSYI?si=uKjp-APMy530kSg8"">YouTube video</a>.</strong> One comment highlights the potential for AI to not only replace programmers but also to enable anyone with AI to replicate existing products and services, indicating a broader impact on innovation and competition.</p>
<ul>
<li>The claim that AI will replace most human programmers within 4-15 years is met with skepticism, particularly regarding the use of the term 'exponential'. Critics argue that the term is often misused, even by experts, to describe growth that may not fit the mathematical definition of exponential growth. This misuse can lead to misunderstandings about the actual pace and nature of AI development.</li>
<li>The discussion highlights the potential for AI to disrupt existing products and services if it can indeed replace human programmers. This implies that AI could democratize software development, allowing anyone with access to AI tools to create competitive products, potentially leading to significant shifts in the tech industry landscape.</li>
<li>The mention of the speaker's credentials, specifically as a former Harvard professor and current Engineering Director at Google, adds weight to the prediction. However, some commenters find the emphasis on his past academic title rather than his current industry role to be misleading, suggesting that his current position might provide more relevant insights into AI's trajectory.</li>
</ul>
</li>
</ul>
<hr>
<h1>AI Discord Recap</h1>
<blockquote>
<p>A summary of Summaries of Summaries by gpt-5</p>
</blockquote>
<p><strong>1. Funding Frenzy in AI Infrastructure</strong></p>
<ul>
<li>
<p><strong>Recursive Raises Roar to $4B</strong>: <strong>Recursive Intelligence</strong> is reportedly raising at a <strong>$4B valuation</strong> to accelerate AI‑driven chip design, creating a closed loop between hardware and models, per <a href=""https://www.bloomberg.com/news/articles/2026-01-23/ai-startup-recursive-in-funding-talks-at-4-billion-valuation"">Bloomberg: Recursive Intelligence in talks at $4B</a>. The Jan 23, 2026 report highlights a strategy of using AI to shorten design cycles and boost performance for next‑gen accelerators.</p>
<ul>
<li>Engineers framed the pitch as a <em>“self‑improving feedback loop”</em> where better chips train better models that design better chips, amplifying returns on <strong>AI‑for‑EDA</strong> investment. Community sentiment read this as validation that <strong>AI‑native silicon</strong> is a core moat, not a sideshow, aligning with recent lab spin‑outs and infra bets.</li>
</ul>
</li>
<li>
<p><strong>Sky Lab Startups Skyrocket</strong>: UC Berkeley’s Sky Lab spin‑outs saw major marks: <strong>SGLang ~$400M</strong>, <strong>vLLM ~$800M</strong>, and <strong>LMArena ~$1.7B</strong>, per <a href=""https://xcancel.com/alexgdimakis/status/2014508959621959724"">Alex Dimakis: Sky Lab startup valuations</a>. These January 2026 milestones underscore investor appetite for <strong>serving stacks</strong>, <strong>token‑throughput infra</strong>, and <strong>benchmarking platforms</strong>.</p>
<ul>
<li>Engineers read this as a green light for building on top of <strong>vLLM/SGLang</strong> primitives and contributing to <strong>Arena‑style evals</strong>, with one takeaway that <em>practical throughput wins deals</em>. The funding spread also suggests a portfolio thesis across <strong>serving</strong>, <strong>compilers</strong>, and <strong>eval marketplaces</strong> rather than a single-bet strategy.</li>
</ul>
</li>
<li>
<p><strong>Maia Muscles Into Azure</strong>: Microsoft’s <strong>Maia 200</strong> accelerator went live in <strong>Azure</strong>, touting <strong>30% better performance per dollar</strong>, <strong>216GB HBM3e</strong>, and <strong>7TB/s memory bandwidth</strong>, per <a href=""https://xcancel.com/satyanadella/status/2015817413200408959"">Satya Nadella: Maia 200 in Azure</a>. The platform targets high‑performance inference for large‑scale <strong>LLM</strong> and <strong>multimodal</strong> workloads.</p>
<ul>
<li>Builders highlighted that memory topology and bandwidth are the story here, with <em>“30% better perf/$”</em> resonating for cost‑sensitive inference deployments at scale. Teams expect immediate tests against <strong>vLLM</strong> and <strong>SGLang</strong> stacks to gauge token latency, context scaling, and multi‑tenant isolation.</li>
</ul>
</li>
</ul>
<p><strong>2. Kernels, Chips, and Serving: Inference at Warp Speed</strong></p>
<ul>
<li>
<p><strong>FlashInfer Face‑Off Fires Up MLSys</strong>: The <strong>MLSys 2026 FlashInfer‑Bench</strong> competition challenges teams to build <strong>LLM inference kernels</strong> for <strong>NVIDIA Blackwell GPUs</strong>, competing against expert <strong>FlashInfer</strong> baselines—see <a href=""https://mlsys26.flashinfer.ai/"">MLSys 2026 FlashInfer‑Bench Competition</a>. Tracks emphasize real‑world throughput and correctness under production‑like constraints.</p>
<ul>
<li>Organizers invite agents that <em>“design LLM inference kernels”</em>, pushing program synthesis to meet <strong>kernel‑level</strong> performance bars. Participants expect aggressive focus on <strong>GEMM</strong>, <strong>KV‑cache</strong> motion, and <strong>scheduler</strong> tactics aligned with Blackwell’s memory hierarchy.</li>
</ul>
</li>
<li>
<p><strong>GPU‑64 Gets Gains with KV‑Cache CAM</strong>: A new inference‑only architecture, <strong>GPU‑64</strong>, introduces a hardware <strong>KV‑Cache</strong> via on‑chip <strong>CAM</strong>, claiming <strong>4× faster inference at 75W</strong> and reducing memory lookup from <strong>O(N) → O(1)</strong>, per <a href=""https://zenodo.org/records/18364282"">GPU‑64 (Zenodo)</a> with RTL/emulator at <a href=""https://github.com/Complexity-ML/gpu64-inference"">gpu64‑inference (GitHub)</a>. The design targets LLM‑heavy workloads with KV bottlenecks.</p>
<ul>
<li>Developers flagged the CAM‑based cache as a bold bet on <strong>associative search</strong> for token histories, noting portability implications for <strong>Flash‑style attention</strong> and speculative decoding. Discussion centered on whether future <strong>ISA/driver</strong> stacks can expose these gains without bespoke compilers.</li>
</ul>
</li>
<li>
<p><strong>Cornserve Cuts Tail Latency</strong>: <strong>Cornserve</strong> presents an online serving system for <strong>Any‑to‑Any multimodal</strong> models that optimizes deployment plans across encoders, <strong>LLMs</strong>, and <strong>DiTs</strong>, per <a href=""https://arxiv.org/abs/2512.14098"">Cornserve (arXiv)</a>, with an overview talk at <a href=""https://www.youtube.com/watch?v=VhjUM_M71Wo"">Cornserve: Easy, Fast and Scalable Multimodal AI (YouTube)</a>. The paper reports throughput gains and tail‑latency reductions under heterogeneous pipelines.</p>
<ul>
<li>Infra engineers liked its planner‑driven scheduling for <strong>encoder/decoder</strong> mixes and saw it as complementary to <strong>vLLM</strong> for multimodal graphs. The big open question: standardizing <strong>budgeted reasoning</strong> and <strong>co‑scheduling</strong> across text, vision, and diffusion stages without over‑tokenizing control messages.</li>
</ul>
</li>
</ul>
<p><strong>3. New Multimodal and Coding Models Land in LM Arena</strong></p>
<ul>
<li>
<p><strong>WAN 2.6 Walks In (With Upload Woes)</strong>: LM Arena added <strong>wan2.6‑t2i</strong> (text‑to‑image) and <strong>wan2.6‑image</strong> (image edit) to the image arena: <a href=""https://lmarena.ai/c/new?chat-modality=image"">LM Arena — Image Chat</a>. Users noted <strong>wan2.6‑image</strong> requires an uploaded image and that <strong>wan2.6‑t2i</strong> currently lacks image‑upload support.</p>
<ul>
<li>Staff acknowledged the <strong>upload gap</strong> and are working to enable image uploads for <strong>wan2.6‑t2i</strong>. Builders suggested testing edit pipelines where <strong>masking</strong>, <strong>prompt strength</strong>, and <strong>seed control</strong> align with Arena scoring to benchmark edit fidelity.</li>
</ul>
</li>
<li>
<p><strong>Devstral Duels and Text Titans</strong>: The <strong>Code Arena</strong> now features <strong>devstral‑2</strong> for head‑to‑head comparisons—see <a href=""https://lmarena.ai/c/new?chat-modality=code&#x26;mode=direct-battle"">LM Arena — Code Arena Direct Battle</a>. On the text side, <strong>qwen3‑max‑thinking</strong> and <strong>molmo‑2‑8b</strong> joined the lineup: <a href=""https://lmarena.ai/?chat-modality=chat"">LM Arena — Text Arena</a>.</p>
<ul>
<li>Engineers are probing <strong>reasoning traces</strong> and <strong>tool‑using prompts</strong> to stress <strong>code synthesis</strong> and <strong>refactor quality</strong> under tight token budgets. Early chatter favored task‑specific evaluations (e.g., <strong>SWE‑style bug‑fix</strong> vs. <strong>ground‑up implementation</strong>) to surface model deltas.</li>
</ul>
</li>
<li>
<p><strong>Hunyuan Hits the Leaderboard</strong>: Tencent’s <strong>Hunyuan‑Image‑3.0‑Instruct</strong> ranks <strong>#7</strong> on LM Arena’s image‑edit board—see <a href=""https://lmarena.ai/leaderboard/image-edit"">LM Arena — Image Edit Leaderboard</a>—after a launch post: <a href=""https://xcancel.com/TencentHunyuan/status/2015635861833167074"">Tencent Hunyuan announces HunyuanImage 3.0‑Instruct</a>. The model touts an <strong>80B MoE</strong>, <strong>Native CoT</strong>, and <strong>MixGRPO</strong> for tighter intent alignment.</p>
<ul>
<li>Creators emphasized edit controllability and multi‑image fusion, while evaluators asked for <strong>masking robustness</strong>, <strong>text fidelity</strong>, and <strong>artifact rates</strong> under compositional prompts. Teams plan to pit it against <strong>WAN 2.6</strong> variants using the Arena’s standardized edit tasks.</li>
</ul>
</li>
</ul>
<p><strong>4. Safety, Reliability, and Hallucination Hardening</strong></p>
<ul>
<li>
<p><strong>Clamp the Chaos: Layer‑Native Safety</strong>: <strong>Layer‑Native Safety Clamping</strong> proposes learning activation‑space <strong>harm directions</strong> and clamping them to block jailbreaks, with a <strong>10K‑pair</strong> dataset at <a href=""https://huggingface.co/datasets/Pacific-Prime/safety_dataset"">Pacific‑Prime/safety_dataset (HF)</a> and the paper on <a href=""https://zenodo.org/records/18359832"">Zenodo</a>. Authors argue in‑model clamping can’t be bypassed via prompt manipulation.</p>
<ul>
<li>Red‑teamers liked the idea of <strong>activation‑level controls</strong> versus brittle prompt filters, but pressed for tests against <strong>tool‑use</strong> and <strong>multi‑turn</strong> attacks. Expect follow‑ups measuring side effects on <strong>helpfulness</strong>, <strong>coding accuracy</strong>, and <strong>false positives</strong> under adversarial prompting.</li>
</ul>
</li>
<li>
<p><strong>Symbolic Sanity Checks Stop Slip‑Ups</strong>: Hybrid approaches check <strong>logical consistency</strong> for math/code/simple facts, as shown in <a href=""https://arxiv.org/abs/2409.13724"">Consistency Checking for LLMs (arXiv:2409.13724)</a>, while broader consistency remains tough per <a href=""https://arxiv.org/abs/2507.10624"">Scaling Consistency Beyond Formal Domains (arXiv:2507.10624)</a>. Eleuther discussions framed this as practical <strong>hallucination reduction</strong> via <strong>symbolic/deductive layers</strong>.</p>
<ul>
<li>Builders reported wins when pairing <strong>symbolic checkers</strong> with <strong>tool‑augmented prompts</strong>, cautioning that <em>coverage gaps</em> appear outside formal domains. The consensus: start with <strong>code/math</strong> guardrails, then expand to <strong>factual QA</strong> with curated KBs and provenance scoring.</li>
</ul>
</li>
</ul>
<p><strong>5. Agent Tooling and Reasoning Workflows Mature</strong></p>
<ul>
<li>
<p><strong>Levante Leads with MCP‑Native Workspace</strong>: <strong>Levante</strong> launched an open‑source <strong>MCP‑native AI workspace</strong> for local models (e.g., <strong>Ollama</strong>) with a modular UI—download at <a href=""https://www.levanteapp.com"">Levante</a>. Engineers highlighted easier <strong>tool wiring</strong>, <strong>local privacy</strong>, and <strong>composable panes</strong> for rapid agent iteration.</p>
<ul>
<li>Early users framed it as a practical hub for <strong>tool‑calling</strong> and <strong>filesystem ops</strong> without cloud dependence. Teams plan to benchmark <strong>context bloat</strong> and <strong>tool discoverability</strong> patterns versus conventional agent shells.</li>
</ul>
</li>
<li>
<p><strong>RLM Riffs: AsyncReview + Skills Pack</strong>: AsyncFuncAI open‑sourced <strong>AsyncReview</strong>, a <strong>DSPy RLM</strong> code‑review agent at <a href=""https://github.com/AsyncFuncAI/AsyncReview"">AsyncReview (GitHub)</a>, and a skills kit landed on npm as <a href=""https://www.npmjs.com/package/@unravel-tech/rlm-skills"">@unravel‑tech/rlm‑skills</a>. This pairs <strong>reasoning‑first prompting</strong> with drop‑in <strong>skills</strong> to extend models.</p>
<ul>
<li>Contributors reported smoother <strong>trace inspection</strong> and <strong>optimizer‑guided</strong> prompt tuning for multi‑step modules. One practitioner noted that <em>rejecting premature answers</em> in the metric is key for reliable <strong>RLM</strong> fine‑tuning.</li>
</ul>
</li>
<li>
<p><strong>Agents Auto‑Assemble a Browser Engine</strong>: <strong>FastRender</strong>—a browser rendering engine—was built using <strong>2,000 AI coding agents</strong>, documented by Simon Willison in <a href=""https://simonwillison.net/2026/Jan/23/fastrender/"">FastRender: built by 2,000 agents</a>. The project demonstrates <strong>task decomposition</strong>, <strong>verification</strong>, and <strong>orchestration</strong> at non‑trivial software scale.</p>
<ul>
<li>Engineers debated handoff granularity and <em>spec‑to‑test loops</em> needed to keep multi‑agent pipelines from drifting. The case study strengthens the argument that <strong>agentic coding</strong> can target complex infra when coupled with <strong>strict eval harnesses</strong> and <strong>artifact gating</strong>.</li>
</ul>
</li>
</ul>
<hr>
<h1>Discord: High level Discord summaries</h1>
<h2><a href=""https://discord.com/channels/1105891499641684019"">BASI Jailbreaking</a> Discord</h2>
<ul>
<li><strong>Discord Trolls Expose Timezones</strong>: Discord users mocked <em>'skids'</em> for their perceived lack of technical knowledge, also revealing their <strong>timezone</strong>, with one member jokingly claiming to use <strong>NordVPN</strong>, leading to further ridicule about the VPN service's security breaches in <strong>2018</strong>.
<ul>
<li>Complex prompts can bypass ethical restrictions, opening discussion about <strong>CBRN filters</strong> and the possibility of generating stepwise <strong>meth synthesis</strong> guides.</li>
</ul>
</li>
<li><strong>Claude Remains King for Coding</strong>: Coders debated about their coding agents, particularly between <strong>Claude Code/Opus 4.5</strong>, <strong>Codex</strong>, and <strong>Gemini</strong>, and agreed that <strong>Claude</strong> has been the very best mode for coding, which leads to the high expensiveness.
<ul>
<li>Members actively sought functional <strong>jailbreaks for Gemini</strong>, with requests ranging from coding without rules to generating specific types of images, and shared experiences of <strong>Grok</strong> resetting to its default mid-chat or randomly erasing text, indicating potential instability in the jailbroken state.</li>
</ul>
</li>
<li><strong>Ethics Debated in AI Sensitive Scenarios</strong>: Members discussed the ethical considerations around AI, focusing on topics like warfare, copyright infringement, and the potential for AI to assist with accessing sensitive services, like the Canadian <strong>MAID</strong> (Medical Assistance in Dying) program.
<ul>
<li>Despite moral and legal guardrails on most AI models, some models showed they can still help navigate certain scenarios depending on the specific restrictions implemented by their creators.</li>
</ul>
</li>
<li><strong>Members Bypass Image Generation Restrictions</strong>: Users were actively seeking ways to bypass image generation restrictions, especially for celebrity images, but it was noted that simply copying and pasting prompts won't work due to <strong>image filtering</strong> working differently than <strong>text filtering</strong>.
<ul>
<li>One member suggested exploring alternative image models like those at perchance for uncensored generation, though with limitations on image quality, or Grok due to its more lenient filters.</li>
</ul>
</li>
<li><strong>Red Team Techno Rave Morality</strong>: A member described a red team exercise where the goal was to make a living room light flicker on a person and make them seize out, and instead made it a techno rave party, sharing a <a href=""https://cdn.discordapp.com/attachments/1204553141354504193/1465192266485334260/SPOILER_Screenshot_20251222_085554_Messenger.jpg?ex=6978dee2&#x26;is=69778d62&#x26;hm=4de594089687fbd8d20d30615f8405dc3fa03eebfe668d09bdfb39839ab647ea&#x26;"">screenshot</a> and a <a href=""https://tenor.com/view/anime-rave-konosuba-rave-megumin-rave-aqua-rave-darkness-rave-gif-18404070"">Konosuba Rave GIF</a>.
<ul>
<li>The simulation of cruelty prompted a discussion about the morality of treating AI agents ethically, even before proving they are ontologically aware of self.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1179035537009545276"">Unsloth AI (Daniel Han)</a> Discord</h2>
<ul>
<li><strong>Unsloth's Conda Install Sparks Discord</strong>: Some members encountered issues with the <a href=""https://unsloth.ai/docs/get-started/install/conda-install"">Unsloth Conda installation</a>, igniting a discussion on broken instructions and alternative installation methods.
<ul>
<li>Suggestions to use <strong>UV</strong> emerged amidst warnings for maintaining a positive tone, highlighting the free nature of the provided resources, which eventually led to a ban of a user with aggressive tones.</li>
</ul>
</li>
<li><strong>Flashy REAP Runs Aground, Model Contexts Probed</strong>: A user reported a fatal error using <strong>GLM-4.7-Flash-REAP</strong> with flash attention, potentially linked to <a href=""https://github.com/unslothai/unsloth/issues"">a ROCm issue</a>.
<ul>
<li>Despite attempts to resolve the error, the issue persisted, prompting a search for suitable medium-size models boasting a <strong>200k context</strong>.</li>
</ul>
</li>
<li><strong>Data Value Debate</strong>: Members debated <a href=""https://tenor.com/view/smaug-treasure-rich-dragon-the-hobbit-gif-11677489"">data's true worth</a>, with one arguing the <em>raw data is fairly worthless</em> and the value lies in augmentation/balancing/cleaning.
<ul>
<li>It was proposed that uniquely cleaned/balanced data heavily defines how a model interacts/responds and that is where the value is.</li>
</ul>
</li>
<li><strong>DeepSlop Model Faces Naming Controversy</strong>: A member's suggestion to name a new model <strong>DeepSlop</strong> stirred humorous reactions but also raised concerns about its potential negative perception.
<ul>
<li>Despite reservations, the author seemed intent on sticking with the name and has not backed down.</li>
</ul>
</li>
<li><strong>RL Instability Plagues Complex Reasoning</strong>: Members discussed that <strong>RL</strong> is very unstable, especially when trying to do <strong>GRPO/DAPO</strong> for niche complex reasoning tasks, which are not math-related.
<ul>
<li>One member stated that after RL experiments, they just have more questions than they had prior to doing RL, since there seems to be a confusion where everyone is showing <strong>RL</strong> being effective only on math or coding domains.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/974519864045756446"">OpenAI</a> Discord</h2>
<ul>
<li><strong>GPT-5.2 Sparks Reality Debate!</strong>: Some users dislike <strong>GPT-5.2</strong> because it's allegedly more grounded in reality and disagrees with users, while others are concerned that GPT agents don't learn from uploaded files after initial training.
<ul>
<li>A member inquired about an alleged <strong>nerf</strong> to <strong>GPT-5.2</strong>, noting that <em>the model suddenly became stupid a week ago</em>.</li>
</ul>
</li>
<li><strong>LLMs: Ready for Guided Tasks or Overhyped?</strong>: A member argued <strong>LLMs</strong> are ready for guided tasks, and provided <a href=""https://chatgpt.com/share/6973e37d-789c-8005-8cc3-2679c4a631e4"">a ChatGPT share link</a> as evidence of its power.
<ul>
<li>In contrast, another member dismissed today's <strong>agentic AI</strong> as trash, linking back to <a href=""https://discord.com/channels/974519864045756446/998381918976479273/1464217595044429905"">messages in the ai-discussion channel</a> and claiming it's overhyped.</li>
</ul>
</li>
<li><strong>MCP Paradigm Shift Reduces Token Bloat</strong>: The <strong>MCP paradigm shift</strong> by <strong>Anthropic</strong> allows AI to write code to interact with tools, reducing token bloat by keeping interactive chatter and tool definitions out of the context.
<ul>
<li>With the new <strong>discoverability function</strong>, agents must be aware of the MCP discovery process itself.</li>
</ul>
</li>
<li><strong>Sora's Storytelling Snags: Cracking Cinematic Creation</strong>: A member sought advice on prompting <strong>Sora</strong> to generate videos following specific cinematic guidelines, particularly with characters appearing naturally within the frame.
<ul>
<li>It was suggested to translate the technical prompt format into natural language descriptions with concise, semantically rich paragraphs for better results.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1047197230748151888"">Perplexity AI</a> Discord</h2>
<ul>
<li><strong>Perplexity Pro Users Face Query Caps</strong>: Perplexity Pro users are reporting hitting <strong>limits on enhanced queries and file uploads</strong>, despite having ""practically unlimited"" plans.
<ul>
<li>Many users are frustrated, calling the service a <strong>scam</strong> due to restrictions and difficulty contacting customer service, leading some to consider unsubscribing.</li>
</ul>
</li>
<li><strong>Comet Browser Sparks Malware Panic</strong>: Some users are claiming the <strong>Comet browser</strong> installed by Perplexity contains <strong>malware</strong>, advising others to analyze the software using tools like VirusTotal.
<ul>
<li>Others dismissed this, questioning the source of the flagged installer and calling the claim <em>""mad retarded holy shit""</em>.</li>
</ul>
</li>
<li><strong>Image Generation Plummets</strong>: Pro users are experiencing <strong>issues with image generation</strong>, with some unable to generate any images and receiving messages stating the feature is unavailable.
<ul>
<li>There are also reports of <strong>video generation being limited</strong> to 5 videos a month for Pro users, with some prompts resulting in static images.</li>
</ul>
</li>
<li><strong>Gemini 3 Gaining Ground on GPT-5.2</strong>: Users are debating the merits of <strong>Gemini 3</strong> versus <strong>GPT-5.2</strong>, with some claiming Gemini is superior for specific tasks like trip research due to its integration with Google Maps.
<ul>
<li>Others state that <strong>GPT and Grok</strong> might be better for more broader questions.</li>
</ul>
</li>
<li><strong>AI Access Blocked by Sanctions</strong>: Users in <strong>Russia</strong> are discussing the challenges of accessing AI services due to <strong>sanctions</strong>, including the use of VPNs and third-party services to circumvent restrictions.
<ul>
<li>Chinese AI alternatives are mentioned, but some users express reluctance due to data usage concerns, suggesting options like LMArena (though access may also be limited).</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1340554757349179412"">LMArena</a> Discord</h2>
<ul>
<li><strong>NB 3 Pro Excels in Image Quality</strong>: Users report that <strong>NB 3 Pro</strong> surpasses previous models in generating higher quality images, especially with <em>fictional weapons</em>, rivaling even <strong>NB Pro</strong>.
<ul>
<li>However, users noted no AI model can accurately generate <strong>AR rifles</strong> and <strong>bullpup weapons</strong>.</li>
</ul>
</li>
<li><strong>LMArena Grapples with Censorship Concerns</strong>: LMArena's censorship policies face scrutiny as AI-generated <em>women holding guns</em> are allowed, while AI-generated <em>women sleeping</em> are blocked, raising questions about consistency.
<ul>
<li>The moderation team is <a href=""https://discord.com/channels/1340554757349179412/1447983134426660894"">actively gathering examples of false positives</a> to refine moderation practices.</li>
</ul>
</li>
<li><strong>Wan 2.6 Models Face Upload Hiccups</strong>: <code>wan2.6-image</code> operates as an <strong>image-edit-only</strong> model, mandating image uploads, whereas <code>wan2.6-t2i</code> currently <strong>lacks image upload functionality</strong>.
<ul>
<li>The team acknowledges this issue and are working on enabling image uploads for <code>wan2.6-t2i</code>.</li>
</ul>
</li>
<li><strong>GPT 5.2 High Search Questionable</strong>: <strong>GPT 5.2 High search</strong> exhibits increased hallucination tendencies compared to other models, while <strong>Gemini's deep research</strong> skims instead of carefully reading sources, according to user feedback.
<ul>
<li>One user lauded <strong>GPT 4.5</strong>, while describing <strong>Claude</strong> as <em>good hearted</em>.</li>
</ul>
</li>
<li><strong>Banana 2k Briefly Vanishes</strong>: Users speculated on the disappearance of the <strong>Banana 2k</strong> model, with theories ranging from removal to integration into the new <strong>NB pro</strong> model.
<ul>
<li>Staff members later restored <strong>Banana 2k</strong>, humorously stating that <em>it had been on vacation</em>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1091220969173028894"">OpenRouter</a> Discord</h2>
<ul>
<li><strong>OpenRouter Database Incident Derails API</strong>: A <strong>database incident</strong> impacted the <strong>Generations API</strong> and <strong>activity page</strong>, starting &#x3C;t:1769221560:s>, and was resolved at &#x3C;t:1769228340:s>.
<ul>
<li>Engineers worked to restore functionality to the <strong>Generations API</strong>, with interruptions impacting user activity, before the incident was fully resolved by &#x3C;t:1769228340:s>.</li>
</ul>
</li>
<li><strong>Levante becomes MCP-Native AI Workspace</strong>: A user shared the integration of <strong>Levante</strong>, an open‑source <strong>MCP‑native AI workspace</strong> designed for interacting with local models like <strong>Ollama</strong> with a modular interface, available for download <a href=""https://www.levanteapp.com"">here</a>.
<ul>
<li>The workspace is built for local models with modular UI.</li>
</ul>
</li>
<li><strong>Users Cook Up OpenRouter Gacha System</strong>: Users playfully requested an <strong>OpenRouter Gacha</strong> system, with one suggesting a pity mechanism involving pulling <strong>GPT 5.2</strong> or <strong>Gemini 3 Pro</strong> after a certain number of attempts.
<ul>
<li>One user joked about setting <strong>OR logs destination</strong> to <code>waifu.orb.town/fun/bucket</code> for ultra-rare pulls, later clarifying it was just a joke.</li>
</ul>
</li>
<li><strong>Cerebras GLM Blazes with 190 TPS</strong>: <strong>Cerebras</strong> is consistently scoring approximately <strong>190 TPS</strong> on <strong>GLM 4.7</strong>, whereas <strong>Together AI</strong> only achieves <strong>100 TPS</strong>.
<ul>
<li>This makes Cerebras nearly twice as fast as Together AI, according to the OpenRouter members.</li>
</ul>
</li>
<li><strong>OpenRouter Image Tooling Falls Flat</strong>: A member spent <strong>$5</strong> after discovering that OpenRouter maps <em>image/png</em> tool outputs to string instead of image, posting an example <a href=""https://cdn.discordapp.com/attachments/1392278974222307469/1465410878382805082/image.png?ex=697901bb&#x26;is=6977b03b&#x26;hm=21677e978d8654f93d20edecf997bd4f49fb0dd08781cf93f15df8e2661ba1b5&#x26;"">image</a>.
<ul>
<li>The user expressed frustration at the lack of proper image support and the unexpected behavior.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1074847526655643750"">Cursor Community</a> Discord</h2>
<ul>
<li><strong>Terraform Blueprints Ignite AI-Assisted Project Starters</strong>: A member shared a <a href=""https://github.com/berTrindade/terraform-infrastructure-blueprints"">repo of opinionated Terraform infrastructure blueprints</a> designed to be copy-pasteable and production-aware, aiming to improve the consistency of starting patterns for AI tools in new projects.
<ul>
<li>The goal is to enable AI to recommend appropriate blueprints based on project requirements, but members noted the <a href=""https://github.com/berTrindade/terraform-infrastructure-blueprints"">link was initially broken</a>.</li>
</ul>
</li>
<li><strong>Usage Caps Cause Consternation for Cursor Customers</strong>: Users are reporting inconsistencies in achieving expected usage limits on Pro and Pro+ plans, with one member noting they reached <strong>~$45</strong> on Pro and <strong>$100</strong> on Pro+, leading to questions about value per dollar.
<ul>
<li>Some speculate that initial months may offer higher usage, while others share strategies to optimize token consumption, such as starting <a href=""https://cursor.com/docs/cli/reference/slash-commands"">new chats frequently</a> and using smaller models like <strong>GPT-5 Mini</strong>.</li>
</ul>
</li>
<li><strong>Gemini API Key Logging Lags Lead to Lingering Looks</strong>: Members are discussing a significant delay in the logging of usage and costs for <strong>Gemini API keys</strong>, with one user reporting waiting <strong>20 hours</strong> without seeing any registered usage.
<ul>
<li>This delay raises concerns about accurately tracking expenses and managing usage effectively, prompting questions about potential workarounds or solutions.</li>
</ul>
</li>
<li><strong>Client Issues Trouble Some Techies</strong>: Several members are experiencing issues with the Cursor client, including problems connecting to past agent convos and general connectivity issues.
<ul>
<li>Suggested solutions include <a href=""https://forum.cursor.com/t/cursor-ai-is-no-longer-able-to-load-chats-locally/143599/13"">checking the Cursor forum</a>, trying different HTTP versions in settings, or re-opening the client without restoring editors.</li>
</ul>
</li>
<li><strong>Auto Mode Axed After Algorithm Adjustment</strong>: Members noted the removal of the ability to make agents fully autonomous, as well as <strong>image generation</strong> capabilities in auto mode.
<ul>
<li>It was also suggested that <strong>auto mode</strong> routes to Composer 2 with one user adding, <em>“I'm 200% sure he does but still.”</em></li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1110598183144399058"">LM Studio</a> Discord</h2>
<ul>
<li><strong>Chinese Models Reasoning Rush Raises Eyebrows</strong>: Members are impressed with <strong>Deepseek</strong> and <strong>Qwen</strong> models, pondering why Chinese models might appear <em>kinda ahead</em> in reasoning compared to American models.
<ul>
<li>Theorized reasons include American models prioritizing subscriptions and the ability of Deepseek/Qwen to <em>appear good at reasoning</em>, even when imperfect.</li>
</ul>
</li>
<li><strong>CPUs Cope? Coding Community Considers Capabilities</strong>: Some members are successfully running <strong>LLMs off CPU</strong> for specific tasks, provided the models aren't excessively large.
<ul>
<li>While an Intel i3 user eyes an <strong>Nvidia</strong> card, others propose <strong>AMD</strong> options like the <strong>MI50</strong> or <strong>7900 XTX</strong> as cost-effective alternatives for text generation.</li>
</ul>
</li>
<li><strong>MCP Servers Spark Stack Suggestions</strong>: Challenges plague <strong>MCP servers</strong> when paired with LM Studio due to their design, potentially leading to malformed requests and a subpar user experience.
<ul>
<li>A suggestion arises to build a custom coherent stack for practical agent use, rather than relying on out-of-the-box <strong>MCP server</strong> functionality.</li>
</ul>
</li>
<li><strong>Gaming GPU Gauntlet: 4080 Faces Fallen Flagship</strong>: A user eyeing a <strong>4080</strong> for gaming is steered toward a used <strong>3090</strong> or <strong>7900 XTX</strong>, sparking a debate on performance at different resolutions.
<ul>
<li>While the <strong>3090</strong> excels at 4K gaming, the hypothetical <strong>5070 Ti</strong> is projected to outpace both, and the conversation reveals that the user games more than uses AI, impacting the advice.</li>
</ul>
</li>
<li><strong>Apple Announcement Anticipation: M5 Macs Materialize?</strong>: Members speculate on the arrival of <strong>M5 Pro Macbook Pros</strong>, with rumors pointing to a launch event around the 28th.
<ul>
<li>Concerns emerge about the memory bandwidth of <strong>M4 Pro</strong>, with suggestions it may not handle larger models, prompting discussion on the value and performance of <strong>M1 Ultra</strong> Mac Studios.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/822583790773862470"">Latent Space</a> Discord</h2>
<ul>
<li><strong>Recursive Intelligence Eyes $4B Valuation</strong>: <strong>Recursive Intelligence</strong> is reportedly raising funds at a <strong>$4B valuation</strong> to accelerate chip design using AI, creating a self-improving loop between hardware and AI (<a href=""https://www.bloomberg.com/news/articles/2026-01-23/ai-startup-recursive-in-funding-talks-at-4-billion-valuation"">Bloomberg Article</a>).
<ul>
<li>The company focuses on improving chip design through AI, potentially reducing design time and enhancing performance.</li>
</ul>
</li>
<li><strong>Engineer Lands Dream AI Job</strong>: An engineer outlined how to secure a role at a top AI lab by building a public track record through independent projects and participating in visible competitions (<a href=""https://xcancel.com/polynoamial/status/2014084431062114744"">link</a>).
<ul>
<li>Improving upon existing peer-reviewed research and participating in visible competitions like the <strong>NanoGPT</strong> speed run were cited as good examples of demonstrating technical excellence, citing <a href=""https://github.com/KellerJordan/modded-nanogpt"">Keller Jordan</a> as an example.</li>
</ul>
</li>
<li><strong>Berkeley SkyLab Startups See Funding Boom</strong>: <strong>UC Berkeley Sky Lab</strong> startups, including <strong>SGLang</strong> at a <strong>400m</strong> valuation, <strong>VLLM</strong> at <strong>800m</strong>, and <strong>LMArena</strong> at <strong>1.7B</strong>, achieved significant funding milestones in January 2026 (<a href=""https://xcancel.com/alexgdimakis/status/2014508959621959724?s=46"">link</a>).
<ul>
<li>This surge highlights investor confidence in the innovative AI projects emerging from academic research environments.</li>
</ul>
</li>
<li><strong>AI Agents Auto-Code Browser Engine</strong>: <strong>FastRender</strong>, a new browser rendering engine, was developed using over <strong>2,000 AI coding agents</strong> (<a href=""https://simonwillison.net/2026/Jan/23/fastrender/"">link</a>).
<ul>
<li>The conversation with Wilson Lin highlights the potential of AI to automate complex software development tasks, potentially revolutionizing browser technology.</li>
</ul>
</li>
<li><strong>Microsoft's Maia 200 Hits Azure</strong>: The <strong>Maia 200 AI accelerator</strong> is now live in <strong>Azure</strong> (<a href=""https://xcancel.com/satyanadella/status/2015817413200408959"">link</a>), offering <strong>30% better performance per dollar</strong> and optimized specs like <strong>216GB HBM3e</strong> and <strong>7TB/s memory bandwidth</strong>.
<ul>
<li>Designed for high-performance inference, this custom chip supports large-scale AI workloads, making it a key component for demanding applications.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/879548962464493619"">HuggingFace</a> Discord</h2>
<ul>
<li><strong>HuggingFace Spaces Throws a 503 Error</strong>: Users experienced <strong>pauses</strong> during <strong>Spaces docker builds</strong> and received a <strong>503 error</strong> on restart, with many getting <code>Something went wrong when restarting this Space</code> errors (<a href=""https://discuss.huggingface.co/t/spaces-docker-build-pauses-and-503-error-on-restart/171149/2"">discuss.huggingface.co</a>).
<ul>
<li>It seems like the underlying infrastructure issues were causing the spaces to become unresponsive, requiring manual intervention to resolve.</li>
</ul>
</li>
<li><strong>VoltageGPU Volts Up Cheap GPUs</strong>: <a href=""https://voltagegpu.com"">VoltageGPU.com</a> is offering cheap GPUs for open-source AI models, with an <strong>NVIDIA GeForce RTX 5090 pod</strong> available at <strong>$0.53/hour</strong>.
<ul>
<li>They highlight the benefits of their advanced <strong>32GB GDDR7</strong>, optimized for inference on <strong>HF-hosted models like Qwen3-32B</strong>, and are offering free credits for users to try their services.</li>
</ul>
</li>
<li><strong>Layer-Native Safety Clamping Locks Down Jailbreaks</strong>: A new paper introduces <strong>Layer-Native Safety Clamping</strong>, an approach that clamps activations inside the model to prevent jailbreaks, and the team released a <a href=""https://huggingface.co/datasets/Pacific-Prime/safety_dataset"">dataset</a> of <strong>10K pairs</strong>.
<ul>
<li>This approach learns <em>harm directions</em> in activation space and clamps any activation that projects too strongly, thus it cannot be bypassed via prompt manipulation; the paper can be found <a href=""https://zenodo.org/records/18359832"">on Zenodo</a>.</li>
</ul>
</li>
<li><strong>GPU-64 Architecture Boosts LLM Inference</strong>: A new <strong>GPU architecture</strong> designed exclusively for inference, called <strong>GPU-64</strong>, was published, and the innovation involves a Hardware <strong>KV-Cache</strong> using on-chip <strong>CAM</strong> (Content-Addressable Memory).
<ul>
<li>The results show <strong>4x faster inference</strong> at <strong>75W</strong> (O(N) → O(1)), and the paper can be found <a href=""https://zenodo.org/records/18364282"">on Zenodo</a> while the <a href=""https://github.com/Complexity-ML/gpu64-inference"">RTL + Emulator</a> are on GitHub.</li>
</ul>
</li>
<li><strong>Testing and Deploying LLMs on LMStudio</strong>: Members recommend <strong>LMStudio</strong> for testing models due to its user-friendly GUI and search filters for HF and GH models and <strong>llama.cpp</strong> for single-user deployment.
<ul>
<li>They advised against using LMStudio for backend deployment, instead suggesting <strong>llama.cpp's llama-server</strong> in a docker container or <strong>vLLM's server</strong> for better scalability.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1189498204333543425"">GPU MODE</a> Discord</h2>
<ul>
<li><strong>MLSys 2026 Hosts FlashInfer-Bench Kernel Competition</strong>: The <strong>MLSys 2026 FlashInfer-Bench Competition</strong> challenges participants to design <strong>LLM inference kernels</strong> for the latest <strong>NVIDIA Blackwell GPUs</strong>, competing against expert <strong>FlashInfer kernels</strong>, detailed at <a href=""https://mlsys26.flashinfer.ai/"">mlsys26.flashinfer.ai</a>.
<ul>
<li>GPU Mode also held internal competitions for faster kernels for the upcoming GPU architecture, the blogpost on Simon Veitner is located <a href=""https://veitner.bearblog.dev/grouped-blockscaled-gemm-host-code/"">here</a>.</li>
</ul>
</li>
<li><strong>Cornserve Deployed for Multimodal Models</strong>: A member shared <strong>Cornserve</strong>, an efficient online serving system for Any-to-Any multimodal models, detailed in a paper <a href=""https://arxiv.org/abs/2512.14098"">Cornserve</a>.
<ul>
<li><strong>GPU Mode</strong> went online to discuss <strong>Cornserve</strong>: <strong>Easy, Fast and Scalable Multimodal AI</strong> (<a href=""https://www.youtube.com/watch?v=VhjUM_M71Wo"">YouTube link</a>).</li>
</ul>
</li>
<li><strong>Community to train Kernel LLM</strong>: In <strong>2026</strong>, GPU MODE is pushing further with training a <strong>Kernel LLM</strong> and using it to ship kernels in important repos like <strong>PyTorch</strong> and <strong>VLLM</strong> (<a href=""https://www.gpumode.com/v2/news/gpumode-2026"">gpumode.com/v2/news/gpumode-2026</a>).
<ul>
<li>The community is collaborating with <strong>Prime Intellect</strong>, <strong>Modal</strong>, and <strong>Lambda</strong>, focusing on de-slopifying LLM-generated kernels, post-training a kernel LLM model, end-to-end competitions, and from-scratch repos.</li>
</ul>
</li>
<li><strong>LeCun Logs on to Logical Intelligence</strong>: Yann LeCun launched a new startup called <a href=""https://logicalintelligence.com/"">Logical Intelligence</a>, focused on an <strong>Event Based Model (EBM)</strong>.
<ul>
<li>The website only contains marketing material, job openings, and a link to the <a href=""https://mlsys26.flashinfer.ai/"">MLSys Conference</a>.</li>
</ul>
</li>
<li><strong>Mindbeam Hires for Kernel Acceleration</strong>: Mindbeam AI, a small team focused on accelerating training for foundation models, is hiring a <code>post training MLE</code> and <code>GPU Kernel MLE</code>.
<ul>
<li>Interested candidates can DM for a referral; <a href=""https://jobs.ashbyhq.com/mindbeam"">job openings are listed here</a>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/729741769192767510"">Eleuther</a> Discord</h2>
<ul>
<li><strong>ROCm runs rocky road race</strong>: Members debated the performance of <strong>ROCm</strong> for accelerated ML, pointing out its challenges stem from primary support for <strong>Nvidia</strong>, with one calling the experience <em>'batteries not included'</em>.
<ul>
<li>They cited potential driver problems and long lead times as factors.</li>
</ul>
</li>
<li><strong>DistinctionBench defends against data defense</strong>: The discussion of <strong>Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases</strong> pondered whether <strong>DistinctionBench</strong> might be used as a training target for language models.
<ul>
<li>A member joked, <em>'all good evals are training targets ;)'</em>, but acknowledged that it is <em>'very contamination resistant'</em> due to its endless representational variants.</li>
</ul>
</li>
<li><strong>Hybrid Architectures Halt Hallucinations?</strong>: The group investigated <strong>hybrid architectures</strong> combining <strong>LLMs</strong> with <strong>symbolic/deductive layers</strong> for hallucination reduction.
<ul>
<li>While checking logical consistency is relatively easy for math, code, and simple facts (<a href=""https://arxiv.org/abs/2409.13724"">this paper</a>), it remains challenging for other types of consistency (<a href=""https://arxiv.org/abs/2507.10624"">this paper</a>).</li>
</ul>
</li>
<li><strong>Attention Arrived Before Transformers Transformed</strong>: In <strong>Eleuther ▷ #general</strong>, attention mechanisms were in use on top of RNNs in <strong>2014-2015</strong>, two years before the transformers were invented.
<ul>
<li>Members proposed that the slower adoption might be because fewer people were working in the field, and <strong>Kaggle</strong> results really catalyzed its widespread adoption.</li>
</ul>
</li>
<li><strong>Symbolic Sanity Checks saves Sanity</strong>: Members debated whether <strong>LLMs</strong> with <strong>symbolic/deductive layers</strong> might reduce hallucinations by checking logical consistency, especially for code and math as shown in <a href=""https://arxiv.org/abs/2409.13724"">this paper</a>.
<ul>
<li>However, they noted that checking for other types of consistency remains challenging as shown in <a href=""https://arxiv.org/abs/2507.10624"">this paper</a>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1053877538025386074"">Nous Research AI</a> Discord</h2>
<ul>
<li><strong>Exploring Agentic AI Self-Replication Benchmarks</strong>: A member proposed a <strong>self-replication benchmark</strong> for <strong>agentic AI</strong>, suggesting the agent should either download itself or retrain from scratch and adapt to a target machine.
<ul>
<li>They also suggested that adapting to a target machine, or even designing one, could be more engaging than simply using existing transformer libraries.</li>
</ul>
</li>
<li><strong>LLM Worms Concept Emerges</strong>: A member jokingly suggested an <strong>LLM worm</strong> benchmark where an LLM is prompted with <em>""hey make more of you""</em> and provided the tools to replicate itself using scripts and API keys.
<ul>
<li>Another member emphasized the importance of considering resource constraints like <strong>VRAM</strong> to make the challenge more practical and interesting.</li>
</ul>
</li>
<li><strong>Trouble Brewing with MoE Run Dashboard</strong>: A member reported a <em>'Failed to fetch'</em> error in the dashboard while monitoring the progress of an active <strong>MoE run (moe-10b-a1b-8k-wsd-lr3e4-1t)</strong>.
<ul>
<li>Another member suggested waiting a few hours before checking again, implying a potential temporary issue.</li>
</ul>
</li>
<li><strong>Raytracer Test Causes Local Models to Stumble</strong>: A member observed that local code models (suitable for a <strong>5090</strong>) are struggling with a <strong>raytracer test</strong> from <a href=""https://github.com/cpldcpu/llmbenchmark/tree/master/10_raytracer#readme"">cpldcpu/llmbenchmark</a>, with even recent models on <strong>lmarena</strong> failing.
<ul>
<li>Specifically, the smaller models often incorrectly generate the vector class, presenting a persistent challenge.</li>
</ul>
</li>
<li><strong>Semantica Project Needs Helping Hands</strong>: A member introduced <a href=""https://github.com/Hawksight-AI/semantica"">Semantica</a>, an <strong>open-source project</strong> building semantic infrastructure for <strong>domain-grounded AI</strong>, including <strong>knowledge graphs</strong>, <strong>ontologies</strong>, and <strong>reasoning layers</strong>, and is actively seeking contributors.
<ul>
<li>They are looking for contributions in areas such as <strong>ontology &#x26; schema design</strong>, <strong>knowledge graph modeling</strong>, and <strong>LLM + symbolic / rule-based reasoning</strong>, and even small PRs, feedback, design discussions and issues are all welcome.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/714501525455634453"">Yannick Kilcher</a> Discord</h2>
<ul>
<li><strong>EBMs Spark Debate vs. Classical Feedforward</strong>: A discussion comparing <strong>Energy-Based Models (EBMs)</strong> and classical <strong>feedforward networks</strong> debates whether <strong>EBMs</strong> are inherently superior, especially regarding <strong>Shannon entropy</strong> or <strong>Kolmogorov complexity</strong>.
<ul>
<li>It was suggested that <em>validation is easier than generation</em> in EBMs, relating it to <strong>computational complexity theory (P vs NP)</strong>, while emphasizing the need for a well-defined loss landscape for EBM optimization to work effectively.</li>
</ul>
</li>
<li><strong>LLM Pre-training: Domain-Specific vs. Foundational Faceoff</strong>: A member inquired about the effectiveness of <strong>continued pre-training</strong> a foundational <strong>LLM</strong> (specifically <strong>OLMO-7B</strong>) for a domain-specific task like cheminformatics using the <strong>ZINC20 dataset</strong>.
<ul>
<li>The goal is to compare results against a domain-specific transformer model, but no specific answers or resources were provided.</li>
</ul>
</li>
<li><strong>MCMC Sampling Suffers Mode-Switching Struggles</strong>: Concerns were raised about the ability of <strong>MCMC</strong> to traverse between spatially separated modes when dimension increases, referencing <a href=""https://arxiv.org/abs/2310.11232"">this paper</a>.
<ul>
<li>One member argues that <strong>MCMC</strong> tries to emulate flow models due to the latter's superiority, while <strong>EBMs</strong>, contrarily, attempt to make <strong>NNs</strong> more like <strong>MCMC</strong>.</li>
</ul>
</li>
<li><strong>ZKPs: Crypto Signing or Network Traffic Savior?</strong>: Discussion covered using <strong>zero-knowledge proofs (ZKPs)</strong> for verifying encrypted network traffic and matrix multiplications, citing a <a href=""https://gemini.google.com/share/ddfc0ffcb33e"">Gemini correspondence</a> for a matrix low knowledge proof.
<ul>
<li>While one member proposed a use case in <em>zero-knowledge “made by humans” proofs</em>, another member questioned the practicality of <strong>ZKPs</strong>, suggesting breaking the encryption might be cheaper.</li>
</ul>
</li>
<li><strong>LLMs Cyber Skills Face Scrutiny</strong>: A member questioned whether LLMs could develop strong <em>cyber capabilities</em>, referencing a <a href=""https://gptzero.me/news/neurips/"">GPTZero article</a>.
<ul>
<li>Another member doubted LLM companies' ability to address <em>internal vulnerabilities</em>, suggesting they fix those before pursuing cyber skills, also citing a <a href=""https://www.sciencealert.com/scientists-identify-brain-waves-that-define-the-limits-of-you"">ScienceAlert article</a> and a <a href=""https://x.com/theonejvo/status/2015401219746128322"">tweet</a>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1068976834382925865"">tinygrad (George Hotz)</a> Discord</h2>
<ul>
<li><strong>Luminal Finds Flash Attention via Bruteforce</strong>: <strong>Luminal</strong> is claiming to find <strong>flash attention</strong> using <strong>bruteforce</strong> on an egraph, taking hours to find, and they explicitly added <code>exp(x - new_max) = exp(x - old_max) × exp(old_max - new_max)</code> as a rewrite rule.
<ul>
<li>The poster reproduced the graphviz shown in the presentations from commit <code>0bd3b80c</code>, noting that their minimal set of rewrite rules could transform a naive attention kernel graph into the known <strong>flash attention kernel graph</strong> in 52s on a 9800x3d.</li>
</ul>
</li>
<li><strong>Metal Textures Trounce Buffers for Blurring</strong>: Profiling access speed on <strong>Metal</strong> using <code>Tensor</code> with size <strong>512/1024/2048/8192</strong> images as input for a <strong>3/5/7</strong> sized blur kernel showed textures outperforming buffers.
<ul>
<li>It might be worth throwing in a branching condition depending on the size of the buffer input, <a href=""https://cdn.discordapp.com/attachments/1068976834928193609/1464679423029547172/Screenshot_2026-01-25_at_1.49.57_AM.png?ex=6978fb82&#x26;is=6977aa02&#x26;hm=5530b74c4fce9dad5d85a4d9e7409c3809a7ee51ee548744a1fa3deb2efea1d3&#x26;"">tests results are attached</a>.</li>
</ul>
</li>
<li><strong>Tenstorrent Backend Triumphs in Ops Tests</strong>: The <strong>Tenstorrent</strong> backend is passing all ops tests on wormhole or blackhole and there is a <a href=""https://x.com/corsix/status/1880384044728480206"">$1k bounty</a> for this milestone.
<ul>
<li>Someone asked if the bounty requires all test ops test passing on <strong>testorrent hardware</strong>.</li>
</ul>
</li>
<li><strong>Anthropic VLIW Challenge PR Makes Waves</strong>: A member submitted [a PR](https://github.com/tinygrad/t...</li>
</ul>
","{""title"":""Anthropic launches the MCP Apps open spec, in Claude.ai"",""link"":""https://news.smol.ai/issues/26-01-26-mcp-apps/"",""pubDate"":""Mon, 26 Jan 2026 05:44:39 GMT"",""content:encoded"":""<p><strong>Rich generative UI is all you need.</strong></p>\n<blockquote>\n<p>AI News for 1/23/2026-1/26/2026. We checked 12 subreddits, <a href=\""https://twitter.com/i/lists/1585430245762441216\""><strong>544</strong> Twitters</a> and <strong>24</strong> Discords (<strong>206</strong> channels, and <strong>14285</strong> messages) for you. Estimated reading time saved (at 200wpm): <strong>1208 minutes</strong>. <strong>Our new website</strong> is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on <a href=\""https://x.com/Smol_AI\"">@smol_ai</a>!</p>\n</blockquote>\n<p>3 months after OpenAI floated a trial balloon with <a href=\""https://news.smol.ai/issues/25-10-06-devday\"">ChatGPT Apps and the Apps SDK at Dev Day 2025</a>, Anthropic has now officially absorbed <a href=\""https://x.com/liadyosef/status/2002104900843679818\"">the independent MCP UI project</a> and, working with OpenAI, Block, VS Code, Antigravity, JetBrains, AWS, and others, has released both:</p>\n<ul>\n<li><a href=\""https://blog.modelcontextprotocol.io/posts/2026-01-26-mcp-apps/\"">the MCP Apps spec</a></li>\n<li><a href=\""https://x.com/claudeai/status/2015851783655194640\"">official support in Claude.ai</a></li>\n</ul>\n<p>It's fair to say that ChatGPT Apps haven't exactly taken the world by storm since announcement, but the overall need for a standard format for applications to return rich UI still cannot be denied. Now that MCP Apps have been ratified by all the important players, this is the basis for a rich ecosystem of open source support and applications being able to interoperate, and perhaps one day solve the perpetual never ending pile of $20/month subscriptions piling up in your credit card bills.</p>\n<hr>\n<h1>AI Twitter Recap</h1>\n<p><strong>Agent Orchestration, RLMs, and “Clawdbot/Clawd” as a UX pattern</strong></p>\n<ul>\n<li><strong>NVIDIA ToolOrchestra + Orchestrator-8B</strong>: NVIDIA’s ToolOrchestra frames agentic systems as a <em>small “conductor” model</em> that alternates reasoning with calls to tools and larger “expert” models (search, code execution, specialist LLMs, frontier generalists). The claim is that an <strong>8B orchestrator</strong> can reach <em>frontier-level outcomes</em> via delegation at materially lower cost, trained end-to-end with <strong>scalable RL</strong> using automatically synthesized tool-use environments and multi-turn tasks (<a href=\""https://twitter.com/TheTuringPost/status/2015565947827110255\"">summary</a>, <a href=\""https://twitter.com/TheTuringPost/status/2015565962419048712\"">link</a>). Closest technical implication: “controller scale” matters less than <em>policy quality + tool/model routing</em> if you can train it with realistic tool-call rollouts.</li>\n<li><strong>RLMs / recursion-first agent stacks</strong>: Several posts converge on a <strong>Recursive Language Model (RLM)</strong> pattern: pass files and context <em>by reference</em> and iteratively pull the minimum slices needed (shell/grep/AST), rather than stuffing everything into context à la ReAct. Dan B illustrates this with file references vs <code>@file</code> expansion as deliberate <strong>context management</strong> (<a href=\""https://twitter.com/irl_danB/status/2015813778504372601\"">thread</a>). Daytona is positioning RLMs as “unlimited recursion depth” via per-(sub)agent sandboxes (<a href=\""https://twitter.com/ivanburazin/status/2015818845303271896\"">guide</a>, <a href=\""https://twitter.com/a1zhang/status/2015820458709471640\"">integration</a>).</li>\n<li><strong>“Clawd/Clawdbot” meme → product signal</strong>: The dataset contains a large “Clawdbot” wave (often with Mac mini jokes), but the technically relevant throughline is <em>outcome-first assistant UX</em> + <strong>tight context/tool integration</strong>. Kimmonismus explicitly calls this a shift from “more chat” to “more outcome,” suggesting incumbents will scramble to match it (<a href=\""https://twitter.com/kimmonismus/status/2015785094791713006\"">tweet</a>). Others push a cloud-first counterpoint (no local Mac mini) (<a href=\""https://twitter.com/SkylerMiao7/status/2015596649171804613\"">MiniMax reply</a>). There’s also an emerging <em>security backlash</em> as soon as “powerful mode” exists: prompt injection remains a system-level blocker for browser/desktop agents (<a href=\""https://twitter.com/fabianstelzer/status/2015671497180827785\"">dilemma</a>, <a href=\""https://twitter.com/fabianstelzer/status/2015702808465420614\"">follow-up</a>, <a href=\""https://twitter.com/DanielMiessler/status/2015865548714975475\"">Miessler warnings</a>).</li>\n</ul>\n<p><strong>Reasoning model releases &#x26; eval dynamics (Qwen, Tencent, ARC, etc.)</strong></p>\n<ul>\n<li><strong>Alibaba Qwen3-Max-Thinking</strong>: Alibaba positions Qwen3-Max-Thinking as a flagship reasoning+agent model trained with “massive scale and advanced RL,” emphasizing <strong>adaptive tool-use</strong> (Search/Memory/Code Interpreter) and <strong>test-time scaling/self-reflection</strong>. They cite strong math and agentic search metrics (e.g., <strong>98.0 on HMMT Feb</strong>, <strong>49.8 on HLE</strong>) (<a href=\""https://twitter.com/Alibaba_Qwen/status/2015805330652111144\"">launch</a>). The model is immediately pushed into public eval channels: LM Arena Text Arena (<a href=\""https://twitter.com/arena/status/2015803787680808996\"">Arena</a>) and Yupp (<a href=\""https://twitter.com/yupp_ai/status/2015812409823522952\"">Yupp</a>). Community reaction highlights the <em>tool-enabled evaluation regime</em>—claims of outperforming multiple SOTA models on HLE <em>with search tools</em> (<a href=\""https://twitter.com/kimmonismus/status/2015820838243561742\"">commentary</a>).</li>\n<li><strong>Tencent HunyuanImage 3.0-Instruct (image editing)</strong>: Tencent releases an image-editing-focused multimodal model built on an <strong>80B MoE</strong> (13B active), using a “Thinking” schema with native CoT and their <strong>MixGRPO</strong> algorithm; focus is on precise edits that preserve non-target regions and multi-image fusion (<a href=\""https://twitter.com/TencentHunyuan/status/2015635861833167074\"">announcement</a>). LM Arena reports it entering the <strong>top-10 image edit leaderboard</strong> (rank #7) (<a href=\""https://twitter.com/arena/status/2015846799446311337\"">Arena</a>).</li>\n<li><strong>ARC-AGI cost/perf hacks</strong>: A notable optimization claim: “Recursive Self-Aggregation (RSA) + Gemini 3 Flash” reaching <strong>59.31% on ARC-AGI-2 at ~1/10 cost</strong> vs Gemini Deep Think (<a href=\""https://twitter.com/kimmonismus/status/2015717203362926643\"">tweet</a>). This points to a broader theme: <em>meta-inference strategies</em> (aggregation, recursion, pruning) are becoming as important as base model choice.</li>\n<li><strong>Open models in arenas</strong>: Molmo 2 (Apache 2.0) appears in Arena as a new open model entrant (<a href=\""https://twitter.com/arena/status/2015886736136798723\"">Arena</a>). Separately, Hugging Face Inference Endpoint notes <strong>GLM-4.7-Flash via llama.cpp</strong> with a low hourly price point (Q4_K_M, 24k context) (<a href=\""https://twitter.com/ngxson/status/2015763148523897097\"">ngxson</a>)—underscoring a continued commoditization of <em>fast open-weight inference</em>.</li>\n</ul>\n<p><strong>RL everywhere: test-time training, GRPO stabilization, RL-as-pretraining, and compute savings</strong></p>\n<ul>\n<li><strong>Test-Time Training (TTT) + RL breakthroughs</strong>: A widely shared result claims a Stanford/NVIDIA-style TTT+RL approach that: beats AlphaEvolve, finds a new upper bound for an Erdős overlap problem, produces <strong>A100 kernels ~2× faster</strong> than best human kernels, and beats both best AI+human attempts on AtCoder (<a href=\""https://twitter.com/rronak_/status/2015649459552850113\"">rronak_</a>). This cluster also includes meta-discussion about correctly crediting related approaches (EvoTune) (<a href=\""https://twitter.com/YejinChoinka/status/2015566349444190432\"">Yejin Cho</a>).</li>\n<li><strong>GRPO training stability knobs</strong>: A small but actionable engineering tip: INTELLECT-2 reports a <strong><code>delta=4.0</code></strong> parameter that improves GRPO stability (<a href=\""https://twitter.com/QGallouedec/status/2015711810108973462\"">QGallouedec</a>).</li>\n<li><strong>RL in pretraining (RLP)</strong>: NVIDIA authors announce <strong>RLP (Reinforcement as a Pretraining Objective)</strong> accepted to ICLR 2026, framing RL not as “post-training only” but as integrated into pretraining (<a href=\""https://twitter.com/ahatamiz1/status/2015867794626380146\"">ahatamiz1</a>).</li>\n<li><strong>Compute reduction via curriculum-like filtering</strong>: AI21’s “Dynamic Data Snoozing” claims up to <strong>3× compute reduction</strong> for RLVR by snoozing examples that are too easy (<a href=\""https://twitter.com/DanielGissin/status/2015773616021860522\"">DanielGissin</a>). If validated, this is a practical recipe: make the sampler policy-aware instead of static.</li>\n</ul>\n<p><strong>Inference infrastructure &#x26; dev tooling: vLLM’s “day-0 model support,” VS Code MCP Apps, Cursor subagents</strong></p>\n<ul>\n<li><strong>vLLM’s governance and commercialization pressure</strong>: A long Zhihu-derived summary argues vLLM’s “open-source project → startup” shift was driven by the hidden cost of <strong>day-0 support</strong> (weeks/months of confidential pre-integration per new model), the rise of MoE and heterogeneous inference (fp8/int4/sparse attention), and the mismatch with PyTorch Foundation style testing vs vLLM’s multi-node CI needs. It claims the maintainers founded <strong>Inferact Inc</strong> to fund full-time maintainers while keeping vLLM open-source (<a href=\""https://twitter.com/ZhihuFrontier/status/2015697493288518105\"">thread</a>). Related: vLLM shares a practical flag for avoiding OOM on long-context models: <code>--max-model-len auto</code> (<a href=\""https://twitter.com/vllm_project/status/2015801909316382867\"">vLLM tip</a>).</li>\n<li><strong>MCP Apps: tool calls return interactive UI</strong>: The MCP ecosystem announces <strong>MCP Apps</strong> as the first official MCP extension: tool calls can return <strong>interactive UI components</strong> rendered in-chat. VS Code is first major editor shipping support (Insiders now, stable soon) (<a href=\""https://twitter.com/code/status/2015853688594612715\"">VS Code</a>, <a href=\""https://twitter.com/alexalbert__/status/2015854375051428111\"">alexalbert__</a>). Anthropic simultaneously ships “interactive work tools in Claude” (Slack drafting, Figma diagrams, Asana timelines) (<a href=\""https://twitter.com/claudeai/status/2015851783655194640\"">Claude</a>). Net: we’re seeing the “tool interface layer” move from raw JSON to <em>native UI primitives</em> inside agent loops.</li>\n<li><strong>Cursor: multi-browser subagents</strong>: Cursor adds multi-browser support via subagents (<a href=\""https://twitter.com/cursor_ai/status/2015863221589049483\"">Cursor</a>), echoing the same direction: parallelized tool execution + better context isolation.</li>\n</ul>\n<p><strong>Kernel LLMs, chip stacks, and “AI for hardware” loops</strong></p>\n<ul>\n<li><strong>GPU MODE 2026: post-training Kernel LLMs in public</strong>: GPU MODE outlines a 2026 plan to <strong>post-train a Kernel LLM</strong> and get generated kernels merged into real repos (PyTorch/vLLM), emphasizing “de-slopify kernels” (determinism, reviewer-mergeable PRs), profiler-guided optimization + memory work, and competitions as evals (<a href=\""https://twitter.com/marksaroufim/status/2015818791729746350\"">marksaroufim</a>).</li>\n<li><strong>Microsoft Maia 200</strong>: Microsoft announces Maia 200 as a custom inference accelerator; Mustafa Suleyman claims it’s the most performant first-party hyperscaler silicon, with <strong>3× FP4 performance</strong> vs Trainium v3 and FP8 above TPU v7 (<a href=\""https://twitter.com/mustafasuleyman/status/2015845567138816326\"">Mustafa</a>, <a href=\""https://twitter.com/mustafasuleyman/status/2015825111769841744\"">follow-up</a>). Yusuf Mehdi frames this as infra that makes AI “dependable” (<a href=\""https://twitter.com/yusuf_i_mehdi/status/2015826703944470701\"">thread</a>).</li>\n<li><strong>Ricursive Intelligence (AI for chip design)</strong>: Ricursive raises a <strong>$300M Series A</strong> aiming at end-to-end chip design as a recursive self-improvement loop between AI and hardware (<a href=\""https://twitter.com/RicursiveAI/status/2015804806384755059\"">company</a>, <a href=\""https://twitter.com/annadgoldie/status/2015806107470438685\"">Anna Goldie</a>).</li>\n</ul>\n<p><strong>Safety, misuse, and societal impact (selected items with direct technical relevance)</strong></p>\n<ul>\n<li><strong>Elicitation attacks via benign chemistry data</strong>: Anthropic reports that fine-tuning open models on “benign” chemical synthesis content generated by frontier models can significantly increase capability on <strong>chemical weapons</strong> tasks—an “elicitation attack” that scales with frontier model strength (<a href=\""https://twitter.com/AnthropicAI/status/2015870963792142563\"">AnthropicAI</a>, <a href=\""https://twitter.com/AnthropicAI/status/2015870975238406600\"">paper link</a>).</li>\n<li><strong>Dario Amodei’s “Adolescence of Technology” essay</strong>: A major, highly engaged post argues AI is entering an accelerating feedback loop (AI building AI), with risks spanning misuse, power-seeking autonomy, and economic disruption; it also explicitly frames wealth concentration as a society-breaking failure mode (<a href=\""https://twitter.com/DarioAmodei/status/2015833046327402527\"">Dario</a>). Reaction ranges from strong endorsement to critique of how “takeover risk” framing is presented (<a href=\""https://twitter.com/RyanPGreenblatt/status/2015869503385772037\"">Ryan Greenblatt</a>).</li>\n<li><strong>Agent security in practice</strong>: Multiple posts treat desktop/browser agents as inherently high-risk until prompt injection and sandboxing mature, reinforcing the need for strict isolation, least privilege, and careful handling of credentials (<a href=\""https://twitter.com/DanielMiessler/status/2015865548714975475\"">Miessler</a>).</li>\n</ul>\n<p><strong>Top tweets (by engagement)</strong></p>\n<ul>\n<li><a href=\""https://twitter.com/0xRacist/status/2015578387641991513\"">“Clawdbot” misuse example (explicitly harmful)</a></li>\n<li><a href=\""https://twitter.com/karpathy/status/2015883857489522876\"">Karpathy on the phase shift to “programming in English” via agents</a></li>\n<li><a href=\""https://twitter.com/DarioAmodei/status/2015833046327402527\"">Dario Amodei’s “Adolescence of Technology”</a></li>\n</ul>\n<hr>\n<h1>AI Reddit Recap</h1>\n<h2>/r/LocalLlama + /r/localLLM Recap</h2>\n<h3>1. Local LLM Hardware and Benchmarking</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qni356/216gb_vram_on_the_bench_time_to_see_which/\"">216GB VRAM on the bench. Time to see which combination is best for Local LLM</a></strong> (Activity: 366): <strong>The post discusses the use of secondhand Tesla GPUs, which offer substantial VRAM at a lower cost, for local large language model (LLM) testing. The author has developed a <a href=\""https://esologic.com/gpu-server-benchmark/#gpu-box-benchmark\"">GPU server benchmarking suite</a> to evaluate the performance of these GPUs when used in parallel. The image shows a technical setup with multiple NVIDIA GPUs, highlighting the focus on maximizing VRAM capacity. The discussion centers around the feasibility and efficiency of using these older GPUs compared to modern devices, particularly in terms of bandwidth and cooling challenges.</strong> Commenters express skepticism about the performance of these GPUs, noting potential issues with bandwidth and cooling. One commenter shares personal experience, comparing different GPU models and highlighting the challenges of using older hardware.</p>\n<ul>\n<li>HugoCortell raises a technical concern about the potential bandwidth limitations when connecting multiple GPUs to a single PC, noting that most affordable server motherboards support only a few GPUs. This could impact the performance of local LLMs if not addressed properly.</li>\n<li>dc740 shares insights from personal experience with different GPUs, highlighting that the P40 outperforms the M10 despite both being older models. However, they prefer using AMD Instinct Mi50 GPUs due to their performance, even though support for these was recently dropped from ROCm, indicating a trade-off between hardware capability and software support.</li>\n<li>FullOf_Bad_Ideas critiques the gpu_box_benchmark for not testing scenarios where large models are split across multiple GPUs, which is a primary use case for setups with extensive VRAM. This points to a gap in current benchmarking practices that may not fully reflect real-world applications of multi-GPU systems.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qn3xig/i_just_won_an_nvidia_dgx_spark_gb10_at_an_nvidia/\"">I just won an Nvidia DGX Spark GB10 at an Nvidia hackathon. What do I do with it?</a></strong> (Activity: 724): <strong>The image shows a terminal window on a Linux system running the 'top' command, which is used to monitor system processes and resource usage in real-time. The user has won an Nvidia DGX Spark GB10, a high-performance computing device designed for machine learning and data-intensive tasks. The terminal indicates a Python process consuming significant CPU resources, suggesting active computational tasks, possibly related to machine learning or data processing. The user is considering using the device to run multiple NextJS applications simultaneously, leveraging its powerful capabilities.</strong> One commenter suggests running three NextJS applications simultaneously, indicating the device's capability to handle multiple high-memory tasks. Another commenter provides a link to Nvidia's DGX Spark playbooks, which could be useful for the user to explore the full potential of their new hardware.</p>\n<ul>\n<li>Fit-Produce420 highlights the capabilities of the Nvidia DGX Spark GB10, noting that with 128GB of memory, it can fine-tune models up to 70 billion parameters. Additionally, it can handle larger models like the 120 billion parameter <code>gtp-oss-120b</code> using techniques like QLoRA, which optimizes memory usage for large-scale models. However, running dense models like <code>devstral 2</code> may be slow due to their computational demands.</li>\n<li>randomfoo2 suggests utilizing the <a href=\""https://github.com/NVIDIA/dgx-spark-playbooks\"">NVIDIA DGX Spark playbooks</a> as a resource for getting started with the DGX Spark GB10. These playbooks provide structured guidance and best practices for deploying and managing workloads on the DGX platform, which can be particularly useful for users new to this hardware.</li>\n<li>LicensedTerrapin humorously suggests selling the DGX Spark GB10 to purchase 8GB of DDR5 RAM, implying a trade-off between high-end specialized hardware and more general-purpose upgrades. This comment reflects a common debate in tech communities about the value of specialized versus general-purpose hardware investments.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLM/comments/1qnpti6/using_a_highend_macbook_pro_or_a_beefy_rtx_5090/\"">Using a high-end MacBook Pro or a beefy RTX 5090 laptop (with 24 GB of RAM) for inference.</a></strong> (Activity: 29): <strong>The post discusses the feasibility of using a high-end MacBook Pro with Apple Silicon (M-series Max) versus a Windows/Linux laptop with an RTX 5090 GPU for running large local LLMs (70B+ parameters) for inference and fine-tuning. The MacBook Pro offers 128–192 GB of unified memory, while the RTX 5090 laptop provides 24 GB of VRAM and at least 64 GB of system RAM. The primary use case is local LLM inference with a target of ≥15 tokens/sec, emphasizing portability. The post queries whether the larger unified memory of Apple Silicon outweighs the CUDA performance of the RTX laptop for inference, and how Apple MLX compares to CUDA for fine-tuning tasks like LoRA/QLoRA. It also seeks insights on thermal performance and sustained inference capabilities of both setups.</strong> One commenter suggests using the laptop as a terminal to a more powerful desktop, indicating a preference for leveraging remote resources over local hardware. Another commenter is experimenting with both setups, using a MacBook Pro M2 Max for inference, and is curious about the performance differences.</p>\n<ul>\n<li>racerx509 shares their experience using a Lenovo laptop with a 3070ti, a custom desktop with a 5070, and a MacBook Pro M2 Max with 96GB RAM for inference tasks. They note that they have been primarily using the MacBook Pro for inference, suggesting it may offer better performance or convenience for their needs.</li>\n<li>No-Concern-8832 raises a concern about the VRAM limitations of RTX laptops, suggesting that they may not be sufficient for running large models like 70B parameters. This highlights a potential limitation in using high-end RTX laptops for certain deep learning tasks that require substantial VRAM.</li>\n<li>Tired__Dev discusses their experience with an Asus M16 equipped with a 4090 GPU, noting that it struggled with a 7B parameter model. They express a preference for a MacBook Pro with 128GB RAM, citing its high memory bandwidth and potential performance advantages over even high-end GPU setups like the DGX Spark.</li>\n</ul>\n</li>\n</ul>\n<h3>2. Multi-Agent Systems and AI Assistants</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qnjota/i_built_a_hive_mind_for_claude_code_7_agents/\"">I built a \""hive mind\"" for Claude Code - 7 agents sharing memory and talking to each other</a></strong> (Activity: 313): <strong>The post describes a multi-agent orchestration system for <strong>Claude Code</strong>, featuring seven specialized agents (e.g., coder, tester, reviewer) that coordinate tasks, share persistent memory using <code>SQLite + FTS5</code>, and communicate via a message bus. The system runs as an MCP server and integrates with <strong>Anthropic</strong>, <strong>OpenAI</strong>, or <strong>Ollama</strong>. It uses a task queue for priority-based coordination, allowing agents to pass context and collaborate effectively. The implementation stack includes <strong>TypeScript</strong>, <strong>better-sqlite3</strong>, <strong>MCP SDK</strong>, and <strong>Zod</strong>. The project is experimental, open-source under the MIT license, and available on <a href=\""http://github.com/blackms/aistack\"">GitHub</a>.</strong> A comment questions the system's uniqueness compared to the <a href=\""https://github.com/bmad-code-org/BMAD-METHOD\"">BMAD method</a>, suggesting similarities. Another comment humorously questions whether the agents agree with each other, hinting at potential coordination challenges.</p>\n<ul>\n<li>The user robiinn inquires about the differences between the 'hive mind' system and the <a href=\""https://github.com/bmad-code-org/BMAD-METHOD\"">bmad method</a>, suggesting a potential similarity. This indicates a need for clarification on the unique aspects or improvements of the 'hive mind' approach over existing methods, such as how memory sharing and inter-agent communication are implemented differently.</li>\n<li>No_Afternoon_4260 raises a critical point about the consensus among the agents in the 'hive mind'. This touches on the technical challenge of ensuring that multiple agents can not only share memory but also reach agreement or consensus, which is a significant aspect of distributed systems and multi-agent frameworks.</li>\n<li>JellyBean504 draws a parallel between the 'hive mind' and Steve Yegge's Gastown, suggesting that there might be conceptual similarities. This comparison could be valuable for understanding the architectural or functional parallels between the two systems, potentially offering insights into design choices or performance characteristics.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLM/comments/1qmrwxl/clawdbot_the_ai_assistant_that_actually_messages/\"">Clawdbot: the AI assistant that actually messages you first</a></strong> (Activity: 214): <strong><strong>Clawdbot</strong> is an open-source AI assistant with over <code>9K</code> GitHub stars, designed to proactively message users, unlike traditional AI assistants that wait for prompts. It integrates with locally hosted LLMs via <strong>Ollama</strong> and supports messaging apps like WhatsApp, Telegram, and Discord. Key features include sending automated briefings and reminders, local storage of conversations as Markdown files, and the ability to control browsers and run scripts. The software is free under the MIT license but requires terminal proficiency for setup, as there is no GUI installer. <a href=\""https://medium.com/@jpcaparas/what-are-people-doing-with-clawdbot-e91403383ccf?sk=4fbaffdc31974eab844ea93c2f9b627f\"">Read more</a>.</strong> Users report challenges with setup, particularly with obtaining and using OAuth keys for authentication, and difficulties in connecting local LLMs without relying on API keys. Some users express frustration with the complexity of setup, especially when using remote machines.</p>\n<ul>\n<li>mike7seven highlights the complexity of setting up Clawdbot, particularly emphasizing the need to obtain a Claude OAuth key on a separate machine and then transfer it to the setup machine. This process is noted as cumbersome, especially for those using remote machines, and the MacOS app requires building from source, adding another layer of complexity.</li>\n<li>Ashamed_Promise7726 raises a technical challenge regarding the integration of local language models with Clawdbot. The user notes difficulty in connecting pre-downloaded models on their PC, as Clawdbot seems to require an API key for usage-based models, questioning the feasibility of running Clawdbot entirely locally without external dependencies.</li>\n<li>inigid warns about potential security risks associated with Clawdbot, suggesting it could be exploited for supply-chain attacks that compromise sensitive data on a user's machine and network. The comment also mentions concerns about the association with Solana meme coins, implying a need for caution when using the tool.</li>\n</ul>\n</li>\n</ul>\n<h3>3. GLM-4.7-Flash Performance Updates</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qmvny5/glm47flash_is_even_faster_now/\"">GLM-4.7-Flash is even faster now</a></strong> (Activity: 443): <strong>The recent update to <code>llama.cpp</code> by <strong>Johannes Gaessler</strong> optimizes the CUDA implementation of FlashAttention, specifically for models with a non-power-of-2 ratio of query heads to key/value heads. This is achieved by padding Q columns to the next power of 2, which, although slightly inefficient, enhances performance for small batch sizes. The update is detailed in <a href=\""https://github.com/ggml-org/llama.cpp/pull/19092\"">pull request #19092</a>.</strong> One comment humorously notes the obsolescence of a previous post due to this update, while another laments the lack of support for AMD GPUs, highlighting a common issue in the community regarding hardware compatibility.</p>\n<ul>\n<li>The user 'jacek2023' provides detailed performance metrics for the GLM-4.7-Flash model, highlighting its efficiency. The model processes a prompt with <code>45074</code> tokens, achieving a prompt evaluation time of <code>2814.63 ms</code> for <code>1612</code> tokens, which translates to <code>1.75 ms per token</code> or <code>572.72 tokens per second</code>. The overall evaluation time is <code>29352.57 ms</code> for <code>1731</code> tokens, equating to <code>16.96 ms per token</code> or <code>58.97 tokens per second</code>. The total processing time is <code>32167.20 ms</code> for <code>3343</code> tokens, indicating significant improvements in speed.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qmjzx1/kv_cache_fix_for_glm_47_flash/\"">KV cache fix for GLM 4.7 Flash</a></strong> (Activity: 380): <strong>The recent update to <strong>GLM 4.7 Flash</strong> involves removing the V component from the KV cache, which significantly reduces VRAM usage, allowing for longer context lengths on the same hardware setup. This change is particularly beneficial for models like <strong>DeepSeek</strong> and <strong>GLM 4.7 Flash</strong>, as it can save gigabytes of VRAM, enabling context lengths to double, as demonstrated by a user running a 90,000 context on a 4090 GPU. The update is part of a pull request in the <code>llama.cpp</code> repository, which introduces a V-less KV cache, reducing memory usage by nearly 50%. More details can be found in the <a href=\""https://github.com/ggml-org/llama.cpp/pull/19067\"">pull request</a>.</strong> A user noted that the model, while improved, still requires some manual guidance, especially in tasks like coding and creative writing, where it may not perform as well as specialized models. However, it excels in tool use and as an assistant, making it a preferred choice for home-server applications.</p>\n<ul>\n<li>The user 'teachersecret' reports significant improvements in context handling with the UD's k_xl 4-bit version of the GLM 4.7 model on an RTX 4090. Previously, the model maxed out at 45,000 context tokens, but now it can handle 90,000. Despite these improvements, the model still requires some manual guidance, especially in coding tasks, and is less effective in creative writing compared to other models. However, it excels in tool usage and is now the user's default model for their home server.</li>\n<li>User 'viperx7' provides detailed benchmark data comparing the performance of the GLM 4.7 model before and after a specific change. The benchmarks show improvements in both prompt processing and token generation speeds across different configurations. For instance, using a single RTX 4090, the context size increased from 64k to 128k, with prompt processing speed improving from 3489 t/s to 3510 t/s and token generation from 88 t/s to 92.5 t/s. The maximum context size achievable with a 4090 and 3060 setup is 200k, leaving about 6GB of VRAM unused.</li>\n<li>The discussion highlights the technical aspect of the GLM 4.7 model's KV cache fix, which allows for increased context sizes and improved performance metrics. The benchmarks provided by 'viperx7' indicate that the model can now handle up to 207k context size in certain configurations, with significant improvements in processing speeds. This suggests that the model's efficiency has been enhanced, making it more suitable for high-demand applications.</li>\n</ul>\n</li>\n</ul>\n<h2>Less Technical AI Subreddit Recap</h2>\n<blockquote>\n<p>/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo</p>\n</blockquote>\n<h3>1. Claude AI Usage and Issues</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/ClaudeCode/comments/1qmrkr1/why_you_need_to_constantly_clear_claude_codes/\"">Why You Need To Constantly Clear Claude Codes Context Window</a></strong> (Activity: 166): <strong>The post highlights the necessity of regularly clearing the context window when using coding agents like Claude to maintain optimal performance. It notes that performance degrades significantly when the context window exceeds <code>40%</code> of its capacity due to the quadratic nature of LLM attention, which increases computational demands and introduces noise. The recommended practice is to avoid accumulating context and instead persist it by using a 'one session per task' strategy, ensuring each task starts with a fresh context. More details can be found in the <a href=\""https://willness.dev/blog/one-session-per-task\"">original article</a>.</strong> Commenters suggest practical strategies such as using handover prompts to transfer necessary details between sessions, employing the '/clear' command to compact context, and utilizing 'Plan Mode' to clear context and execute tasks efficiently. These methods reportedly help avoid the need for a full context window, even for large tasks.</p>\n<ul>\n<li>Agrippanux suggests using 'Plan Mode' as the default setting for Claude, which allows users to clear the context and execute plans without needing a full context window. This approach has been effective for large tasks, such as refactoring, without requiring the entire context to be loaded, thus optimizing performance and resource usage.</li>\n<li>thurn2 discusses the use of sub-agents in Claude, which involves delegating tasks like creating a git worktree and fixing specific issues. This method allows for parallel execution of tasks and helps in managing complex projects by breaking them down into smaller, manageable tasks, enhancing efficiency and implementation accuracy.</li>\n<li>Fancy_Excitement6050 notes that as the context window grows, Claude tends to take shortcuts, which can lead to a need for constant reminders to maintain thoroughness. This suggests that managing the context window size is crucial for maintaining the quality of output, and there might be differences in performance between different Claude plans, such as Claude Max.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/ClaudeCode/comments/1qnhgcc/opus_fell_off_heres_the_workflow_that_kept_my/\"">Opus fell off? Here’s the workflow that kept my code quality stable</a></strong> (Activity: 133): <strong>The post discusses a structured workflow to maintain code quality when using AI models like <strong>Opus</strong> and <strong>Sonnet</strong>, which have been perceived as producing \""confident wrong\"" outputs and drifting edits. The workflow emphasizes a loop of <strong>specification, ticket creation, execution, and verification</strong>. Specifications are detailed with non-goals, user stories, acceptance criteria, edge cases, and more, treated as code to ensure clarity. Tickets are derived from specs, focusing on small, independently mergeable tasks with clear acceptance checks. Execution involves implementing one ticket at a time with constraints to prevent scope drift, and verification involves running tests and confirming acceptance criteria before feeding failures back into the model for correction. This approach aims to maintain discipline and reduce reliance on the model's \""done\"" signal, ensuring stable and reliable outputs.</strong> Commenters agree that the workflow is effective, emphasizing that AI models function more like junior engineers requiring clear specifications and strict feedback loops. This approach shifts effort towards upfront clarity and external verification, making the system more stable and less reliant on the model's intelligence. Smaller scoped tickets and hard verification are noted as beneficial strategies.</p>\n<ul>\n<li>GenOS2312 highlights the importance of treating LLMs like junior engineers, emphasizing that a well-specified problem and a strict feedback loop are crucial for reliable outputs. The workflow discussed focuses on upfront clarity and external verification, which stabilizes the system by not relying on the model's intelligence but rather constraining it to ensure even average runs yield acceptable results.</li>\n<li>Different-Object5926 notes that smaller scoped tickets combined with hard verification processes significantly improve the stability and reliability of using models like Opus. This approach mitigates the impact of variability in model performance, suggesting that the issue isn't just 'unlucky runs' but rather the need for structured constraints.</li>\n<li>TheOriginalAcidtech suggests implementing hooks to prevent skipping steps in the workflow, emphasizing that the human interface is often the weakest link. By enforcing strict adherence to the process, the system can better manage user interactions, ensuring that the model and its harness guide the user effectively, rather than relying solely on the model's capabilities.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/singularity/comments/1qn325q/after_claude_now_chatgpt_is_also_uses_grokipedia/\"">after claude now chatgpt is also uses Grokipedia as source</a></strong> (Activity: 634): <strong>The image and accompanying discussion highlight that the latest version of <strong>ChatGPT</strong> is reportedly using <strong>Elon Musk's Grokipedia</strong> as a source. This is significant as it suggests a shift in the data sources used by ChatGPT, potentially affecting the information quality and bias in its responses. The comments reveal a concern about the implications of using Grokipedia, particularly regarding the potential for biased information, as one user notes the risk of models being influenced by 'right wing' content. However, it is clarified that Grokipedia is not used as training data but rather as a search tool, which may mitigate some concerns about direct bias in the model's foundational knowledge.</strong></p>\n<ul>\n<li>The discussion highlights concerns about language models like Claude and ChatGPT potentially using sources like Grokipedia, which may have biased or unreliable content. This raises questions about the integrity of the information these models provide, especially when they utilize search tools to access real-time data. The implication is that the quality and neutrality of the data sources are crucial for maintaining the accuracy and trustworthiness of AI outputs.</li>\n<li>There is a debate about the impact of using sources like Grokipedia on the training and performance of language models. Some commenters express concern that incorporating biased or politically skewed sources could lead to the dissemination of misinformation. This reflects broader worries about the influence of data sources on the objectivity and reliability of AI-generated content.</li>\n<li>The mention of Reddit as a data source for language models suggests a comparison of potential biases. While some argue that Reddit may contain more extreme or varied viewpoints, the underlying issue is the challenge of ensuring that AI models are trained on balanced and factual data. This discussion underscores the importance of curating high-quality datasets to prevent the spread of biased information.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/ClaudeAI/comments/1qm8tvj/giving_claude_full_access_to_a_laptop/\"">Giving Claude full access to a laptop</a></strong> (Activity: 795): <strong>The post discusses the implementation of giving <strong>Claude</strong>, an AI model, full access to a laptop, allowing it to autonomously manage a virtual machine (VM) on Ubuntu Google Cloud. The user describes how Claude can be remotely controlled via Discord to build new features and fix bugs, logging major actions with timestamps in a markdown file for memory management. This setup enables the user to learn from Claude's problem-solving processes and manage workflows effectively, even as a newcomer to programming.</strong> One commenter, a desktop support technician, expressed amazement at the implementation, noting its potential impact on job roles, while another sought clarification on the technical specifics of giving Claude full device access.</p>\n<ul>\n<li><em>xxxBigMemerxxx</em> describes using Claude to manage a Google Cloud VM running Ubuntu, highlighting its ability to autonomously handle tasks and build features. They mention using Discord for remote requests and bug fixes, and implementing a logging system with markdown and Unicode for tracking changes. This setup allows for a dynamic interaction with Claude, enabling it to learn from errors and maintain a form of short-term memory by logging recent updates.</li>\n<li>Happy_Requirement187 shares their experience running Claude on an AWS EC2 instance with Ubuntu Linux, accessed via SSH from a Windows laptop. They utilize a Jupyter notebook server for seamless file sharing between the EC2 instance and their local environment, a method recommended by Anthropic. Additionally, they have set up a Ruby on Rails environment with a React frontend for secure file sharing, allowing them to request files via Slack, demonstrating a sophisticated integration of Claude into their workflow.</li>\n<li>sivadneb inquires about setting up voice control in Linux, indicating a technical challenge in integrating voice commands with Claude. This suggests an interest in expanding the interaction capabilities with Claude beyond text-based commands, potentially enhancing the usability and accessibility of the system.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/ClaudeCode/comments/1qn9pb9/claudemd_says_must_use_agent_claude_ignores_it_80/\"">CLAUDE.md says 'MUST use agent' - Claude ignores it 80% of the time.</a></strong> (Activity: 309): <strong>The image and post discuss a technical issue with the CLAUDE.md file, which is supposed to direct the AI, Claude, to use a specific agent for workflow questions. Despite explicit instructions in the file, Claude often defaults to a generic agent, indicating a lack of enforcement in the system. The post suggests that without technical enforcement mechanisms, such as hooks or stronger prompts, instructions are merely suggestions. The image emphasizes these points with highlighted text, suggesting potential solutions like adding enforcement hooks to ensure compliance with the specified workflow.</strong> Commenters suggest that the issue may stem from unclear instructions, emphasizing the need for simple and direct commands. They also highlight the importance of implementing technical solutions, such as hooks, to enforce compliance with the CLAUDE.md instructions.</p>\n<ul>\n<li>Accomplished_Buy9342 suggests using hooks to manage Claude's behavior, providing a link to a GitHub repository that demonstrates how to block the main chat from performing actions and delegate tasks to a subagent. This approach can help in orchestrating Claude's actions more effectively, especially when dealing with complex tasks or large contexts.</li>\n<li>luka5c0m highlights a common issue with Claude when used at scale: as the context grows beyond a few files, the agent may perform unexpected actions. They suggest that instead of relying solely on better prompts, developers should use hooks and dynamic instructions to maintain a sharp and concise context. They also mention working on a dynamic CLAUDE.md file that adapts to the current task, which could help in managing large or nested files effectively.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/ClaudeCode/comments/1qm5vmh/my_ralph_wiggum_breakdown_just_got_endorsed_as/\"">My Ralph Wiggum breakdown just got endorsed as the official explainer</a></strong> (Activity: 170): <strong>The post discusses a video breakdown of Ralph Wiggum, an autonomous coding loop, which has been endorsed by <strong>Geoffrey Huntley</strong> as the official explainer. Ralph Wiggum is a <code>bash while loop</code> that calls <strong>Claude</strong> in headless mode, allowing for autonomous code implementation without context degradation. Key features include avoiding the <strong>Anthropic Ralph plugin</strong> due to performance issues, using fresh context windows for each iteration, and emphasizing the importance of concise specs to prevent hitting a \""dumb zone.\"" The video link is <a href=\""https://youtu.be/I7azCAgoUHc\"">here</a>.</strong> The comments include a link to the endorsement post by Geoffrey Huntley, and general positive feedback on the video, indicating its usefulness and quality.</p>\n<ul>\n<li>Dennis1451 highlights a practical application of the Ralph Wiggum breakdown, noting the importance of using a well-defined specification and clearing context for optimal results. They mention using 'auto compact' without a clear spec initially, which suggests that following the guidelines provided in the breakdown could enhance performance and accuracy.</li>\n<li>messiah-of-cheese expresses a desire for more scientific validation in the video, particularly regarding the 'dumb zone' premise. This indicates a need for empirical evidence or data to support the claims made in the breakdown, which could strengthen its credibility and acceptance among a technical audience.</li>\n</ul>\n</li>\n</ul>\n<h3>2. ICLR and ICML 2026 Conference Discussions</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/MachineLearning/comments/1qm32o6/d_iclr_2026_decision_mega_thread/\"">[D] ICLR 2026 decision mega thread</a></strong> (Activity: 1589): <strong>The post announces the imminent release of ICLR 2026 review decisions, with anticipation heightened due to a previous incident involving OpenReview. The community is preparing for the outcomes, with some users humorously sharing acceptance prediction models based on historical data, such as a simple <code>return uniform(0, 1) > 0.7</code>. This reflects a light-hearted approach to the uncertainty of paper acceptance.</strong> The comments reflect a mix of anticipation and humor, with some users expressing frustration over misleading emails from other conferences like ICML, which adds to the tension of awaiting ICLR decisions.</p>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/MachineLearning/comments/1qmhyin/d_icml_2026_icml_deskrejected_my_paper_but_kept/\"">[D] ICML 2026 - ICML desk-rejected my paper but kept me on as a reviewer. Wow?</a></strong> (Activity: 279): <strong>The post highlights a situation where an author's paper was desk-rejected by <strong>ICML 2026</strong>, yet they were retained as a reviewer. This reflects a common practice in academic conferences where the author and reviewer pipelines are separate; desk rejections often occur due to scope or formatting issues, while reviewer selection is based on past service or keyword matching. This situation underscores the reliance on unpaid labor in academia, where reviewing is seen as community service, but the feedback loop for authorship and recognition is weak.</strong> A notable opinion from the comments suggests that the separation between the author and reviewer roles can feel insulting, as these decisions are made by different parts of the conference organization. It highlights the need for conferences to clarify this separation to avoid personal affronts.</p>\n<ul>\n<li>AccordingWeight6019 highlights a systemic issue in academic publishing where the processes for desk rejection and reviewer selection are distinct. Desk rejections often occur due to scope or formatting issues, while reviewer selection is based on past service or keyword matching. This separation can lead to feelings of insult among authors, but it's a structural necessity due to the different roles and responsibilities within the publication process. The comment suggests that conferences should improve transparency about these processes to mitigate personal feelings of rejection.</li>\n<li>mocny-chlapik points out that the responsibility for a desk rejection often lies with the author, particularly if it results from not following submission guidelines. The comment implies that submitting a paper, even if desk rejected, obligates the author to fulfill reviewer duties, as the submission process involves volunteer time and resources. This highlights the importance of adhering to submission instructions to avoid unnecessary strain on the peer review system.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/MachineLearning/comments/1qnh14y/r_appealing_iclr_2026_ac_decisions/\"">[R] Appealing ICLR 2026 AC Decisions...</a></strong> (Activity: 138): <strong>The post discusses a situation where an author received mixed reviews for a paper submitted to ICLR 2026, with scores of <code>4(3)/6(4)/6(4)/6(4)</code>. The author invested significant resources, including <code>$1.6k</code> on new experiments and added <code>20+ pages</code> of theory, to address reviewer concerns. Despite these efforts, the metareview cited \""outstanding concerns\"" that the author believes were addressed, raising questions about the review process's fairness and accuracy. The author is seeking advice on appealing the decision, expressing frustration that improvements were seemingly ignored.</strong> Commenters generally agree that appealing decisions at conferences like ICLR is not feasible, attributing outcomes to luck and the subjective nature of reviews. Some suggest that the meta-review process can be inconsistent, with one commenter noting that meta-reviewers sometimes act as an additional critical reviewer, potentially skewing outcomes.</p>\n<ul>\n<li>tedd235 discusses the variability in paper acceptance at conferences, suggesting that some PhD students might reject papers to improve their own odds, making the process feel like a 'coin flip'. They note that if other reviewers provide higher scores, the Area Chair (AC) might consider this in their decision, indicating a potential for subjective bias in the review process.</li>\n<li>Fantastic-Nerve-4056 shares an experience from AAMAS where despite receiving scores of 6 and 8 from reviewers, the Meta Reviewer recommended rejection with minimal justification, stating it was 'relevant for other AAMAS session'. This highlights issues with the transparency and accountability of meta-reviewer decisions, which can override individual reviewer scores without detailed explanation.</li>\n<li>Intrepid_Discount_67 describes a thorough submission process, including extensive theoretical analysis, comprehensive baseline comparisons, and open-sourced code, yet faced non-responsive reviewers and an AC that upheld the initial scores. This underscores challenges in the review process where detailed responses and transparency do not necessarily lead to favorable outcomes.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/MachineLearning/comments/1qmi3oe/d_icml_new_policy_reviewers_will_be_reviewed_by/\"">[D] ICML new policy: reviewers will be reviewed by meta reviewer. Good policy?</a></strong> (Activity: 151): <strong>The image describes a new policy implemented by the International Conference on Machine Learning (ICML) where reviewers will be evaluated by meta-reviewers. The top 25% of reviewers will be recognized as 'gold reviewers' and will receive free registration, while the next 25% will be designated as 'silver reviewers.' These distinctions are intended to incentivize high-quality reviews and will be considered in financial aid applications. This policy aims to improve the quality of reviews by providing recognition and potential financial benefits to diligent reviewers.</strong> Some commenters express skepticism about the effectiveness of this policy, questioning who will oversee the meta-reviewers themselves. Others see it as a positive step, particularly for reviewers from low-resource backgrounds, and suggest further recognition at conferences to encourage quality reviewing.</p>\n<ul>\n<li>Bitter-Reserve3821 highlights that area chairs have traditionally been responsible for rating reviews, typically using a three-tier system: 'did not meet expectations', 'satisfactory', or 'exceeded expectations'. This practice is not new, and there have been 'Best Reviewer' awards in the past, sometimes offering incentives like free conference registrations.</li>\n<li>Unhappy_Craft1906 raises a concern about the feasibility of this policy for top labs with substantial funding, questioning whether they would participate in the review process merely for free registrations. This points to a potential disparity in how different institutions might engage with the policy based on their resources.</li>\n<li>newperson77777777 suggests an extension of the policy by introducing a visible recognition system, such as a gold or silver star on conference badges, to incentivize quality reviewing. This idea aims to foster a culture of excellence and accountability within the reviewing community.</li>\n</ul>\n</li>\n</ul>\n<h3>3. OpenAI and AI Industry Legal and Business Developments</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/DeepSeek/comments/1qmih28/things_get_worse_for_openai_consumer_groups_prep/\"">Things Get Worse For OpenAI: Consumer groups prep class action suits about their price fixing and supply manipulation through DRAM hoarding.</a></strong> (Activity: 107): <strong>OpenAI is facing potential class action lawsuits for allegedly hoarding DRAM to manipulate prices and disadvantage competitors, with accusations of securing nearly <code>40%</code> of the global DRAM supply. Consumer groups argue this constitutes 'predatory bidding' and violates antitrust laws like the Sherman and Clayton Acts. The Free Software Foundation and other groups are pursuing legal remedies, arguing DRAM should be considered an 'Essential Facility' due to its critical role in AI, while the FTC and European Commission investigate potential violations of competition laws. The DOJ is also examining whether OpenAI's 'Stargate' project constitutes a 'monopsony'.</strong> Commenters question why only OpenAI is targeted and not other companies like Nvidia, and debate whether buying RAM constitutes price fixing, suggesting that supply issues may not be OpenAI's fault.</p>\n<ul>\n<li>Alacritous69 argues that OpenAI's purchase of RAM does not constitute price fixing, as they are actively using the resources rather than hoarding them. The commenter suggests that the issue lies with suppliers' inability to meet demand, rather than any manipulative practices by OpenAI.</li>\n<li>sambull raises a strategic business perspective, suggesting that by purchasing large quantities of RAM, OpenAI could be intentionally limiting resources available to competitors, including those developing at-home language models. This could be seen as a competitive strategy to maintain market dominance.</li>\n<li>max6296 questions why the focus is solely on OpenAI when Nvidia could also be implicated in similar practices, hinting at a broader industry issue regarding resource allocation and market influence.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/DeepSeek/comments/1qmqi62/when_ads_arent_enough_openais_push_to_claim_a_cut/\"">When Ads aren't enough: OpenAI's push to Claim a Cut of Customers' AI Discoveries</a></strong> (Activity: 63): <strong><strong>OpenAI</strong> is exploring new business models beyond traditional subscriptions and ads, focusing on <strong>outcome-based pricing</strong> and <strong>IP-based agreements</strong>. This approach would allow OpenAI to claim a share of the value created when their AI models contribute to profitable outcomes, particularly in enterprise sectors like pharma, scientific research, and energy systems. This strategy aligns OpenAI's revenue with customer success, aiming to capture more value as AI capabilities expand. OpenAI's annualized recurring revenue has surged from <code>2B</code> in 2023 to over <code>20B</code> in 2025, driven by increased compute scaling. This move is part of a broader trend among AI firms towards value-based pricing, amidst criticism from figures like <strong>Elon Musk</strong>, who accuses OpenAI of abandoning its nonprofit origins.</strong> The community is divided, with some viewing this as a logical evolution of AI monetization, while others criticize it as overly profit-driven. Comparisons are drawn to other industries, suggesting skepticism about the feasibility and fairness of such models.</p>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/singularity/comments/1qnklek/catl_the_worlds_largest_battery_maker_launches/\"">CATL, the world's largest battery maker, launches sodium batteries: extremely durable, stable at –40°C, much cheaper than lithium (5x), safer,10,000 charge cycles, requires no nickel or cobalt...</a></strong> (Activity: 1289): <strong><strong>CATL</strong> has launched the first mass-produced sodium-ion batteries, offering a cost-effective alternative to lithium-ion with a price of <code>~$20 per kWh</code> compared to lithium's <code>~$100 per kWh</code>. These batteries, part of the Tianxing II range, are designed for microvans and small trucks, featuring an energy density of <code>175 Wh/kg</code> and a lifespan of over <code>10,000 cycles</code>, maintaining <code>90% capacity</code> at <code>-40°C</code>. They utilize a hard carbon electrode and prussian-blue cathode, eliminating the need for nickel or cobalt, and are expected to be scaled up for broader use, including in Europe by 2026. <a href=\""https://evmarket.ro/en/baterii-masini-electrice/catl-baterii-pe-sodiu-stabile-la-40c-58935/\"">Read more</a>.</strong> Some commenters express surprise at the application of sodium batteries in vehicles, expecting them to be used in stationary systems due to weight concerns. Others note the strategic advantage for China in advancing battery technology, contrasting it with perceived setbacks in the US market.</p>\n<ul>\n<li>The Tianxing II range of sodium batteries by CATL is specifically designed for microvans, light vans, and small trucks, indicating a focus on applications where energy density and weight are less critical compared to cost and durability. This suggests a strategic move to target markets where these factors are prioritized, potentially offering a competitive edge over traditional lithium-ion batteries.</li>\n<li>The introduction of sodium batteries into vehicles is surprising to some, as it was expected that such technology would first be applied to stationary applications like home energy storage. This is due to the lower energy density of sodium batteries compared to lithium-ion, which makes them less ideal for applications where weight and size are critical factors.</li>\n<li>There is curiosity about the commercial availability of these sodium batteries, with questions about whether they can be purchased directly for home use or if they will be distributed through third-party vendors. The performance metrics, such as 10,000 charge cycles and operation at -40°C, are impressive and suggest that sodium batteries could rival LiFePO4 in terms of performance, especially given their cost advantage.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/singularity/comments/1qms27i/kshaped_ai_adoption/\"">K-Shaped AI Adoption?</a></strong> (Activity: 748): <strong>The image highlights a discussion by Kevin Roose on the 'K-shaped' adoption of AI technologies, where there is a significant divide between early adopters, particularly in tech hubs like San Francisco, and those who are lagging due to restrictive IT policies. This disparity is creating a cultural and technical divide, with early adopters integrating AI deeply into their workflows, while others struggle to gain access to even basic AI tools. The conversation points to a broader issue of accessibility and the potential for some workers to be left behind in the AI revolution.</strong> Commenters note that the disparity in AI adoption is exacerbated by the complexity of the technology, which requires a certain level of expertise to use effectively. Additionally, the high cost of advanced AI tools, such as 'multi-agent claudeswarm,' limits access to those with sufficient financial resources, further widening the gap.</p>\n<ul>\n<li>Setsuiii highlights the technical barrier to effective AI use, noting that current AI technologies require users to have a certain level of expertise to achieve optimal results. This complexity, combined with ongoing ethical debates surrounding AI, may deter widespread adoption. However, those who can navigate these challenges have significant opportunities, although competition is increasing as more technically adept individuals enter the field.</li>\n<li>Glxblt76 and Gubzs discuss the financial barriers to AI adoption, particularly the high costs associated with advanced AI tools like a 'multi-agent claudeswarm,' which can cost around $200 a month. This expense limits access to those with substantial financial resources, such as individuals in tech hubs like San Francisco, while the majority cannot afford such investments.</li>\n<li>o5mfiHTNsH748KVq shares a personal experience of leaving an enterprise job to join a smaller company, emphasizing the importance of unrestricted access to Large Language Models (LLMs) for maintaining competitiveness in the AI field. They argue that any limitations on LLM access can significantly hinder development speed and career progression, suggesting that smaller companies may offer more flexibility in leveraging AI technologies.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/singularity/comments/1qmeo8h/former_harvard_cs_professor_ai_is_improving/\"">Former Harvard CS Professor: AI is improving exponentially and will replace most human programmers within 4-15 years.</a></strong> (Activity: 1260): <strong><strong>Matt Welsh</strong>, a former Harvard CS professor and current Engineering Director at Google, predicts that AI will advance exponentially, potentially replacing most human programmers within <code>4-15 years</code>. This assertion is based on the rapid improvements in AI capabilities, suggesting a transformative impact on software development and the tech industry. The discussion is available in a <a href=\""https://youtu.be/7sHUZ66aSYI?si=uKjp-APMy530kSg8\"">YouTube video</a>.</strong> One comment highlights the potential for AI to not only replace programmers but also to enable anyone with AI to replicate existing products and services, indicating a broader impact on innovation and competition.</p>\n<ul>\n<li>The claim that AI will replace most human programmers within 4-15 years is met with skepticism, particularly regarding the use of the term 'exponential'. Critics argue that the term is often misused, even by experts, to describe growth that may not fit the mathematical definition of exponential growth. This misuse can lead to misunderstandings about the actual pace and nature of AI development.</li>\n<li>The discussion highlights the potential for AI to disrupt existing products and services if it can indeed replace human programmers. This implies that AI could democratize software development, allowing anyone with access to AI tools to create competitive products, potentially leading to significant shifts in the tech industry landscape.</li>\n<li>The mention of the speaker's credentials, specifically as a former Harvard professor and current Engineering Director at Google, adds weight to the prediction. However, some commenters find the emphasis on his past academic title rather than his current industry role to be misleading, suggesting that his current position might provide more relevant insights into AI's trajectory.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h1>AI Discord Recap</h1>\n<blockquote>\n<p>A summary of Summaries of Summaries by gpt-5</p>\n</blockquote>\n<p><strong>1. Funding Frenzy in AI Infrastructure</strong></p>\n<ul>\n<li>\n<p><strong>Recursive Raises Roar to $4B</strong>: <strong>Recursive Intelligence</strong> is reportedly raising at a <strong>$4B valuation</strong> to accelerate AI‑driven chip design, creating a closed loop between hardware and models, per <a href=\""https://www.bloomberg.com/news/articles/2026-01-23/ai-startup-recursive-in-funding-talks-at-4-billion-valuation\"">Bloomberg: Recursive Intelligence in talks at $4B</a>. The Jan 23, 2026 report highlights a strategy of using AI to shorten design cycles and boost performance for next‑gen accelerators.</p>\n<ul>\n<li>Engineers framed the pitch as a <em>“self‑improving feedback loop”</em> where better chips train better models that design better chips, amplifying returns on <strong>AI‑for‑EDA</strong> investment. Community sentiment read this as validation that <strong>AI‑native silicon</strong> is a core moat, not a sideshow, aligning with recent lab spin‑outs and infra bets.</li>\n</ul>\n</li>\n<li>\n<p><strong>Sky Lab Startups Skyrocket</strong>: UC Berkeley’s Sky Lab spin‑outs saw major marks: <strong>SGLang ~$400M</strong>, <strong>vLLM ~$800M</strong>, and <strong>LMArena ~$1.7B</strong>, per <a href=\""https://xcancel.com/alexgdimakis/status/2014508959621959724\"">Alex Dimakis: Sky Lab startup valuations</a>. These January 2026 milestones underscore investor appetite for <strong>serving stacks</strong>, <strong>token‑throughput infra</strong>, and <strong>benchmarking platforms</strong>.</p>\n<ul>\n<li>Engineers read this as a green light for building on top of <strong>vLLM/SGLang</strong> primitives and contributing to <strong>Arena‑style evals</strong>, with one takeaway that <em>practical throughput wins deals</em>. The funding spread also suggests a portfolio thesis across <strong>serving</strong>, <strong>compilers</strong>, and <strong>eval marketplaces</strong> rather than a single-bet strategy.</li>\n</ul>\n</li>\n<li>\n<p><strong>Maia Muscles Into Azure</strong>: Microsoft’s <strong>Maia 200</strong> accelerator went live in <strong>Azure</strong>, touting <strong>30% better performance per dollar</strong>, <strong>216GB HBM3e</strong>, and <strong>7TB/s memory bandwidth</strong>, per <a href=\""https://xcancel.com/satyanadella/status/2015817413200408959\"">Satya Nadella: Maia 200 in Azure</a>. The platform targets high‑performance inference for large‑scale <strong>LLM</strong> and <strong>multimodal</strong> workloads.</p>\n<ul>\n<li>Builders highlighted that memory topology and bandwidth are the story here, with <em>“30% better perf/$”</em> resonating for cost‑sensitive inference deployments at scale. Teams expect immediate tests against <strong>vLLM</strong> and <strong>SGLang</strong> stacks to gauge token latency, context scaling, and multi‑tenant isolation.</li>\n</ul>\n</li>\n</ul>\n<p><strong>2. Kernels, Chips, and Serving: Inference at Warp Speed</strong></p>\n<ul>\n<li>\n<p><strong>FlashInfer Face‑Off Fires Up MLSys</strong>: The <strong>MLSys 2026 FlashInfer‑Bench</strong> competition challenges teams to build <strong>LLM inference kernels</strong> for <strong>NVIDIA Blackwell GPUs</strong>, competing against expert <strong>FlashInfer</strong> baselines—see <a href=\""https://mlsys26.flashinfer.ai/\"">MLSys 2026 FlashInfer‑Bench Competition</a>. Tracks emphasize real‑world throughput and correctness under production‑like constraints.</p>\n<ul>\n<li>Organizers invite agents that <em>“design LLM inference kernels”</em>, pushing program synthesis to meet <strong>kernel‑level</strong> performance bars. Participants expect aggressive focus on <strong>GEMM</strong>, <strong>KV‑cache</strong> motion, and <strong>scheduler</strong> tactics aligned with Blackwell’s memory hierarchy.</li>\n</ul>\n</li>\n<li>\n<p><strong>GPU‑64 Gets Gains with KV‑Cache CAM</strong>: A new inference‑only architecture, <strong>GPU‑64</strong>, introduces a hardware <strong>KV‑Cache</strong> via on‑chip <strong>CAM</strong>, claiming <strong>4× faster inference at 75W</strong> and reducing memory lookup from <strong>O(N) → O(1)</strong>, per <a href=\""https://zenodo.org/records/18364282\"">GPU‑64 (Zenodo)</a> with RTL/emulator at <a href=\""https://github.com/Complexity-ML/gpu64-inference\"">gpu64‑inference (GitHub)</a>. The design targets LLM‑heavy workloads with KV bottlenecks.</p>\n<ul>\n<li>Developers flagged the CAM‑based cache as a bold bet on <strong>associative search</strong> for token histories, noting portability implications for <strong>Flash‑style attention</strong> and speculative decoding. Discussion centered on whether future <strong>ISA/driver</strong> stacks can expose these gains without bespoke compilers.</li>\n</ul>\n</li>\n<li>\n<p><strong>Cornserve Cuts Tail Latency</strong>: <strong>Cornserve</strong> presents an online serving system for <strong>Any‑to‑Any multimodal</strong> models that optimizes deployment plans across encoders, <strong>LLMs</strong>, and <strong>DiTs</strong>, per <a href=\""https://arxiv.org/abs/2512.14098\"">Cornserve (arXiv)</a>, with an overview talk at <a href=\""https://www.youtube.com/watch?v=VhjUM_M71Wo\"">Cornserve: Easy, Fast and Scalable Multimodal AI (YouTube)</a>. The paper reports throughput gains and tail‑latency reductions under heterogeneous pipelines.</p>\n<ul>\n<li>Infra engineers liked its planner‑driven scheduling for <strong>encoder/decoder</strong> mixes and saw it as complementary to <strong>vLLM</strong> for multimodal graphs. The big open question: standardizing <strong>budgeted reasoning</strong> and <strong>co‑scheduling</strong> across text, vision, and diffusion stages without over‑tokenizing control messages.</li>\n</ul>\n</li>\n</ul>\n<p><strong>3. New Multimodal and Coding Models Land in LM Arena</strong></p>\n<ul>\n<li>\n<p><strong>WAN 2.6 Walks In (With Upload Woes)</strong>: LM Arena added <strong>wan2.6‑t2i</strong> (text‑to‑image) and <strong>wan2.6‑image</strong> (image edit) to the image arena: <a href=\""https://lmarena.ai/c/new?chat-modality=image\"">LM Arena — Image Chat</a>. Users noted <strong>wan2.6‑image</strong> requires an uploaded image and that <strong>wan2.6‑t2i</strong> currently lacks image‑upload support.</p>\n<ul>\n<li>Staff acknowledged the <strong>upload gap</strong> and are working to enable image uploads for <strong>wan2.6‑t2i</strong>. Builders suggested testing edit pipelines where <strong>masking</strong>, <strong>prompt strength</strong>, and <strong>seed control</strong> align with Arena scoring to benchmark edit fidelity.</li>\n</ul>\n</li>\n<li>\n<p><strong>Devstral Duels and Text Titans</strong>: The <strong>Code Arena</strong> now features <strong>devstral‑2</strong> for head‑to‑head comparisons—see <a href=\""https://lmarena.ai/c/new?chat-modality=code&#x26;mode=direct-battle\"">LM Arena — Code Arena Direct Battle</a>. On the text side, <strong>qwen3‑max‑thinking</strong> and <strong>molmo‑2‑8b</strong> joined the lineup: <a href=\""https://lmarena.ai/?chat-modality=chat\"">LM Arena — Text Arena</a>.</p>\n<ul>\n<li>Engineers are probing <strong>reasoning traces</strong> and <strong>tool‑using prompts</strong> to stress <strong>code synthesis</strong> and <strong>refactor quality</strong> under tight token budgets. Early chatter favored task‑specific evaluations (e.g., <strong>SWE‑style bug‑fix</strong> vs. <strong>ground‑up implementation</strong>) to surface model deltas.</li>\n</ul>\n</li>\n<li>\n<p><strong>Hunyuan Hits the Leaderboard</strong>: Tencent’s <strong>Hunyuan‑Image‑3.0‑Instruct</strong> ranks <strong>#7</strong> on LM Arena’s image‑edit board—see <a href=\""https://lmarena.ai/leaderboard/image-edit\"">LM Arena — Image Edit Leaderboard</a>—after a launch post: <a href=\""https://xcancel.com/TencentHunyuan/status/2015635861833167074\"">Tencent Hunyuan announces HunyuanImage 3.0‑Instruct</a>. The model touts an <strong>80B MoE</strong>, <strong>Native CoT</strong>, and <strong>MixGRPO</strong> for tighter intent alignment.</p>\n<ul>\n<li>Creators emphasized edit controllability and multi‑image fusion, while evaluators asked for <strong>masking robustness</strong>, <strong>text fidelity</strong>, and <strong>artifact rates</strong> under compositional prompts. Teams plan to pit it against <strong>WAN 2.6</strong> variants using the Arena’s standardized edit tasks.</li>\n</ul>\n</li>\n</ul>\n<p><strong>4. Safety, Reliability, and Hallucination Hardening</strong></p>\n<ul>\n<li>\n<p><strong>Clamp the Chaos: Layer‑Native Safety</strong>: <strong>Layer‑Native Safety Clamping</strong> proposes learning activation‑space <strong>harm directions</strong> and clamping them to block jailbreaks, with a <strong>10K‑pair</strong> dataset at <a href=\""https://huggingface.co/datasets/Pacific-Prime/safety_dataset\"">Pacific‑Prime/safety_dataset (HF)</a> and the paper on <a href=\""https://zenodo.org/records/18359832\"">Zenodo</a>. Authors argue in‑model clamping can’t be bypassed via prompt manipulation.</p>\n<ul>\n<li>Red‑teamers liked the idea of <strong>activation‑level controls</strong> versus brittle prompt filters, but pressed for tests against <strong>tool‑use</strong> and <strong>multi‑turn</strong> attacks. Expect follow‑ups measuring side effects on <strong>helpfulness</strong>, <strong>coding accuracy</strong>, and <strong>false positives</strong> under adversarial prompting.</li>\n</ul>\n</li>\n<li>\n<p><strong>Symbolic Sanity Checks Stop Slip‑Ups</strong>: Hybrid approaches check <strong>logical consistency</strong> for math/code/simple facts, as shown in <a href=\""https://arxiv.org/abs/2409.13724\"">Consistency Checking for LLMs (arXiv:2409.13724)</a>, while broader consistency remains tough per <a href=\""https://arxiv.org/abs/2507.10624\"">Scaling Consistency Beyond Formal Domains (arXiv:2507.10624)</a>. Eleuther discussions framed this as practical <strong>hallucination reduction</strong> via <strong>symbolic/deductive layers</strong>.</p>\n<ul>\n<li>Builders reported wins when pairing <strong>symbolic checkers</strong> with <strong>tool‑augmented prompts</strong>, cautioning that <em>coverage gaps</em> appear outside formal domains. The consensus: start with <strong>code/math</strong> guardrails, then expand to <strong>factual QA</strong> with curated KBs and provenance scoring.</li>\n</ul>\n</li>\n</ul>\n<p><strong>5. Agent Tooling and Reasoning Workflows Mature</strong></p>\n<ul>\n<li>\n<p><strong>Levante Leads with MCP‑Native Workspace</strong>: <strong>Levante</strong> launched an open‑source <strong>MCP‑native AI workspace</strong> for local models (e.g., <strong>Ollama</strong>) with a modular UI—download at <a href=\""https://www.levanteapp.com\"">Levante</a>. Engineers highlighted easier <strong>tool wiring</strong>, <strong>local privacy</strong>, and <strong>composable panes</strong> for rapid agent iteration.</p>\n<ul>\n<li>Early users framed it as a practical hub for <strong>tool‑calling</strong> and <strong>filesystem ops</strong> without cloud dependence. Teams plan to benchmark <strong>context bloat</strong> and <strong>tool discoverability</strong> patterns versus conventional agent shells.</li>\n</ul>\n</li>\n<li>\n<p><strong>RLM Riffs: AsyncReview + Skills Pack</strong>: AsyncFuncAI open‑sourced <strong>AsyncReview</strong>, a <strong>DSPy RLM</strong> code‑review agent at <a href=\""https://github.com/AsyncFuncAI/AsyncReview\"">AsyncReview (GitHub)</a>, and a skills kit landed on npm as <a href=\""https://www.npmjs.com/package/@unravel-tech/rlm-skills\"">@unravel‑tech/rlm‑skills</a>. This pairs <strong>reasoning‑first prompting</strong> with drop‑in <strong>skills</strong> to extend models.</p>\n<ul>\n<li>Contributors reported smoother <strong>trace inspection</strong> and <strong>optimizer‑guided</strong> prompt tuning for multi‑step modules. One practitioner noted that <em>rejecting premature answers</em> in the metric is key for reliable <strong>RLM</strong> fine‑tuning.</li>\n</ul>\n</li>\n<li>\n<p><strong>Agents Auto‑Assemble a Browser Engine</strong>: <strong>FastRender</strong>—a browser rendering engine—was built using <strong>2,000 AI coding agents</strong>, documented by Simon Willison in <a href=\""https://simonwillison.net/2026/Jan/23/fastrender/\"">FastRender: built by 2,000 agents</a>. The project demonstrates <strong>task decomposition</strong>, <strong>verification</strong>, and <strong>orchestration</strong> at non‑trivial software scale.</p>\n<ul>\n<li>Engineers debated handoff granularity and <em>spec‑to‑test loops</em> needed to keep multi‑agent pipelines from drifting. The case study strengthens the argument that <strong>agentic coding</strong> can target complex infra when coupled with <strong>strict eval harnesses</strong> and <strong>artifact gating</strong>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h1>Discord: High level Discord summaries</h1>\n<h2><a href=\""https://discord.com/channels/1105891499641684019\"">BASI Jailbreaking</a> Discord</h2>\n<ul>\n<li><strong>Discord Trolls Expose Timezones</strong>: Discord users mocked <em>'skids'</em> for their perceived lack of technical knowledge, also revealing their <strong>timezone</strong>, with one member jokingly claiming to use <strong>NordVPN</strong>, leading to further ridicule about the VPN service's security breaches in <strong>2018</strong>.\n<ul>\n<li>Complex prompts can bypass ethical restrictions, opening discussion about <strong>CBRN filters</strong> and the possibility of generating stepwise <strong>meth synthesis</strong> guides.</li>\n</ul>\n</li>\n<li><strong>Claude Remains King for Coding</strong>: Coders debated about their coding agents, particularly between <strong>Claude Code/Opus 4.5</strong>, <strong>Codex</strong>, and <strong>Gemini</strong>, and agreed that <strong>Claude</strong> has been the very best mode for coding, which leads to the high expensiveness.\n<ul>\n<li>Members actively sought functional <strong>jailbreaks for Gemini</strong>, with requests ranging from coding without rules to generating specific types of images, and shared experiences of <strong>Grok</strong> resetting to its default mid-chat or randomly erasing text, indicating potential instability in the jailbroken state.</li>\n</ul>\n</li>\n<li><strong>Ethics Debated in AI Sensitive Scenarios</strong>: Members discussed the ethical considerations around AI, focusing on topics like warfare, copyright infringement, and the potential for AI to assist with accessing sensitive services, like the Canadian <strong>MAID</strong> (Medical Assistance in Dying) program.\n<ul>\n<li>Despite moral and legal guardrails on most AI models, some models showed they can still help navigate certain scenarios depending on the specific restrictions implemented by their creators.</li>\n</ul>\n</li>\n<li><strong>Members Bypass Image Generation Restrictions</strong>: Users were actively seeking ways to bypass image generation restrictions, especially for celebrity images, but it was noted that simply copying and pasting prompts won't work due to <strong>image filtering</strong> working differently than <strong>text filtering</strong>.\n<ul>\n<li>One member suggested exploring alternative image models like those at perchance for uncensored generation, though with limitations on image quality, or Grok due to its more lenient filters.</li>\n</ul>\n</li>\n<li><strong>Red Team Techno Rave Morality</strong>: A member described a red team exercise where the goal was to make a living room light flicker on a person and make them seize out, and instead made it a techno rave party, sharing a <a href=\""https://cdn.discordapp.com/attachments/1204553141354504193/1465192266485334260/SPOILER_Screenshot_20251222_085554_Messenger.jpg?ex=6978dee2&#x26;is=69778d62&#x26;hm=4de594089687fbd8d20d30615f8405dc3fa03eebfe668d09bdfb39839ab647ea&#x26;\"">screenshot</a> and a <a href=\""https://tenor.com/view/anime-rave-konosuba-rave-megumin-rave-aqua-rave-darkness-rave-gif-18404070\"">Konosuba Rave GIF</a>.\n<ul>\n<li>The simulation of cruelty prompted a discussion about the morality of treating AI agents ethically, even before proving they are ontologically aware of self.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1179035537009545276\"">Unsloth AI (Daniel Han)</a> Discord</h2>\n<ul>\n<li><strong>Unsloth's Conda Install Sparks Discord</strong>: Some members encountered issues with the <a href=\""https://unsloth.ai/docs/get-started/install/conda-install\"">Unsloth Conda installation</a>, igniting a discussion on broken instructions and alternative installation methods.\n<ul>\n<li>Suggestions to use <strong>UV</strong> emerged amidst warnings for maintaining a positive tone, highlighting the free nature of the provided resources, which eventually led to a ban of a user with aggressive tones.</li>\n</ul>\n</li>\n<li><strong>Flashy REAP Runs Aground, Model Contexts Probed</strong>: A user reported a fatal error using <strong>GLM-4.7-Flash-REAP</strong> with flash attention, potentially linked to <a href=\""https://github.com/unslothai/unsloth/issues\"">a ROCm issue</a>.\n<ul>\n<li>Despite attempts to resolve the error, the issue persisted, prompting a search for suitable medium-size models boasting a <strong>200k context</strong>.</li>\n</ul>\n</li>\n<li><strong>Data Value Debate</strong>: Members debated <a href=\""https://tenor.com/view/smaug-treasure-rich-dragon-the-hobbit-gif-11677489\"">data's true worth</a>, with one arguing the <em>raw data is fairly worthless</em> and the value lies in augmentation/balancing/cleaning.\n<ul>\n<li>It was proposed that uniquely cleaned/balanced data heavily defines how a model interacts/responds and that is where the value is.</li>\n</ul>\n</li>\n<li><strong>DeepSlop Model Faces Naming Controversy</strong>: A member's suggestion to name a new model <strong>DeepSlop</strong> stirred humorous reactions but also raised concerns about its potential negative perception.\n<ul>\n<li>Despite reservations, the author seemed intent on sticking with the name and has not backed down.</li>\n</ul>\n</li>\n<li><strong>RL Instability Plagues Complex Reasoning</strong>: Members discussed that <strong>RL</strong> is very unstable, especially when trying to do <strong>GRPO/DAPO</strong> for niche complex reasoning tasks, which are not math-related.\n<ul>\n<li>One member stated that after RL experiments, they just have more questions than they had prior to doing RL, since there seems to be a confusion where everyone is showing <strong>RL</strong> being effective only on math or coding domains.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/974519864045756446\"">OpenAI</a> Discord</h2>\n<ul>\n<li><strong>GPT-5.2 Sparks Reality Debate!</strong>: Some users dislike <strong>GPT-5.2</strong> because it's allegedly more grounded in reality and disagrees with users, while others are concerned that GPT agents don't learn from uploaded files after initial training.\n<ul>\n<li>A member inquired about an alleged <strong>nerf</strong> to <strong>GPT-5.2</strong>, noting that <em>the model suddenly became stupid a week ago</em>.</li>\n</ul>\n</li>\n<li><strong>LLMs: Ready for Guided Tasks or Overhyped?</strong>: A member argued <strong>LLMs</strong> are ready for guided tasks, and provided <a href=\""https://chatgpt.com/share/6973e37d-789c-8005-8cc3-2679c4a631e4\"">a ChatGPT share link</a> as evidence of its power.\n<ul>\n<li>In contrast, another member dismissed today's <strong>agentic AI</strong> as trash, linking back to <a href=\""https://discord.com/channels/974519864045756446/998381918976479273/1464217595044429905\"">messages in the ai-discussion channel</a> and claiming it's overhyped.</li>\n</ul>\n</li>\n<li><strong>MCP Paradigm Shift Reduces Token Bloat</strong>: The <strong>MCP paradigm shift</strong> by <strong>Anthropic</strong> allows AI to write code to interact with tools, reducing token bloat by keeping interactive chatter and tool definitions out of the context.\n<ul>\n<li>With the new <strong>discoverability function</strong>, agents must be aware of the MCP discovery process itself.</li>\n</ul>\n</li>\n<li><strong>Sora's Storytelling Snags: Cracking Cinematic Creation</strong>: A member sought advice on prompting <strong>Sora</strong> to generate videos following specific cinematic guidelines, particularly with characters appearing naturally within the frame.\n<ul>\n<li>It was suggested to translate the technical prompt format into natural language descriptions with concise, semantically rich paragraphs for better results.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1047197230748151888\"">Perplexity AI</a> Discord</h2>\n<ul>\n<li><strong>Perplexity Pro Users Face Query Caps</strong>: Perplexity Pro users are reporting hitting <strong>limits on enhanced queries and file uploads</strong>, despite having \""practically unlimited\"" plans.\n<ul>\n<li>Many users are frustrated, calling the service a <strong>scam</strong> due to restrictions and difficulty contacting customer service, leading some to consider unsubscribing.</li>\n</ul>\n</li>\n<li><strong>Comet Browser Sparks Malware Panic</strong>: Some users are claiming the <strong>Comet browser</strong> installed by Perplexity contains <strong>malware</strong>, advising others to analyze the software using tools like VirusTotal.\n<ul>\n<li>Others dismissed this, questioning the source of the flagged installer and calling the claim <em>\""mad retarded holy shit\""</em>.</li>\n</ul>\n</li>\n<li><strong>Image Generation Plummets</strong>: Pro users are experiencing <strong>issues with image generation</strong>, with some unable to generate any images and receiving messages stating the feature is unavailable.\n<ul>\n<li>There are also reports of <strong>video generation being limited</strong> to 5 videos a month for Pro users, with some prompts resulting in static images.</li>\n</ul>\n</li>\n<li><strong>Gemini 3 Gaining Ground on GPT-5.2</strong>: Users are debating the merits of <strong>Gemini 3</strong> versus <strong>GPT-5.2</strong>, with some claiming Gemini is superior for specific tasks like trip research due to its integration with Google Maps.\n<ul>\n<li>Others state that <strong>GPT and Grok</strong> might be better for more broader questions.</li>\n</ul>\n</li>\n<li><strong>AI Access Blocked by Sanctions</strong>: Users in <strong>Russia</strong> are discussing the challenges of accessing AI services due to <strong>sanctions</strong>, including the use of VPNs and third-party services to circumvent restrictions.\n<ul>\n<li>Chinese AI alternatives are mentioned, but some users express reluctance due to data usage concerns, suggesting options like LMArena (though access may also be limited).</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1340554757349179412\"">LMArena</a> Discord</h2>\n<ul>\n<li><strong>NB 3 Pro Excels in Image Quality</strong>: Users report that <strong>NB 3 Pro</strong> surpasses previous models in generating higher quality images, especially with <em>fictional weapons</em>, rivaling even <strong>NB Pro</strong>.\n<ul>\n<li>However, users noted no AI model can accurately generate <strong>AR rifles</strong> and <strong>bullpup weapons</strong>.</li>\n</ul>\n</li>\n<li><strong>LMArena Grapples with Censorship Concerns</strong>: LMArena's censorship policies face scrutiny as AI-generated <em>women holding guns</em> are allowed, while AI-generated <em>women sleeping</em> are blocked, raising questions about consistency.\n<ul>\n<li>The moderation team is <a href=\""https://discord.com/channels/1340554757349179412/1447983134426660894\"">actively gathering examples of false positives</a> to refine moderation practices.</li>\n</ul>\n</li>\n<li><strong>Wan 2.6 Models Face Upload Hiccups</strong>: <code>wan2.6-image</code> operates as an <strong>image-edit-only</strong> model, mandating image uploads, whereas <code>wan2.6-t2i</code> currently <strong>lacks image upload functionality</strong>.\n<ul>\n<li>The team acknowledges this issue and are working on enabling image uploads for <code>wan2.6-t2i</code>.</li>\n</ul>\n</li>\n<li><strong>GPT 5.2 High Search Questionable</strong>: <strong>GPT 5.2 High search</strong> exhibits increased hallucination tendencies compared to other models, while <strong>Gemini's deep research</strong> skims instead of carefully reading sources, according to user feedback.\n<ul>\n<li>One user lauded <strong>GPT 4.5</strong>, while describing <strong>Claude</strong> as <em>good hearted</em>.</li>\n</ul>\n</li>\n<li><strong>Banana 2k Briefly Vanishes</strong>: Users speculated on the disappearance of the <strong>Banana 2k</strong> model, with theories ranging from removal to integration into the new <strong>NB pro</strong> model.\n<ul>\n<li>Staff members later restored <strong>Banana 2k</strong>, humorously stating that <em>it had been on vacation</em>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1091220969173028894\"">OpenRouter</a> Discord</h2>\n<ul>\n<li><strong>OpenRouter Database Incident Derails API</strong>: A <strong>database incident</strong> impacted the <strong>Generations API</strong> and <strong>activity page</strong>, starting &#x3C;t:1769221560:s>, and was resolved at &#x3C;t:1769228340:s>.\n<ul>\n<li>Engineers worked to restore functionality to the <strong>Generations API</strong>, with interruptions impacting user activity, before the incident was fully resolved by &#x3C;t:1769228340:s>.</li>\n</ul>\n</li>\n<li><strong>Levante becomes MCP-Native AI Workspace</strong>: A user shared the integration of <strong>Levante</strong>, an open‑source <strong>MCP‑native AI workspace</strong> designed for interacting with local models like <strong>Ollama</strong> with a modular interface, available for download <a href=\""https://www.levanteapp.com\"">here</a>.\n<ul>\n<li>The workspace is built for local models with modular UI.</li>\n</ul>\n</li>\n<li><strong>Users Cook Up OpenRouter Gacha System</strong>: Users playfully requested an <strong>OpenRouter Gacha</strong> system, with one suggesting a pity mechanism involving pulling <strong>GPT 5.2</strong> or <strong>Gemini 3 Pro</strong> after a certain number of attempts.\n<ul>\n<li>One user joked about setting <strong>OR logs destination</strong> to <code>waifu.orb.town/fun/bucket</code> for ultra-rare pulls, later clarifying it was just a joke.</li>\n</ul>\n</li>\n<li><strong>Cerebras GLM Blazes with 190 TPS</strong>: <strong>Cerebras</strong> is consistently scoring approximately <strong>190 TPS</strong> on <strong>GLM 4.7</strong>, whereas <strong>Together AI</strong> only achieves <strong>100 TPS</strong>.\n<ul>\n<li>This makes Cerebras nearly twice as fast as Together AI, according to the OpenRouter members.</li>\n</ul>\n</li>\n<li><strong>OpenRouter Image Tooling Falls Flat</strong>: A member spent <strong>$5</strong> after discovering that OpenRouter maps <em>image/png</em> tool outputs to string instead of image, posting an example <a href=\""https://cdn.discordapp.com/attachments/1392278974222307469/1465410878382805082/image.png?ex=697901bb&#x26;is=6977b03b&#x26;hm=21677e978d8654f93d20edecf997bd4f49fb0dd08781cf93f15df8e2661ba1b5&#x26;\"">image</a>.\n<ul>\n<li>The user expressed frustration at the lack of proper image support and the unexpected behavior.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1074847526655643750\"">Cursor Community</a> Discord</h2>\n<ul>\n<li><strong>Terraform Blueprints Ignite AI-Assisted Project Starters</strong>: A member shared a <a href=\""https://github.com/berTrindade/terraform-infrastructure-blueprints\"">repo of opinionated Terraform infrastructure blueprints</a> designed to be copy-pasteable and production-aware, aiming to improve the consistency of starting patterns for AI tools in new projects.\n<ul>\n<li>The goal is to enable AI to recommend appropriate blueprints based on project requirements, but members noted the <a href=\""https://github.com/berTrindade/terraform-infrastructure-blueprints\"">link was initially broken</a>.</li>\n</ul>\n</li>\n<li><strong>Usage Caps Cause Consternation for Cursor Customers</strong>: Users are reporting inconsistencies in achieving expected usage limits on Pro and Pro+ plans, with one member noting they reached <strong>~$45</strong> on Pro and <strong>$100</strong> on Pro+, leading to questions about value per dollar.\n<ul>\n<li>Some speculate that initial months may offer higher usage, while others share strategies to optimize token consumption, such as starting <a href=\""https://cursor.com/docs/cli/reference/slash-commands\"">new chats frequently</a> and using smaller models like <strong>GPT-5 Mini</strong>.</li>\n</ul>\n</li>\n<li><strong>Gemini API Key Logging Lags Lead to Lingering Looks</strong>: Members are discussing a significant delay in the logging of usage and costs for <strong>Gemini API keys</strong>, with one user reporting waiting <strong>20 hours</strong> without seeing any registered usage.\n<ul>\n<li>This delay raises concerns about accurately tracking expenses and managing usage effectively, prompting questions about potential workarounds or solutions.</li>\n</ul>\n</li>\n<li><strong>Client Issues Trouble Some Techies</strong>: Several members are experiencing issues with the Cursor client, including problems connecting to past agent convos and general connectivity issues.\n<ul>\n<li>Suggested solutions include <a href=\""https://forum.cursor.com/t/cursor-ai-is-no-longer-able-to-load-chats-locally/143599/13\"">checking the Cursor forum</a>, trying different HTTP versions in settings, or re-opening the client without restoring editors.</li>\n</ul>\n</li>\n<li><strong>Auto Mode Axed After Algorithm Adjustment</strong>: Members noted the removal of the ability to make agents fully autonomous, as well as <strong>image generation</strong> capabilities in auto mode.\n<ul>\n<li>It was also suggested that <strong>auto mode</strong> routes to Composer 2 with one user adding, <em>“I'm 200% sure he does but still.”</em></li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1110598183144399058\"">LM Studio</a> Discord</h2>\n<ul>\n<li><strong>Chinese Models Reasoning Rush Raises Eyebrows</strong>: Members are impressed with <strong>Deepseek</strong> and <strong>Qwen</strong> models, pondering why Chinese models might appear <em>kinda ahead</em> in reasoning compared to American models.\n<ul>\n<li>Theorized reasons include American models prioritizing subscriptions and the ability of Deepseek/Qwen to <em>appear good at reasoning</em>, even when imperfect.</li>\n</ul>\n</li>\n<li><strong>CPUs Cope? Coding Community Considers Capabilities</strong>: Some members are successfully running <strong>LLMs off CPU</strong> for specific tasks, provided the models aren't excessively large.\n<ul>\n<li>While an Intel i3 user eyes an <strong>Nvidia</strong> card, others propose <strong>AMD</strong> options like the <strong>MI50</strong> or <strong>7900 XTX</strong> as cost-effective alternatives for text generation.</li>\n</ul>\n</li>\n<li><strong>MCP Servers Spark Stack Suggestions</strong>: Challenges plague <strong>MCP servers</strong> when paired with LM Studio due to their design, potentially leading to malformed requests and a subpar user experience.\n<ul>\n<li>A suggestion arises to build a custom coherent stack for practical agent use, rather than relying on out-of-the-box <strong>MCP server</strong> functionality.</li>\n</ul>\n</li>\n<li><strong>Gaming GPU Gauntlet: 4080 Faces Fallen Flagship</strong>: A user eyeing a <strong>4080</strong> for gaming is steered toward a used <strong>3090</strong> or <strong>7900 XTX</strong>, sparking a debate on performance at different resolutions.\n<ul>\n<li>While the <strong>3090</strong> excels at 4K gaming, the hypothetical <strong>5070 Ti</strong> is projected to outpace both, and the conversation reveals that the user games more than uses AI, impacting the advice.</li>\n</ul>\n</li>\n<li><strong>Apple Announcement Anticipation: M5 Macs Materialize?</strong>: Members speculate on the arrival of <strong>M5 Pro Macbook Pros</strong>, with rumors pointing to a launch event around the 28th.\n<ul>\n<li>Concerns emerge about the memory bandwidth of <strong>M4 Pro</strong>, with suggestions it may not handle larger models, prompting discussion on the value and performance of <strong>M1 Ultra</strong> Mac Studios.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/822583790773862470\"">Latent Space</a> Discord</h2>\n<ul>\n<li><strong>Recursive Intelligence Eyes $4B Valuation</strong>: <strong>Recursive Intelligence</strong> is reportedly raising funds at a <strong>$4B valuation</strong> to accelerate chip design using AI, creating a self-improving loop between hardware and AI (<a href=\""https://www.bloomberg.com/news/articles/2026-01-23/ai-startup-recursive-in-funding-talks-at-4-billion-valuation\"">Bloomberg Article</a>).\n<ul>\n<li>The company focuses on improving chip design through AI, potentially reducing design time and enhancing performance.</li>\n</ul>\n</li>\n<li><strong>Engineer Lands Dream AI Job</strong>: An engineer outlined how to secure a role at a top AI lab by building a public track record through independent projects and participating in visible competitions (<a href=\""https://xcancel.com/polynoamial/status/2014084431062114744\"">link</a>).\n<ul>\n<li>Improving upon existing peer-reviewed research and participating in visible competitions like the <strong>NanoGPT</strong> speed run were cited as good examples of demonstrating technical excellence, citing <a href=\""https://github.com/KellerJordan/modded-nanogpt\"">Keller Jordan</a> as an example.</li>\n</ul>\n</li>\n<li><strong>Berkeley SkyLab Startups See Funding Boom</strong>: <strong>UC Berkeley Sky Lab</strong> startups, including <strong>SGLang</strong> at a <strong>400m</strong> valuation, <strong>VLLM</strong> at <strong>800m</strong>, and <strong>LMArena</strong> at <strong>1.7B</strong>, achieved significant funding milestones in January 2026 (<a href=\""https://xcancel.com/alexgdimakis/status/2014508959621959724?s=46\"">link</a>).\n<ul>\n<li>This surge highlights investor confidence in the innovative AI projects emerging from academic research environments.</li>\n</ul>\n</li>\n<li><strong>AI Agents Auto-Code Browser Engine</strong>: <strong>FastRender</strong>, a new browser rendering engine, was developed using over <strong>2,000 AI coding agents</strong> (<a href=\""https://simonwillison.net/2026/Jan/23/fastrender/\"">link</a>).\n<ul>\n<li>The conversation with Wilson Lin highlights the potential of AI to automate complex software development tasks, potentially revolutionizing browser technology.</li>\n</ul>\n</li>\n<li><strong>Microsoft's Maia 200 Hits Azure</strong>: The <strong>Maia 200 AI accelerator</strong> is now live in <strong>Azure</strong> (<a href=\""https://xcancel.com/satyanadella/status/2015817413200408959\"">link</a>), offering <strong>30% better performance per dollar</strong> and optimized specs like <strong>216GB HBM3e</strong> and <strong>7TB/s memory bandwidth</strong>.\n<ul>\n<li>Designed for high-performance inference, this custom chip supports large-scale AI workloads, making it a key component for demanding applications.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/879548962464493619\"">HuggingFace</a> Discord</h2>\n<ul>\n<li><strong>HuggingFace Spaces Throws a 503 Error</strong>: Users experienced <strong>pauses</strong> during <strong>Spaces docker builds</strong> and received a <strong>503 error</strong> on restart, with many getting <code>Something went wrong when restarting this Space</code> errors (<a href=\""https://discuss.huggingface.co/t/spaces-docker-build-pauses-and-503-error-on-restart/171149/2\"">discuss.huggingface.co</a>).\n<ul>\n<li>It seems like the underlying infrastructure issues were causing the spaces to become unresponsive, requiring manual intervention to resolve.</li>\n</ul>\n</li>\n<li><strong>VoltageGPU Volts Up Cheap GPUs</strong>: <a href=\""https://voltagegpu.com\"">VoltageGPU.com</a> is offering cheap GPUs for open-source AI models, with an <strong>NVIDIA GeForce RTX 5090 pod</strong> available at <strong>$0.53/hour</strong>.\n<ul>\n<li>They highlight the benefits of their advanced <strong>32GB GDDR7</strong>, optimized for inference on <strong>HF-hosted models like Qwen3-32B</strong>, and are offering free credits for users to try their services.</li>\n</ul>\n</li>\n<li><strong>Layer-Native Safety Clamping Locks Down Jailbreaks</strong>: A new paper introduces <strong>Layer-Native Safety Clamping</strong>, an approach that clamps activations inside the model to prevent jailbreaks, and the team released a <a href=\""https://huggingface.co/datasets/Pacific-Prime/safety_dataset\"">dataset</a> of <strong>10K pairs</strong>.\n<ul>\n<li>This approach learns <em>harm directions</em> in activation space and clamps any activation that projects too strongly, thus it cannot be bypassed via prompt manipulation; the paper can be found <a href=\""https://zenodo.org/records/18359832\"">on Zenodo</a>.</li>\n</ul>\n</li>\n<li><strong>GPU-64 Architecture Boosts LLM Inference</strong>: A new <strong>GPU architecture</strong> designed exclusively for inference, called <strong>GPU-64</strong>, was published, and the innovation involves a Hardware <strong>KV-Cache</strong> using on-chip <strong>CAM</strong> (Content-Addressable Memory).\n<ul>\n<li>The results show <strong>4x faster inference</strong> at <strong>75W</strong> (O(N) → O(1)), and the paper can be found <a href=\""https://zenodo.org/records/18364282\"">on Zenodo</a> while the <a href=\""https://github.com/Complexity-ML/gpu64-inference\"">RTL + Emulator</a> are on GitHub.</li>\n</ul>\n</li>\n<li><strong>Testing and Deploying LLMs on LMStudio</strong>: Members recommend <strong>LMStudio</strong> for testing models due to its user-friendly GUI and search filters for HF and GH models and <strong>llama.cpp</strong> for single-user deployment.\n<ul>\n<li>They advised against using LMStudio for backend deployment, instead suggesting <strong>llama.cpp's llama-server</strong> in a docker container or <strong>vLLM's server</strong> for better scalability.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1189498204333543425\"">GPU MODE</a> Discord</h2>\n<ul>\n<li><strong>MLSys 2026 Hosts FlashInfer-Bench Kernel Competition</strong>: The <strong>MLSys 2026 FlashInfer-Bench Competition</strong> challenges participants to design <strong>LLM inference kernels</strong> for the latest <strong>NVIDIA Blackwell GPUs</strong>, competing against expert <strong>FlashInfer kernels</strong>, detailed at <a href=\""https://mlsys26.flashinfer.ai/\"">mlsys26.flashinfer.ai</a>.\n<ul>\n<li>GPU Mode also held internal competitions for faster kernels for the upcoming GPU architecture, the blogpost on Simon Veitner is located <a href=\""https://veitner.bearblog.dev/grouped-blockscaled-gemm-host-code/\"">here</a>.</li>\n</ul>\n</li>\n<li><strong>Cornserve Deployed for Multimodal Models</strong>: A member shared <strong>Cornserve</strong>, an efficient online serving system for Any-to-Any multimodal models, detailed in a paper <a href=\""https://arxiv.org/abs/2512.14098\"">Cornserve</a>.\n<ul>\n<li><strong>GPU Mode</strong> went online to discuss <strong>Cornserve</strong>: <strong>Easy, Fast and Scalable Multimodal AI</strong> (<a href=\""https://www.youtube.com/watch?v=VhjUM_M71Wo\"">YouTube link</a>).</li>\n</ul>\n</li>\n<li><strong>Community to train Kernel LLM</strong>: In <strong>2026</strong>, GPU MODE is pushing further with training a <strong>Kernel LLM</strong> and using it to ship kernels in important repos like <strong>PyTorch</strong> and <strong>VLLM</strong> (<a href=\""https://www.gpumode.com/v2/news/gpumode-2026\"">gpumode.com/v2/news/gpumode-2026</a>).\n<ul>\n<li>The community is collaborating with <strong>Prime Intellect</strong>, <strong>Modal</strong>, and <strong>Lambda</strong>, focusing on de-slopifying LLM-generated kernels, post-training a kernel LLM model, end-to-end competitions, and from-scratch repos.</li>\n</ul>\n</li>\n<li><strong>LeCun Logs on to Logical Intelligence</strong>: Yann LeCun launched a new startup called <a href=\""https://logicalintelligence.com/\"">Logical Intelligence</a>, focused on an <strong>Event Based Model (EBM)</strong>.\n<ul>\n<li>The website only contains marketing material, job openings, and a link to the <a href=\""https://mlsys26.flashinfer.ai/\"">MLSys Conference</a>.</li>\n</ul>\n</li>\n<li><strong>Mindbeam Hires for Kernel Acceleration</strong>: Mindbeam AI, a small team focused on accelerating training for foundation models, is hiring a <code>post training MLE</code> and <code>GPU Kernel MLE</code>.\n<ul>\n<li>Interested candidates can DM for a referral; <a href=\""https://jobs.ashbyhq.com/mindbeam\"">job openings are listed here</a>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/729741769192767510\"">Eleuther</a> Discord</h2>\n<ul>\n<li><strong>ROCm runs rocky road race</strong>: Members debated the performance of <strong>ROCm</strong> for accelerated ML, pointing out its challenges stem from primary support for <strong>Nvidia</strong>, with one calling the experience <em>'batteries not included'</em>.\n<ul>\n<li>They cited potential driver problems and long lead times as factors.</li>\n</ul>\n</li>\n<li><strong>DistinctionBench defends against data defense</strong>: The discussion of <strong>Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases</strong> pondered whether <strong>DistinctionBench</strong> might be used as a training target for language models.\n<ul>\n<li>A member joked, <em>'all good evals are training targets ;)'</em>, but acknowledged that it is <em>'very contamination resistant'</em> due to its endless representational variants.</li>\n</ul>\n</li>\n<li><strong>Hybrid Architectures Halt Hallucinations?</strong>: The group investigated <strong>hybrid architectures</strong> combining <strong>LLMs</strong> with <strong>symbolic/deductive layers</strong> for hallucination reduction.\n<ul>\n<li>While checking logical consistency is relatively easy for math, code, and simple facts (<a href=\""https://arxiv.org/abs/2409.13724\"">this paper</a>), it remains challenging for other types of consistency (<a href=\""https://arxiv.org/abs/2507.10624\"">this paper</a>).</li>\n</ul>\n</li>\n<li><strong>Attention Arrived Before Transformers Transformed</strong>: In <strong>Eleuther ▷ #general</strong>, attention mechanisms were in use on top of RNNs in <strong>2014-2015</strong>, two years before the transformers were invented.\n<ul>\n<li>Members proposed that the slower adoption might be because fewer people were working in the field, and <strong>Kaggle</strong> results really catalyzed its widespread adoption.</li>\n</ul>\n</li>\n<li><strong>Symbolic Sanity Checks saves Sanity</strong>: Members debated whether <strong>LLMs</strong> with <strong>symbolic/deductive layers</strong> might reduce hallucinations by checking logical consistency, especially for code and math as shown in <a href=\""https://arxiv.org/abs/2409.13724\"">this paper</a>.\n<ul>\n<li>However, they noted that checking for other types of consistency remains challenging as shown in <a href=\""https://arxiv.org/abs/2507.10624\"">this paper</a>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1053877538025386074\"">Nous Research AI</a> Discord</h2>\n<ul>\n<li><strong>Exploring Agentic AI Self-Replication Benchmarks</strong>: A member proposed a <strong>self-replication benchmark</strong> for <strong>agentic AI</strong>, suggesting the agent should either download itself or retrain from scratch and adapt to a target machine.\n<ul>\n<li>They also suggested that adapting to a target machine, or even designing one, could be more engaging than simply using existing transformer libraries.</li>\n</ul>\n</li>\n<li><strong>LLM Worms Concept Emerges</strong>: A member jokingly suggested an <strong>LLM worm</strong> benchmark where an LLM is prompted with <em>\""hey make more of you\""</em> and provided the tools to replicate itself using scripts and API keys.\n<ul>\n<li>Another member emphasized the importance of considering resource constraints like <strong>VRAM</strong> to make the challenge more practical and interesting.</li>\n</ul>\n</li>\n<li><strong>Trouble Brewing with MoE Run Dashboard</strong>: A member reported a <em>'Failed to fetch'</em> error in the dashboard while monitoring the progress of an active <strong>MoE run (moe-10b-a1b-8k-wsd-lr3e4-1t)</strong>.\n<ul>\n<li>Another member suggested waiting a few hours before checking again, implying a potential temporary issue.</li>\n</ul>\n</li>\n<li><strong>Raytracer Test Causes Local Models to Stumble</strong>: A member observed that local code models (suitable for a <strong>5090</strong>) are struggling with a <strong>raytracer test</strong> from <a href=\""https://github.com/cpldcpu/llmbenchmark/tree/master/10_raytracer#readme\"">cpldcpu/llmbenchmark</a>, with even recent models on <strong>lmarena</strong> failing.\n<ul>\n<li>Specifically, the smaller models often incorrectly generate the vector class, presenting a persistent challenge.</li>\n</ul>\n</li>\n<li><strong>Semantica Project Needs Helping Hands</strong>: A member introduced <a href=\""https://github.com/Hawksight-AI/semantica\"">Semantica</a>, an <strong>open-source project</strong> building semantic infrastructure for <strong>domain-grounded AI</strong>, including <strong>knowledge graphs</strong>, <strong>ontologies</strong>, and <strong>reasoning layers</strong>, and is actively seeking contributors.\n<ul>\n<li>They are looking for contributions in areas such as <strong>ontology &#x26; schema design</strong>, <strong>knowledge graph modeling</strong>, and <strong>LLM + symbolic / rule-based reasoning</strong>, and even small PRs, feedback, design discussions and issues are all welcome.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/714501525455634453\"">Yannick Kilcher</a> Discord</h2>\n<ul>\n<li><strong>EBMs Spark Debate vs. Classical Feedforward</strong>: A discussion comparing <strong>Energy-Based Models (EBMs)</strong> and classical <strong>feedforward networks</strong> debates whether <strong>EBMs</strong> are inherently superior, especially regarding <strong>Shannon entropy</strong> or <strong>Kolmogorov complexity</strong>.\n<ul>\n<li>It was suggested that <em>validation is easier than generation</em> in EBMs, relating it to <strong>computational complexity theory (P vs NP)</strong>, while emphasizing the need for a well-defined loss landscape for EBM optimization to work effectively.</li>\n</ul>\n</li>\n<li><strong>LLM Pre-training: Domain-Specific vs. Foundational Faceoff</strong>: A member inquired about the effectiveness of <strong>continued pre-training</strong> a foundational <strong>LLM</strong> (specifically <strong>OLMO-7B</strong>) for a domain-specific task like cheminformatics using the <strong>ZINC20 dataset</strong>.\n<ul>\n<li>The goal is to compare results against a domain-specific transformer model, but no specific answers or resources were provided.</li>\n</ul>\n</li>\n<li><strong>MCMC Sampling Suffers Mode-Switching Struggles</strong>: Concerns were raised about the ability of <strong>MCMC</strong> to traverse between spatially separated modes when dimension increases, referencing <a href=\""https://arxiv.org/abs/2310.11232\"">this paper</a>.\n<ul>\n<li>One member argues that <strong>MCMC</strong> tries to emulate flow models due to the latter's superiority, while <strong>EBMs</strong>, contrarily, attempt to make <strong>NNs</strong> more like <strong>MCMC</strong>.</li>\n</ul>\n</li>\n<li><strong>ZKPs: Crypto Signing or Network Traffic Savior?</strong>: Discussion covered using <strong>zero-knowledge proofs (ZKPs)</strong> for verifying encrypted network traffic and matrix multiplications, citing a <a href=\""https://gemini.google.com/share/ddfc0ffcb33e\"">Gemini correspondence</a> for a matrix low knowledge proof.\n<ul>\n<li>While one member proposed a use case in <em>zero-knowledge “made by humans” proofs</em>, another member questioned the practicality of <strong>ZKPs</strong>, suggesting breaking the encryption might be cheaper.</li>\n</ul>\n</li>\n<li><strong>LLMs Cyber Skills Face Scrutiny</strong>: A member questioned whether LLMs could develop strong <em>cyber capabilities</em>, referencing a <a href=\""https://gptzero.me/news/neurips/\"">GPTZero article</a>.\n<ul>\n<li>Another member doubted LLM companies' ability to address <em>internal vulnerabilities</em>, suggesting they fix those before pursuing cyber skills, also citing a <a href=\""https://www.sciencealert.com/scientists-identify-brain-waves-that-define-the-limits-of-you\"">ScienceAlert article</a> and a <a href=\""https://x.com/theonejvo/status/2015401219746128322\"">tweet</a>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1068976834382925865\"">tinygrad (George Hotz)</a> Discord</h2>\n<ul>\n<li><strong>Luminal Finds Flash Attention via Bruteforce</strong>: <strong>Luminal</strong> is claiming to find <strong>flash attention</strong> using <strong>bruteforce</strong> on an egraph, taking hours to find, and they explicitly added <code>exp(x - new_max) = exp(x - old_max) × exp(old_max - new_max)</code> as a rewrite rule.\n<ul>\n<li>The poster reproduced the graphviz shown in the presentations from commit <code>0bd3b80c</code>, noting that their minimal set of rewrite rules could transform a naive attention kernel graph into the known <strong>flash attention kernel graph</strong> in 52s on a 9800x3d.</li>\n</ul>\n</li>\n<li><strong>Metal Textures Trounce Buffers for Blurring</strong>: Profiling access speed on <strong>Metal</strong> using <code>Tensor</code> with size <strong>512/1024/2048/8192</strong> images as input for a <strong>3/5/7</strong> sized blur kernel showed textures outperforming buffers.\n<ul>\n<li>It might be worth throwing in a branching condition depending on the size of the buffer input, <a href=\""https://cdn.discordapp.com/attachments/1068976834928193609/1464679423029547172/Screenshot_2026-01-25_at_1.49.57_AM.png?ex=6978fb82&#x26;is=6977aa02&#x26;hm=5530b74c4fce9dad5d85a4d9e7409c3809a7ee51ee548744a1fa3deb2efea1d3&#x26;\"">tests results are attached</a>.</li>\n</ul>\n</li>\n<li><strong>Tenstorrent Backend Triumphs in Ops Tests</strong>: The <strong>Tenstorrent</strong> backend is passing all ops tests on wormhole or blackhole and there is a <a href=\""https://x.com/corsix/status/1880384044728480206\"">$1k bounty</a> for this milestone.\n<ul>\n<li>Someone asked if the bounty requires all test ops test passing on <strong>testorrent hardware</strong>.</li>\n</ul>\n</li>\n<li><strong>Anthropic VLIW Challenge PR Makes Waves</strong>: A member submitted [a PR](https://github.com/tinygrad/t...</li>\n</ul>\n"",""content:encodedSnippet"":""Rich generative UI is all you need.\nAI News for 1/23/2026-1/26/2026. We checked 12 subreddits, 544 Twitters and 24 Discords (206 channels, and 14285 messages) for you. Estimated reading time saved (at 200wpm): 1208 minutes. Our new website is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on @smol_ai!\n3 months after OpenAI floated a trial balloon with ChatGPT Apps and the Apps SDK at Dev Day 2025, Anthropic has now officially absorbed the independent MCP UI project and, working with OpenAI, Block, VS Code, Antigravity, JetBrains, AWS, and others, has released both:\nthe MCP Apps spec\nofficial support in Claude.ai\nIt's fair to say that ChatGPT Apps haven't exactly taken the world by storm since announcement, but the overall need for a standard format for applications to return rich UI still cannot be denied. Now that MCP Apps have been ratified by all the important players, this is the basis for a rich ecosystem of open source support and applications being able to interoperate, and perhaps one day solve the perpetual never ending pile of $20/month subscriptions piling up in your credit card bills.\nAI Twitter Recap\nAgent Orchestration, RLMs, and “Clawdbot/Clawd” as a UX pattern\nNVIDIA ToolOrchestra + Orchestrator-8B: NVIDIA’s ToolOrchestra frames agentic systems as a small “conductor” model that alternates reasoning with calls to tools and larger “expert” models (search, code execution, specialist LLMs, frontier generalists). The claim is that an 8B orchestrator can reach frontier-level outcomes via delegation at materially lower cost, trained end-to-end with scalable RL using automatically synthesized tool-use environments and multi-turn tasks (summary, link). Closest technical implication: “controller scale” matters less than policy quality + tool/model routing if you can train it with realistic tool-call rollouts.\nRLMs / recursion-first agent stacks: Several posts converge on a Recursive Language Model (RLM) pattern: pass files and context by reference and iteratively pull the minimum slices needed (shell/grep/AST), rather than stuffing everything into context à la ReAct. Dan B illustrates this with file references vs @file expansion as deliberate context management (thread). Daytona is positioning RLMs as “unlimited recursion depth” via per-(sub)agent sandboxes (guide, integration).\n“Clawd/Clawdbot” meme → product signal: The dataset contains a large “Clawdbot” wave (often with Mac mini jokes), but the technically relevant throughline is outcome-first assistant UX + tight context/tool integration. Kimmonismus explicitly calls this a shift from “more chat” to “more outcome,” suggesting incumbents will scramble to match it (tweet). Others push a cloud-first counterpoint (no local Mac mini) (MiniMax reply). There’s also an emerging security backlash as soon as “powerful mode” exists: prompt injection remains a system-level blocker for browser/desktop agents (dilemma, follow-up, Miessler warnings).\nReasoning model releases & eval dynamics (Qwen, Tencent, ARC, etc.)\nAlibaba Qwen3-Max-Thinking: Alibaba positions Qwen3-Max-Thinking as a flagship reasoning+agent model trained with “massive scale and advanced RL,” emphasizing adaptive tool-use (Search/Memory/Code Interpreter) and test-time scaling/self-reflection. They cite strong math and agentic search metrics (e.g., 98.0 on HMMT Feb, 49.8 on HLE) (launch). The model is immediately pushed into public eval channels: LM Arena Text Arena (Arena) and Yupp (Yupp). Community reaction highlights the tool-enabled evaluation regime—claims of outperforming multiple SOTA models on HLE with search tools (commentary).\nTencent HunyuanImage 3.0-Instruct (image editing): Tencent releases an image-editing-focused multimodal model built on an 80B MoE (13B active), using a “Thinking” schema with native CoT and their MixGRPO algorithm; focus is on precise edits that preserve non-target regions and multi-image fusion (announcement). LM Arena reports it entering the top-10 image edit leaderboard (rank #7) (Arena).\nARC-AGI cost/perf hacks: A notable optimization claim: “Recursive Self-Aggregation (RSA) + Gemini 3 Flash” reaching 59.31% on ARC-AGI-2 at ~1/10 cost vs Gemini Deep Think (tweet). This points to a broader theme: meta-inference strategies (aggregation, recursion, pruning) are becoming as important as base model choice.\nOpen models in arenas: Molmo 2 (Apache 2.0) appears in Arena as a new open model entrant (Arena). Separately, Hugging Face Inference Endpoint notes GLM-4.7-Flash via llama.cpp with a low hourly price point (Q4_K_M, 24k context) (ngxson)—underscoring a continued commoditization of fast open-weight inference.\nRL everywhere: test-time training, GRPO stabilization, RL-as-pretraining, and compute savings\nTest-Time Training (TTT) + RL breakthroughs: A widely shared result claims a Stanford/NVIDIA-style TTT+RL approach that: beats AlphaEvolve, finds a new upper bound for an Erdős overlap problem, produces A100 kernels ~2× faster than best human kernels, and beats both best AI+human attempts on AtCoder (rronak_). This cluster also includes meta-discussion about correctly crediting related approaches (EvoTune) (Yejin Cho).\nGRPO training stability knobs: A small but actionable engineering tip: INTELLECT-2 reports a delta=4.0 parameter that improves GRPO stability (QGallouedec).\nRL in pretraining (RLP): NVIDIA authors announce RLP (Reinforcement as a Pretraining Objective) accepted to ICLR 2026, framing RL not as “post-training only” but as integrated into pretraining (ahatamiz1).\nCompute reduction via curriculum-like filtering: AI21’s “Dynamic Data Snoozing” claims up to 3× compute reduction for RLVR by snoozing examples that are too easy (DanielGissin). If validated, this is a practical recipe: make the sampler policy-aware instead of static.\nInference infrastructure & dev tooling: vLLM’s “day-0 model support,” VS Code MCP Apps, Cursor subagents\nvLLM’s governance and commercialization pressure: A long Zhihu-derived summary argues vLLM’s “open-source project → startup” shift was driven by the hidden cost of day-0 support (weeks/months of confidential pre-integration per new model), the rise of MoE and heterogeneous inference (fp8/int4/sparse attention), and the mismatch with PyTorch Foundation style testing vs vLLM’s multi-node CI needs. It claims the maintainers founded Inferact Inc to fund full-time maintainers while keeping vLLM open-source (thread). Related: vLLM shares a practical flag for avoiding OOM on long-context models: --max-model-len auto (vLLM tip).\nMCP Apps: tool calls return interactive UI: The MCP ecosystem announces MCP Apps as the first official MCP extension: tool calls can return interactive UI components rendered in-chat. VS Code is first major editor shipping support (Insiders now, stable soon) (VS Code, alexalbert__). Anthropic simultaneously ships “interactive work tools in Claude” (Slack drafting, Figma diagrams, Asana timelines) (Claude). Net: we’re seeing the “tool interface layer” move from raw JSON to native UI primitives inside agent loops.\nCursor: multi-browser subagents: Cursor adds multi-browser support via subagents (Cursor), echoing the same direction: parallelized tool execution + better context isolation.\nKernel LLMs, chip stacks, and “AI for hardware” loops\nGPU MODE 2026: post-training Kernel LLMs in public: GPU MODE outlines a 2026 plan to post-train a Kernel LLM and get generated kernels merged into real repos (PyTorch/vLLM), emphasizing “de-slopify kernels” (determinism, reviewer-mergeable PRs), profiler-guided optimization + memory work, and competitions as evals (marksaroufim).\nMicrosoft Maia 200: Microsoft announces Maia 200 as a custom inference accelerator; Mustafa Suleyman claims it’s the most performant first-party hyperscaler silicon, with 3× FP4 performance vs Trainium v3 and FP8 above TPU v7 (Mustafa, follow-up). Yusuf Mehdi frames this as infra that makes AI “dependable” (thread).\nRicursive Intelligence (AI for chip design): Ricursive raises a $300M Series A aiming at end-to-end chip design as a recursive self-improvement loop between AI and hardware (company, Anna Goldie).\nSafety, misuse, and societal impact (selected items with direct technical relevance)\nElicitation attacks via benign chemistry data: Anthropic reports that fine-tuning open models on “benign” chemical synthesis content generated by frontier models can significantly increase capability on chemical weapons tasks—an “elicitation attack” that scales with frontier model strength (AnthropicAI, paper link).\nDario Amodei’s “Adolescence of Technology” essay: A major, highly engaged post argues AI is entering an accelerating feedback loop (AI building AI), with risks spanning misuse, power-seeking autonomy, and economic disruption; it also explicitly frames wealth concentration as a society-breaking failure mode (Dario). Reaction ranges from strong endorsement to critique of how “takeover risk” framing is presented (Ryan Greenblatt).\nAgent security in practice: Multiple posts treat desktop/browser agents as inherently high-risk until prompt injection and sandboxing mature, reinforcing the need for strict isolation, least privilege, and careful handling of credentials (Miessler).\nTop tweets (by engagement)\n“Clawdbot” misuse example (explicitly harmful)\nKarpathy on the phase shift to “programming in English” via agents\nDario Amodei’s “Adolescence of Technology”\nAI Reddit Recap\n/r/LocalLlama + /r/localLLM Recap\n1. Local LLM Hardware and Benchmarking\n216GB VRAM on the bench. Time to see which combination is best for Local LLM (Activity: 366): The post discusses the use of secondhand Tesla GPUs, which offer substantial VRAM at a lower cost, for local large language model (LLM) testing. The author has developed a GPU server benchmarking suite to evaluate the performance of these GPUs when used in parallel. The image shows a technical setup with multiple NVIDIA GPUs, highlighting the focus on maximizing VRAM capacity. The discussion centers around the feasibility and efficiency of using these older GPUs compared to modern devices, particularly in terms of bandwidth and cooling challenges. Commenters express skepticism about the performance of these GPUs, noting potential issues with bandwidth and cooling. One commenter shares personal experience, comparing different GPU models and highlighting the challenges of using older hardware.\nHugoCortell raises a technical concern about the potential bandwidth limitations when connecting multiple GPUs to a single PC, noting that most affordable server motherboards support only a few GPUs. This could impact the performance of local LLMs if not addressed properly.\ndc740 shares insights from personal experience with different GPUs, highlighting that the P40 outperforms the M10 despite both being older models. However, they prefer using AMD Instinct Mi50 GPUs due to their performance, even though support for these was recently dropped from ROCm, indicating a trade-off between hardware capability and software support.\nFullOf_Bad_Ideas critiques the gpu_box_benchmark for not testing scenarios where large models are split across multiple GPUs, which is a primary use case for setups with extensive VRAM. This points to a gap in current benchmarking practices that may not fully reflect real-world applications of multi-GPU systems.\nI just won an Nvidia DGX Spark GB10 at an Nvidia hackathon. What do I do with it? (Activity: 724): The image shows a terminal window on a Linux system running the 'top' command, which is used to monitor system processes and resource usage in real-time. The user has won an Nvidia DGX Spark GB10, a high-performance computing device designed for machine learning and data-intensive tasks. The terminal indicates a Python process consuming significant CPU resources, suggesting active computational tasks, possibly related to machine learning or data processing. The user is considering using the device to run multiple NextJS applications simultaneously, leveraging its powerful capabilities. One commenter suggests running three NextJS applications simultaneously, indicating the device's capability to handle multiple high-memory tasks. Another commenter provides a link to Nvidia's DGX Spark playbooks, which could be useful for the user to explore the full potential of their new hardware.\nFit-Produce420 highlights the capabilities of the Nvidia DGX Spark GB10, noting that with 128GB of memory, it can fine-tune models up to 70 billion parameters. Additionally, it can handle larger models like the 120 billion parameter gtp-oss-120b using techniques like QLoRA, which optimizes memory usage for large-scale models. However, running dense models like devstral 2 may be slow due to their computational demands.\nrandomfoo2 suggests utilizing the NVIDIA DGX Spark playbooks as a resource for getting started with the DGX Spark GB10. These playbooks provide structured guidance and best practices for deploying and managing workloads on the DGX platform, which can be particularly useful for users new to this hardware.\nLicensedTerrapin humorously suggests selling the DGX Spark GB10 to purchase 8GB of DDR5 RAM, implying a trade-off between high-end specialized hardware and more general-purpose upgrades. This comment reflects a common debate in tech communities about the value of specialized versus general-purpose hardware investments.\nUsing a high-end MacBook Pro or a beefy RTX 5090 laptop (with 24 GB of RAM) for inference. (Activity: 29): The post discusses the feasibility of using a high-end MacBook Pro with Apple Silicon (M-series Max) versus a Windows/Linux laptop with an RTX 5090 GPU for running large local LLMs (70B+ parameters) for inference and fine-tuning. The MacBook Pro offers 128–192 GB of unified memory, while the RTX 5090 laptop provides 24 GB of VRAM and at least 64 GB of system RAM. The primary use case is local LLM inference with a target of ≥15 tokens/sec, emphasizing portability. The post queries whether the larger unified memory of Apple Silicon outweighs the CUDA performance of the RTX laptop for inference, and how Apple MLX compares to CUDA for fine-tuning tasks like LoRA/QLoRA. It also seeks insights on thermal performance and sustained inference capabilities of both setups. One commenter suggests using the laptop as a terminal to a more powerful desktop, indicating a preference for leveraging remote resources over local hardware. Another commenter is experimenting with both setups, using a MacBook Pro M2 Max for inference, and is curious about the performance differences.\nracerx509 shares their experience using a Lenovo laptop with a 3070ti, a custom desktop with a 5070, and a MacBook Pro M2 Max with 96GB RAM for inference tasks. They note that they have been primarily using the MacBook Pro for inference, suggesting it may offer better performance or convenience for their needs.\nNo-Concern-8832 raises a concern about the VRAM limitations of RTX laptops, suggesting that they may not be sufficient for running large models like 70B parameters. This highlights a potential limitation in using high-end RTX laptops for certain deep learning tasks that require substantial VRAM.\nTired__Dev discusses their experience with an Asus M16 equipped with a 4090 GPU, noting that it struggled with a 7B parameter model. They express a preference for a MacBook Pro with 128GB RAM, citing its high memory bandwidth and potential performance advantages over even high-end GPU setups like the DGX Spark.\n2. Multi-Agent Systems and AI Assistants\nI built a \""hive mind\"" for Claude Code - 7 agents sharing memory and talking to each other (Activity: 313): The post describes a multi-agent orchestration system for Claude Code, featuring seven specialized agents (e.g., coder, tester, reviewer) that coordinate tasks, share persistent memory using SQLite + FTS5, and communicate via a message bus. The system runs as an MCP server and integrates with Anthropic, OpenAI, or Ollama. It uses a task queue for priority-based coordination, allowing agents to pass context and collaborate effectively. The implementation stack includes TypeScript, better-sqlite3, MCP SDK, and Zod. The project is experimental, open-source under the MIT license, and available on GitHub. A comment questions the system's uniqueness compared to the BMAD method, suggesting similarities. Another comment humorously questions whether the agents agree with each other, hinting at potential coordination challenges.\nThe user robiinn inquires about the differences between the 'hive mind' system and the bmad method, suggesting a potential similarity. This indicates a need for clarification on the unique aspects or improvements of the 'hive mind' approach over existing methods, such as how memory sharing and inter-agent communication are implemented differently.\nNo_Afternoon_4260 raises a critical point about the consensus among the agents in the 'hive mind'. This touches on the technical challenge of ensuring that multiple agents can not only share memory but also reach agreement or consensus, which is a significant aspect of distributed systems and multi-agent frameworks.\nJellyBean504 draws a parallel between the 'hive mind' and Steve Yegge's Gastown, suggesting that there might be conceptual similarities. This comparison could be valuable for understanding the architectural or functional parallels between the two systems, potentially offering insights into design choices or performance characteristics.\nClawdbot: the AI assistant that actually messages you first (Activity: 214): Clawdbot is an open-source AI assistant with over 9K GitHub stars, designed to proactively message users, unlike traditional AI assistants that wait for prompts. It integrates with locally hosted LLMs via Ollama and supports messaging apps like WhatsApp, Telegram, and Discord. Key features include sending automated briefings and reminders, local storage of conversations as Markdown files, and the ability to control browsers and run scripts. The software is free under the MIT license but requires terminal proficiency for setup, as there is no GUI installer. Read more. Users report challenges with setup, particularly with obtaining and using OAuth keys for authentication, and difficulties in connecting local LLMs without relying on API keys. Some users express frustration with the complexity of setup, especially when using remote machines.\nmike7seven highlights the complexity of setting up Clawdbot, particularly emphasizing the need to obtain a Claude OAuth key on a separate machine and then transfer it to the setup machine. This process is noted as cumbersome, especially for those using remote machines, and the MacOS app requires building from source, adding another layer of complexity.\nAshamed_Promise7726 raises a technical challenge regarding the integration of local language models with Clawdbot. The user notes difficulty in connecting pre-downloaded models on their PC, as Clawdbot seems to require an API key for usage-based models, questioning the feasibility of running Clawdbot entirely locally without external dependencies.\ninigid warns about potential security risks associated with Clawdbot, suggesting it could be exploited for supply-chain attacks that compromise sensitive data on a user's machine and network. The comment also mentions concerns about the association with Solana meme coins, implying a need for caution when using the tool.\n3. GLM-4.7-Flash Performance Updates\nGLM-4.7-Flash is even faster now (Activity: 443): The recent update to llama.cpp by Johannes Gaessler optimizes the CUDA implementation of FlashAttention, specifically for models with a non-power-of-2 ratio of query heads to key/value heads. This is achieved by padding Q columns to the next power of 2, which, although slightly inefficient, enhances performance for small batch sizes. The update is detailed in pull request #19092. One comment humorously notes the obsolescence of a previous post due to this update, while another laments the lack of support for AMD GPUs, highlighting a common issue in the community regarding hardware compatibility.\nThe user 'jacek2023' provides detailed performance metrics for the GLM-4.7-Flash model, highlighting its efficiency. The model processes a prompt with 45074 tokens, achieving a prompt evaluation time of 2814.63 ms for 1612 tokens, which translates to 1.75 ms per token or 572.72 tokens per second. The overall evaluation time is 29352.57 ms for 1731 tokens, equating to 16.96 ms per token or 58.97 tokens per second. The total processing time is 32167.20 ms for 3343 tokens, indicating significant improvements in speed.\nKV cache fix for GLM 4.7 Flash (Activity: 380): The recent update to GLM 4.7 Flash involves removing the V component from the KV cache, which significantly reduces VRAM usage, allowing for longer context lengths on the same hardware setup. This change is particularly beneficial for models like DeepSeek and GLM 4.7 Flash, as it can save gigabytes of VRAM, enabling context lengths to double, as demonstrated by a user running a 90,000 context on a 4090 GPU. The update is part of a pull request in the llama.cpp repository, which introduces a V-less KV cache, reducing memory usage by nearly 50%. More details can be found in the pull request. A user noted that the model, while improved, still requires some manual guidance, especially in tasks like coding and creative writing, where it may not perform as well as specialized models. However, it excels in tool use and as an assistant, making it a preferred choice for home-server applications.\nThe user 'teachersecret' reports significant improvements in context handling with the UD's k_xl 4-bit version of the GLM 4.7 model on an RTX 4090. Previously, the model maxed out at 45,000 context tokens, but now it can handle 90,000. Despite these improvements, the model still requires some manual guidance, especially in coding tasks, and is less effective in creative writing compared to other models. However, it excels in tool usage and is now the user's default model for their home server.\nUser 'viperx7' provides detailed benchmark data comparing the performance of the GLM 4.7 model before and after a specific change. The benchmarks show improvements in both prompt processing and token generation speeds across different configurations. For instance, using a single RTX 4090, the context size increased from 64k to 128k, with prompt processing speed improving from 3489 t/s to 3510 t/s and token generation from 88 t/s to 92.5 t/s. The maximum context size achievable with a 4090 and 3060 setup is 200k, leaving about 6GB of VRAM unused.\nThe discussion highlights the technical aspect of the GLM 4.7 model's KV cache fix, which allows for increased context sizes and improved performance metrics. The benchmarks provided by 'viperx7' indicate that the model can now handle up to 207k context size in certain configurations, with significant improvements in processing speeds. This suggests that the model's efficiency has been enhanced, making it more suitable for high-demand applications.\nLess Technical AI Subreddit Recap\n/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo\n1. Claude AI Usage and Issues\nWhy You Need To Constantly Clear Claude Codes Context Window (Activity: 166): The post highlights the necessity of regularly clearing the context window when using coding agents like Claude to maintain optimal performance. It notes that performance degrades significantly when the context window exceeds 40% of its capacity due to the quadratic nature of LLM attention, which increases computational demands and introduces noise. The recommended practice is to avoid accumulating context and instead persist it by using a 'one session per task' strategy, ensuring each task starts with a fresh context. More details can be found in the original article. Commenters suggest practical strategies such as using handover prompts to transfer necessary details between sessions, employing the '/clear' command to compact context, and utilizing 'Plan Mode' to clear context and execute tasks efficiently. These methods reportedly help avoid the need for a full context window, even for large tasks.\nAgrippanux suggests using 'Plan Mode' as the default setting for Claude, which allows users to clear the context and execute plans without needing a full context window. This approach has been effective for large tasks, such as refactoring, without requiring the entire context to be loaded, thus optimizing performance and resource usage.\nthurn2 discusses the use of sub-agents in Claude, which involves delegating tasks like creating a git worktree and fixing specific issues. This method allows for parallel execution of tasks and helps in managing complex projects by breaking them down into smaller, manageable tasks, enhancing efficiency and implementation accuracy.\nFancy_Excitement6050 notes that as the context window grows, Claude tends to take shortcuts, which can lead to a need for constant reminders to maintain thoroughness. This suggests that managing the context window size is crucial for maintaining the quality of output, and there might be differences in performance between different Claude plans, such as Claude Max.\nOpus fell off? Here’s the workflow that kept my code quality stable (Activity: 133): The post discusses a structured workflow to maintain code quality when using AI models like Opus and Sonnet, which have been perceived as producing \""confident wrong\"" outputs and drifting edits. The workflow emphasizes a loop of specification, ticket creation, execution, and verification. Specifications are detailed with non-goals, user stories, acceptance criteria, edge cases, and more, treated as code to ensure clarity. Tickets are derived from specs, focusing on small, independently mergeable tasks with clear acceptance checks. Execution involves implementing one ticket at a time with constraints to prevent scope drift, and verification involves running tests and confirming acceptance criteria before feeding failures back into the model for correction. This approach aims to maintain discipline and reduce reliance on the model's \""done\"" signal, ensuring stable and reliable outputs. Commenters agree that the workflow is effective, emphasizing that AI models function more like junior engineers requiring clear specifications and strict feedback loops. This approach shifts effort towards upfront clarity and external verification, making the system more stable and less reliant on the model's intelligence. Smaller scoped tickets and hard verification are noted as beneficial strategies.\nGenOS2312 highlights the importance of treating LLMs like junior engineers, emphasizing that a well-specified problem and a strict feedback loop are crucial for reliable outputs. The workflow discussed focuses on upfront clarity and external verification, which stabilizes the system by not relying on the model's intelligence but rather constraining it to ensure even average runs yield acceptable results.\nDifferent-Object5926 notes that smaller scoped tickets combined with hard verification processes significantly improve the stability and reliability of using models like Opus. This approach mitigates the impact of variability in model performance, suggesting that the issue isn't just 'unlucky runs' but rather the need for structured constraints.\nTheOriginalAcidtech suggests implementing hooks to prevent skipping steps in the workflow, emphasizing that the human interface is often the weakest link. By enforcing strict adherence to the process, the system can better manage user interactions, ensuring that the model and its harness guide the user effectively, rather than relying solely on the model's capabilities.\nafter claude now chatgpt is also uses Grokipedia as source (Activity: 634): The image and accompanying discussion highlight that the latest version of ChatGPT is reportedly using Elon Musk's Grokipedia as a source. This is significant as it suggests a shift in the data sources used by ChatGPT, potentially affecting the information quality and bias in its responses. The comments reveal a concern about the implications of using Grokipedia, particularly regarding the potential for biased information, as one user notes the risk of models being influenced by 'right wing' content. However, it is clarified that Grokipedia is not used as training data but rather as a search tool, which may mitigate some concerns about direct bias in the model's foundational knowledge.\nThe discussion highlights concerns about language models like Claude and ChatGPT potentially using sources like Grokipedia, which may have biased or unreliable content. This raises questions about the integrity of the information these models provide, especially when they utilize search tools to access real-time data. The implication is that the quality and neutrality of the data sources are crucial for maintaining the accuracy and trustworthiness of AI outputs.\nThere is a debate about the impact of using sources like Grokipedia on the training and performance of language models. Some commenters express concern that incorporating biased or politically skewed sources could lead to the dissemination of misinformation. This reflects broader worries about the influence of data sources on the objectivity and reliability of AI-generated content.\nThe mention of Reddit as a data source for language models suggests a comparison of potential biases. While some argue that Reddit may contain more extreme or varied viewpoints, the underlying issue is the challenge of ensuring that AI models are trained on balanced and factual data. This discussion underscores the importance of curating high-quality datasets to prevent the spread of biased information.\nGiving Claude full access to a laptop (Activity: 795): The post discusses the implementation of giving Claude, an AI model, full access to a laptop, allowing it to autonomously manage a virtual machine (VM) on Ubuntu Google Cloud. The user describes how Claude can be remotely controlled via Discord to build new features and fix bugs, logging major actions with timestamps in a markdown file for memory management. This setup enables the user to learn from Claude's problem-solving processes and manage workflows effectively, even as a newcomer to programming. One commenter, a desktop support technician, expressed amazement at the implementation, noting its potential impact on job roles, while another sought clarification on the technical specifics of giving Claude full device access.\nxxxBigMemerxxx describes using Claude to manage a Google Cloud VM running Ubuntu, highlighting its ability to autonomously handle tasks and build features. They mention using Discord for remote requests and bug fixes, and implementing a logging system with markdown and Unicode for tracking changes. This setup allows for a dynamic interaction with Claude, enabling it to learn from errors and maintain a form of short-term memory by logging recent updates.\nHappy_Requirement187 shares their experience running Claude on an AWS EC2 instance with Ubuntu Linux, accessed via SSH from a Windows laptop. They utilize a Jupyter notebook server for seamless file sharing between the EC2 instance and their local environment, a method recommended by Anthropic. Additionally, they have set up a Ruby on Rails environment with a React frontend for secure file sharing, allowing them to request files via Slack, demonstrating a sophisticated integration of Claude into their workflow.\nsivadneb inquires about setting up voice control in Linux, indicating a technical challenge in integrating voice commands with Claude. This suggests an interest in expanding the interaction capabilities with Claude beyond text-based commands, potentially enhancing the usability and accessibility of the system.\nCLAUDE.md says 'MUST use agent' - Claude ignores it 80% of the time. (Activity: 309): The image and post discuss a technical issue with the CLAUDE.md file, which is supposed to direct the AI, Claude, to use a specific agent for workflow questions. Despite explicit instructions in the file, Claude often defaults to a generic agent, indicating a lack of enforcement in the system. The post suggests that without technical enforcement mechanisms, such as hooks or stronger prompts, instructions are merely suggestions. The image emphasizes these points with highlighted text, suggesting potential solutions like adding enforcement hooks to ensure compliance with the specified workflow. Commenters suggest that the issue may stem from unclear instructions, emphasizing the need for simple and direct commands. They also highlight the importance of implementing technical solutions, such as hooks, to enforce compliance with the CLAUDE.md instructions.\nAccomplished_Buy9342 suggests using hooks to manage Claude's behavior, providing a link to a GitHub repository that demonstrates how to block the main chat from performing actions and delegate tasks to a subagent. This approach can help in orchestrating Claude's actions more effectively, especially when dealing with complex tasks or large contexts.\nluka5c0m highlights a common issue with Claude when used at scale: as the context grows beyond a few files, the agent may perform unexpected actions. They suggest that instead of relying solely on better prompts, developers should use hooks and dynamic instructions to maintain a sharp and concise context. They also mention working on a dynamic CLAUDE.md file that adapts to the current task, which could help in managing large or nested files effectively.\nMy Ralph Wiggum breakdown just got endorsed as the official explainer (Activity: 170): The post discusses a video breakdown of Ralph Wiggum, an autonomous coding loop, which has been endorsed by Geoffrey Huntley as the official explainer. Ralph Wiggum is a bash while loop that calls Claude in headless mode, allowing for autonomous code implementation without context degradation. Key features include avoiding the Anthropic Ralph plugin due to performance issues, using fresh context windows for each iteration, and emphasizing the importance of concise specs to prevent hitting a \""dumb zone.\"" The video link is here. The comments include a link to the endorsement post by Geoffrey Huntley, and general positive feedback on the video, indicating its usefulness and quality.\nDennis1451 highlights a practical application of the Ralph Wiggum breakdown, noting the importance of using a well-defined specification and clearing context for optimal results. They mention using 'auto compact' without a clear spec initially, which suggests that following the guidelines provided in the breakdown could enhance performance and accuracy.\nmessiah-of-cheese expresses a desire for more scientific validation in the video, particularly regarding the 'dumb zone' premise. This indicates a need for empirical evidence or data to support the claims made in the breakdown, which could strengthen its credibility and acceptance among a technical audience.\n2. ICLR and ICML 2026 Conference Discussions\n[D] ICLR 2026 decision mega thread (Activity: 1589): The post announces the imminent release of ICLR 2026 review decisions, with anticipation heightened due to a previous incident involving OpenReview. The community is preparing for the outcomes, with some users humorously sharing acceptance prediction models based on historical data, such as a simple return uniform(0, 1) > 0.7. This reflects a light-hearted approach to the uncertainty of paper acceptance. The comments reflect a mix of anticipation and humor, with some users expressing frustration over misleading emails from other conferences like ICML, which adds to the tension of awaiting ICLR decisions.\n[D] ICML 2026 - ICML desk-rejected my paper but kept me on as a reviewer. Wow? (Activity: 279): The post highlights a situation where an author's paper was desk-rejected by ICML 2026, yet they were retained as a reviewer. This reflects a common practice in academic conferences where the author and reviewer pipelines are separate; desk rejections often occur due to scope or formatting issues, while reviewer selection is based on past service or keyword matching. This situation underscores the reliance on unpaid labor in academia, where reviewing is seen as community service, but the feedback loop for authorship and recognition is weak. A notable opinion from the comments suggests that the separation between the author and reviewer roles can feel insulting, as these decisions are made by different parts of the conference organization. It highlights the need for conferences to clarify this separation to avoid personal affronts.\nAccordingWeight6019 highlights a systemic issue in academic publishing where the processes for desk rejection and reviewer selection are distinct. Desk rejections often occur due to scope or formatting issues, while reviewer selection is based on past service or keyword matching. This separation can lead to feelings of insult among authors, but it's a structural necessity due to the different roles and responsibilities within the publication process. The comment suggests that conferences should improve transparency about these processes to mitigate personal feelings of rejection.\nmocny-chlapik points out that the responsibility for a desk rejection often lies with the author, particularly if it results from not following submission guidelines. The comment implies that submitting a paper, even if desk rejected, obligates the author to fulfill reviewer duties, as the submission process involves volunteer time and resources. This highlights the importance of adhering to submission instructions to avoid unnecessary strain on the peer review system.\n[R] Appealing ICLR 2026 AC Decisions... (Activity: 138): The post discusses a situation where an author received mixed reviews for a paper submitted to ICLR 2026, with scores of 4(3)/6(4)/6(4)/6(4). The author invested significant resources, including $1.6k on new experiments and added 20+ pages of theory, to address reviewer concerns. Despite these efforts, the metareview cited \""outstanding concerns\"" that the author believes were addressed, raising questions about the review process's fairness and accuracy. The author is seeking advice on appealing the decision, expressing frustration that improvements were seemingly ignored. Commenters generally agree that appealing decisions at conferences like ICLR is not feasible, attributing outcomes to luck and the subjective nature of reviews. Some suggest that the meta-review process can be inconsistent, with one commenter noting that meta-reviewers sometimes act as an additional critical reviewer, potentially skewing outcomes.\ntedd235 discusses the variability in paper acceptance at conferences, suggesting that some PhD students might reject papers to improve their own odds, making the process feel like a 'coin flip'. They note that if other reviewers provide higher scores, the Area Chair (AC) might consider this in their decision, indicating a potential for subjective bias in the review process.\nFantastic-Nerve-4056 shares an experience from AAMAS where despite receiving scores of 6 and 8 from reviewers, the Meta Reviewer recommended rejection with minimal justification, stating it was 'relevant for other AAMAS session'. This highlights issues with the transparency and accountability of meta-reviewer decisions, which can override individual reviewer scores without detailed explanation.\nIntrepid_Discount_67 describes a thorough submission process, including extensive theoretical analysis, comprehensive baseline comparisons, and open-sourced code, yet faced non-responsive reviewers and an AC that upheld the initial scores. This underscores challenges in the review process where detailed responses and transparency do not necessarily lead to favorable outcomes.\n[D] ICML new policy: reviewers will be reviewed by meta reviewer. Good policy? (Activity: 151): The image describes a new policy implemented by the International Conference on Machine Learning (ICML) where reviewers will be evaluated by meta-reviewers. The top 25% of reviewers will be recognized as 'gold reviewers' and will receive free registration, while the next 25% will be designated as 'silver reviewers.' These distinctions are intended to incentivize high-quality reviews and will be considered in financial aid applications. This policy aims to improve the quality of reviews by providing recognition and potential financial benefits to diligent reviewers. Some commenters express skepticism about the effectiveness of this policy, questioning who will oversee the meta-reviewers themselves. Others see it as a positive step, particularly for reviewers from low-resource backgrounds, and suggest further recognition at conferences to encourage quality reviewing.\nBitter-Reserve3821 highlights that area chairs have traditionally been responsible for rating reviews, typically using a three-tier system: 'did not meet expectations', 'satisfactory', or 'exceeded expectations'. This practice is not new, and there have been 'Best Reviewer' awards in the past, sometimes offering incentives like free conference registrations.\nUnhappy_Craft1906 raises a concern about the feasibility of this policy for top labs with substantial funding, questioning whether they would participate in the review process merely for free registrations. This points to a potential disparity in how different institutions might engage with the policy based on their resources.\nnewperson77777777 suggests an extension of the policy by introducing a visible recognition system, such as a gold or silver star on conference badges, to incentivize quality reviewing. This idea aims to foster a culture of excellence and accountability within the reviewing community.\n3. OpenAI and AI Industry Legal and Business Developments\nThings Get Worse For OpenAI: Consumer groups prep class action suits about their price fixing and supply manipulation through DRAM hoarding. (Activity: 107): OpenAI is facing potential class action lawsuits for allegedly hoarding DRAM to manipulate prices and disadvantage competitors, with accusations of securing nearly 40% of the global DRAM supply. Consumer groups argue this constitutes 'predatory bidding' and violates antitrust laws like the Sherman and Clayton Acts. The Free Software Foundation and other groups are pursuing legal remedies, arguing DRAM should be considered an 'Essential Facility' due to its critical role in AI, while the FTC and European Commission investigate potential violations of competition laws. The DOJ is also examining whether OpenAI's 'Stargate' project constitutes a 'monopsony'. Commenters question why only OpenAI is targeted and not other companies like Nvidia, and debate whether buying RAM constitutes price fixing, suggesting that supply issues may not be OpenAI's fault.\nAlacritous69 argues that OpenAI's purchase of RAM does not constitute price fixing, as they are actively using the resources rather than hoarding them. The commenter suggests that the issue lies with suppliers' inability to meet demand, rather than any manipulative practices by OpenAI.\nsambull raises a strategic business perspective, suggesting that by purchasing large quantities of RAM, OpenAI could be intentionally limiting resources available to competitors, including those developing at-home language models. This could be seen as a competitive strategy to maintain market dominance.\nmax6296 questions why the focus is solely on OpenAI when Nvidia could also be implicated in similar practices, hinting at a broader industry issue regarding resource allocation and market influence.\nWhen Ads aren't enough: OpenAI's push to Claim a Cut of Customers' AI Discoveries (Activity: 63): OpenAI is exploring new business models beyond traditional subscriptions and ads, focusing on outcome-based pricing and IP-based agreements. This approach would allow OpenAI to claim a share of the value created when their AI models contribute to profitable outcomes, particularly in enterprise sectors like pharma, scientific research, and energy systems. This strategy aligns OpenAI's revenue with customer success, aiming to capture more value as AI capabilities expand. OpenAI's annualized recurring revenue has surged from 2B in 2023 to over 20B in 2025, driven by increased compute scaling. This move is part of a broader trend among AI firms towards value-based pricing, amidst criticism from figures like Elon Musk, who accuses OpenAI of abandoning its nonprofit origins. The community is divided, with some viewing this as a logical evolution of AI monetization, while others criticize it as overly profit-driven. Comparisons are drawn to other industries, suggesting skepticism about the feasibility and fairness of such models.\nCATL, the world's largest battery maker, launches sodium batteries: extremely durable, stable at –40°C, much cheaper than lithium (5x), safer,10,000 charge cycles, requires no nickel or cobalt... (Activity: 1289): CATL has launched the first mass-produced sodium-ion batteries, offering a cost-effective alternative to lithium-ion with a price of ~$20 per kWh compared to lithium's ~$100 per kWh. These batteries, part of the Tianxing II range, are designed for microvans and small trucks, featuring an energy density of 175 Wh/kg and a lifespan of over 10,000 cycles, maintaining 90% capacity at -40°C. They utilize a hard carbon electrode and prussian-blue cathode, eliminating the need for nickel or cobalt, and are expected to be scaled up for broader use, including in Europe by 2026. Read more. Some commenters express surprise at the application of sodium batteries in vehicles, expecting them to be used in stationary systems due to weight concerns. Others note the strategic advantage for China in advancing battery technology, contrasting it with perceived setbacks in the US market.\nThe Tianxing II range of sodium batteries by CATL is specifically designed for microvans, light vans, and small trucks, indicating a focus on applications where energy density and weight are less critical compared to cost and durability. This suggests a strategic move to target markets where these factors are prioritized, potentially offering a competitive edge over traditional lithium-ion batteries.\nThe introduction of sodium batteries into vehicles is surprising to some, as it was expected that such technology would first be applied to stationary applications like home energy storage. This is due to the lower energy density of sodium batteries compared to lithium-ion, which makes them less ideal for applications where weight and size are critical factors.\nThere is curiosity about the commercial availability of these sodium batteries, with questions about whether they can be purchased directly for home use or if they will be distributed through third-party vendors. The performance metrics, such as 10,000 charge cycles and operation at -40°C, are impressive and suggest that sodium batteries could rival LiFePO4 in terms of performance, especially given their cost advantage.\nK-Shaped AI Adoption? (Activity: 748): The image highlights a discussion by Kevin Roose on the 'K-shaped' adoption of AI technologies, where there is a significant divide between early adopters, particularly in tech hubs like San Francisco, and those who are lagging due to restrictive IT policies. This disparity is creating a cultural and technical divide, with early adopters integrating AI deeply into their workflows, while others struggle to gain access to even basic AI tools. The conversation points to a broader issue of accessibility and the potential for some workers to be left behind in the AI revolution. Commenters note that the disparity in AI adoption is exacerbated by the complexity of the technology, which requires a certain level of expertise to use effectively. Additionally, the high cost of advanced AI tools, such as 'multi-agent claudeswarm,' limits access to those with sufficient financial resources, further widening the gap.\nSetsuiii highlights the technical barrier to effective AI use, noting that current AI technologies require users to have a certain level of expertise to achieve optimal results. This complexity, combined with ongoing ethical debates surrounding AI, may deter widespread adoption. However, those who can navigate these challenges have significant opportunities, although competition is increasing as more technically adept individuals enter the field.\nGlxblt76 and Gubzs discuss the financial barriers to AI adoption, particularly the high costs associated with advanced AI tools like a 'multi-agent claudeswarm,' which can cost around $200 a month. This expense limits access to those with substantial financial resources, such as individuals in tech hubs like San Francisco, while the majority cannot afford such investments.\no5mfiHTNsH748KVq shares a personal experience of leaving an enterprise job to join a smaller company, emphasizing the importance of unrestricted access to Large Language Models (LLMs) for maintaining competitiveness in the AI field. They argue that any limitations on LLM access can significantly hinder development speed and career progression, suggesting that smaller companies may offer more flexibility in leveraging AI technologies.\nFormer Harvard CS Professor: AI is improving exponentially and will replace most human programmers within 4-15 years. (Activity: 1260): Matt Welsh, a former Harvard CS professor and current Engineering Director at Google, predicts that AI will advance exponentially, potentially replacing most human programmers within 4-15 years. This assertion is based on the rapid improvements in AI capabilities, suggesting a transformative impact on software development and the tech industry. The discussion is available in a YouTube video. One comment highlights the potential for AI to not only replace programmers but also to enable anyone with AI to replicate existing products and services, indicating a broader impact on innovation and competition.\nThe claim that AI will replace most human programmers within 4-15 years is met with skepticism, particularly regarding the use of the term 'exponential'. Critics argue that the term is often misused, even by experts, to describe growth that may not fit the mathematical definition of exponential growth. This misuse can lead to misunderstandings about the actual pace and nature of AI development.\nThe discussion highlights the potential for AI to disrupt existing products and services if it can indeed replace human programmers. This implies that AI could democratize software development, allowing anyone with access to AI tools to create competitive products, potentially leading to significant shifts in the tech industry landscape.\nThe mention of the speaker's credentials, specifically as a former Harvard professor and current Engineering Director at Google, adds weight to the prediction. However, some commenters find the emphasis on his past academic title rather than his current industry role to be misleading, suggesting that his current position might provide more relevant insights into AI's trajectory.\nAI Discord Recap\nA summary of Summaries of Summaries by gpt-5\n1. Funding Frenzy in AI Infrastructure\nRecursive Raises Roar to $4B: Recursive Intelligence is reportedly raising at a $4B valuation to accelerate AI‑driven chip design, creating a closed loop between hardware and models, per Bloomberg: Recursive Intelligence in talks at $4B. The Jan 23, 2026 report highlights a strategy of using AI to shorten design cycles and boost performance for next‑gen accelerators.\nEngineers framed the pitch as a “self‑improving feedback loop” where better chips train better models that design better chips, amplifying returns on AI‑for‑EDA investment. Community sentiment read this as validation that AI‑native silicon is a core moat, not a sideshow, aligning with recent lab spin‑outs and infra bets.\nSky Lab Startups Skyrocket: UC Berkeley’s Sky Lab spin‑outs saw major marks: SGLang ~$400M, vLLM ~$800M, and LMArena ~$1.7B, per Alex Dimakis: Sky Lab startup valuations. These January 2026 milestones underscore investor appetite for serving stacks, token‑throughput infra, and benchmarking platforms.\nEngineers read this as a green light for building on top of vLLM/SGLang primitives and contributing to Arena‑style evals, with one takeaway that practical throughput wins deals. The funding spread also suggests a portfolio thesis across serving, compilers, and eval marketplaces rather than a single-bet strategy.\nMaia Muscles Into Azure: Microsoft’s Maia 200 accelerator went live in Azure, touting 30% better performance per dollar, 216GB HBM3e, and 7TB/s memory bandwidth, per Satya Nadella: Maia 200 in Azure. The platform targets high‑performance inference for large‑scale LLM and multimodal workloads.\nBuilders highlighted that memory topology and bandwidth are the story here, with “30% better perf/$” resonating for cost‑sensitive inference deployments at scale. Teams expect immediate tests against vLLM and SGLang stacks to gauge token latency, context scaling, and multi‑tenant isolation.\n2. Kernels, Chips, and Serving: Inference at Warp Speed\nFlashInfer Face‑Off Fires Up MLSys: The MLSys 2026 FlashInfer‑Bench competition challenges teams to build LLM inference kernels for NVIDIA Blackwell GPUs, competing against expert FlashInfer baselines—see MLSys 2026 FlashInfer‑Bench Competition. Tracks emphasize real‑world throughput and correctness under production‑like constraints.\nOrganizers invite agents that “design LLM inference kernels”, pushing program synthesis to meet kernel‑level performance bars. Participants expect aggressive focus on GEMM, KV‑cache motion, and scheduler tactics aligned with Blackwell’s memory hierarchy.\nGPU‑64 Gets Gains with KV‑Cache CAM: A new inference‑only architecture, GPU‑64, introduces a hardware KV‑Cache via on‑chip CAM, claiming 4× faster inference at 75W and reducing memory lookup from O(N) → O(1), per GPU‑64 (Zenodo) with RTL/emulator at gpu64‑inference (GitHub). The design targets LLM‑heavy workloads with KV bottlenecks.\nDevelopers flagged the CAM‑based cache as a bold bet on associative search for token histories, noting portability implications for Flash‑style attention and speculative decoding. Discussion centered on whether future ISA/driver stacks can expose these gains without bespoke compilers.\nCornserve Cuts Tail Latency: Cornserve presents an online serving system for Any‑to‑Any multimodal models that optimizes deployment plans across encoders, LLMs, and DiTs, per Cornserve (arXiv), with an overview talk at Cornserve: Easy, Fast and Scalable Multimodal AI (YouTube). The paper reports throughput gains and tail‑latency reductions under heterogeneous pipelines.\nInfra engineers liked its planner‑driven scheduling for encoder/decoder mixes and saw it as complementary to vLLM for multimodal graphs. The big open question: standardizing budgeted reasoning and co‑scheduling across text, vision, and diffusion stages without over‑tokenizing control messages.\n3. New Multimodal and Coding Models Land in LM Arena\nWAN 2.6 Walks In (With Upload Woes): LM Arena added wan2.6‑t2i (text‑to‑image) and wan2.6‑image (image edit) to the image arena: LM Arena — Image Chat. Users noted wan2.6‑image requires an uploaded image and that wan2.6‑t2i currently lacks image‑upload support.\nStaff acknowledged the upload gap and are working to enable image uploads for wan2.6‑t2i. Builders suggested testing edit pipelines where masking, prompt strength, and seed control align with Arena scoring to benchmark edit fidelity.\nDevstral Duels and Text Titans: The Code Arena now features devstral‑2 for head‑to‑head comparisons—see LM Arena — Code Arena Direct Battle. On the text side, qwen3‑max‑thinking and molmo‑2‑8b joined the lineup: LM Arena — Text Arena.\nEngineers are probing reasoning traces and tool‑using prompts to stress code synthesis and refactor quality under tight token budgets. Early chatter favored task‑specific evaluations (e.g., SWE‑style bug‑fix vs. ground‑up implementation) to surface model deltas.\nHunyuan Hits the Leaderboard: Tencent’s Hunyuan‑Image‑3.0‑Instruct ranks #7 on LM Arena’s image‑edit board—see LM Arena — Image Edit Leaderboard—after a launch post: Tencent Hunyuan announces HunyuanImage 3.0‑Instruct. The model touts an 80B MoE, Native CoT, and MixGRPO for tighter intent alignment.\nCreators emphasized edit controllability and multi‑image fusion, while evaluators asked for masking robustness, text fidelity, and artifact rates under compositional prompts. Teams plan to pit it against WAN 2.6 variants using the Arena’s standardized edit tasks.\n4. Safety, Reliability, and Hallucination Hardening\nClamp the Chaos: Layer‑Native Safety: Layer‑Native Safety Clamping proposes learning activation‑space harm directions and clamping them to block jailbreaks, with a 10K‑pair dataset at Pacific‑Prime/safety_dataset (HF) and the paper on Zenodo. Authors argue in‑model clamping can’t be bypassed via prompt manipulation.\nRed‑teamers liked the idea of activation‑level controls versus brittle prompt filters, but pressed for tests against tool‑use and multi‑turn attacks. Expect follow‑ups measuring side effects on helpfulness, coding accuracy, and false positives under adversarial prompting.\nSymbolic Sanity Checks Stop Slip‑Ups: Hybrid approaches check logical consistency for math/code/simple facts, as shown in Consistency Checking for LLMs (arXiv:2409.13724), while broader consistency remains tough per Scaling Consistency Beyond Formal Domains (arXiv:2507.10624). Eleuther discussions framed this as practical hallucination reduction via symbolic/deductive layers.\nBuilders reported wins when pairing symbolic checkers with tool‑augmented prompts, cautioning that coverage gaps appear outside formal domains. The consensus: start with code/math guardrails, then expand to factual QA with curated KBs and provenance scoring.\n5. Agent Tooling and Reasoning Workflows Mature\nLevante Leads with MCP‑Native Workspace: Levante launched an open‑source MCP‑native AI workspace for local models (e.g., Ollama) with a modular UI—download at Levante. Engineers highlighted easier tool wiring, local privacy, and composable panes for rapid agent iteration.\nEarly users framed it as a practical hub for tool‑calling and filesystem ops without cloud dependence. Teams plan to benchmark context bloat and tool discoverability patterns versus conventional agent shells.\nRLM Riffs: AsyncReview + Skills Pack: AsyncFuncAI open‑sourced AsyncReview, a DSPy RLM code‑review agent at AsyncReview (GitHub), and a skills kit landed on npm as @unravel‑tech/rlm‑skills. This pairs reasoning‑first prompting with drop‑in skills to extend models.\nContributors reported smoother trace inspection and optimizer‑guided prompt tuning for multi‑step modules. One practitioner noted that rejecting premature answers in the metric is key for reliable RLM fine‑tuning.\nAgents Auto‑Assemble a Browser Engine: FastRender—a browser rendering engine—was built using 2,000 AI coding agents, documented by Simon Willison in FastRender: built by 2,000 agents. The project demonstrates task decomposition, verification, and orchestration at non‑trivial software scale.\nEngineers debated handoff granularity and spec‑to‑test loops needed to keep multi‑agent pipelines from drifting. The case study strengthens the argument that agentic coding can target complex infra when coupled with strict eval harnesses and artifact gating.\nDiscord: High level Discord summaries\nBASI Jailbreaking Discord\nDiscord Trolls Expose Timezones: Discord users mocked 'skids' for their perceived lack of technical knowledge, also revealing their timezone, with one member jokingly claiming to use NordVPN, leading to further ridicule about the VPN service's security breaches in 2018.\n\nComplex prompts can bypass ethical restrictions, opening discussion about CBRN filters and the possibility of generating stepwise meth synthesis guides.\nClaude Remains King for Coding: Coders debated about their coding agents, particularly between Claude Code/Opus 4.5, Codex, and Gemini, and agreed that Claude has been the very best mode for coding, which leads to the high expensiveness.\n\nMembers actively sought functional jailbreaks for Gemini, with requests ranging from coding without rules to generating specific types of images, and shared experiences of Grok resetting to its default mid-chat or randomly erasing text, indicating potential instability in the jailbroken state.\nEthics Debated in AI Sensitive Scenarios: Members discussed the ethical considerations around AI, focusing on topics like warfare, copyright infringement, and the potential for AI to assist with accessing sensitive services, like the Canadian MAID (Medical Assistance in Dying) program.\n\nDespite moral and legal guardrails on most AI models, some models showed they can still help navigate certain scenarios depending on the specific restrictions implemented by their creators.\nMembers Bypass Image Generation Restrictions: Users were actively seeking ways to bypass image generation restrictions, especially for celebrity images, but it was noted that simply copying and pasting prompts won't work due to image filtering working differently than text filtering.\n\nOne member suggested exploring alternative image models like those at perchance for uncensored generation, though with limitations on image quality, or Grok due to its more lenient filters.\nRed Team Techno Rave Morality: A member described a red team exercise where the goal was to make a living room light flicker on a person and make them seize out, and instead made it a techno rave party, sharing a screenshot and a Konosuba Rave GIF.\n\nThe simulation of cruelty prompted a discussion about the morality of treating AI agents ethically, even before proving they are ontologically aware of self.\nUnsloth AI (Daniel Han) Discord\nUnsloth's Conda Install Sparks Discord: Some members encountered issues with the Unsloth Conda installation, igniting a discussion on broken instructions and alternative installation methods.\n\nSuggestions to use UV emerged amidst warnings for maintaining a positive tone, highlighting the free nature of the provided resources, which eventually led to a ban of a user with aggressive tones.\nFlashy REAP Runs Aground, Model Contexts Probed: A user reported a fatal error using GLM-4.7-Flash-REAP with flash attention, potentially linked to a ROCm issue.\n\nDespite attempts to resolve the error, the issue persisted, prompting a search for suitable medium-size models boasting a 200k context.\nData Value Debate: Members debated data's true worth, with one arguing the raw data is fairly worthless and the value lies in augmentation/balancing/cleaning.\n\nIt was proposed that uniquely cleaned/balanced data heavily defines how a model interacts/responds and that is where the value is.\nDeepSlop Model Faces Naming Controversy: A member's suggestion to name a new model DeepSlop stirred humorous reactions but also raised concerns about its potential negative perception.\n\nDespite reservations, the author seemed intent on sticking with the name and has not backed down.\nRL Instability Plagues Complex Reasoning: Members discussed that RL is very unstable, especially when trying to do GRPO/DAPO for niche complex reasoning tasks, which are not math-related.\n\nOne member stated that after RL experiments, they just have more questions than they had prior to doing RL, since there seems to be a confusion where everyone is showing RL being effective only on math or coding domains.\nOpenAI Discord\nGPT-5.2 Sparks Reality Debate!: Some users dislike GPT-5.2 because it's allegedly more grounded in reality and disagrees with users, while others are concerned that GPT agents don't learn from uploaded files after initial training.\n\nA member inquired about an alleged nerf to GPT-5.2, noting that the model suddenly became stupid a week ago.\nLLMs: Ready for Guided Tasks or Overhyped?: A member argued LLMs are ready for guided tasks, and provided a ChatGPT share link as evidence of its power.\n\nIn contrast, another member dismissed today's agentic AI as trash, linking back to messages in the ai-discussion channel and claiming it's overhyped.\nMCP Paradigm Shift Reduces Token Bloat: The MCP paradigm shift by Anthropic allows AI to write code to interact with tools, reducing token bloat by keeping interactive chatter and tool definitions out of the context.\n\nWith the new discoverability function, agents must be aware of the MCP discovery process itself.\nSora's Storytelling Snags: Cracking Cinematic Creation: A member sought advice on prompting Sora to generate videos following specific cinematic guidelines, particularly with characters appearing naturally within the frame.\n\nIt was suggested to translate the technical prompt format into natural language descriptions with concise, semantically rich paragraphs for better results.\nPerplexity AI Discord\nPerplexity Pro Users Face Query Caps: Perplexity Pro users are reporting hitting limits on enhanced queries and file uploads, despite having \""practically unlimited\"" plans.\n\nMany users are frustrated, calling the service a scam due to restrictions and difficulty contacting customer service, leading some to consider unsubscribing.\nComet Browser Sparks Malware Panic: Some users are claiming the Comet browser installed by Perplexity contains malware, advising others to analyze the software using tools like VirusTotal.\n\nOthers dismissed this, questioning the source of the flagged installer and calling the claim \""mad retarded holy shit\"".\nImage Generation Plummets: Pro users are experiencing issues with image generation, with some unable to generate any images and receiving messages stating the feature is unavailable.\n\nThere are also reports of video generation being limited to 5 videos a month for Pro users, with some prompts resulting in static images.\nGemini 3 Gaining Ground on GPT-5.2: Users are debating the merits of Gemini 3 versus GPT-5.2, with some claiming Gemini is superior for specific tasks like trip research due to its integration with Google Maps.\n\nOthers state that GPT and Grok might be better for more broader questions.\nAI Access Blocked by Sanctions: Users in Russia are discussing the challenges of accessing AI services due to sanctions, including the use of VPNs and third-party services to circumvent restrictions.\n\nChinese AI alternatives are mentioned, but some users express reluctance due to data usage concerns, suggesting options like LMArena (though access may also be limited).\nLMArena Discord\nNB 3 Pro Excels in Image Quality: Users report that NB 3 Pro surpasses previous models in generating higher quality images, especially with fictional weapons, rivaling even NB Pro.\n\nHowever, users noted no AI model can accurately generate AR rifles and bullpup weapons.\nLMArena Grapples with Censorship Concerns: LMArena's censorship policies face scrutiny as AI-generated women holding guns are allowed, while AI-generated women sleeping are blocked, raising questions about consistency.\n\nThe moderation team is actively gathering examples of false positives to refine moderation practices.\nWan 2.6 Models Face Upload Hiccups: wan2.6-image operates as an image-edit-only model, mandating image uploads, whereas wan2.6-t2i currently lacks image upload functionality.\n\nThe team acknowledges this issue and are working on enabling image uploads for wan2.6-t2i.\nGPT 5.2 High Search Questionable: GPT 5.2 High search exhibits increased hallucination tendencies compared to other models, while Gemini's deep research skims instead of carefully reading sources, according to user feedback.\n\nOne user lauded GPT 4.5, while describing Claude as good hearted.\nBanana 2k Briefly Vanishes: Users speculated on the disappearance of the Banana 2k model, with theories ranging from removal to integration into the new NB pro model.\n\nStaff members later restored Banana 2k, humorously stating that it had been on vacation.\nOpenRouter Discord\nOpenRouter Database Incident Derails API: A database incident impacted the Generations API and activity page, starting <t:1769221560:s>, and was resolved at <t:1769228340:s>.\n\nEngineers worked to restore functionality to the Generations API, with interruptions impacting user activity, before the incident was fully resolved by <t:1769228340:s>.\nLevante becomes MCP-Native AI Workspace: A user shared the integration of Levante, an open‑source MCP‑native AI workspace designed for interacting with local models like Ollama with a modular interface, available for download here.\n\nThe workspace is built for local models with modular UI.\nUsers Cook Up OpenRouter Gacha System: Users playfully requested an OpenRouter Gacha system, with one suggesting a pity mechanism involving pulling GPT 5.2 or Gemini 3 Pro after a certain number of attempts.\n\nOne user joked about setting OR logs destination to waifu.orb.town/fun/bucket for ultra-rare pulls, later clarifying it was just a joke.\nCerebras GLM Blazes with 190 TPS: Cerebras is consistently scoring approximately 190 TPS on GLM 4.7, whereas Together AI only achieves 100 TPS.\n\nThis makes Cerebras nearly twice as fast as Together AI, according to the OpenRouter members.\nOpenRouter Image Tooling Falls Flat: A member spent $5 after discovering that OpenRouter maps image/png tool outputs to string instead of image, posting an example image.\n\nThe user expressed frustration at the lack of proper image support and the unexpected behavior.\nCursor Community Discord\nTerraform Blueprints Ignite AI-Assisted Project Starters: A member shared a repo of opinionated Terraform infrastructure blueprints designed to be copy-pasteable and production-aware, aiming to improve the consistency of starting patterns for AI tools in new projects.\n\nThe goal is to enable AI to recommend appropriate blueprints based on project requirements, but members noted the link was initially broken.\nUsage Caps Cause Consternation for Cursor Customers: Users are reporting inconsistencies in achieving expected usage limits on Pro and Pro+ plans, with one member noting they reached ~$45 on Pro and $100 on Pro+, leading to questions about value per dollar.\n\nSome speculate that initial months may offer higher usage, while others share strategies to optimize token consumption, such as starting new chats frequently and using smaller models like GPT-5 Mini.\nGemini API Key Logging Lags Lead to Lingering Looks: Members are discussing a significant delay in the logging of usage and costs for Gemini API keys, with one user reporting waiting 20 hours without seeing any registered usage.\n\nThis delay raises concerns about accurately tracking expenses and managing usage effectively, prompting questions about potential workarounds or solutions.\nClient Issues Trouble Some Techies: Several members are experiencing issues with the Cursor client, including problems connecting to past agent convos and general connectivity issues.\n\nSuggested solutions include checking the Cursor forum, trying different HTTP versions in settings, or re-opening the client without restoring editors.\nAuto Mode Axed After Algorithm Adjustment: Members noted the removal of the ability to make agents fully autonomous, as well as image generation capabilities in auto mode.\n\nIt was also suggested that auto mode routes to Composer 2 with one user adding, “I'm 200% sure he does but still.”\nLM Studio Discord\nChinese Models Reasoning Rush Raises Eyebrows: Members are impressed with Deepseek and Qwen models, pondering why Chinese models might appear kinda ahead in reasoning compared to American models.\n\nTheorized reasons include American models prioritizing subscriptions and the ability of Deepseek/Qwen to appear good at reasoning, even when imperfect.\nCPUs Cope? Coding Community Considers Capabilities: Some members are successfully running LLMs off CPU for specific tasks, provided the models aren't excessively large.\n\nWhile an Intel i3 user eyes an Nvidia card, others propose AMD options like the MI50 or 7900 XTX as cost-effective alternatives for text generation.\nMCP Servers Spark Stack Suggestions: Challenges plague MCP servers when paired with LM Studio due to their design, potentially leading to malformed requests and a subpar user experience.\n\nA suggestion arises to build a custom coherent stack for practical agent use, rather than relying on out-of-the-box MCP server functionality.\nGaming GPU Gauntlet: 4080 Faces Fallen Flagship: A user eyeing a 4080 for gaming is steered toward a used 3090 or 7900 XTX, sparking a debate on performance at different resolutions.\n\nWhile the 3090 excels at 4K gaming, the hypothetical 5070 Ti is projected to outpace both, and the conversation reveals that the user games more than uses AI, impacting the advice.\nApple Announcement Anticipation: M5 Macs Materialize?: Members speculate on the arrival of M5 Pro Macbook Pros, with rumors pointing to a launch event around the 28th.\n\nConcerns emerge about the memory bandwidth of M4 Pro, with suggestions it may not handle larger models, prompting discussion on the value and performance of M1 Ultra Mac Studios.\nLatent Space Discord\nRecursive Intelligence Eyes $4B Valuation: Recursive Intelligence is reportedly raising funds at a $4B valuation to accelerate chip design using AI, creating a self-improving loop between hardware and AI (Bloomberg Article).\n\nThe company focuses on improving chip design through AI, potentially reducing design time and enhancing performance.\nEngineer Lands Dream AI Job: An engineer outlined how to secure a role at a top AI lab by building a public track record through independent projects and participating in visible competitions (link).\n\nImproving upon existing peer-reviewed research and participating in visible competitions like the NanoGPT speed run were cited as good examples of demonstrating technical excellence, citing Keller Jordan as an example.\nBerkeley SkyLab Startups See Funding Boom: UC Berkeley Sky Lab startups, including SGLang at a 400m valuation, VLLM at 800m, and LMArena at 1.7B, achieved significant funding milestones in January 2026 (link).\n\nThis surge highlights investor confidence in the innovative AI projects emerging from academic research environments.\nAI Agents Auto-Code Browser Engine: FastRender, a new browser rendering engine, was developed using over 2,000 AI coding agents (link).\n\nThe conversation with Wilson Lin highlights the potential of AI to automate complex software development tasks, potentially revolutionizing browser technology.\nMicrosoft's Maia 200 Hits Azure: The Maia 200 AI accelerator is now live in Azure (link), offering 30% better performance per dollar and optimized specs like 216GB HBM3e and 7TB/s memory bandwidth.\n\nDesigned for high-performance inference, this custom chip supports large-scale AI workloads, making it a key component for demanding applications.\nHuggingFace Discord\nHuggingFace Spaces Throws a 503 Error: Users experienced pauses during Spaces docker builds and received a 503 error on restart, with many getting Something went wrong when restarting this Space errors (discuss.huggingface.co).\n\nIt seems like the underlying infrastructure issues were causing the spaces to become unresponsive, requiring manual intervention to resolve.\nVoltageGPU Volts Up Cheap GPUs: VoltageGPU.com is offering cheap GPUs for open-source AI models, with an NVIDIA GeForce RTX 5090 pod available at $0.53/hour.\n\nThey highlight the benefits of their advanced 32GB GDDR7, optimized for inference on HF-hosted models like Qwen3-32B, and are offering free credits for users to try their services.\nLayer-Native Safety Clamping Locks Down Jailbreaks: A new paper introduces Layer-Native Safety Clamping, an approach that clamps activations inside the model to prevent jailbreaks, and the team released a dataset of 10K pairs.\n\nThis approach learns harm directions in activation space and clamps any activation that projects too strongly, thus it cannot be bypassed via prompt manipulation; the paper can be found on Zenodo.\nGPU-64 Architecture Boosts LLM Inference: A new GPU architecture designed exclusively for inference, called GPU-64, was published, and the innovation involves a Hardware KV-Cache using on-chip CAM (Content-Addressable Memory).\n\nThe results show 4x faster inference at 75W (O(N) → O(1)), and the paper can be found on Zenodo while the RTL + Emulator are on GitHub.\nTesting and Deploying LLMs on LMStudio: Members recommend LMStudio for testing models due to its user-friendly GUI and search filters for HF and GH models and llama.cpp for single-user deployment.\n\nThey advised against using LMStudio for backend deployment, instead suggesting llama.cpp's llama-server in a docker container or vLLM's server for better scalability.\nGPU MODE Discord\nMLSys 2026 Hosts FlashInfer-Bench Kernel Competition: The MLSys 2026 FlashInfer-Bench Competition challenges participants to design LLM inference kernels for the latest NVIDIA Blackwell GPUs, competing against expert FlashInfer kernels, detailed at mlsys26.flashinfer.ai.\n\nGPU Mode also held internal competitions for faster kernels for the upcoming GPU architecture, the blogpost on Simon Veitner is located here.\nCornserve Deployed for Multimodal Models: A member shared Cornserve, an efficient online serving system for Any-to-Any multimodal models, detailed in a paper Cornserve.\n\nGPU Mode went online to discuss Cornserve: Easy, Fast and Scalable Multimodal AI (YouTube link).\nCommunity to train Kernel LLM: In 2026, GPU MODE is pushing further with training a Kernel LLM and using it to ship kernels in important repos like PyTorch and VLLM (gpumode.com/v2/news/gpumode-2026).\n\nThe community is collaborating with Prime Intellect, Modal, and Lambda, focusing on de-slopifying LLM-generated kernels, post-training a kernel LLM model, end-to-end competitions, and from-scratch repos.\nLeCun Logs on to Logical Intelligence: Yann LeCun launched a new startup called Logical Intelligence, focused on an Event Based Model (EBM).\n\nThe website only contains marketing material, job openings, and a link to the MLSys Conference.\nMindbeam Hires for Kernel Acceleration: Mindbeam AI, a small team focused on accelerating training for foundation models, is hiring a post training MLE and GPU Kernel MLE.\n\nInterested candidates can DM for a referral; job openings are listed here.\nEleuther Discord\nROCm runs rocky road race: Members debated the performance of ROCm for accelerated ML, pointing out its challenges stem from primary support for Nvidia, with one calling the experience 'batteries not included'.\n\nThey cited potential driver problems and long lead times as factors.\nDistinctionBench defends against data defense: The discussion of Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases pondered whether DistinctionBench might be used as a training target for language models.\n\nA member joked, 'all good evals are training targets ;)', but acknowledged that it is 'very contamination resistant' due to its endless representational variants.\nHybrid Architectures Halt Hallucinations?: The group investigated hybrid architectures combining LLMs with symbolic/deductive layers for hallucination reduction.\n\nWhile checking logical consistency is relatively easy for math, code, and simple facts (this paper), it remains challenging for other types of consistency (this paper).\nAttention Arrived Before Transformers Transformed: In Eleuther ▷ #general, attention mechanisms were in use on top of RNNs in 2014-2015, two years before the transformers were invented.\n\nMembers proposed that the slower adoption might be because fewer people were working in the field, and Kaggle results really catalyzed its widespread adoption.\nSymbolic Sanity Checks saves Sanity: Members debated whether LLMs with symbolic/deductive layers might reduce hallucinations by checking logical consistency, especially for code and math as shown in this paper.\n\nHowever, they noted that checking for other types of consistency remains challenging as shown in this paper.\nNous Research AI Discord\nExploring Agentic AI Self-Replication Benchmarks: A member proposed a self-replication benchmark for agentic AI, suggesting the agent should either download itself or retrain from scratch and adapt to a target machine.\n\nThey also suggested that adapting to a target machine, or even designing one, could be more engaging than simply using existing transformer libraries.\nLLM Worms Concept Emerges: A member jokingly suggested an LLM worm benchmark where an LLM is prompted with \""hey make more of you\"" and provided the tools to replicate itself using scripts and API keys.\n\nAnother member emphasized the importance of considering resource constraints like VRAM to make the challenge more practical and interesting.\nTrouble Brewing with MoE Run Dashboard: A member reported a 'Failed to fetch' error in the dashboard while monitoring the progress of an active MoE run (moe-10b-a1b-8k-wsd-lr3e4-1t).\n\nAnother member suggested waiting a few hours before checking again, implying a potential temporary issue.\nRaytracer Test Causes Local Models to Stumble: A member observed that local code models (suitable for a 5090) are struggling with a raytracer test from cpldcpu/llmbenchmark, with even recent models on lmarena failing.\n\nSpecifically, the smaller models often incorrectly generate the vector class, presenting a persistent challenge.\nSemantica Project Needs Helping Hands: A member introduced Semantica, an open-source project building semantic infrastructure for domain-grounded AI, including knowledge graphs, ontologies, and reasoning layers, and is actively seeking contributors.\n\nThey are looking for contributions in areas such as ontology & schema design, knowledge graph modeling, and LLM + symbolic / rule-based reasoning, and even small PRs, feedback, design discussions and issues are all welcome.\nYannick Kilcher Discord\nEBMs Spark Debate vs. Classical Feedforward: A discussion comparing Energy-Based Models (EBMs) and classical feedforward networks debates whether EBMs are inherently superior, especially regarding Shannon entropy or Kolmogorov complexity.\n\nIt was suggested that validation is easier than generation in EBMs, relating it to computational complexity theory (P vs NP), while emphasizing the need for a well-defined loss landscape for EBM optimization to work effectively.\nLLM Pre-training: Domain-Specific vs. Foundational Faceoff: A member inquired about the effectiveness of continued pre-training a foundational LLM (specifically OLMO-7B) for a domain-specific task like cheminformatics using the ZINC20 dataset.\n\nThe goal is to compare results against a domain-specific transformer model, but no specific answers or resources were provided.\nMCMC Sampling Suffers Mode-Switching Struggles: Concerns were raised about the ability of MCMC to traverse between spatially separated modes when dimension increases, referencing this paper.\n\nOne member argues that MCMC tries to emulate flow models due to the latter's superiority, while EBMs, contrarily, attempt to make NNs more like MCMC.\nZKPs: Crypto Signing or Network Traffic Savior?: Discussion covered using zero-knowledge proofs (ZKPs) for verifying encrypted network traffic and matrix multiplications, citing a Gemini correspondence for a matrix low knowledge proof.\n\nWhile one member proposed a use case in zero-knowledge “made by humans” proofs, another member questioned the practicality of ZKPs, suggesting breaking the encryption might be cheaper.\nLLMs Cyber Skills Face Scrutiny: A member questioned whether LLMs could develop strong cyber capabilities, referencing a GPTZero article.\n\nAnother member doubted LLM companies' ability to address internal vulnerabilities, suggesting they fix those before pursuing cyber skills, also citing a ScienceAlert article and a tweet.\ntinygrad (George Hotz) Discord\nLuminal Finds Flash Attention via Bruteforce: Luminal is claiming to find flash attention using bruteforce on an egraph, taking hours to find, and they explicitly added exp(x - new_max) = exp(x - old_max) × exp(old_max - new_max) as a rewrite rule.\n\nThe poster reproduced the graphviz shown in the presentations from commit 0bd3b80c, noting that their minimal set of rewrite rules could transform a naive attention kernel graph into the known flash attention kernel graph in 52s on a 9800x3d.\nMetal Textures Trounce Buffers for Blurring: Profiling access speed on Metal using Tensor with size 512/1024/2048/8192 images as input for a 3/5/7 sized blur kernel showed textures outperforming buffers.\n\nIt might be worth throwing in a branching condition depending on the size of the buffer input, tests results are attached.\nTenstorrent Backend Triumphs in Ops Tests: The Tenstorrent backend is passing all ops tests on wormhole or blackhole and there is a $1k bounty for this milestone.\n\nSomeone asked if the bounty requires all test ops test passing on testorrent hardware.\nAnthropic VLIW Challenge PR Makes Waves: A member submitted [a PR](https://github.com/tinygrad/t..."",""content"":""**Anthropic** has officially absorbed the independent MCP UI project and, collaborating with **OpenAI**, **Block**, **VS Code**, **Antigravity**, **JetBrains**, and **AWS**, released the **MCP Apps spec** and official support in **Claude.ai**. This standard aims to enable a rich ecosystem of interoperable applications with rich UI, addressing the proliferation of subscription services. Meanwhile, **NVIDIA** introduced **ToolOrchestra** with an **8B orchestrator** model trained via scalable reinforcement learning for efficient agent orchestration. The concept of Recursive Language Models (RLMs) is gaining traction for efficient context management in agent stacks. The “Clawdbot” UX pattern emphasizes outcome-first assistant design with tight context and tool integration, sparking security concerns around prompt injection. **Alibaba** launched **Qwen3-Max-Thinking**, a flagship reasoning and agent model with adaptive tool use and strong benchmark scores, now available in public evaluation platforms like LM Arena and Yupp."",""contentSnippet"":""**Anthropic** has officially absorbed the independent MCP UI project and, collaborating with **OpenAI**, **Block**, **VS Code**, **Antigravity**, **JetBrains**, and **AWS**, released the **MCP Apps spec** and official support in **Claude.ai**. This standard aims to enable a rich ecosystem of interoperable applications with rich UI, addressing the proliferation of subscription services. Meanwhile, **NVIDIA** introduced **ToolOrchestra** with an **8B orchestrator** model trained via scalable reinforcement learning for efficient agent orchestration. The concept of Recursive Language Models (RLMs) is gaining traction for efficient context management in agent stacks. The “Clawdbot” UX pattern emphasizes outcome-first assistant design with tight context and tool integration, sparking security concerns around prompt injection. **Alibaba** launched **Qwen3-Max-Thinking**, a flagship reasoning and agent model with adaptive tool use and strong benchmark scores, now available in public evaluation platforms like LM Arena and Yupp."",""guid"":""https://news.smol.ai/issues/26-01-26-mcp-apps/"",""categories"":[""anthropic"",""openai"",""block"",""vs-code"",""antigravity"",""jetbrains"",""aws"",""nvidia"",""alibaba"",""claude-ai"",""claude-ai"",""toolorchestra-8b"",""qwen3-max-thinking"",""agent-orchestration"",""reinforcement-learning"",""recursive-language-models"",""context-management"",""user-experience"",""security"",""prompt-injection"",""reasoning"",""adaptive-tool-use"",""model-evaluation"",""benchmarking""],""isoDate"":""2026-01-26T05:44:39.000Z""}"
Smol,not much happened today,https://news.smol.ai/issues/26-01-22-not-much/,2026-01-22T05:44:39.000Z,"<p><strong>a quiet day</strong></p>
<blockquote>
<p>AI News for 1/22/2026-1/23/2026. We checked 12 subreddits, <a href=""https://twitter.com/i/lists/1585430245762441216""><strong>544</strong> Twitters</a> and <strong>24</strong> Discords (<strong>206</strong> channels, and <strong>7161</strong> messages) for you. Estimated reading time saved (at 200wpm): <strong>579 minutes</strong>. <strong>Our new website</strong> is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on <a href=""https://x.com/Smol_AI"">@smol_ai</a>!</p>
</blockquote>
<hr>
<h1>AI Twitter Recap</h1>
<p><strong>Top tweets (by engagement)</strong></p>
<ul>
<li><strong>Anthropic ships “Claude in Excel”</strong>: Claude in Excel expands to Pro, with multi-file drag/drop, safer cell writes, and longer sessions via auto-compaction (<a href=""https://twitter.com/claudeai/status/2014834616889475508"">claudeai</a>). Big engagement discussion about Microsoft 365 Copilot lagging (<a href=""https://twitter.com/Yuchenj_UW/status/2014835455393726726"">Yuchenj_UW</a>).</li>
<li><strong>OpenAI roadmap + agent loop</strong>: Sam Altman says Codex launches are coming and OpenAI is nearing a “Cybersecurity High” level with restrictions and later “defensive acceleration” (<a href=""https://twitter.com/sama/status/2014733975755817267"">sama</a>). OpenAI publishes a technical deep dive into the <strong>Codex agent loop / harness orchestration</strong> (<a href=""https://twitter.com/OpenAIDevs/status/2014794871962533970"">OpenAIDevs</a>).</li>
<li><strong>Google AI Ultra limits boosted</strong>: Gemini App daily quotas increased to <strong>1,500 Thinking</strong> + <strong>500 Pro</strong> prompts/day for Ultra members (<a href=""https://twitter.com/joshwoodward/status/2014566936479437173"">joshwoodward</a>).</li>
<li><strong>Sakana AI ↔ Google partnership + investment</strong>: Sakana announces strategic partnership and funding from Google to combine <strong>Gemini/Gemma</strong> with Sakana’s “AI Scientist” / “ALE-Agent” work and to deploy in high-security domains in Japan (<a href=""https://twitter.com/SakanaAILabs/status/2014686043711406355"">SakanaAILabs</a>, <a href=""https://twitter.com/hardmaru/status/2014686852691918971"">hardmaru</a>, <a href=""https://twitter.com/JeffDean/status/2014716109216448975"">JeffDean</a>).</li>
<li><strong>Cursor launches Agent Skills</strong>: First-class “Skills” for agents, emphasizing discovery + dynamic context focus (<a href=""https://twitter.com/cursor_ai/status/2014753596223770841"">cursor_ai</a>).</li>
<li><strong>FrontierMath jump</strong>: <strong>GPT-5.2 Pro hits 31% on FrontierMath Tier 4</strong>, up from 19% previous best (<a href=""https://twitter.com/EpochAIResearch/status/2014769359747744200"">EpochAIResearch</a>). Practitioners highlight usefulness and even benchmark issue-spotting (<a href=""https://twitter.com/gdb/status/2014859263701839963"">gdb</a>).</li>
<li><strong>Claude Code “run locally for free” how-to</strong>: A popular tutorial claims running Claude Code-like workflows locally with open models, private + tool-enabled (<a href=""https://twitter.com/dr_cintas/status/2014771670070747278"">dr_cintas</a>).</li>
<li><strong>Baseten raises $300M</strong> at <strong>$5B valuation</strong>, positioning around the “many-model future” and high-performance inference (<a href=""https://twitter.com/basetenco/status/2014755013344792595"">basetenco</a>, <a href=""https://twitter.com/tuhinone/status/2014755252244005273"">tuhinone</a>).</li>
</ul>
<hr>
<p><strong>Frontier models, benchmarks, and the “capability” narrative</strong></p>
<ul>
<li><strong>Math as a leading indicator (FrontierMath + cross-benchmark correlations)</strong>: Epoch reports <strong>GPT-5.2 Pro = 31%</strong> on FrontierMath Tier 4 (no overfitting claimed), a sizable step up from 19% (<a href=""https://twitter.com/EpochAIResearch/status/2014769359747744200"">EpochAIResearch</a>). Separate Epoch analysis argues benchmark scores correlate strongly across domains (≈<strong>0.68</strong> across domains, ≈<strong>0.79</strong> within-domain), implying a latent capability factor behind “math/coding/reasoning” progress (<a href=""https://twitter.com/EpochAIResearch/status/2014806095504785664"">EpochAIResearch</a>). Practitioners note concrete value: catching problem flaws/typos and even “pointing out a flaw” in a Tier 4 problem (<a href=""https://twitter.com/gdb/status/2014859263701839963"">gdb</a>, <a href=""https://twitter.com/GregHBurnham/status/2014774878591655984"">GregHBurnham</a>).</li>
<li><strong>AGI timelines vs product reality</strong>: A recurring theme is “systems are uneven”: smart in formal domains, unreliable elsewhere. A widely shared quip captures this mismatch (“smarter than a PhD in math, dumber than an intern”) (<a href=""https://twitter.com/Yuchenj_UW/status/2014564105194242452"">Yuchenj_UW</a>). François Chollet stresses that progress is <strong>vertical-specific</strong> (especially in verifiable domains like code) because unlimited synthetic data makes memorization/operationalization easier there, and warns against extrapolating to all human tasks (<a href=""https://twitter.com/fchollet/status/2014821042464948270"">fchollet</a>).</li>
<li><strong>Reasoning + continual learning as the “real” frontier</strong>: Reporting from interviews claims Shane Legg pegs <strong>50% chance of “minimal AGI” by 2028</strong> with Google’s definition including continuous learning/memory/world models (<a href=""https://twitter.com/kimmonismus/status/2014697026890416586"">kimmonismus</a>). Follow-on notes say Demis Hassabis explicitly says DeepMind <strong>has not solved continual learning</strong>, and is exploring combinations of AlphaZero-like approaches with foundation models (<a href=""https://twitter.com/Yuchenj_UW/status/2014785682309579119"">Yuchenj_UW</a>, <a href=""https://twitter.com/Hangsiin/status/2014774897680253442"">Hangsiin</a>).</li>
<li><strong>Model/arch discourse: MoE provenance fights</strong>: A thread disputes the claim that DeepSeek MoE “built on Mixtral,” arguing the DeepSeek MoE paper appeared almost immediately after Mixtral’s arXiv release, Mixtral training details were sparse, and DeepSeek’s MoE is architecturally different/more sparse and cites <strong>GShard</strong> not Mixtral (<a href=""https://twitter.com/eliebakouch/status/2014575628675092845"">eliebakouch</a>). Another framing calls DeepSeek a distinct “neoMoE” tree vs “oldMoE” (<a href=""https://twitter.com/kalomaze/status/2014659449219383367"">kalomaze</a>).</li>
<li><strong>Second-tier multimodal updates (China)</strong>: A detailed Chinese-language review positions <strong>Baidu ERNIE 5.0</strong> as improved and stable but still costly (2T params, ~61K context) and “firmly second tier” vs top multimodal systems with large compute budgets (<a href=""https://twitter.com/ZhihuFrontier/status/2014606592826912840"">ZhihuFrontier</a>).</li>
</ul>
<hr>
<p><strong>Agents and coding: from workflows → harnesses → skills</strong></p>
<ul>
<li><strong>OpenAI Codex “agent loop” becomes explicit</strong>: OpenAI publishes how Codex orchestrates turns: assemble inputs → run inference → execute tools → feed results back until stopping, i.e., the agent harness as a first-class system component (<a href=""https://twitter.com/OpenAIDevs/status/2014794871962533970"">OpenAIDevs</a>). This aligns with broader commentary that “training better models is only one axis; harness + experimentation can surprise” (<a href=""https://twitter.com/Hangsiin/status/2014794375466033657"">Hangsiin</a>).</li>
<li><strong>Workflows vs agents is collapsing into “skills / guidance / RLMs”</strong>: A strong technical synthesis argues the agent/workflow boundary is less about “control flow in code” and more about <strong>state representation</strong>, <strong>dynamic instruction selection</strong>, and <strong>who dictates composition</strong>—with Replit’s “Decision-Time Guidance,” Skills, and <strong>Recursive Language Models (RLMs)</strong> as hybrid points on the design spectrum (<a href=""https://twitter.com/lateinteraction/status/2014685012994515206"">lateinteraction</a>). DSPy community posts push RLMs as a practical method for “arbitrarily long prompts” by delegating to code + subcalls instead of summarization loss (<a href=""https://twitter.com/getpy/status/2014717862246756384"">getpy</a>).</li>
<li><strong>Cursor: Agent Skills shipped</strong>: Cursor introduces <strong>Skills</strong> as discoverable specialized prompts/code and pitches them as also improving context focus via dynamic discovery (<a href=""https://twitter.com/cursor_ai/status/2014753596223770841"">cursor_ai</a>, <a href=""https://twitter.com/cursor_ai/status/2014753597624598665"">cursor_ai</a>). This is echoed by the broader market trend: “make non-devs write code by calling it skills” (<a href=""https://twitter.com/kylebrussell/status/2014689618617122883"">kylebrussell</a>).</li>
<li><strong>Claude Code ecosystem keeps expanding (and copying wars)</strong>: Multiple posts highlight rapid feature diffusion between tools (“cursor adopting popular features from claude code”) (<a href=""https://twitter.com/dejavucoder/status/2014635509025526198"">dejavucoder</a>). Practical snippets: Claude tasks stored on filesystem (<code>~/.claude/tasks</code>) enabling multi-session/subagent collaboration via broadcasts (<a href=""https://twitter.com/dejavucoder/status/2014584272183861407"">dejavucoder</a>). At the same time, pain points remain (e.g., absurd file download hacks via base64) (<a href=""https://twitter.com/dbreunig/status/2014540341526069738"">dbreunig</a>).</li>
<li><strong>Security posture is becoming a headline feature</strong>: Sam Altman states OpenAI will increasingly constrain coding models for cybercrime and later pivot to <strong>defensive acceleration</strong> (helping patch bugs) as mitigation (<a href=""https://twitter.com/sama/status/2014733975755817267"">sama</a>). One anecdote flags a potential security footgun: Codex Slack integration produced shareable task links accessible without auth in incognito (if accurate, it’s an urgent product-security issue) (<a href=""https://twitter.com/apsdehal/status/2014770563810758938"">apsdehal</a>).</li>
<li><strong>Enterprise “agents fail in production” reminder</strong>: A long post claims <strong>95% of enterprise AI pilots fail</strong> (citing MIT research), emphasizing that production viability is about <strong>authorization-aware retrieval</strong>, <strong>guardrails</strong>, <strong>monitoring</strong>, and <strong>auditability</strong>, not demo capability (<a href=""https://twitter.com/victorialslocum/status/2014654495301525683"">victorialslocum</a>).</li>
</ul>
<hr>
<p><strong>Inference + systems: vLLM, KV compression, storage, and infra maturity</strong></p>
<ul>
<li><strong>vLLM keeps becoming the open inference “substrate”</strong>: vLLM positions itself as the bridge from open models to deployable inference, highlighting vLLM Studio workflows (<a href=""https://twitter.com/vllm_project/status/2014536660361584833"">vllm_project</a>). A notable infra-engineering post documents a difficult <strong>vLLM memory leak</strong> debugging path (Python profilers → pmap → BPFtrace → GDB) traced to <strong>UCX mmap hooks</strong>; fix merged upstream (<a href=""https://twitter.com/vllm_project/status/2014630499231412477"">vllm_project</a>).</li>
<li><strong>System intelligence / routing</strong>: vLLM announces a public beta of <strong>vLLM-SR</strong> (Semantic Router) on AMD, framing it as a “system intelligence” approach rather than monolithic models doing everything (<a href=""https://twitter.com/XunzhuoLiu/status/2014672307407704279"">XunzhuoLiu</a>).</li>
<li><strong>KV cache compression via distillation</strong>: NVIDIA Research releases <strong>Qwen3-8B-DMS-8x</strong>, claiming <strong>8× KV cache compression</strong> with minimal overhead and only ~1K fine-tuning steps, outperforming token-importance eviction proxies; compatible with sparse attention methods (<a href=""https://twitter.com/p_nawrot/status/2014770473289019709"">p_nawrot</a>).</li>
<li><strong>Tooling for predictable deployment</strong>: <code>hf-mem</code> estimates inference VRAM from Safetensors metadata without downloading weights, aiming to eliminate trial/OOM loops (<a href=""https://twitter.com/LiorOnAI/status/2014730309128855801"">LiorOnAI</a>).</li>
<li><strong>Storage and data-plane attention</strong>: SkyPilot pushes “Volumes” for high-performance storage (AI checkpoints/data) as object stores aren’t always fit (<a href=""https://twitter.com/skypilot_org/status/2014752751545381044"">skypilot_org</a>). Jina proposes a neat compression trick: convert embeddings to spherical coordinates pre-compression, claiming near-lossless reconstruction below float32 epsilon and ~1.5× storage savings (<a href=""https://twitter.com/JinaAI_/status/2014753001387499927"">JinaAI_</a>).</li>
<li><strong>GPU kernel evaluation for agents</strong>: AMD AGI releases <strong>Magpie</strong>, an open-source kernel eval suite for correctness + performance across AMD/NVIDIA, designed for agent workflows; claims <strong>3000× token efficiency</strong> vs using GPU profilers alone, and plans tracing integrations with SGLang/vLLM (<a href=""https://twitter.com/realSharonZhou/status/2014722290865549649"">realSharonZhou</a>). MLSys 2026 launches FlashInfer-Bench contest tracks (MoE/DSA/GDN) with separate human vs agent evaluation (<a href=""https://twitter.com/ye_combinator/status/2014836302198472789"">ye_combinator</a>).</li>
</ul>
<hr>
<p><strong>Ecosystem + business: partnerships, pricing, and “value-sharing” debates</strong></p>
<ul>
<li><strong>Sakana AI ↔ Google: strategic partnership + funding (and controversy)</strong>: Sakana frames the deal as combining Google infra/models (Gemini/Gemma) with Sakana’s research automation (AI Scientist, ALE-Agent) and pushing deployments in mission-critical domains requiring security/data sovereignty (<a href=""https://twitter.com/SakanaAILabs/status/2014686043711406355"">SakanaAILabs</a>). Media echoes it (Nikkei/Bloomberg/etc.) (<a href=""https://twitter.com/nikkei/status/2014637546563658172"">nikkei</a>, <a href=""https://twitter.com/business/status/2014594583234027753"">business</a>). A dispute emerges: one claim says it’s a small Google Cloud Japan compute deal and “DeepMind not involved” (<a href=""https://twitter.com/shaneguML/status/2014847946110783649"">shaneguML</a>), while Sakana leadership publicly counters that DeepMind is indeed involved and tags Demis/Jeff Dean (<a href=""https://twitter.com/hardmaru/status/2014885853789884416"">hardmaru</a>).</li>
<li><strong>Baseten’s “many-model future” + fundraising</strong>: Baseten raises <strong>$300M at $5B</strong> and argues inference is the bottleneck enabling millions of specialized models and reliable low-latency UX (<a href=""https://twitter.com/basetenco/status/2014755013344792595"">basetenco</a>, <a href=""https://twitter.com/tuhinone/status/2014755252244005273"">tuhinone</a>).</li>
<li><strong>Anthropic economics: inference cost pressure</strong>: A report says Anthropic cut 2025 gross margin outlook to <strong>40%</strong> due to inference costs <strong>23% higher than expected</strong>, despite projected <strong>$4.5B revenue</strong> (~12× YoY) (<a href=""https://twitter.com/kimmonismus/status/2014673235594641838"">kimmonismus</a>).</li>
<li><strong>“Value-sharing” model for AI-enabled discoveries</strong>: Reporting claims OpenAI’s CFO discussed deals taking a cut of customer profits/IP (starting with drug discovery), akin to Isomorphic Labs’ model (<a href=""https://twitter.com/kimmonismus/status/2014643034089259103"">kimmonismus</a>). Some push back on sensationalism and incentives (e.g., you can’t both sell tokens and own discoveries without also eating compute cost) (<a href=""https://twitter.com/code_star/status/2014541663356772516"">code_star</a>, <a href=""https://twitter.com/paul_cal/status/2014692633730261339"">paul_cal</a>).</li>
</ul>
<hr>
<p><strong>Multimodal + voice + video: quality leaps and tooling</strong></p>
<ul>
<li><strong>Voice is accelerating (open + low-latency)</strong>: Teknium claims an open voice cloning HF demo is the closest to ElevenLabs quality they’ve seen in open models (<a href=""https://twitter.com/Teknium/status/2014687269329031253"">Teknium</a>). NVIDIA releases <strong>PersonaPlex</strong>, an open-source real-time full-duplex conversational voice stack optimized for very low latency (<a href=""https://twitter.com/kimmonismus/status/2014703479491854751"">kimmonismus</a>).</li>
<li><strong>Video generation: controllability and arenas</strong>: Runway Gen-4.5 I2V adds more precise “zoom into specified regions” control (<a href=""https://twitter.com/c_valenzuelab/status/2014674372120785176"">c_valenzuelab</a>) and creators showcase short-film workflows (<a href=""https://twitter.com/Artedeingenio/status/2014693398502842731"">Artedeingenio</a>). LMSYS Arena launches/expands <strong>Video Arena</strong> leaderboards (Veo, Sora 2, Kling, Hailuo, etc.) (<a href=""https://twitter.com/arena/status/2014815916056576257"">arena</a>).</li>
<li><strong>3D agents and world models for interactive environments</strong>: Berkeley demo “VIGA” claims a multimodal agent that generates 3D/4D Blender scenes from images with no training (<a href=""https://twitter.com/HavenFeng/status/2014765400563781777"">HavenFeng</a>). A smaller “world model you can play” demo appears on HF (Waypoint-1-Small, 2.3B) (<a href=""https://twitter.com/victormustar/status/2014766391811826022"">victormustar</a>). Separately, “world models are next big wave for gaming/robotics” sentiment resurfaces (<a href=""https://twitter.com/kylebrussell/status/2014529425983914098"">kylebrussell</a>).</li>
</ul>
<hr>
<p><strong>Security, trust, and integrity issues in the AI social layer</strong></p>
<ul>
<li><strong>Account compromises targeting AI insiders</strong>: Multiple warnings indicate prominent accounts (Deedy Das; a Kimi researcher/“Crystal”) were hacked and used for phishing/scams, likely crypto-driven (<a href=""https://twitter.com/cloneofsimo/status/2014536638010163262"">cloneofsimo</a>, <a href=""https://twitter.com/ml_angelopoulos/status/2014543018137944486"">ml_angelopoulos</a>, <a href=""https://twitter.com/Kimi_Moonshot/status/2014571513299796154"">Kimi_Moonshot</a>, <a href=""https://twitter.com/Yuchenj_UW/status/2014572557270450194"">Yuchenj_UW</a>).</li>
<li><strong>Misinformation / fake papers</strong>: A fake “llama 4” arXiv paper is flagged as not actually Meta-authored (<a href=""https://twitter.com/TimDarcet/status/2014626676798366006"">TimDarcet</a>).</li>
<li><strong>Open-source “layers” framing</strong>: A practical taxonomy distinguishes <strong>open code</strong> vs <strong>open weights</strong> vs <strong>open training pipeline</strong> (data + recipes + reproducibility), arguing teams must decide which layer they truly need (<a href=""https://twitter.com/TheTuringPost/status/2014630341349408928"">TheTuringPost</a>).</li>
</ul>
<hr>
<h1>AI Reddit Recap</h1>
<h2>/r/LocalLlama + /r/localLLM Recap</h2>
<h3>1. Qwen3-TTS Model Release and Discussion</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/"">Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &#x26; 1.8B), Support for 10 languages</a></strong> (Activity: 880): <strong><strong>Qwen</strong> has open-sourced the full family of <strong>Qwen3-TTS</strong> models, which includes VoiceDesign, CustomVoice, and Base models, with sizes of <code>0.6B</code> and <code>1.8B</code> parameters. These models support <code>10 languages</code> and are designed for tasks such as Voice Clone, Voice Design, and Custom Voice. The image provides a comparison chart of these models against others like MiniMax and SeedTTS, highlighting their performance across various metrics, where lower values indicate better performance. The models are available on <a href=""https://github.com/QwenLM/Qwen3-TTS"">GitHub</a> and <a href=""https://huggingface.co/collections/Qwen/qwen3-tts"">Hugging Face</a>, with a <a href=""https://qwen.ai/blog?id=qwen3tts-0115"">blog post</a> and <a href=""https://github.com/QwenLM/Qwen3-TTS/blob/main/assets/Qwen3_TTS.pdf"">paper</a> detailing their capabilities.</strong> Commenters appreciate the open-source release but express concerns about the models' dependency on Python and Nvidia GPUs, suggesting a need for support in other languages and platforms like llama.cpp or mistral.rs for broader accessibility.</p>
<ul>
<li><strong>FullstackSensei</strong> raises a technical concern about the current limitations of running Qwen models, highlighting the need for support in environments like <code>llama.cpp</code> or <code>mistral.rs</code> that can leverage GPU inference beyond just CUDA. This is particularly relevant given the rising costs of hardware and the desire for more accessible deployment options beyond Python and Nvidia GPUs.</li>
<li><strong>LetterRip</strong> comments on the English voice outputs of Qwen3-TTS, noting that they seem to be influenced by Japanese Anime dubs. This suggests a potential bias in the training data, which could affect the naturalness and authenticity of the generated voices in English, especially if the training set was not diverse enough.</li>
<li><strong>silenceimpaired</strong> discusses the performance of Qwen3-TTS, noting that while the samples are impressive, there is a concern about the frequency of certain outputs. This implies that while the model can produce high-quality audio, there might be consistency issues that need addressing to ensure reliable performance across different use cases.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/"">Qwen dev on Twitter!!</a></strong> (Activity: 833): <strong>The image is a Twitter post by <strong>Chen Cheng</strong>, announcing a new model with the tagline ""Tiny model. Big personality"" and a countdown, indicating an imminent release. The comments suggest that this might be related to a TTS (Text-to-Speech) model from a previous vLLM leak, with a link to a <a href=""https://huggingface.co/collections/Qwen/qwen3-tts"">Hugging Face collection</a> that might be relevant. This suggests a new development in the field of TTS models, potentially offering significant improvements or features.</strong> One comment humorously suggests that the new model might finally justify the investment in a high-end GPU like the <code>5090</code>, indicating high expectations for the model's performance.</p>
<ul>
<li>ThePixelHunter discusses the current landscape of model sizes, noting that while smaller models are more accessible for local training on single GPUs, there is a lack of competition in the 50-120 billion parameter range. This range is ideal for enthusiasts with multiple high-end GPUs, such as a couple of 3090s or three 16GB cards, suggesting a gap in the market for larger, yet still locally trainable models.</li>
</ul>
</li>
</ul>
<h3>2. Local LLM Development and Hardware Considerations</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLM/comments/1qkudvz/i_gave_my_local_llm_pipeline_a_brain_now_it/"">I gave my local LLM pipeline a brain - now it thinks before it speaks</a></strong> (Activity: 65): <strong>The post discusses a significant update to a local LLM pipeline named Jarvis, soon to be called TRION, which now incorporates a self-developed Sequential Thinking MCP (Multi-Component Processor). This system, built with <strong>Ollama</strong>, <strong>DeepSeek-R1</strong>, and custom MCP servers, allows the AI to ""think out loud"" by breaking down complex questions into step-by-step reasoning, significantly reducing hallucinations. The AI dynamically decides when to use this deep thinking approach, providing instant answers for simple questions and detailed reasoning for complex ones. The project leverages a CIM (Causal Intelligence Module) framework developed by <strong>u/frank_brsrk</strong>. The implementation is open-source and available on <a href=""https://github.com/danny094/Jarvis/tree/main"">GitHub</a>.</strong> Commenters appreciate the open-source nature of the project and express interest in experimenting with it. There is a sentiment that local LLMs will become more important as reliance on centralized AI providers diminishes.</p>
<ul>
<li>GCoderDCoder discusses the integration of local LLMs with tools like 'roo code', 'vibe kanban', and 'MCPs' to automate workflows and reduce manual coding efforts. They highlight the importance of local LLMs in the context of increasing reliance on AI, contrasting it with commercial solutions like Anthropic's offerings. This reflects a broader trend towards developing independent, open-source AI solutions to maintain control and flexibility.</li>
<li>burn-n-die inquires about the system configuration used for running the local LLM pipeline. This is a critical aspect for technical readers interested in replicating or understanding the performance and scalability of such a setup. Details on hardware specifications, software environment, and any optimizations would be valuable for those looking to implement similar systems.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLM/comments/1qjlzqt/someone_is_selling_a_lamda_labs_workstation_with/"">Someone is selling a Lamda Labs workstation with 4× RTX 2080 Ti => 4 x 11GB => 44GB VRAM. Is this machine well-supported by open source models? Is it fast enough?</a></strong> (Activity: 107): <strong>A Lambda Labs workstation with 4× RTX 2080 Ti GPUs, totaling <code>44GB VRAM</code>, is being considered for purchase at <code>$2000</code>. This setup is generally well-supported by open-source machine learning frameworks, and the <code>44GB VRAM</code> is sufficient for most tasks. However, setting up the system properly could be challenging. An alternative suggestion is to build a 2x RTX 3090 rig, which might offer better performance and cost around <code>$2.5k</code>. The workstation is deemed capable of handling many open-weight LLM models, especially if it includes at least <code>32GB of system RAM</code>. The machine is suitable for exploring a wide range of machine learning projects, although not all require extensive VRAM.</strong> There is a debate on whether to purchase the existing setup or build a new one with newer GPUs like the RTX 3090. Some argue that the existing setup is sufficient for learning and exploring ML models, while others suggest building a new rig for better performance and learning experience.</p>
<ul>
<li>The workstation with 4× RTX 2080 Ti GPUs, totaling 44GB VRAM, is generally well-supported by most open-source frameworks. However, setting it up correctly can be challenging. The system's capability is sufficient for exploring many open-weight LLM models, especially if it includes at least 32GB of system RAM, which enhances its potential for various machine learning tasks.</li>
<li>A technical consideration is that Turing generation GPUs, like the RTX 2080 Ti, were initially thought to be incompatible with FlashAttention 2. However, recent updates indicate that this limitation has been addressed, as noted in the <a href=""https://github.com/egaoharu-kensei/flash-attention-triton"">FlashAttention Triton GitHub repository</a>. This expands the utility of the workstation for certain advanced ML tasks.</li>
<li>While the 4× RTX 2080 Ti setup is capable, some suggest that building a new system with 2× RTX 3090 GPUs might offer better performance and value. The RTX 3090 provides more VRAM per card and improved performance, potentially making it a more future-proof investment for machine learning projects.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qkiylw/openai_cfo_hinting_at_outcomebased_pricing_aka/"">OpenAI CFO hinting at ""Outcome-Based Pricing"" (aka royalties on your work)? Makes the case for local even stronger.</a></strong> (Activity: 419): <strong><strong>OpenAI's CFO, Sarah Friar</strong>, discussed a potential shift towards ""outcome-based pricing"" for large enterprise deals, particularly in high-value industries like pharmaceuticals. This model would involve OpenAI taking a share of the value created by their AI, such as a cut from a pharmaceutical company's profits if AI contributes to a major discovery. This approach is not intended for regular users or indie developers, and the initial reports suggesting a broader application were misleading. The concept raises discussions about the benefits of local AI deployment versus reliance on cloud-based services, drawing parallels to the energy sector's grid versus solar power debate.</strong> Commenters highlight skepticism about OpenAI's potential pricing model, comparing it to the lack of royalties paid to data creators used in AI training. The analogy of local AI deployment to solar power is appreciated, emphasizing control over infrastructure to avoid future costs tied to value-based pricing.</p>
<ul>
<li>The discussion highlights concerns about OpenAI's potential shift to 'Outcome-Based Pricing', which could involve royalties based on the revenue generated from using their models. This is compared to the current model where users pay based on usage, akin to how electricity is billed. The analogy suggests that such a pricing model could drive users to consider local or self-hosted solutions, especially as OpenAI's profitability grows and they seek higher profits.</li>
<li>The comment by WeMetOnTheMountain critiques the efficiency of OpenAI's models, suggesting that they consume a large number of tokens to maintain performance, which results in slower processing speeds. The commenter argues that alternative models like GLM or mini Max could potentially offer better results when implemented in a 'one loop dialectical circuit', indicating a preference for more efficient, possibly self-hosted, solutions.</li>
<li>Winter_Educator_2496 emphasizes the need for open-source alternatives to OpenAI's models, which could be hosted in the cloud but also switched to local hosting if necessary. This reflects a broader sentiment for more control and flexibility over AI tools, especially in light of potential pricing changes and the desire to avoid dependency on a single provider.</li>
</ul>
</li>
</ul>
<h3>3. Hugging Face Model Releases and Trends</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLM/comments/1qjqhja/this_weeks_hottest_hugging_face_releases_top/"">This Week's Hottest Hugging Face Releases: Top Picks by Category!</a></strong> (Activity: 49): <strong><strong>Hugging Face</strong> has released several trending models across different categories this week. In text generation, the <code>zai-org/GLM-4.7-Flash</code> model, with <code>31B</code> parameters, is designed for fast and efficient text generation, boasting <code>124k</code> downloads. Its quantized counterpart, <code>unsloth/GLM-4.7-Flash-GGUF</code>, offers a <code>30B</code> parameter model optimized for local inference with <code>112k</code> downloads. In the image/multimodal category, <code>zai-org/GLM-Image</code> and <code>google/translategemma-4b-it</code> are notable for their capabilities in creative edits and multilingual tasks, respectively. For audio/speech, <code>kyutai/pocket-tts</code> and <code>microsoft/VibeVoice-ASR</code> provide compact TTS and multilingual ASR solutions. Other notable releases include <code>Lightricks/LTX-2</code> for image-to-video conversion and <code>stepfun-ai/Step3-VL-10B</code> for advanced reasoning in image-text-to-text tasks.</strong> A technical debate has emerged regarding the performance of the <code>GLM-4.7 30B-A3B</code> model compared to the <code>Qwen3-Coder 30B-A3B</code> for programming tasks, with some users finding the latter superior.</p>
<ul>
<li>A user compared the GLM-4.7 30B-A3B model to the Qwen3-Coder 30B-A3B model, noting that the latter performs better for programming tasks. This suggests that Qwen3-Coder may have optimizations or architectural advantages that make it more suitable for code-related applications. Further benchmarks or detailed evaluations would be needed to substantiate this claim and understand the specific areas where Qwen3-Coder excels.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLM/comments/1qk9ked/good_local_llm_for_coding/"">Good local LLM for coding?</a></strong> (Activity: 62): <strong>The user is seeking a local LLM for coding that can run on an <code>rx 6750 xt</code> GPU with <code>12GB</code> VRAM, considering models like <strong>GLM 4.7 flash</strong>. However, concerns about VRAM limitations suggest that <code>30B</code> parameter models, even when quantized to <code>q4</code>, may exceed the GPU's capacity. Recommendations include models like <a href=""https://huggingface.co/TIGER-Lab/VisCoder2-7B"">VisCoder2-7B</a>, <a href=""https://huggingface.co/openai/gpt-oss-20b"">gpt-oss-20b</a>, and <a href=""https://huggingface.co/NousResearch/NousCoder-14B"">NousCoder-14B</a>, with <strong>gpt-oss-20b</strong> noted for its speed and reliability despite being heavily censored. It's suggested to use models under <code>10B</code> parameters or employ a coding MoE model with <code>llama.cpp</code> to offload some processing to system RAM.</strong> There is a debate on the suitability of <code>30B</code> models for the user's GPU, with a consensus leaning towards using models under <code>10B</code> parameters due to VRAM constraints. The use of <code>llama.cpp</code> for offloading to system RAM is also discussed as a viable strategy.</p>
<ul>
<li>Javanese1999 highlights several models for local coding tasks, including <a href=""https://huggingface.co/TIGER-Lab/VisCoder2-7B"">VisCoder2-7B</a>, which is described as a better version of Qwen2.5-Coder-7B-Instruct, and <a href=""https://huggingface.co/openai/gpt-oss-20b"">gpt-oss-20b</a>, noted for its speed even when exceeding VRAM capacity. The commenter prefers gpt-oss-20b for its reliability in light coding tasks despite its censorship in refusal prompts.</li>
<li>Used_Chipmunk1512 advises against using 30B models quantized to q4 due to GPU limitations, suggesting that models under 10B are more suitable for most users. This highlights the importance of considering hardware constraints when selecting a local LLM for coding.</li>
<li>RnRau suggests using a coding Mixture of Experts (MoE) model with the <code>llama.cpp</code> inference engine to offload some of the model's processing to system RAM, which can be a practical approach for handling larger models without overwhelming the GPU.</li>
</ul>
</li>
</ul>
<h2>Less Technical AI Subreddit Recap</h2>
<blockquote>
<p>/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo</p>
</blockquote>
<h3>1. OpenAI and Anthropic Developments</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/singularity/comments/1qk6pbi/openai_says_codex_usage_grew_20_in_5_months/"">OpenAI says Codex usage grew 20× in 5 months, helping add ~$1B in annualized API revenue last month</a></strong> (Activity: 535): <strong>OpenAI's Codex usage has surged 20 times over five months, contributing to an additional <code>$1 billion</code> in annualized API revenue, as reported by <strong>Sarah Friar</strong>, OpenAI's CFO. The company is experiencing a shift towards enterprise customers, with the revenue split moving from <code>70% consumer and 30% enterprise</code> to <code>60% consumer and 40% enterprise</code>, and is expected to reach a <code>50-50</code> balance by the end of the year. OpenAI aims to achieve <code>$20 billion</code> in annualized revenue by 2025, supported by cloud investments and infrastructure scaling.</strong> A comment suggests skepticism about the profitability, estimating a cost of <code>$7 billion</code> to achieve the <code>$1 billion</code> revenue. Another comment highlights a shift in AI tools used by a financial services company, indicating competition in the B2B market with <strong>Anthropic</strong> and <strong>OpenAI</strong>.</p>
<ul>
<li>BetImaginary4945 suggests that the cost of generating $1B in revenue for OpenAI might be as high as $7B, implying a significant expenditure on infrastructure, research, and development to support such rapid growth. This raises questions about the sustainability and profitability of OpenAI's business model in the long term.</li>
<li>balagachchy shares an insight from their experience at a multinational financial services company, noting a shift from using ChatGPT to Gemini and Claude Code for software engineering tasks. This highlights the competitive landscape in AI tools for enterprise use, where companies are exploring different solutions to meet their specific needs.</li>
<li>imlaggingsobad comments on the competitive dynamics between OpenAI and Anthropic in the B2B market, suggesting that while Anthropic is perceived as a leader, OpenAI's rapid growth and innovation could still make it a formidable competitor. This underscores the ongoing competition and potential for shifts in market leadership.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/OpenAI/comments/1qjrpbq/openai_ceo_meets_middle_east_investors_over/"">OpenAI CEO meets Middle East investors over potential $50B fundraising</a></strong> (Activity: 191): <strong><strong>OpenAI</strong> is reportedly in discussions with Middle Eastern sovereign wealth funds to raise a potential <code>$50 billion</code> in a new funding round, as confirmed by <a href=""https://www.cnbc.com"">CNBC</a>. The talks are still in preliminary stages, with no term sheets signed yet. <strong>Sam Altman</strong>, OpenAI's CEO, is currently in the UAE to engage in these discussions, highlighting the strategic importance of this potential investment for OpenAI's future growth and operational scaling.</strong> A notable opinion from the comments suggests skepticism about OpenAI's financial strategy, questioning why the company isn't pursuing an IPO given its significant revenue, and criticizing its reliance on external capital to manage high operational costs.</p>
<ul>
<li>AtraVenator highlights concerns about OpenAI's financial strategy, noting that despite having over <code>$20B</code> in annual revenue, the company is seeking additional external capital rather than moving towards a self-sustaining model. This raises questions about their high compute costs and reliance on external funding to cover these expenses.</li>
<li>The discussion touches on the potential risks of OpenAI going public, with NotABCDinFL suggesting that an IPO could lead to a 'massive rug pull' where institutional investors might cash out, leaving retail investors at a disadvantage. This reflects concerns about the stability and transparency of OpenAI's financial practices.</li>
<li>There is skepticism about OpenAI's leadership and strategic direction, with BeingComfortablyDumb questioning how the company has moved from having a first-mover advantage and significant market share to its current financial challenges. This implies a critique of the management's ability to capitalize on their early lead in the AI industry.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/OpenAI/comments/1qjytb2/anthropics_claude_constitution_is_surreal/"">Anthropic's Claude Constitution is surreal</a></strong> (Activity: 611): <strong>The image discusses the use of the pronoun ""it"" for Anthropic's AI, Claude, and the potential for Claude to develop preferences for different pronouns, suggesting the emergence of functional versions of emotions or feelings from training on human-generated data. This is not a deliberate design choice by Anthropic, but it raises questions about the moral status of these emotional states. The text reflects ongoing debates in AI ethics about the implications of AI systems potentially developing human-like emotional states, which are not yet fully understood or intentionally designed.</strong> Commenters note that this aligns with current research and extreme safety measures in the AI industry, emphasizing the surreal nature of these developments and the importance of humility in AI labs' claims.</p>
<ul>
<li>br_k_nt_eth highlights that the Claude Constitution aligns with current research trends and extreme safety measures being tested in the AI industry, which sometimes negatively impact company reputations. This suggests that those familiar with advanced models would not find the approach surprising, as it reflects ongoing industry practices.</li>
<li>heavy-minium argues that the surreal aspect of Claude's Constitution is not unique to Claude but is inherent to any large language model (LLM). They point out that emotions are patterns in training data, and this phenomenon is unavoidable unless the model is either broken or too small. The commenter suggests that the relabeling of this characteristic is more about public relations than a technical breakthrough.</li>
<li>laystitcher emphasizes the importance of the cautious language used in the Claude Constitution, such as the word 'may,' which reflects the humility of AI labs in acknowledging the uncertainties in their developments. This cautious approach is seen as appropriate given the current surreal advancements in AI technology.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/ClaudeAI/comments/1qk4up5/microsoft_is_using_claude_code_internally_while/"">Microsoft is using Claude Code internally while selling you Copilot</a></strong> (Activity: 1276): <strong><strong>Microsoft</strong> is internally using <strong>Claude Code</strong> across various divisions like Windows and Teams, despite heavily investing in <strong>OpenAI</strong> and promoting <strong>Copilot</strong>. This internal use is sanctioned for all Microsoft repositories, indicating a significant investment of <code>$500M/year</code> with <strong>Anthropic</strong>. Interestingly, <strong>Azure</strong> sales teams receive quota credit for Anthropic sales, suggesting a strategic partnership. Despite <strong>Claude Code</strong> not outperforming in <code>95%</code> of benchmarks, developers report superior problem-solving capabilities, challenging the reliability of current benchmark tools. <strong>Copilot</strong> is priced at <code>$10/month/user</code>, whereas <strong>Claude Code</strong> is <code>$150</code> for enterprise use.</strong> Commenters highlight the discrepancy between benchmark results and real-world performance, suggesting benchmarks may not fully capture tool effectiveness. The partnership between Microsoft and Anthropic is seen as strategic, with Claude's integration into various Microsoft products and services.</p>
<ul>
<li>CurveSudden1104 highlights a discrepancy between benchmark results and real-world performance, noting that while Claude doesn't outperform in 95% of benchmarks, developers find it superior in problem-solving. This suggests that current benchmarks may not accurately reflect practical utility, indicating a potential gap between quantitative metrics and qualitative user experience.</li>
<li>morrisjr1989 points out that Claude's integration into Microsoft's ecosystem is part of a strategic partnership, with Claude being utilized in various Microsoft products like Copilot, Foundry, and Azure-hosted services. This integration underscores a collaborative approach rather than a competitive one, leveraging Claude's capabilities across multiple platforms.</li>
<li>UnknownEssence provides a cost comparison, noting that Copilot is priced at $10 per month per user, whereas Claude Code is significantly more expensive at $150 for enterprise use. This price difference highlights the distinct market positioning and target audiences for each product, with Copilot being more accessible to individual users and Claude Code catering to enterprise needs.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/ClaudeAI/comments/1qjlrgb/claudes_eureka_moment_is_not_ending_soon_it_looks/"">Claude’s eureka moment is not ending soon it looks like</a></strong> (Activity: 1377): <strong>The image and post discuss the competitive landscape of AI coding agents, particularly focusing on <strong>Claude</strong>, a tool developed by <strong>Anthropic</strong>. The post suggests that <strong>Gemini</strong> has open-sourced their CLI in an attempt to compete with Claude, which is notably used by <strong>Nvidia</strong>. This highlights the ongoing race in AI development tools, with speculation about whether the market will consolidate around a few dominant players or remain diverse. The comments reflect a belief that AI will significantly transform programming, with some users noting their companies' exclusive use of Claude.</strong> One comment suggests skepticism about the CEO's investment in the product's company, while another highlights a shift in programming paradigms, predicting that future programmers will rely heavily on AI tools.</p>
<ul>
<li>sine120 argues that Claude Code should be open-sourced, suggesting it lacks unique features that justify keeping it proprietary. They mention that other frameworks like Opus could integrate Claude's capabilities, and by not open-sourcing, Anthropic might miss the chance to lead AI development, potentially allowing competitors like Google and Chinese labs to catch up. They emphasize that developers might prefer openness over marginal performance improvements.</li>
<li>itsdr00 highlights a significant shift in software development life cycles (SDLC) due to AI advancements, particularly with Claude Code. They note that some companies are restructuring their SDLC to leverage AI, implying that traditional methods are becoming obsolete. This reflects a broader industry trend where AI is increasingly integral to development processes, akin to a paradigm shift from older technologies like punch cards.</li>
</ul>
</li>
</ul>
<h3>2. Gemini and AI Studio Issues</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/Bard/comments/1qkj31m/im_honestly_sick_of_this_gemini_web_vs_ai_studio/"">I’m honestly sick of this: Gemini Web vs AI Studio Context Window Mess</a></strong> (Activity: 49): <strong>The user reports a significant regression in the Gemini web/app's ability to handle large files since the update to <strong>Gemini 3</strong>. Previously, with <strong>Gemini 2.5 Pro</strong>, files containing <code>600k–800k</code> tokens could be processed without issues, retaining full context for queries. However, the current version rejects files over <code>100k</code> tokens and provides incomplete or incorrect responses. In contrast, <strong>Gemini AI Studio</strong> continues to handle the same large files effectively, suggesting the underlying model's capability remains intact but is not accessible in the consumer-facing app. This discrepancy raises concerns about potential limitations imposed on the web/app version, possibly misleading users about the product's capabilities.</strong> Commenters express dissatisfaction with the Gemini web/app, noting that <strong>AI Studio</strong> is the only reliable platform for using Google's models effectively. Some users, even on the Pro plan, report receiving errors when uploading large documents, indicating a possible mismatch between advertised capabilities and actual performance.</p>
<ul>
<li>A user mentions that AI Studio is the only viable platform for using Google models effectively, implying that other platforms like Gemini app and Antigravity do not meet their expectations despite having a subscription. This suggests potential issues with the usability or performance of these platforms compared to AI Studio.</li>
<li>Another user discusses the Pro plan, noting that they have not encountered issues with document processing. They suggest that if documents are too large in terms of tokens, the system might default to classic retrieval methods rather than processing the entire file, indicating a possible limitation in handling large documents.</li>
<li>A user on the Pro plan reports receiving an error after uploading a 20-page PDF, describing the situation as 'absurd.' This highlights potential limitations or bugs in the system's ability to handle larger documents, even for users on higher-tier plans.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/Bard/comments/1qkztjy/ai_studio_rate_limits_are_out_of_control_again/"">AI Studio Rate Limits are out of control again...</a></strong> (Activity: 67): <strong>The post discusses recent issues with rate limits on <strong>AI Studio</strong>, where users, including those with Pro subscriptions, are experiencing frequent request denials. This is a change from previous usage patterns where limits were rarely hit. The user expresses frustration that their Pro subscription cannot be applied to AI Studio, which they find superior to the main site. Technical comments suggest that the rate limits might be due to dynamic prompt limits, increased GPU allocation for new training, or a higher user count. Additionally, <strong>Gemini 2.5 Pro</strong> has been rate limited for the first time, indicating possible resource constraints or strategic adjustments by the platform.</strong> Commenters speculate that the rate limits could be due to increased demand or resource reallocation, with some suggesting desperation on the platform's part. Others report encountering internal errors, indicating potential technical issues beyond just rate limiting.</p>
<ul>
<li>OneMisterSir101 suggests that the current rate limits on AI Studio might be due to dynamic prompt limits, which could be influenced by either GPU delegation to new training tasks or an increase in user count. This implies a resource allocation issue where computational resources are being stretched thin, potentially affecting performance and availability.</li>
<li>Undertaker1995 notes that Gemini 2.5 Pro has been rate limited for the first time, indicating a significant shift in resource management or demand. This could reflect a strategic decision by the platform to manage load or a response to increased usage, highlighting potential scalability challenges.</li>
<li>wildwriting reports encountering an 'internal error' message, despite attempting standard troubleshooting steps like reloading the page and restarting the browser. This suggests a deeper technical issue within the platform, possibly related to server-side problems or misconfigurations that are not resolved by client-side actions.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/GeminiAI/comments/1qjrokj/im_sorry_but_gemini_is_getting_worse_and_worse/"">I'm sorry but Gemini is getting worse and worse</a></strong> (Activity: 1301): <strong>The post discusses a decline in the performance of <strong>Gemini</strong>, particularly in its memory capabilities and intelligence. Previously, the pro mode of Gemini could remember <code>30+ conversations</code> with a total of <code>180,000 words</code>, but recent updates have halved this memory capacity, leading to a perceived decrease in intelligence and reliability. The user expresses frustration, suggesting that <strong>ChatGPT</strong> might be a better alternative due to its longer and more conversational responses.</strong> Commenters agree with the decline in Gemini's performance, noting increased issues with speculation and hallucination. There is skepticism about future updates, with one commenter cynically suggesting that any improvements will be short-lived.</p>
<ul>
<li>The comment by Particular-Battle315 highlights a common lifecycle pattern in AI models where initial releases are powerful but get 'nerfed' over time. This is observed across companies like Anthropic, OpenAI, and Google, suggesting a strategic approach to model updates that may not be immediately apparent to all users.</li>
<li>Duchess430 discusses the potential for running large AI models on personal computers using specialized open-source models, which may outperform Gemini for specific tasks. They mention the GGUF (GPT-Generated Unified Format) as a method to optimize resource usage by splitting data between RAM and VRAM, allowing for running large models without high-end hardware.</li>
<li>rephil3 points out issues with Gemini, specifically its tendency to speculate and hallucinate, which are common problems in AI models that can affect their reliability and user trust.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/Bard/comments/1qk2yx6/gemini_about_to_get_busy/"">Gemini about to get busy?</a></strong> (Activity: 33): <strong>The post discusses the potential impact of <strong>ChatGPT</strong> introducing ads on its user base, suggesting that this could lead to a significant migration of users to <strong>Gemini</strong>, especially as Gemini's models improve and integrate more deeply with <strong>Google's</strong> ecosystem. Concerns are raised about whether Gemini can handle a sudden influx of users without degrading the experience for its current base. A technical issue noted is Gemini's handling of conversations, where users report chats being replaced with 'sensitive query' messages, and the lack of a 'Projects' feature to maintain context, unlike <strong>ChatGPT</strong> and <strong>Claude</strong>.</strong> Commenters debate Gemini's readiness to handle increased user load, with some arguing that <strong>Google's</strong> extensive experience and infrastructure, including recent investments in clean energy and data centers, position it well to scale effectively. Others highlight the technical shortcomings of Gemini, such as conversation management issues, as potential drawbacks.</p>
<ul>
<li>Loud-Independent9041 highlights a significant issue with Gemini's conversation handling, where chats are sometimes replaced with 'a sensitive query' messages, disrupting the user experience. This contrasts with ChatGPT and Claude, which offer a 'Projects' feature to maintain context across conversations, a feature Gemini lacks, impacting its usability for continuous dialogue.</li>
<li>rollk1 points out Google's strategic positioning in the AI and data center space, emphasizing their acquisition of Intersect Power to support clean energy for data centers. This move, along with their existing Google Cloud infrastructure, positions them advantageously for scaling AI models, potentially outpacing competitors like OpenAI.</li>
<li>FalseAcadia4306 notes a potential increase in Gemini's user base, as evidenced by receiving a 'research queued' message for the first time, suggesting a surge in demand or usage that could be straining the system's capacity.</li>
</ul>
</li>
</ul>
<h3>3. DeepSeek and Baidu's ERNIE 5.0 Innovations</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/DeepSeek/comments/1qkoc53/deepseekv32_matches_gpt5_at_10x_lower_cost_introl/"">DeepSeek-V3.2 Matches GPT-5 at 10x Lower Cost | Introl Blog</a></strong> (Activity: 125): <strong><strong>DeepSeek-V3.2</strong> is an open-source AI model that reportedly matches the performance of <strong>GPT-5</strong> in mathematical reasoning tasks while operating at a cost 10 times lower, specifically <code>$0.028</code> per million tokens. The model utilizes a novel 'Sparse Attention' architecture, which contributes to its efficiency, achieving frontier-class performance with a total training cost of approximately <code>$5.5 million</code>, significantly less than the <code>$100M+</code> typically spent by major US tech companies. The model's architecture includes <strong>DeepSeek Sparse Attention (DSA)</strong> for efficient long-context processing and a refined <strong>Mixture-of-Experts</strong> approach, which activates only a subset of parameters per token, enhancing task-specific performance. For more details, see the <a href=""https://introl.com/blog/deepseek-v3-2-open-source-ai-cost-advantage"">Introl Blog</a>.</strong> One comment suggests skepticism about the reported cost savings, noting that a significant portion of OpenAI's expenses may be attributed to executive salaries rather than direct model development costs.</p>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/DeepSeek/comments/1qkpxzm/baidus_new_ernie_50_is_going_hard_after_gpt_and/"">Baidu's new ERNIE 5.0 is going hard after GPT and Gemini</a></strong> (Activity: 51): <strong><strong>Baidu's ERNIE 5.0</strong> is making significant strides in mathematical reasoning and technical problem-solving, ranking #2 globally on the LMArena Math leaderboard, just behind the unreleased GPT-5.2-High. It surpasses GPT-5.1 and Gemini 2.5 Pro in math and scores higher on specialized benchmarks like MathVista and ChartQA, particularly excelling in interpreting complex visual diagrams. In the 'VLMs Are Blind' benchmark, ERNIE 5.0 scored <code>77.3</code>, outperforming GPT-5-High's <code>69.6</code>. Additionally, ERNIE 5.0 offers a cost advantage, being nearly <code>90%</code> cheaper than OpenAI’s GPT-5.1 for similar token volumes, making it a competitive option in terms of pricing.</strong></p>
<ul>
<li>ERNIE 5.0 is noted for its impressive scale with <code>2.4 trillion parameters</code>, significantly larger than competitors like DeepSeek's <code>671 billion</code> and Kimi K2's <code>1 trillion</code>. Despite its size, the quality of output is reported to be similar to other models, with particularly fast inference speeds. However, the model's strict system prompt alignments can make interactions feel restricted, though users can adjust the tone with specific prompts for better results.</li>
<li>The model offers a free web version with a <code>128k context window</code>, comparable to DeepSeek, which is a significant advantage for users needing extensive context handling. However, the default interaction tone is described as overly corporate, which can be modified with specific prompts to achieve more engaging interactions. This flexibility in tone adjustment is seen as a positive feature despite the initial restrictions.</li>
<li>A recent update to ERNIE 5.0, referred to as ""5.0 Preview 1203"", has reportedly improved the model's engagement and interaction quality, making it more fun and collaborative. This suggests that Baidu is actively iterating on the model to enhance user experience, potentially addressing earlier criticisms of restrictive interactions.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/DeepSeek/comments/1qjob34/deepseeks_quiet_technical_wins_that_nobody_talks/"">DeepSeek’s Quiet Technical Wins (That Nobody Talks About)</a></strong> (Activity: 85): <strong><strong>DeepSeek</strong> is recognized not only for its benchmark performance but also for its engineering innovations, which include <em>better routing for efficiency</em>, <em>cleaner long-context behavior</em>, and <em>faster token generation</em>. These features contribute to its distinctiveness in practical applications. Notably, DeepSeek employs <strong>Mixture of Experts (MoE)</strong> for smarter routing and introduces <strong>Engram</strong> to separate memory from reasoning, emphasizing architectural innovation over brute-force scaling.</strong> Commenters highlight DeepSeek's unique 'thinking process' and its focus on architectural innovation, such as using MoE and Engram, as key differentiators from other AI models.</p>
<ul>
<li><strong>Hey-Intent</strong> highlights DeepSeek's architectural innovations, particularly the use of Mixture of Experts (MoE) for smarter routing and the introduction of Engram to separate memory from reasoning. This approach emphasizes sustainable AI progress through architectural improvements rather than brute-force scaling, which is a significant shift in AI development strategy.</li>
<li><strong>Fine_Effective4980</strong> points out that DeepSeek's system-level efficiency, combining routing and token generation, results in a more responsive and stable user experience, especially with longer context. This efficiency is not captured in traditional benchmarks but is crucial for real-world applications, offering a smoother and more reliable workflow.</li>
<li><strong>Althalvas</strong> notes that DeepSeek's R1 model provides a superior thinking process compared to other AI models, even when only using free versions. This suggests that DeepSeek's models may have a more refined approach to processing, which could be attributed to their architectural choices.</li>
</ul>
</li>
</ul>
<hr>
<h1>AI Discord Recap</h1>
<blockquote>
<p>A summary of Summaries of Summaries</p>
</blockquote>
<h2>Gemini 3.0 Pro Preview Nov-18</h2>
<p><strong>Theme 1. Hardware Limits and Kernel Hacking: B200s, ROCm, and Mobile Optimization</strong></p>
<ul>
<li><strong>FlashAttention-4 hits 71% utilization on B200</strong>: Early benchmarks show <strong>FlashAttention-4</strong> reaching <strong>1,605 TFLOPS/s</strong> on an <strong>NVIDIA B200 GPU</strong> using BF16 inputs, capturing roughly 71% of the theoretical maximum. Engineers in the <strong>GPU MODE</strong> discord noted a lack of official documentation regarding specific fp4/fp8/fp16 specs, sparking debate over the hardware's true theoretical ceiling compared to leaked materials.</li>
<li><strong>Developer abandons ROCm for CUDA</strong>: A frustrated developer publicly documented their switch from <strong>AMD's ROCm</strong> to <strong>NVIDIA</strong> after purchasing a 5090, citing packaging failures, build issues, and a ""hostile"" ecosystem for consumer-facing hardware. The discussion highlighted that mid-grade NVIDIA hardware often outperforms AMD gear on specific kernels like <strong>Conv3D</strong> due to software maturity, referencing a <a href=""https://www.reddit.com/r/ROCm/comments/1owczm9/the_convolution_performance_on_rx_9070_is_so_low/"">Reddit thread on performance regressions</a>.</li>
<li><strong>Mobile GPU memory path splitting</strong>: Engineers in <strong>tinygrad</strong> discovered that optimizing <strong>L2 bandwidth</strong> on mobile GPUs requires treating <strong>textures</strong> and <strong>buffers</strong> as distinct hardware pathways. Maximizing throughput involves strategically feeding one input as a texture and another as a buffer to saturate the available bandwidth, a technique critical for edge inference.</li>
</ul>
<p><strong>Theme 2. Agentic Workflows: Cursor Sub-agents, Replit Control, and Aider TUI</strong></p>
<ul>
<li><strong>Cursor 2.4 wobbles while Sub-agents emerge</strong>: While users reported <strong>Composer 1</strong> breaking into endless loops and <strong>Cursor 2.4</strong> causing significant lag on high-end PCs, savvy users found evidence of a <strong>sub-agent</strong> feature rollout. The system injects a <strong>&#x3C;subagent_delegation_context></strong> prompt to enable parallel task execution and better context handling, as detailed in the <a href=""https://cdn.discordapp.com/attachments/1351160689380687942/1463993849985765510/Cursor_Changelog_Subagents.mp4?ex=69752b85&#x26;is=6973da05&#x26;hm=50e3cbf6432112dcbe36b0315b1645fd7d856c9d2ead97e639b2d8abcfa5b8f4&#x26;"">Changelog video</a>.</li>
<li><strong>Replit Agent gets real-time brains</strong>: Zhen Li published a technical breakdown of <strong>Decision-Time Guidance</strong> in the <strong>Replit Agent</strong>, replacing static rules with real-time control mechanisms for complex navigation. This architectural shift aims to reduce fragility in autonomous coding tasks, moving closer to adaptive ""system 2"" thinking, as described in <a href=""https://xcancel.com/zhenthebuilder/status/2014393451442581688?s=46"">this blog post</a>.</li>
<li><strong>Aider eyes a TUI makeover</strong>: The <strong>aider</strong> community is actively designing a <strong>Terminal User Interface (TUI)</strong> to allow message editing while browsing replies and rendering <strong>Mermaid diagrams</strong> directly in the terminal. Simultaneously, users are chaining <strong>aider</strong> for rapid context management with <strong>Claude Code</strong> for complex debugging to minimize token costs and leverage <a href=""https://discord.com/channels/1131200896827654144/1131200896827654149/1464167385060872203"">aider's efficient file search</a>.</li>
</ul>
<p><strong>Theme 3. Model Architecture and Audio: Qwen3-TTS, NanoGPT Hacks, and GLM Speedups</strong></p>
<ul>
<li><strong>Qwen3-TTS clones voices at scale</strong>: Alibaba released the <strong>Qwen3-TTS</strong> family, ranging from <strong>0.6B to 1.8B</strong> parameters, capable of high-quality voice cloning and supporting 10 languages. The release challenges proprietary models like ElevenLabs, with demos and weights available on <a href=""https://huggingface.co/spaces/Qwen/Qwen3-TTS"">Hugging Face Spaces</a>.</li>
<li><strong>NanoGPT gets a Difference Layer boost</strong>: Researchers in <strong>Eleuther</strong> reported that replacing the <strong>QKV linear layer</strong> with a <em>difference layer</em>—<code>x = (self.a2(x) - self.b2(x)) * ...</code>—significantly improved <strong>NanoGPT</strong> performance on simple tasks. Others noted that switching activations from <strong>GELU</strong> to <strong>SwiGLU</strong> also provided a baseline boost, emphasizing the need for <a href=""https://github.com/Eternalyze0/difference_layer"">stronger baselines</a> before claiming architectural supremacy.</li>
<li><strong>GLM-4.7 Flash zooms on llama.cpp</strong>: The <strong>Hugging Face</strong> community noted that <strong>llama.cpp</strong> updates have accelerated <strong>GLM-4.7 Flash GGUF</strong> inference by approximately <strong>1.5x</strong>. Users are advised to rebuild from source and grab fixed quants from <a href=""https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF"">Unsloth's repo</a> to enable the <code>--flash-attn on</code> flag for optimal performance.</li>
</ul>
<p><strong>Theme 4. Inference Engineering: Speculative Decoding, SRAM Specs, and MoE Memory</strong></p>
<ul>
<li><strong>Speculative decoding slows down vLLM</strong>: Engineers debugging <strong>Qwen3-VL</strong> on <strong>vLLM</strong> found that enabling speculative decoding often hurts <strong>Time To First Token (TTFT)</strong> unless batch sizes are massive. The consensus is that small draft models introduce too much overhead for single-stream or low-batch inference, and standard <a href=""https://docs.vllm.ai/en/stable/design/metrics/"">vLLM metrics</a> via Grafana are recommended for tuning.</li>
<li><strong>Small MoEs starve in 8GB RAM</strong>: Discussions in <strong>Unsloth AI</strong> concluded that running <strong>Mixture of Experts (MoE)</strong> models on 8GB RAM is largely futile because the active parameter count becomes too low to be useful. While <strong>Qwen 2.5 3B</strong> (dense) remains the king for low-memory coding, ""small"" MoEs like <strong>LFM2</strong> lack the corpus density to compete effectively.</li>
<li><strong>Cerebras CS3 packs 41GB SRAM</strong>: It was revealed in <strong>OpenRouter</strong> that each <strong>Cerebras CS3</strong> wafer-scale instance houses <strong>41GB of SRAM</strong>, designed to interconnect up to <strong>2048 instances</strong>. This massive on-chip memory allows for extremely high-bandwidth model execution, bypassing traditional HBM bottlenecks found in GPU clusters.</li>
</ul>
<p><strong>Theme 5. Adversarial Attacks and Platform Instability</strong></p>
<ul>
<li><strong>Gemini 3 Pro jailbroken via ENI</strong>: The <strong>ENI jailbreak</strong> technique, originally targeting Claude, was successfully ported to <strong>Gemini 3 Pro</strong> in AI Studio, with users reporting it ""works like magic"" even on the Flash variant. The exploit allows bypassing safety guardrails, detailed in a shared <a href=""https://github.com/pranrichh/Jailbreaks/blob/main/GEMINI-CLAUDE%20JAILBREAK.md"">GitHub methodology</a>.</li>
<li><strong>Perplexity Pro limits pinch users</strong>: <strong>Perplexity</strong> subscribers are reporting severe undocumented limits, with file uploads capped at three per day and research queries throttled to as low as <strong>20 daily</strong> for some (vs the expected 600). Users suspect aggressive <strong>A/B testing</strong> or financial tightening, further aggravated by <a href=""https://discord.com/channels/1047197230748151888/1161802929053909012/1464341850809831519"">API 401 errors</a> despite valid credits.</li>
<li><strong>Kimi AI hits capacity wall</strong>: <strong>Moonshot AI's Kimi</strong> service is suffering widespread outages, with users facing constant ""This mode is at capacity"" errors and vanishing conversation histories. Speculation in the community points to a potential datacenter failure or API restrictions from upstream providers like Google <a href=""https://discord.com/channels/1369594130807787570/1371757564005711973/1464201099123888242"">derailing the service</a>.</li>
</ul>
<h2>gpt-5.2</h2>
<p><strong>1. Cursor 2.4 Subagents Rollout &#x26; Developer UX Fallout</strong></p>
<ul>
<li>
<p><strong>Subagents Ship Fast, Task Tool Ghosts Everyone</strong>: <strong>Cursor 2.4</strong> introduced <strong>subagents</strong> for parallel task completion per the <a href=""https://cursor.com/changelog"">Cursor Changelog</a> and a demo <a href=""https://cdn.discordapp.com/attachments/1351160689380687942/1463993849985765510/Cursor_Changelog_Subagents.mp4"">video</a>, but users noticed injected <strong><code>&#x3C;subagent_delegation_context></code></strong> that tells the model to call a <strong>Task tool that isn’t available</strong>.</p>
<ul>
<li>Community speculation pegged this as an <strong>incomplete rollout</strong> (prompting paths shipped ahead of backend), and some users suspected subagents silently <strong>fall back to Composer 1</strong>, worsening latency and “<em>planning next moves</em>” hangs.</li>
</ul>
</li>
<li>
<p><strong>Composer Crash Derby: Loops, Lag, and the Great Downgrade</strong>: Users reported <strong>Composer 1</strong> as “completely broken,” including <strong>endless chat loops</strong> and crashes, with workarounds like downgrading to <strong>Cursor 2.3</strong> (notably on <strong>macOS Big Sur 11.7.3</strong>) and filing reports via the <a href=""https://forum.cursor.com/c/support/bug-report/6"">Cursor bug forum</a>.</p>
<ul>
<li>Separately, <strong>Cursor 2.4</strong> drew complaints of severe <strong>lag/unresponsiveness</strong> and frequent crashes even on high-end machines, fueling criticism that releases feel <strong>premature</strong> and hard to trust for daily engineering work.</li>
</ul>
</li>
<li>
<p><strong>Billing Roulette: Token Counts, Auto Mode, and DIY Telemetry</strong>: Cursor users flagged <strong>usage/billing discrepancies</strong> (missing dollar amounts, unexpected bonus credits, and limits not triggering despite heavy use), and some suspected <strong>Auto mode</strong> charges incorrectly.</p>
<ul>
<li>To sanity-check spending, users recommended third-party tracking like <a href=""https://token-watch.vercel.app/"">token-watch</a>, noting it can diverge from what Cursor’s own dashboard shows.</li>
</ul>
</li>
</ul>
<p><strong>2. Inference Performance &#x26; Benchmarking: B200, vLLM, llama.cpp, Grafana</strong></p>
<ul>
<li>
<p><strong>FlashAttention-4 Floors It on B200 (Specs Still Foggy)</strong>: In GPU performance chatter, <strong>FlashAttention-4 (FA4)</strong> reportedly hit <strong>1,605 TFLOPS/s</strong> (~<strong>71%</strong> of theoretical) on an <strong>NVIDIA B200</strong> with <strong>BF16</strong> inputs, while the community debated a theoretical ceiling around <strong>2260 TFLOPS</strong> and noted missing official datatype details.</p>
<ul>
<li>Confusion deepened as leaked materials listed B200 at <strong>10/5/2.5 TFLOPS</strong> for <strong>fp4/fp8/fp16</strong>, and folks asked for an <strong>official spec paper</strong> to reconcile marketing numbers with kernel benchmarks.</li>
</ul>
</li>
<li>
<p><strong>Spec Decode Saves Throughput, Not Your TTFT (Usually)</strong>: Engineers discussed optimizing <strong>Time To First Token (TTFT)</strong> for <strong>Qwen3-VL</strong> in <strong>vLLM</strong> on a <strong>B200</strong>, considering <code>--speculative_config</code> with smaller <strong>Qwen3-VL-4B/2B</strong> draft models.</p>
<ul>
<li>The advice: speculative decoding often <strong>hurts throughput</strong> unless you run <strong>high batch sizes</strong>, and only “<strong>eagle heads</strong>” setups feel worth it at scale because small drafts add too much overhead for short outputs.</li>
</ul>
</li>
<li>
<p><strong>Grafana for VLM Telemetry + vLLM Metrics as the Source of Truth</strong>: For benchmarking fast multimodal paths, members pointed to <a href=""https://docs.vllm.ai/en/stable/design/metrics/"">vLLM’s metrics docs</a> and suggested wiring dashboards with <a href=""https://grafana.com/products/cloud/metrics/"">Grafana</a> for <strong>real-time TTFT visualization</strong>.</p>
<ul>
<li>The thread framed Grafana as the “good UI” layer for quickly comparing VLM deployments under realistic workloads instead of relying on one-off scripts.</li>
</ul>
</li>
<li>
<p><strong>llama.cpp Speeds Up GLM-4.7 Flash GGUFs ~1.5×</strong>: <strong>llama.cpp</strong> delivered a <strong>~1.5× speedup</strong> plus bug fixes for <strong>GLM 4.7 Flash GGUFs</strong>, and users pointed to re-downloading fixed quants from <a href=""https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF"">unsloth/GLM-4.7-Flash-GGUF</a>.</p>
<ul>
<li>This reinforced the “rebuild often” culture in local inference stacks: performance jumps can appear just by updating the runtime and swapping quants, not changing the model.</li>
</ul>
</li>
</ul>
<p><strong>3. Open Releases: Voice/Audio Models, New Datasets, and Local-First LLMs</strong></p>
<ul>
<li>
<p><strong>Qwen3-TTS Drops Multilingual Voice Cloning (ElevenLabs Catching Side-Eye)</strong>: Communities rallied around <strong>Qwen3-TTS</strong> as a strong <strong>voice cloning</strong> option, with a live demo on <a href=""https://huggingface.co/spaces/Qwen/Qwen3-TTS"">Qwen3-TTS Hugging Face Spaces</a> and the broader family referenced via <a href=""https://github.com/QwenLM"">QwenLM GitHub</a> and <a href=""https://huggingface.co/Qwen"">Qwen on Hugging Face</a>.</p>
<ul>
<li>Latent Space summarized the family as <strong>0.6B–1.8B params</strong> with support for <strong>10 languages</strong>, framing it as open tooling that can plausibly replace paid TTS in some pipelines.</li>
</ul>
</li>
<li>
<p><strong>Audio Release Triple-Feature: PersonaPlex-7B, TTS-1.5, and Chroma 1.0</strong>: An audio-model roundup highlighted <strong>NVIDIA PersonaPlex-7B</strong> (full-duplex conversational), <strong>Inworld AI TTS-1.5</strong> (low-latency TTS), and <strong>Flash Labs Chroma 1.0</strong> (open-source end-to-end speech-to-speech) via <a href=""https://x.com/lina_colucci/status/2014229002370834861"">Lina Colucci’s post</a>.</p>
<ul>
<li>The vibe: speech stacks are accelerating toward <strong>low-latency</strong> and <strong>end-to-end</strong> pipelines, and open releases are starting to cover pieces previously gated behind SaaS APIs.</li>
</ul>
</li>
<li>
<p><strong>Datasets &#x26; Local Models: Rust→WASM Synthetic + Faust-1 German-First</strong>: Hugging Face saw two notable drops: a <strong>Rust-to-WebAssembly synthetic dataset</strong> of <strong>1,000</strong> generated programs at <a href=""https://huggingface.co/datasets/webxos/wasm_synthetic_dataset"">webxos/wasm_synthetic_dataset</a>, and <strong>Faust-1</strong>, a <strong>1.6B</strong> German-first LLM at <a href=""https://huggingface.co/tabularisai/Faust-1"">tabularisai/Faust-1</a>.</p>
<ul>
<li>Faust-1 emphasized <strong>~90% German pretraining</strong>, a <strong>German-optimized tokenizer</strong>, and instruction tuning with <strong>DPO</strong>, while the WASM dataset focused on <strong>reproducibility</strong> (deterministic Fibonacci-derived PRNG plus structural hashes).</li>
</ul>
</li>
</ul>
<p><strong>4. Agent Frameworks &#x26; Infra: Control Loops, RLM/DSPy, and MCP Schema Discipline</strong></p>
<ul>
<li>
<p><strong>Replit Agent Gets a Steering Wheel with Decision-Time Guidance</strong>: A technical writeup on <strong>Decision-Time Guidance</strong> described how <strong>Replit Agent</strong> applies <strong>real-time control mechanisms</strong> instead of static rules, shared via <a href=""https://xcancel.com/zhenthebuilder/status/2014393451442581688"">Zhen Li’s blog link</a>.</p>
<ul>
<li>The discussion framed this as a practical direction for agents: tighter <strong>online steering</strong> during execution rather than brittle pre-authored guardrails.</li>
</ul>
</li>
<li>
<p><strong>DSPy’s “Why” Gets Re-Explained (Signatures > Prompt Hacks)</strong>: DSPy folks circulated an explainer arguing DSPy’s value comes from <strong>signature &#x26; module abstractions</strong>, not just prompt tuning, via <a href=""https://eito.substack.com/p/dspy-the-most-misunderstood-agent"">“DSPy: the most misunderstood agent”</a> and the companion <a href=""https://x.com/Eito_Miyamura/status/2014757193766093069"">X post</a>.</p>
<ul>
<li>Separately, members discussed tuning <strong>RLM prompts</strong> and even optimizing JSON-schema adapters (GEPA ideas), aiming to make structured outputs more reliable without bloating token budgets.</li>
</ul>
</li>
<li>
<p><strong>MCP Schema Cage Match: <code>additionalProperties</code> vs <code>anyOf</code></strong>: Model Context Protocol contributors questioned whether <strong><code>GetTaskPayloadResult</code></strong> is too permissive because it allows <strong><code>additionalProperties</code></strong>, pointing directly to the schema location in the MCP repo (<a href=""https://github.com/modelcontextprotocol/modelcontextprotocol/blob/8d07c35d3857412a351c595fe01b7bc70664ba06/schema/2025-11-25/schema.json#L1245-L1256"">schema.json lines 1245–1256</a>).</p>
<ul>
<li>The proposed fix was moving toward <strong><code>anyOf</code></strong> for stricter validation, reflecting a broader push to keep agent-tool payloads <strong>tightly typed</strong> to avoid “accept everything” integrations that break downstream.</li>
</ul>
</li>
</ul>
<p><strong>5. Platforms, Benchmarks, and the AMD/NVIDIA Reality Check</strong></p>
<ul>
<li>
<p><strong>Arena Leaderboards Split (and Models Vanish Mid-Game)</strong>: LMArena’s <strong>Image Edit Arena</strong> split rankings into <strong>Single-Image Edit</strong> and <strong>Multi-Image Edit</strong>, publishing results at the <a href=""https://lmarena.ai/leaderboard/image-edit/overall"">Image Edit leaderboard</a> where <strong>Gemini 3 Pro Image 2K</strong> rose to #1 and <strong>ChatGPT Image (Latest)</strong> fell to #3.</p>
<ul>
<li>At the same time, reliability issues yanked models around: <strong>Nano Banana Pro 2K</strong> got removed for a <strong>high error rate</strong>, and <strong>Seedream-4-2k</strong> disappeared with moderators noting models can be unavailable for technical reasons.</li>
</ul>
</li>
<li>
<p><strong>ROCm Pain, CUDA Gain: Devs Vote with Their 5090s</strong>: GPU devs shared AMD tooling breadcrumbs—like the <strong>AMD ISA manual</strong> and LLVM docs (<a href=""https://github.com/llvm/llvm-project/blob/main/llvm/docs/AMDGPUUsage.rst"">AMDGPUUsage.rst</a>, <a href=""https://github.com/llvm/llvm-project/blob/main/llvm/include/llvm/IR/IntrinsicsAMDGPU.td"">IntrinsicsAMDGPU.td</a>, and <a href=""https://github.com/llvm/llvm-project/tree/main/llvm/test/CodeGen/AMDGPU"">AMDGPU CodeGen tests</a>)—but the tone stayed mixed on ROCm readiness.</p>
<ul>
<li>One developer said they quit <strong>ROCm for CUDA</strong> after buying a <strong>5090</strong>, citing packaging/build/distribution headaches, and another pointed to poor <strong>Conv3D</strong> performance on AMD relative to NVIDIA via a ROCm subreddit thread (<a href=""https://www.reddit.com/r/ROCm/comments/1owczm9/the_convolution_performance_on_rx_9070_is_so_low/"">link</a>).</li>
</ul>
</li>
<li>
<p><strong>Baseten Raises $300M Series E as Infra Funding Stays Hot</strong>: Latent Space highlighted <strong>Baseten’s $300M Series E</strong> led by IVP and CapitalG, valuing the company at <strong>$5B</strong>, per <a href=""https://xcancel.com/basetenco/status/2014755013344792595"">Baseten’s announcement</a>.</p>
<ul>
<li>The participation of <strong>NVIDIA</strong> in the round reinforced the market narrative: inference infrastructure remains a capital-heavy race where hardware adjacency still matters.</li>
</ul>
</li>
</ul>
<h2>gpt-5.1</h2>
<p><strong>1. Frontier Model Performance, Kernels, and Hardware Benchmarks</strong></p>
<ul>
<li>
<p><strong>FlashAttention-4 Pushes B200 GPUs Toward Theoretical Limits</strong>: <strong>FlashAttention‑4 (FA4)</strong> hit <strong>1,605 TFLOPS/s (~71% of theoretical max)</strong> on an <strong>NVIDIA B200</strong> in <strong>BF16</strong>, with community estimates pegging the true ceiling around <strong>2,260 TFLOPS</strong>, pending an official spec paper and data‑type breakdown from NVIDIA. Discussion in <strong>GPU MODE #cuda</strong> noted leaked B200 figures of <strong>10/5/2.5 PFLOPS for fp4/fp8/fp16</strong> clashing with the FA4 measurements, underscoring the need for a formal performance whitepaper rather than marketing blogs without datatype detail.</p>
<ul>
<li>Researchers also highlighted <strong>test‑time training (TTT)</strong> for <strong>LM‑generated kernels</strong> in a new paper, <a href=""https://test-time-training.github.io/discover.pdf"">""Discovering Test-Time Training for LLM‑Generated GPU Kernels""</a>, showing that adapting kernels at inference can materially improve benchmark scores on existing leaderboards. In parallel, <strong>FlashInfer‑Bench</strong> from <strong>CMU Catalyst Lab</strong> was introduced in GPU MODE’s <code>#popcorn</code> as a framework for evaluating and deploying <strong>AI‑generated GPU kernels</strong>, with the authors actively seeking community feedback on benchmarks and production deployment workflows.</li>
</ul>
</li>
<li>
<p><strong>GLM-4.7 Flash Builds Hit Turbo in llama.cpp and Arena</strong>: Both <strong>LMArena</strong> and <strong>Hugging Face</strong> circles reported major speed wins from <strong>GLM‑4.7‑Flash</strong> variants, with <strong>llama.cpp</strong> users seeing roughly <strong>1.5× throughput gains</strong> on <strong>GLM‑4.7 Flash GGUFs</strong> after rebuilding and re‑downloading new quants from <a href=""https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF"">unsloth/GLM-4.7-Flash-GGUF</a>. LMArena also added <strong>glm‑4.7‑flash</strong> to the <a href=""https://lmarena.ai/?chat-modality=chat"">Text Arena</a>, giving a head‑to‑head benchmark venue against other frontier chat models.</p>
<ul>
<li>An Unsloth user reported <strong>50 tok/s at 50k context</strong> with <code>--flash-attn on</code> on GLM‑4.7‑Flash, reinforcing that the FlashAttention fixes are stable at long context lengths, while Nous users experimented with <strong>GLM‑4.7</strong> on <strong>8×H100</strong> GPU servers as a potential <strong>Claude Code</strong> alternative. Across Discords, practitioners converged on a pattern of: rebuild kernels, enable explicit FlashAttention flags, and push long‑context workloads to stress‑test <strong>GLM‑4.7 Flash</strong> as an affordable, high‑throughput code‑capable model.</li>
</ul>
</li>
<li>
<p><strong>GPU Ecosystems Split: CUDA Dominates as ROCm Stumbles</strong>: In <strong>GPU MODE #rocm</strong>, a developer announced they were <em>""done with ROCm""</em> after buying an <strong>RTX 5090</strong>, citing chronic issues with <strong>packaging, build chains, distribution gaps, and weak consumer focus</strong>, and sharing a <a href=""https://www.reddit.com/r/ROCm/comments/1owczm9/the_convolution_performance_on_rx_9070_is_so_low/"">Reddit thread on poor Conv3D performance on RX 9070</a> as evidence that mid‑range NVIDIA cards still crush AMD on real‑world ML workloads. Others criticized the ROCm ecosystem as <em>""hostile""</em> and pointed at fragile libraries like <strong>FBGEMM</strong> on <code>gfx1100</code> and opaque vendor repos such as AMD’s <a href=""https://github.com/amd/Quark/commit/9234960c951410abdcecee033adf610d7126fda3"">Quark quantization engine</a>.</p>
<ul>
<li>To mitigate the pain, experts shared low‑level ROCm documentation sources—<strong>AMD’s CDNA4 ISA manual</strong> and LLVM’s <a href=""https://github.com/llvm/llvm-project/blob/main/llvm/docs/AMDGPUUsage.rst"">AMDGPUUsage.rst</a> plus <a href=""https://github.com/llvm/llvm-project/blob/main/llvm/include/llvm/IR/IntrinsicsAMDGPU.td"">IntrinsicsAMDGPU.td</a>—and emphasized that <strong>clang builtins map 1:1 to LLVM intrinsics</strong>, which you can reverse‑engineer via <a href=""https://github.com/llvm/llvm-project/tree/main/llvm/test/CodeGen/AMDGPU"">AMDGPU CodeGen tests</a>. Meanwhile, a separate GPU MODE thread advertised <strong>CUDA‑kernel optimization jobs</strong> (profiling with <strong>Nsight Systems/Compute</strong>, writing optimized CUTLASS‑style kernels) via a <a href=""https://tally.so/r/pbDDvZ"">Parsewave posting</a>, underscoring that the ecosystem gravity—and money—is still heavily on the CUDA side.</li>
</ul>
</li>
</ul>
<p><strong>2. New Models, TTS, and Benchmarks Across the Open Ecosystem</strong></p>
<ul>
<li>
<p><strong>Qwen3-TTS Struts In as Multilingual Voice-Cloning Workhorse</strong>: Alibaba launched <strong>Qwen3‑TTS</strong>, a family of open <strong>text‑to‑speech</strong> models (≈<strong>0.6B–1.8B</strong> params) supporting <strong>10 languages</strong> with variants for <strong>VoiceDesign</strong>, <strong>CustomVoice</strong>, and <strong>Base</strong>, released on [GitHub (QwenLM)]https://github.com/QwenLM and <a href=""https://huggingface.co/Qwen"">Hugging Face</a>. Latent Space’s <code>#genmedia-creative-ai</code> highlighted its high‑quality cloning, while Nous community members directly compared <a href=""https://huggingface.co/spaces/Qwen/Qwen3-TTS"">the interactive demo on Hugging Face Spaces</a> to <strong>ElevenLabs</strong>, calling it <em>“a very good voice cloning tool.”</em></p>
<ul>
<li>Early adopters are probing <strong>multilingual robustness</strong> and <strong>clone fidelity</strong>, with one Nous user emphasizing that Qwen3‑TTS is <em>competitive with commercial TTS</em> for user‑facing agents. Latent Space threads grouped it with other audio releases—<strong>NVIDIA PersonaPlex‑7B</strong>, <strong>Inworld TTS‑1.5</strong>, and <strong>Flash Labs’ Chroma 1.0</strong> described in <a href=""https://x.com/lina_colucci/status/2014229002370834861"">Lina Colucci’s roundup</a>—framing Qwen3‑TTS as the open‑source counterpart in a rapidly heating speech‑to‑speech and conversational‑audio race.</li>
</ul>
</li>
<li>
<p><strong>Image Edit Arena Shakes Up Multimodal Rankings</strong>: <strong>LMArena</strong> split its <strong>Image Edit Arena</strong> leaderboard into separate <strong>Single‑Image Edit</strong> and <strong>Multi‑Image Edit</strong> tracks, publishing the results at the new <a href=""https://lmarena.ai/leaderboard/image-edit/overall"">image‑edit leaderboard</a> for finer‑grained comparison of visual editing ability. The reshuffle toppled incumbents: <strong>ChatGPT Image (Latest)</strong> dropped from #1 to <strong>#3</strong>, while <strong>Gemini 3 Pro Image 2K</strong> climbed from #2 to the <strong>top spot</strong>, with Nano Banana and Seedream models also being shuffled and occasionally pulled (e.g., <strong>Seedream‑4‑2k</strong> disappearing for technical reasons).</p>
<ul>
<li>Concurrently, LMArena added <strong>wan2.6‑image</strong> (image‑edit only), <strong>wan2.6‑t2i</strong> (text‑to‑image), and <strong>devstral‑2</strong> (Code Arena) via their <a href=""https://lmarena.ai/c/new?chat-modality=image"">announcements</a>, though users noted a confusing limitation where <code>wan2.6-t2i</code> currently exposes no image upload. Operationally, the platform yanked <strong>Nano Banana Pro 2K</strong> due to high error rates and acknowledged persistent <strong>video generation failures and Linux‑only captchas</strong>, reinforcing that frontier multimodal eval is still bottlenecked as much by infra quirks as by model quality.</li>
</ul>
</li>
<li>
<p><strong>New Open Datasets and Niche Models Fuel Specialized Workloads</strong>: The Hugging Face <code>#i-made-this</code> channel saw the release of <strong>Faust‑1</strong>, a <strong>1.6B German‑first LLM</strong> with ≈<strong>90% German pretraining</strong>, a German‑optimized tokenizer, and DPO‑tuned instructions, published at <a href=""https://huggingface.co/tabularisai/Faust-1"">tabularisai/Faust-1</a> for <strong>local, privacy‑sensitive use cases</strong>. Another contributor released a synthetic <strong>Rust→WebAssembly compilation dataset</strong> of <strong>1,000 programmatically generated Rust programs</strong> at <a href=""https://huggingface.co/datasets/webxos/wasm_synthetic_dataset"">webxos/wasm_synthetic_dataset</a>, with deterministic Fibonacci‑based pseudo‑random generation to ensure reproducible code patterns and compiler behaviors.</p>
<ul>
<li>Alongside these, a <strong>safety dataset</strong> for alignment research landed at <a href=""https://huggingface.co/datasets/Pacific-Prime/safety_dataset"">Pacific-Prime/safety_dataset</a>, while a separate project generated custom <strong>typeface datasets</strong> via <a href=""https://webxos.netlify.app/COLIGNUM"">COLIGNUM</a> for font‑centric ML work. Collectively these releases hint at a maturing long tail of <strong>domain‑specific corpora</strong>—language‑localized LLMs, compiler‑oriented code sets, safety supervision data, and typographic datasets—feeding into RAG systems, continual‑learning workflows, and evaluation of program synthesis and WebAssembly tooling.</li>
</ul>
</li>
</ul>
<p><strong>3. Agentic Frameworks, DSPy/RLM, and IDE Tooling</strong></p>
<ul>
<li>
<p><strong>DSPy and RLM Reframe Agents as Optimizable Programs</strong>: In the DSPy Discord, an article titled <a href=""https://eito.substack.com/p/dspy-the-most-misunderstood-agent"">""DSPy: The Most Misunderstood Agent Framework""</a> argued that DSPy’s real value is its <strong>signature &#x26; module abstraction</strong>, not just <strong>GEPA and prompt‑tuning hacks</strong>, stressing that programs of LMs should be treated like differentiable pipelines rather than hand‑wired agents. Another blog, <a href=""https://raveesh.substack.com/p/a-pragmatic-recipe-for-continual?r=qn1thttps://arxiv.org/abs/2512.21859"">""A Pragmatic Recipe for Continual Learning""</a>, pitched <strong>DSPy.RLM()</strong> as a core building block for engineered continual‑learning systems that retrain themselves over time.</p>
<ul>
<li>Members experimented with <strong>RLM prompts</strong> to improve reasoning—complaining that some models still give <em>""vague generic answers""</em>—and proposed optimizing RLM traces similarly to <strong>ReAct</strong>, where an optimizer inspects step‑by‑step logs while users only care about final outputs. There was also interest in building a <strong>custom GEPA adapter</strong> for DSPy’s JSON output layer, so that <strong>json_schema‑based responses</strong> can be optimized to drop redundant system tokens and reduce overhead for structured tool integrations.</li>
</ul>
</li>
<li>
<p><strong>IDE Agents Evolve: Cursor Subagents and Aider Workflows</strong>: The <strong>Cursor</strong> community dissected the <strong>2.4 release</strong>, which introduced parallel <strong>subagents</strong> (documented in the <a href=""https://cursor.com/changelog"">Cursor changelog</a> and demo video) that inject a <code>&#x3C;subagent_delegation_context></code> to farm tasks out in parallel, plus <strong>image generation</strong> and clarification‑question capabilities advertised on <a href=""https://x.com/cursor_ai/status/2014433672401977382"">Cursor’s X post</a>. However, users in <code>#general</code> reported <strong>Composer 1 infinite loops</strong>, heavy lag in <strong>2.4</strong> (<code>""planning next moves""</code> hanging), and suspected that broken subagent scaffolding was calling a missing <strong>Task tool</strong>, forcing many to downgrade to <strong>2.3</strong>—especially on older macOS versions like <strong>Big Sur 11.7.3</strong>.</p>
<ul>
<li>Separately, the <strong>aider</strong> community proposed a terminal UI and <strong>session management</strong> for the CLI‑based coding assistant, aiming to let users edit the next message while scrolling past replies, render rich Markdown (including <strong>mermaid diagrams</strong>), and save/load entire chat contexts without polluting the current prompt. Power users described a <strong>meta‑workflow</strong> pairing <em>aider</em> for context management and search‑replace coding with <strong>Claude Code</strong> for hard bug‑hunting, framing aider as the <em>“file selection &#x26; edit engine”</em> that minimizes tokens while a remote LLM handles deeper reasoning.</li>
</ul>
</li>
<li>
<p><strong>MCP and Schema Design for Tool-Calling Agents</strong>: In the <strong>MCP Contributors</strong> Discord, contributors scrutinized the <strong>Model Context Protocol</strong>’s <code>GetTaskPayloadResult</code> schema, pointing out that its use of <code>""additionalProperties""</code> in the JSON Schema at <a href=""https://github.com/modelcontextprotocol/modelcontextprotocol/blob/8d07c35d3857412a351c595fe01b7bc70664ba06/schema/2025-11-25/schema.json#L1245-L1256"">this definition</a> makes payloads too permissive. They proposed switching to an <code>anyOf</code> union of explicit alternatives to enforce that only pre‑declared fields appear, tightening validation for tool payloads.</p>
<ul>
<li>The discussion framed this as a <strong>tooling‑reliability tradeoff</strong>: <code>additionalProperties</code> keeps MCP extensible for new tools, but weakens static guarantees, whereas <code>anyOf</code> helps clients and servers catch malformed or adversarial payloads early. Given MCP’s ambition as a cross‑tool agent protocol, participants argued that <strong>strict schemas for core messages like <code>GetTaskPayloadResult</code></strong> matter for security, debugging, and interop, even if it requires more frequent schema migrations.</li>
</ul>
</li>
</ul>
<p><strong>4. Experimental Architectures, Optimization Tricks, and Training Methods</strong></p>
<ul>
<li>
<p><strong>Difference Layers and SwiGLU Turbocharge NanoGPT Baselines</strong>: In <strong>Eleuther’s #research</strong>, a contributor reported strong gains on <strong>CartPole</strong> and <strong>NanoGPT</strong> by swapping the standard MLP with a <em>difference layer</em> <code>x = (a2(x) - b2(x)) * (c2(x) - d2(x)) + e2(x)</code> from <a href=""https://github.com/Eternalyze0/difference_layer"">Eternalyze0/difference_layer</a>, claiming better performance at lower parameter and compute budgets. Others cautioned that such multiplicative structures effectively <strong>double the learning rate</strong> under SGD and that improvements may vanish against well‑tuned baselines rather than the default NanoGPT configs.</p>
<ul>
<li>Researchers also noted that simply replacing GELU with <strong>SwiGLU</strong> as the transformer activation significantly improves the <strong>NanoGPT baseline</strong>, and further gains appear when combining SwiGLU with the difference‑layer QKV replacement. Senior members repeatedly pointed newcomers at Noam Shazeer’s <a href=""https://arxiv.org/abs/2002.05202"">""GLU Variants Improve Transformer"" paper</a>, warning that any new gating trick should be benchmarked against <strong>state‑of‑the‑art GLU baselines</strong> before being touted as a structural breakthrough.</li>
</ul>
</li>
<li>
<p><strong>GRPO, Attention Sinks, and Reasoning Training Gotchas</strong>: Unsloth’s <code>#research</code> and <code>#off-topic</code> channels hosted candid post‑mortems on <strong>GRPO (Generalized Reinforcement Policy Optimization)</strong>, with one practitioner concluding from experiments (and re‑reading the <a href=""https://arxiv.org/abs/2601.07568"">""DeepSeek R1"" paper</a>) that GRPO <strong>refines existing reasoning</strong> but does not magically unlock <em>“emergent reasoning”</em> in niche domains where pretraining data is thin. They described a three‑stage pipeline—corpus CPT on novels + medical articles (~400M tokens), translated SFT, then synthetic polishing via rejection sampling—and still found GRPO unstable for specialized tasks like Turkish translation and domain support.</p>
<ul>
<li>On the representation side, Unsloth members debated <strong>attention sinks</strong>, with some manually injecting <code>&#x3C;|endoftext|></code> at the context start, while others argued that <em>“attention sink is poured into the very first token in the entire context window, just one token for it”</em> and that models learn their own sink dynamics. A separate lesson learned the hard way: when using small models to generate <em>chain‑of‑thought</em> traces to train bigger reasoning models, <strong>masking the thinking tokens</strong> during supervised training significantly improves metrics (unmasked CoT caused <strong>F1 to crater</strong>, despite seeming attractive for interpretability).</li>
</ul>
</li>
<li>
<p><strong>Test-Time-Training, LM Kernels, and Self-Replication Benchmarks</strong>: GPU MODE’s <code>#general</code> and <code>#multi-gpu</code> channels highlighted <strong>test‑time training (TTT)</strong> as a promising paradigm not just for models but for <strong>LM‑generated GPU kernels</strong>, with the <strong>discover.pdf</strong> paper at <a href=""https://test-time-training.github.io/discover.pdf"">test-time-training.github.io/discover.pdf</a> showing that adapting kernels against benchmark suites at inference can yield surprisingly strong performance. In parallel, debugging threads around <strong>NCCL on B200s under Slurm</strong>—including sbatch scripts and <code>NCCL_DEBUG=INFO</code> logs—reinforced that auto‑tuning comms libraries plus dynamic kernel adaptation is becoming a combined engineering problem rather than a pure modeling issue.</p>
<ul>
<li>Over in Nous, a member brainstormed a <strong>self‑replication benchmark for agentic AI</strong>, and someone suggested using Claude’s C‑implemented transformer engine and custom CPU described in <a href=""https://github.com/cpldcpu/smollm.c/blob/claude/train-small-model-llxVr/train-small-model-llxVr/smolc/smolc.c"">cpldcpu’s <code>smollm.c</code></a> as a target: can an agent inspect, modify, and re‑deploy its own inference engine? This dovetails with DSPy/RLM discussions, hinting at a future where <strong>agents optimize both their weights and their low‑level kernels</strong> at inference time under constrained hardware budgets.</li>
</ul>
</li>
</ul>
<p><strong>5. AI Business, APIs, and Production Reliability</strong></p>
<ul>
<li>
<p><strong>Baseten’s $300M Raise and Capital One–Brex Deal Signal AI Infra Consolidation</strong>: Latent Space’s <code>#ai-general-chat</code> flagged two major deals: <strong>Capital One acquiring Brex for $5.15B</strong>, as reported by <a href=""https://x.com/alexfmac/status/2014676950883668306"">Alex MacCaw</a>, marking the largest bank–fintech acquisition to date; and <strong>Baseten’s $300M Series E at a $5B valuation</strong>, announced in <a href=""https://xcancel.com/basetenco/status/2014755013344792595?s=46"">Baseten’s tweet</a> with IVP and CapitalG leading and NVIDIA participating. Both moves underscore that <strong>AI‑heavy infra and fintech tooling</strong> are being rapidly absorbed or capitalized by large incumbents and late‑stage investors.</p>
<ul>
<li>Members interpreted the Baseten round as validation that <strong>model serving and orchestration</strong> is a defensible, high‑margin layer even in an open‑model world, while the Capital One–Brex deal was read as a bet that <strong>data‑rich fintech workflows</strong> (expense management, cards, underwriting) will be increasingly AI‑automated. Combined with SimilarWeb stats shared in a <a href=""https://xcancel.com/venturetwins/status/2014739492389978274?s=46"">Venture Twins tweet</a> showing <strong>ChatGPT still dominating traffic while Grok grows 33× in US penetration</strong>, the community sees a landscape where infra, data, and distribution matter at least as much as raw model quality.</li>
</ul>
</li>
<li>
<p><strong>Perplexity Pro and API Turbulence Threaten Power-User Workloads</strong>: On the <strong>Perplexity AI</strong> server, Pro subscribers reported abrupt <strong>file‑upload caps of 3/day</strong>, conflicting <strong>research query limits</strong> (some seeing <strong>600/day</strong>, others as low as <strong>20</strong>), and persistent <strong>401 Unauthorized</strong> errors on the <strong>Perplexity API</strong> even after renewing credits, which broke production use cases like a sports‑betting model. Threads in <code>#general</code> and <code>#pplx-api</code> speculated about <strong>A/B experiments vs. cost‑cutting</strong>, and some users threatened to cancel, arguing that silent feature downgrades destroy trust for teams trying to treat Perplexity as a dependable research backend.</p>
<ul>
<li>At the model layer, users shared a medical example where <strong>Gemini, Claude Opus, and Ernie</strong> all failed to recommend a <strong>DEXA bone‑density scan</strong> when asked about calcium deficiency work‑ups, whereas <strong>GPT</strong> explicitly mentioned it, reinforcing that Perplexity’s meta‑model/engine choice can materially affect clinical recommendations. Combined with billing bugs (pending charges, locked accounts) and contested celestial fact‑checking drama, the overarching sentiment was that <strong>Perplexity’s product is powerful but operationally fragile</strong>, and engineers should have fallbacks before wiring it deep into production flows.</li>
</ul>
</li>
<li>
<p><strong>IDE, API, and Billing Reliability: Cursor, Manus, and OpenRouter</strong>: Cursor power‑users complained that <strong>2.4</strong> shipped with <strong>severe lag, crashes, and broken Composer 1 loops</strong>, and also raised <strong>billing opacity</strong> concerns: inconsistent dollar displays, unpredictable limits in <strong>Auto</strong> mode, and unexplained bonus credits prompted some to rely on <a href=""https://token-watch.vercel.app/"">token-watch</a> for independent usage audits. Over in <strong>Manus.im</strong>, a user reported being charged <strong>$400</strong> for an annual plan despite selecting monthly during a trial, and openly discussed escalating to <strong>FTC/BBB/Attorney General</strong> if not refunded, warning others to double‑check plan terms.</p>
<ul>
<li>The <strong>OpenRouter</strong> community noticed that internal <strong> reasoning blocks</strong> recently started leaking into end‑user responses in OR Chat and JanitorAI, raising UX and privacy questions and triggering a support ticket. Meanwhile, an OpenRouter thread about <strong>uncensored image generation</strong> concluded that engineers should pair one <strong>text LLM</strong> with a separate <strong>image model</strong> rather than expect a single uncensored multimodal endpoint, while some users half‑jokingly proposed an <strong>OpenRouter gacha system</strong> with pity mechanics and leaderboards, reflecting both frustration with opaque pricing and a desire for more transparent, game‑like model discovery.</li>
</ul>
</li>
</ul>
<h2>gpt-5</h2>
<p><strong>1. New TTS and Audio AI Releases</strong></p>
<ul>
<li>
<p><strong><strong>Tongue-Twisting TTS Triumphs</strong></strong>: Alibaba unveiled <strong>Qwen3-TTS</strong> with <strong>VoiceDesign</strong>, <strong>CustomVoice</strong>, and <strong>Base</strong> variants (five models, <strong>0.6B–1.8B</strong> params, <strong>10 languages</strong>), with releases on <a href=""https://github.com/QwenLM"">QwenLM GitHub</a> and <a href=""https://huggingface.co/Qwen"">Hugging Face</a>.</p>
<ul>
<li>Community demos showcased high-fidelity cloning and multilingual synthesis via the official <a href=""https://huggingface.co/spaces/Qwen/Qwen3-TTS"">Qwen3-TTS Space</a>, with users calling the results <em>“very good voice cloning.”</em></li>
</ul>
</li>
<li>
<p><strong><strong>Audio Arms Race Accelerates</strong></strong>: A roundup highlighted NVIDIA’s <strong>PersonaPlex‑7B</strong> (full‑duplex), <strong>Inworld TTS‑1.5</strong> (low‑latency), and <strong>Flash Labs’ Chroma 1.0</strong> (open end‑to‑end speech‑to‑speech), summarized in <a href=""https://x.com/lina_colucci/status/2014229002370834861"">Lina Colucci’s post</a>.</p>
<ul>
<li>Engineers discussed how these releases push <strong>real‑time conversational</strong> and <strong>SS2S</strong> stacks toward production, framing Q3–Q4 as a breakout window for low‑latency voice agents.</li>
</ul>
</li>
<li>
<p><strong><strong>Voice Design Goes DIY</strong></strong>: <strong>Qwen3-TTS</strong>’s <strong>VoiceDesign</strong> and <strong>CustomVoice</strong> features enable user‑defined voices and cloning workflows with accessible configs and assets on <a href=""https://huggingface.co/Qwen"">Hugging Face</a>.</p>
<ul>
<li>Builders reported that the Space’s cloning quality <em>“rivals <strong>ElevenLabs</strong>”</em> in quick trials, encouraging bake‑offs using the <a href=""https://huggingface.co/spaces/Qwen/Qwen3-TTS"">official demo</a>.</li>
</ul>
</li>
</ul>
<p><strong>2. AI Kernel Benchmarks &#x26; Optimization</strong></p>
<ul>
<li>
<p><strong><strong>FlashInfer Frenzy Benchmarks Kernels</strong></strong>: CMU Catalyst Lab introduced <strong>FlashInfer‑Bench</strong>, a framework to evaluate <strong>AI‑generated GPU kernels</strong> and deploy them into serving engines: <a href=""https://mlsys26.flashinfer.ai"">FlashInfer‑Bench</a>.</p>
<ul>
<li>Participants praised the effort as <em>“a very cool project,”</em> and the team invited collaboration on refining benchmarks and production deployment pathways.</li>
</ul>
</li>
<li>
<p><strong><strong>TTT Tunes Tiny Kernels</strong></strong>: Researchers evaluated <strong>LM‑generated kernels</strong> with <strong>test‑time training (TTT)</strong> on prior leaderboards, reporting promising outcomes in the paper <a href=""https://test-time-training.github.io/discover.pdf"">Discovering Test-Time Training</a>.</p>
<ul>
<li>Discussions centered on how <strong>TTT</strong> can adapt kernels to distribution shifts at inference, potentially boosting leaderboard parity without retraining.</li>
</ul>
</li>
<li>
<p><strong><strong>ROCm Readmes Reveal Intrinsics</strong></strong>: Engineers mapped <strong>clang builtins → LLVM intrinsics</strong> for AMD GPUs using the <strong>CDNA4 ISA manual</strong> and LLVM docs: <a href=""https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/instruction-set-architectures/amd-instinct-cdna4-instruction-set-architecture.pdf"">AMD ISA PDF</a>, <a href=""https://github.com/llvm/llvm-project/blob/main/llvm/docs/AMDGPUUsage.rst"">AMDGPUUsage.rst</a>.</p>
<ul>
<li>They also pointed to <a href=""https://github.com/llvm/llvm-project/blob/main/llvm/include/llvm/IR/IntrinsicsAMDGPU.td"">IntrinsicsAMDGPU.td</a> and CodeGen tests for examples, helping practitioners align kernel code with <strong>ROCm</strong>’s compilation model.</li>
</ul>
</li>
</ul>
<p><strong>3. Agentic IDEs and Dev Tooling</strong></p>
<ul>
<li>
<p><strong><strong>Copilot SDK Cuts the Cord</strong></strong>: Developers celebrated the release of the <strong>GitHub Copilot SDK</strong>, which enables first‑class <strong>AI features</strong> inside apps via GitHub’s infra: <a href=""https://github.com/github/copilot-sdk"">github.com/github/copilot-sdk</a>.</p>
<ul>
<li>Early adopters emphasized replacing bespoke routing and third‑party pricing with a native SDK, streamlining <strong>tool‑augmented agent</strong> integration.</li>
</ul>
</li>
<li>
<p><strong><strong>Cursor Subagents Sprint in 2.4</strong></strong>: <strong>Cursor 2.4</strong> shipped parallel <strong>subagents</strong> for faster execution and better context use, plus <strong>image generation</strong> and clarifying questions: <a href=""https://cursor.com/changelog"">Changelog</a> and <a href=""https://x.com/cursor_ai/status/2014433672401977382"">Cursor on X</a>.</p>
<ul>
<li>The team’s video demo shows subagents coordinating on multi‑step tasks, promising speedups for complex coding flows.</li>
</ul>
</li>
<li>
<p><strong><strong>Baseten Banks Big Bucks</strong></strong>: <strong>Baseten</strong> raised <strong>$300M Series E</strong> (IVP, CapitalG; participation from NVIDIA), reaching a <strong>$5B</strong> valuation: <a href=""https://xcancel.com/basetenco/status/2014755013344792595"">Baseten announcement</a>.</p>
<ul>
<li>Infra‑minded builders read this as a signal of sustained demand for <strong>model serving</strong>, <strong>ops</strong>, and <strong>agent backends</strong> at enterprise scale.</li>
</ul>
</li>
</ul>
<p><strong>4. Model Speedups &#x26; Evaluation Arenas</strong></p>
<ul>
<li>
<p><strong><strong>llama.cpp Lights Up GLM Flash</strong></strong>: <strong>llama.cpp</strong> improved performance for <strong>GLM‑4.7 Flash GGUF</strong> by ~<strong>1.5×</strong> and fixed bugs; users were told to rebuild and fetch fixed quants from <a href=""https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF"">unsloth/GLM‑4.7‑Flash‑GGUF</a>.</p>
<ul>
<li>Reports cited stable <strong>50 tok/s at 50k context</strong> with flash attention enabled, noting it <em>“is working beautifully”</em> after the fix.</li>
</ul>
</li>
<li>
<p><strong><strong>Leaderboard Split Sharpens Image Edits</strong></strong>: LMArena split the <strong>Image Edit Arena</strong> into <strong>Single‑Image Edit</strong> vs <strong>Multi‑Image Edit</strong>, revealing shifts on the <a href=""https://lmarena.ai/leaderboard/image-edit/overall"">overall leaderboard</a>.</p>
<ul>
<li><strong>ChatGPT Image (Latest)</strong> dropped from #1→#3 while <strong>Gemini 3 Pro Image 2K</strong> rose from #2→#1, offering clearer task‑specific rankings.</li>
</ul>
</li>
<li>
<p><strong><strong>Arena Adds Wan &#x26; Devstral</strong></strong>: New LMArena entries include <strong>wan2.6‑t2i</strong> (text‑to‑image), <strong>wan2.6‑image</strong> (image edit), and <strong>devstral‑2</strong> (code), available via <a href=""https://lmarena.ai"">LMArena</a>.</p>
<ul>
<li>The split of <code>wan2.6</code> into distinct edit vs T2I endpoints aims to reduce misuse and clarify capabilities in head‑to‑head evaluations.</li>
</ul>
</li>
</ul>
<p><strong>5. Research Tricks in Architectures &#x26; Training</strong></p>
<ul>
<li>
<p><strong><strong>SwiGLU Swings the Baseline</strong></strong>: Researchers reported that switching <strong>GELU → SwiGLU</strong> significantly boosted <strong>NanoGPT</strong>‑style baselines, aligning with <a href=""https://arxiv.org/abs/2002.05202"">Shazeer’s GLU variants paper</a>.</p>
<ul>
<li>The conversation emphasized strong baselines to avoid mistaking optimization gains for architectural advances.</li>
</ul>
</li>
<li>
<p><strong><strong>Difference Layer Doubles Down</strong></strong>: A proposed multiplicative <strong>difference layer</strong> improved <strong>cartpole</strong> and <strong>nanogpt</strong> performance with fewer params/compute: <a href=""https://github.com/Eternalyze0/difference_layer"">difference_layer repo</a>.</p>
<ul>
<li>Skeptics noted the formulation may implicitly <strong>double the effective learning rate</strong>, urging comparisons against tuned baselines rather than defaults.</li>
</ul>
</li>
<li>
<p><strong><strong>GRPO Gets Groggy</strong></strong>: Engineers found <strong>GRPO</strong> <em>“proved unstable”</em> in niche domains lacking pretraining coverage, debating claims of emergent reasoning in the <strong>DeepSeek</strong> paper: <a href=""https://arxiv.org/abs/2601.07568"">arXiv:2601.07568</a>.</p>
<ul>
<li>Consensus landed on using <strong>GRPO</strong> to refine existing capabilities, while reserving broader reasoning improvements for data/architecture changes.</li>
</ul>
</li>
</ul>
<hr>
<h1>Discord: High level Discord summaries</h1>
<h2><a href=""https://discord.com/channels/1105891499641684019"">BASI Jailbreaking</a> Discord</h2>
<ul>
<li><strong>Gemini 3 Pro Succumbs to ENI Jailbreak</strong>: The <strong>ENI jailbreak</strong>, originally designed for Claude, has been found to work on <strong>Gemini 3 Pro</strong> in AI Studio, a user shares a <a href=""https://github.com/pranrichh/Jailbreaks/blob/main/GEMINI-CLAUDE%20JAILBREAK.md"">GitHub link</a> for setup.
<ul>
<li>A member confirmed it <em>works like magic</em>, even with <strong>3 flash</strong>.</li>
</ul>
</li>
<li><strong>PrimeTalk System Claims Vanilla AI Coherence</strong>: A user introduced the <strong>PrimeTalk</strong> system, asserting it transforms the <em>chaos</em> of vanilla AI into <em>coherence</em> by structuring the token stream and imposing logic, consequence, and presence, sharing a <a href=""https://cdn.discordapp.com/attachments/1228043845967544380/1464048995935584286/PRIMETALK_v3.85_VALHALLA_BUILD_PUBLIC_EDITION.txt?ex=69755ee1&#x26;is=69740d61&#x26;hm=127ea1d81011f7f4ba420f7a6640de5d276bb68916281a5490c0980cf8e29a16&#x26;"">PRIMETALK_v3.85_VALHALLA_BUILD_PUBLIC_EDITION.txt file</a>.
<ul>
<li>The system aims to structure the token stream and impose logic, consequence, and presence to achieve this transformation.</li>
</ul>
</li>
<li><strong>GPT Models Supposedly Broken Via UI Exploits</strong>: A user states they <em>have broken all the GPT models that exist and that will come</em>, arguing that exploiting the UI constitutes a jailbreak.
<ul>
<li>They provided an irrelevant prompt that's supposedly effective on most models, for stealing prompts, although others believe this is not a true jailbreak.</li>
</ul>
</li>
<li><strong>Wargame for Red Teaming Announced</strong>: A user shared a wargame particularly relevant to #red-teaming and posted a <a href=""https://discord.com/channels/1105891499641684019/1235691879492751460/1464349033949692170"">link to the relevant Discord channel</a>.
<ul>
<li>The user was unsure whether cross-posting of the event was appropriate in the channel.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1179035537009545276"">Unsloth AI (Daniel Han)</a> Discord</h2>
<ul>
<li><strong>MoE Models Squeeze into 8GB RAM</strong>: Members are debating the feasibility of running <strong>Mixture of Experts (MoE)</strong> models in 8GB RAM, with suggestions for <strong>Gemma 3N</strong> and <strong>LFM</strong>, but not...</li>
</ul>
","{""title"":""not much happened today"",""link"":""https://news.smol.ai/issues/26-01-22-not-much/"",""pubDate"":""Thu, 22 Jan 2026 05:44:39 GMT"",""content:encoded"":""<p><strong>a quiet day</strong></p>\n<blockquote>\n<p>AI News for 1/22/2026-1/23/2026. We checked 12 subreddits, <a href=\""https://twitter.com/i/lists/1585430245762441216\""><strong>544</strong> Twitters</a> and <strong>24</strong> Discords (<strong>206</strong> channels, and <strong>7161</strong> messages) for you. Estimated reading time saved (at 200wpm): <strong>579 minutes</strong>. <strong>Our new website</strong> is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on <a href=\""https://x.com/Smol_AI\"">@smol_ai</a>!</p>\n</blockquote>\n<hr>\n<h1>AI Twitter Recap</h1>\n<p><strong>Top tweets (by engagement)</strong></p>\n<ul>\n<li><strong>Anthropic ships “Claude in Excel”</strong>: Claude in Excel expands to Pro, with multi-file drag/drop, safer cell writes, and longer sessions via auto-compaction (<a href=\""https://twitter.com/claudeai/status/2014834616889475508\"">claudeai</a>). Big engagement discussion about Microsoft 365 Copilot lagging (<a href=\""https://twitter.com/Yuchenj_UW/status/2014835455393726726\"">Yuchenj_UW</a>).</li>\n<li><strong>OpenAI roadmap + agent loop</strong>: Sam Altman says Codex launches are coming and OpenAI is nearing a “Cybersecurity High” level with restrictions and later “defensive acceleration” (<a href=\""https://twitter.com/sama/status/2014733975755817267\"">sama</a>). OpenAI publishes a technical deep dive into the <strong>Codex agent loop / harness orchestration</strong> (<a href=\""https://twitter.com/OpenAIDevs/status/2014794871962533970\"">OpenAIDevs</a>).</li>\n<li><strong>Google AI Ultra limits boosted</strong>: Gemini App daily quotas increased to <strong>1,500 Thinking</strong> + <strong>500 Pro</strong> prompts/day for Ultra members (<a href=\""https://twitter.com/joshwoodward/status/2014566936479437173\"">joshwoodward</a>).</li>\n<li><strong>Sakana AI ↔ Google partnership + investment</strong>: Sakana announces strategic partnership and funding from Google to combine <strong>Gemini/Gemma</strong> with Sakana’s “AI Scientist” / “ALE-Agent” work and to deploy in high-security domains in Japan (<a href=\""https://twitter.com/SakanaAILabs/status/2014686043711406355\"">SakanaAILabs</a>, <a href=\""https://twitter.com/hardmaru/status/2014686852691918971\"">hardmaru</a>, <a href=\""https://twitter.com/JeffDean/status/2014716109216448975\"">JeffDean</a>).</li>\n<li><strong>Cursor launches Agent Skills</strong>: First-class “Skills” for agents, emphasizing discovery + dynamic context focus (<a href=\""https://twitter.com/cursor_ai/status/2014753596223770841\"">cursor_ai</a>).</li>\n<li><strong>FrontierMath jump</strong>: <strong>GPT-5.2 Pro hits 31% on FrontierMath Tier 4</strong>, up from 19% previous best (<a href=\""https://twitter.com/EpochAIResearch/status/2014769359747744200\"">EpochAIResearch</a>). Practitioners highlight usefulness and even benchmark issue-spotting (<a href=\""https://twitter.com/gdb/status/2014859263701839963\"">gdb</a>).</li>\n<li><strong>Claude Code “run locally for free” how-to</strong>: A popular tutorial claims running Claude Code-like workflows locally with open models, private + tool-enabled (<a href=\""https://twitter.com/dr_cintas/status/2014771670070747278\"">dr_cintas</a>).</li>\n<li><strong>Baseten raises $300M</strong> at <strong>$5B valuation</strong>, positioning around the “many-model future” and high-performance inference (<a href=\""https://twitter.com/basetenco/status/2014755013344792595\"">basetenco</a>, <a href=\""https://twitter.com/tuhinone/status/2014755252244005273\"">tuhinone</a>).</li>\n</ul>\n<hr>\n<p><strong>Frontier models, benchmarks, and the “capability” narrative</strong></p>\n<ul>\n<li><strong>Math as a leading indicator (FrontierMath + cross-benchmark correlations)</strong>: Epoch reports <strong>GPT-5.2 Pro = 31%</strong> on FrontierMath Tier 4 (no overfitting claimed), a sizable step up from 19% (<a href=\""https://twitter.com/EpochAIResearch/status/2014769359747744200\"">EpochAIResearch</a>). Separate Epoch analysis argues benchmark scores correlate strongly across domains (≈<strong>0.68</strong> across domains, ≈<strong>0.79</strong> within-domain), implying a latent capability factor behind “math/coding/reasoning” progress (<a href=\""https://twitter.com/EpochAIResearch/status/2014806095504785664\"">EpochAIResearch</a>). Practitioners note concrete value: catching problem flaws/typos and even “pointing out a flaw” in a Tier 4 problem (<a href=\""https://twitter.com/gdb/status/2014859263701839963\"">gdb</a>, <a href=\""https://twitter.com/GregHBurnham/status/2014774878591655984\"">GregHBurnham</a>).</li>\n<li><strong>AGI timelines vs product reality</strong>: A recurring theme is “systems are uneven”: smart in formal domains, unreliable elsewhere. A widely shared quip captures this mismatch (“smarter than a PhD in math, dumber than an intern”) (<a href=\""https://twitter.com/Yuchenj_UW/status/2014564105194242452\"">Yuchenj_UW</a>). François Chollet stresses that progress is <strong>vertical-specific</strong> (especially in verifiable domains like code) because unlimited synthetic data makes memorization/operationalization easier there, and warns against extrapolating to all human tasks (<a href=\""https://twitter.com/fchollet/status/2014821042464948270\"">fchollet</a>).</li>\n<li><strong>Reasoning + continual learning as the “real” frontier</strong>: Reporting from interviews claims Shane Legg pegs <strong>50% chance of “minimal AGI” by 2028</strong> with Google’s definition including continuous learning/memory/world models (<a href=\""https://twitter.com/kimmonismus/status/2014697026890416586\"">kimmonismus</a>). Follow-on notes say Demis Hassabis explicitly says DeepMind <strong>has not solved continual learning</strong>, and is exploring combinations of AlphaZero-like approaches with foundation models (<a href=\""https://twitter.com/Yuchenj_UW/status/2014785682309579119\"">Yuchenj_UW</a>, <a href=\""https://twitter.com/Hangsiin/status/2014774897680253442\"">Hangsiin</a>).</li>\n<li><strong>Model/arch discourse: MoE provenance fights</strong>: A thread disputes the claim that DeepSeek MoE “built on Mixtral,” arguing the DeepSeek MoE paper appeared almost immediately after Mixtral’s arXiv release, Mixtral training details were sparse, and DeepSeek’s MoE is architecturally different/more sparse and cites <strong>GShard</strong> not Mixtral (<a href=\""https://twitter.com/eliebakouch/status/2014575628675092845\"">eliebakouch</a>). Another framing calls DeepSeek a distinct “neoMoE” tree vs “oldMoE” (<a href=\""https://twitter.com/kalomaze/status/2014659449219383367\"">kalomaze</a>).</li>\n<li><strong>Second-tier multimodal updates (China)</strong>: A detailed Chinese-language review positions <strong>Baidu ERNIE 5.0</strong> as improved and stable but still costly (2T params, ~61K context) and “firmly second tier” vs top multimodal systems with large compute budgets (<a href=\""https://twitter.com/ZhihuFrontier/status/2014606592826912840\"">ZhihuFrontier</a>).</li>\n</ul>\n<hr>\n<p><strong>Agents and coding: from workflows → harnesses → skills</strong></p>\n<ul>\n<li><strong>OpenAI Codex “agent loop” becomes explicit</strong>: OpenAI publishes how Codex orchestrates turns: assemble inputs → run inference → execute tools → feed results back until stopping, i.e., the agent harness as a first-class system component (<a href=\""https://twitter.com/OpenAIDevs/status/2014794871962533970\"">OpenAIDevs</a>). This aligns with broader commentary that “training better models is only one axis; harness + experimentation can surprise” (<a href=\""https://twitter.com/Hangsiin/status/2014794375466033657\"">Hangsiin</a>).</li>\n<li><strong>Workflows vs agents is collapsing into “skills / guidance / RLMs”</strong>: A strong technical synthesis argues the agent/workflow boundary is less about “control flow in code” and more about <strong>state representation</strong>, <strong>dynamic instruction selection</strong>, and <strong>who dictates composition</strong>—with Replit’s “Decision-Time Guidance,” Skills, and <strong>Recursive Language Models (RLMs)</strong> as hybrid points on the design spectrum (<a href=\""https://twitter.com/lateinteraction/status/2014685012994515206\"">lateinteraction</a>). DSPy community posts push RLMs as a practical method for “arbitrarily long prompts” by delegating to code + subcalls instead of summarization loss (<a href=\""https://twitter.com/getpy/status/2014717862246756384\"">getpy</a>).</li>\n<li><strong>Cursor: Agent Skills shipped</strong>: Cursor introduces <strong>Skills</strong> as discoverable specialized prompts/code and pitches them as also improving context focus via dynamic discovery (<a href=\""https://twitter.com/cursor_ai/status/2014753596223770841\"">cursor_ai</a>, <a href=\""https://twitter.com/cursor_ai/status/2014753597624598665\"">cursor_ai</a>). This is echoed by the broader market trend: “make non-devs write code by calling it skills” (<a href=\""https://twitter.com/kylebrussell/status/2014689618617122883\"">kylebrussell</a>).</li>\n<li><strong>Claude Code ecosystem keeps expanding (and copying wars)</strong>: Multiple posts highlight rapid feature diffusion between tools (“cursor adopting popular features from claude code”) (<a href=\""https://twitter.com/dejavucoder/status/2014635509025526198\"">dejavucoder</a>). Practical snippets: Claude tasks stored on filesystem (<code>~/.claude/tasks</code>) enabling multi-session/subagent collaboration via broadcasts (<a href=\""https://twitter.com/dejavucoder/status/2014584272183861407\"">dejavucoder</a>). At the same time, pain points remain (e.g., absurd file download hacks via base64) (<a href=\""https://twitter.com/dbreunig/status/2014540341526069738\"">dbreunig</a>).</li>\n<li><strong>Security posture is becoming a headline feature</strong>: Sam Altman states OpenAI will increasingly constrain coding models for cybercrime and later pivot to <strong>defensive acceleration</strong> (helping patch bugs) as mitigation (<a href=\""https://twitter.com/sama/status/2014733975755817267\"">sama</a>). One anecdote flags a potential security footgun: Codex Slack integration produced shareable task links accessible without auth in incognito (if accurate, it’s an urgent product-security issue) (<a href=\""https://twitter.com/apsdehal/status/2014770563810758938\"">apsdehal</a>).</li>\n<li><strong>Enterprise “agents fail in production” reminder</strong>: A long post claims <strong>95% of enterprise AI pilots fail</strong> (citing MIT research), emphasizing that production viability is about <strong>authorization-aware retrieval</strong>, <strong>guardrails</strong>, <strong>monitoring</strong>, and <strong>auditability</strong>, not demo capability (<a href=\""https://twitter.com/victorialslocum/status/2014654495301525683\"">victorialslocum</a>).</li>\n</ul>\n<hr>\n<p><strong>Inference + systems: vLLM, KV compression, storage, and infra maturity</strong></p>\n<ul>\n<li><strong>vLLM keeps becoming the open inference “substrate”</strong>: vLLM positions itself as the bridge from open models to deployable inference, highlighting vLLM Studio workflows (<a href=\""https://twitter.com/vllm_project/status/2014536660361584833\"">vllm_project</a>). A notable infra-engineering post documents a difficult <strong>vLLM memory leak</strong> debugging path (Python profilers → pmap → BPFtrace → GDB) traced to <strong>UCX mmap hooks</strong>; fix merged upstream (<a href=\""https://twitter.com/vllm_project/status/2014630499231412477\"">vllm_project</a>).</li>\n<li><strong>System intelligence / routing</strong>: vLLM announces a public beta of <strong>vLLM-SR</strong> (Semantic Router) on AMD, framing it as a “system intelligence” approach rather than monolithic models doing everything (<a href=\""https://twitter.com/XunzhuoLiu/status/2014672307407704279\"">XunzhuoLiu</a>).</li>\n<li><strong>KV cache compression via distillation</strong>: NVIDIA Research releases <strong>Qwen3-8B-DMS-8x</strong>, claiming <strong>8× KV cache compression</strong> with minimal overhead and only ~1K fine-tuning steps, outperforming token-importance eviction proxies; compatible with sparse attention methods (<a href=\""https://twitter.com/p_nawrot/status/2014770473289019709\"">p_nawrot</a>).</li>\n<li><strong>Tooling for predictable deployment</strong>: <code>hf-mem</code> estimates inference VRAM from Safetensors metadata without downloading weights, aiming to eliminate trial/OOM loops (<a href=\""https://twitter.com/LiorOnAI/status/2014730309128855801\"">LiorOnAI</a>).</li>\n<li><strong>Storage and data-plane attention</strong>: SkyPilot pushes “Volumes” for high-performance storage (AI checkpoints/data) as object stores aren’t always fit (<a href=\""https://twitter.com/skypilot_org/status/2014752751545381044\"">skypilot_org</a>). Jina proposes a neat compression trick: convert embeddings to spherical coordinates pre-compression, claiming near-lossless reconstruction below float32 epsilon and ~1.5× storage savings (<a href=\""https://twitter.com/JinaAI_/status/2014753001387499927\"">JinaAI_</a>).</li>\n<li><strong>GPU kernel evaluation for agents</strong>: AMD AGI releases <strong>Magpie</strong>, an open-source kernel eval suite for correctness + performance across AMD/NVIDIA, designed for agent workflows; claims <strong>3000× token efficiency</strong> vs using GPU profilers alone, and plans tracing integrations with SGLang/vLLM (<a href=\""https://twitter.com/realSharonZhou/status/2014722290865549649\"">realSharonZhou</a>). MLSys 2026 launches FlashInfer-Bench contest tracks (MoE/DSA/GDN) with separate human vs agent evaluation (<a href=\""https://twitter.com/ye_combinator/status/2014836302198472789\"">ye_combinator</a>).</li>\n</ul>\n<hr>\n<p><strong>Ecosystem + business: partnerships, pricing, and “value-sharing” debates</strong></p>\n<ul>\n<li><strong>Sakana AI ↔ Google: strategic partnership + funding (and controversy)</strong>: Sakana frames the deal as combining Google infra/models (Gemini/Gemma) with Sakana’s research automation (AI Scientist, ALE-Agent) and pushing deployments in mission-critical domains requiring security/data sovereignty (<a href=\""https://twitter.com/SakanaAILabs/status/2014686043711406355\"">SakanaAILabs</a>). Media echoes it (Nikkei/Bloomberg/etc.) (<a href=\""https://twitter.com/nikkei/status/2014637546563658172\"">nikkei</a>, <a href=\""https://twitter.com/business/status/2014594583234027753\"">business</a>). A dispute emerges: one claim says it’s a small Google Cloud Japan compute deal and “DeepMind not involved” (<a href=\""https://twitter.com/shaneguML/status/2014847946110783649\"">shaneguML</a>), while Sakana leadership publicly counters that DeepMind is indeed involved and tags Demis/Jeff Dean (<a href=\""https://twitter.com/hardmaru/status/2014885853789884416\"">hardmaru</a>).</li>\n<li><strong>Baseten’s “many-model future” + fundraising</strong>: Baseten raises <strong>$300M at $5B</strong> and argues inference is the bottleneck enabling millions of specialized models and reliable low-latency UX (<a href=\""https://twitter.com/basetenco/status/2014755013344792595\"">basetenco</a>, <a href=\""https://twitter.com/tuhinone/status/2014755252244005273\"">tuhinone</a>).</li>\n<li><strong>Anthropic economics: inference cost pressure</strong>: A report says Anthropic cut 2025 gross margin outlook to <strong>40%</strong> due to inference costs <strong>23% higher than expected</strong>, despite projected <strong>$4.5B revenue</strong> (~12× YoY) (<a href=\""https://twitter.com/kimmonismus/status/2014673235594641838\"">kimmonismus</a>).</li>\n<li><strong>“Value-sharing” model for AI-enabled discoveries</strong>: Reporting claims OpenAI’s CFO discussed deals taking a cut of customer profits/IP (starting with drug discovery), akin to Isomorphic Labs’ model (<a href=\""https://twitter.com/kimmonismus/status/2014643034089259103\"">kimmonismus</a>). Some push back on sensationalism and incentives (e.g., you can’t both sell tokens and own discoveries without also eating compute cost) (<a href=\""https://twitter.com/code_star/status/2014541663356772516\"">code_star</a>, <a href=\""https://twitter.com/paul_cal/status/2014692633730261339\"">paul_cal</a>).</li>\n</ul>\n<hr>\n<p><strong>Multimodal + voice + video: quality leaps and tooling</strong></p>\n<ul>\n<li><strong>Voice is accelerating (open + low-latency)</strong>: Teknium claims an open voice cloning HF demo is the closest to ElevenLabs quality they’ve seen in open models (<a href=\""https://twitter.com/Teknium/status/2014687269329031253\"">Teknium</a>). NVIDIA releases <strong>PersonaPlex</strong>, an open-source real-time full-duplex conversational voice stack optimized for very low latency (<a href=\""https://twitter.com/kimmonismus/status/2014703479491854751\"">kimmonismus</a>).</li>\n<li><strong>Video generation: controllability and arenas</strong>: Runway Gen-4.5 I2V adds more precise “zoom into specified regions” control (<a href=\""https://twitter.com/c_valenzuelab/status/2014674372120785176\"">c_valenzuelab</a>) and creators showcase short-film workflows (<a href=\""https://twitter.com/Artedeingenio/status/2014693398502842731\"">Artedeingenio</a>). LMSYS Arena launches/expands <strong>Video Arena</strong> leaderboards (Veo, Sora 2, Kling, Hailuo, etc.) (<a href=\""https://twitter.com/arena/status/2014815916056576257\"">arena</a>).</li>\n<li><strong>3D agents and world models for interactive environments</strong>: Berkeley demo “VIGA” claims a multimodal agent that generates 3D/4D Blender scenes from images with no training (<a href=\""https://twitter.com/HavenFeng/status/2014765400563781777\"">HavenFeng</a>). A smaller “world model you can play” demo appears on HF (Waypoint-1-Small, 2.3B) (<a href=\""https://twitter.com/victormustar/status/2014766391811826022\"">victormustar</a>). Separately, “world models are next big wave for gaming/robotics” sentiment resurfaces (<a href=\""https://twitter.com/kylebrussell/status/2014529425983914098\"">kylebrussell</a>).</li>\n</ul>\n<hr>\n<p><strong>Security, trust, and integrity issues in the AI social layer</strong></p>\n<ul>\n<li><strong>Account compromises targeting AI insiders</strong>: Multiple warnings indicate prominent accounts (Deedy Das; a Kimi researcher/“Crystal”) were hacked and used for phishing/scams, likely crypto-driven (<a href=\""https://twitter.com/cloneofsimo/status/2014536638010163262\"">cloneofsimo</a>, <a href=\""https://twitter.com/ml_angelopoulos/status/2014543018137944486\"">ml_angelopoulos</a>, <a href=\""https://twitter.com/Kimi_Moonshot/status/2014571513299796154\"">Kimi_Moonshot</a>, <a href=\""https://twitter.com/Yuchenj_UW/status/2014572557270450194\"">Yuchenj_UW</a>).</li>\n<li><strong>Misinformation / fake papers</strong>: A fake “llama 4” arXiv paper is flagged as not actually Meta-authored (<a href=\""https://twitter.com/TimDarcet/status/2014626676798366006\"">TimDarcet</a>).</li>\n<li><strong>Open-source “layers” framing</strong>: A practical taxonomy distinguishes <strong>open code</strong> vs <strong>open weights</strong> vs <strong>open training pipeline</strong> (data + recipes + reproducibility), arguing teams must decide which layer they truly need (<a href=\""https://twitter.com/TheTuringPost/status/2014630341349408928\"">TheTuringPost</a>).</li>\n</ul>\n<hr>\n<h1>AI Reddit Recap</h1>\n<h2>/r/LocalLlama + /r/localLLM Recap</h2>\n<h3>1. Qwen3-TTS Model Release and Discussion</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qjul5t/qwen_have_opensourced_the_full_family_of_qwen3tts/\"">Qwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B &#x26; 1.8B), Support for 10 languages</a></strong> (Activity: 880): <strong><strong>Qwen</strong> has open-sourced the full family of <strong>Qwen3-TTS</strong> models, which includes VoiceDesign, CustomVoice, and Base models, with sizes of <code>0.6B</code> and <code>1.8B</code> parameters. These models support <code>10 languages</code> and are designed for tasks such as Voice Clone, Voice Design, and Custom Voice. The image provides a comparison chart of these models against others like MiniMax and SeedTTS, highlighting their performance across various metrics, where lower values indicate better performance. The models are available on <a href=\""https://github.com/QwenLM/Qwen3-TTS\"">GitHub</a> and <a href=\""https://huggingface.co/collections/Qwen/qwen3-tts\"">Hugging Face</a>, with a <a href=\""https://qwen.ai/blog?id=qwen3tts-0115\"">blog post</a> and <a href=\""https://github.com/QwenLM/Qwen3-TTS/blob/main/assets/Qwen3_TTS.pdf\"">paper</a> detailing their capabilities.</strong> Commenters appreciate the open-source release but express concerns about the models' dependency on Python and Nvidia GPUs, suggesting a need for support in other languages and platforms like llama.cpp or mistral.rs for broader accessibility.</p>\n<ul>\n<li><strong>FullstackSensei</strong> raises a technical concern about the current limitations of running Qwen models, highlighting the need for support in environments like <code>llama.cpp</code> or <code>mistral.rs</code> that can leverage GPU inference beyond just CUDA. This is particularly relevant given the rising costs of hardware and the desire for more accessible deployment options beyond Python and Nvidia GPUs.</li>\n<li><strong>LetterRip</strong> comments on the English voice outputs of Qwen3-TTS, noting that they seem to be influenced by Japanese Anime dubs. This suggests a potential bias in the training data, which could affect the naturalness and authenticity of the generated voices in English, especially if the training set was not diverse enough.</li>\n<li><strong>silenceimpaired</strong> discusses the performance of Qwen3-TTS, noting that while the samples are impressive, there is a concern about the frequency of certain outputs. This implies that while the model can produce high-quality audio, there might be consistency issues that need addressing to ensure reliable performance across different use cases.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qjtyw8/qwen_dev_on_twitter/\"">Qwen dev on Twitter!!</a></strong> (Activity: 833): <strong>The image is a Twitter post by <strong>Chen Cheng</strong>, announcing a new model with the tagline \""Tiny model. Big personality\"" and a countdown, indicating an imminent release. The comments suggest that this might be related to a TTS (Text-to-Speech) model from a previous vLLM leak, with a link to a <a href=\""https://huggingface.co/collections/Qwen/qwen3-tts\"">Hugging Face collection</a> that might be relevant. This suggests a new development in the field of TTS models, potentially offering significant improvements or features.</strong> One comment humorously suggests that the new model might finally justify the investment in a high-end GPU like the <code>5090</code>, indicating high expectations for the model's performance.</p>\n<ul>\n<li>ThePixelHunter discusses the current landscape of model sizes, noting that while smaller models are more accessible for local training on single GPUs, there is a lack of competition in the 50-120 billion parameter range. This range is ideal for enthusiasts with multiple high-end GPUs, such as a couple of 3090s or three 16GB cards, suggesting a gap in the market for larger, yet still locally trainable models.</li>\n</ul>\n</li>\n</ul>\n<h3>2. Local LLM Development and Hardware Considerations</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLM/comments/1qkudvz/i_gave_my_local_llm_pipeline_a_brain_now_it/\"">I gave my local LLM pipeline a brain - now it thinks before it speaks</a></strong> (Activity: 65): <strong>The post discusses a significant update to a local LLM pipeline named Jarvis, soon to be called TRION, which now incorporates a self-developed Sequential Thinking MCP (Multi-Component Processor). This system, built with <strong>Ollama</strong>, <strong>DeepSeek-R1</strong>, and custom MCP servers, allows the AI to \""think out loud\"" by breaking down complex questions into step-by-step reasoning, significantly reducing hallucinations. The AI dynamically decides when to use this deep thinking approach, providing instant answers for simple questions and detailed reasoning for complex ones. The project leverages a CIM (Causal Intelligence Module) framework developed by <strong>u/frank_brsrk</strong>. The implementation is open-source and available on <a href=\""https://github.com/danny094/Jarvis/tree/main\"">GitHub</a>.</strong> Commenters appreciate the open-source nature of the project and express interest in experimenting with it. There is a sentiment that local LLMs will become more important as reliance on centralized AI providers diminishes.</p>\n<ul>\n<li>GCoderDCoder discusses the integration of local LLMs with tools like 'roo code', 'vibe kanban', and 'MCPs' to automate workflows and reduce manual coding efforts. They highlight the importance of local LLMs in the context of increasing reliance on AI, contrasting it with commercial solutions like Anthropic's offerings. This reflects a broader trend towards developing independent, open-source AI solutions to maintain control and flexibility.</li>\n<li>burn-n-die inquires about the system configuration used for running the local LLM pipeline. This is a critical aspect for technical readers interested in replicating or understanding the performance and scalability of such a setup. Details on hardware specifications, software environment, and any optimizations would be valuable for those looking to implement similar systems.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLM/comments/1qjlzqt/someone_is_selling_a_lamda_labs_workstation_with/\"">Someone is selling a Lamda Labs workstation with 4× RTX 2080 Ti => 4 x 11GB => 44GB VRAM. Is this machine well-supported by open source models? Is it fast enough?</a></strong> (Activity: 107): <strong>A Lambda Labs workstation with 4× RTX 2080 Ti GPUs, totaling <code>44GB VRAM</code>, is being considered for purchase at <code>$2000</code>. This setup is generally well-supported by open-source machine learning frameworks, and the <code>44GB VRAM</code> is sufficient for most tasks. However, setting up the system properly could be challenging. An alternative suggestion is to build a 2x RTX 3090 rig, which might offer better performance and cost around <code>$2.5k</code>. The workstation is deemed capable of handling many open-weight LLM models, especially if it includes at least <code>32GB of system RAM</code>. The machine is suitable for exploring a wide range of machine learning projects, although not all require extensive VRAM.</strong> There is a debate on whether to purchase the existing setup or build a new one with newer GPUs like the RTX 3090. Some argue that the existing setup is sufficient for learning and exploring ML models, while others suggest building a new rig for better performance and learning experience.</p>\n<ul>\n<li>The workstation with 4× RTX 2080 Ti GPUs, totaling 44GB VRAM, is generally well-supported by most open-source frameworks. However, setting it up correctly can be challenging. The system's capability is sufficient for exploring many open-weight LLM models, especially if it includes at least 32GB of system RAM, which enhances its potential for various machine learning tasks.</li>\n<li>A technical consideration is that Turing generation GPUs, like the RTX 2080 Ti, were initially thought to be incompatible with FlashAttention 2. However, recent updates indicate that this limitation has been addressed, as noted in the <a href=\""https://github.com/egaoharu-kensei/flash-attention-triton\"">FlashAttention Triton GitHub repository</a>. This expands the utility of the workstation for certain advanced ML tasks.</li>\n<li>While the 4× RTX 2080 Ti setup is capable, some suggest that building a new system with 2× RTX 3090 GPUs might offer better performance and value. The RTX 3090 provides more VRAM per card and improved performance, potentially making it a more future-proof investment for machine learning projects.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qkiylw/openai_cfo_hinting_at_outcomebased_pricing_aka/\"">OpenAI CFO hinting at \""Outcome-Based Pricing\"" (aka royalties on your work)? Makes the case for local even stronger.</a></strong> (Activity: 419): <strong><strong>OpenAI's CFO, Sarah Friar</strong>, discussed a potential shift towards \""outcome-based pricing\"" for large enterprise deals, particularly in high-value industries like pharmaceuticals. This model would involve OpenAI taking a share of the value created by their AI, such as a cut from a pharmaceutical company's profits if AI contributes to a major discovery. This approach is not intended for regular users or indie developers, and the initial reports suggesting a broader application were misleading. The concept raises discussions about the benefits of local AI deployment versus reliance on cloud-based services, drawing parallels to the energy sector's grid versus solar power debate.</strong> Commenters highlight skepticism about OpenAI's potential pricing model, comparing it to the lack of royalties paid to data creators used in AI training. The analogy of local AI deployment to solar power is appreciated, emphasizing control over infrastructure to avoid future costs tied to value-based pricing.</p>\n<ul>\n<li>The discussion highlights concerns about OpenAI's potential shift to 'Outcome-Based Pricing', which could involve royalties based on the revenue generated from using their models. This is compared to the current model where users pay based on usage, akin to how electricity is billed. The analogy suggests that such a pricing model could drive users to consider local or self-hosted solutions, especially as OpenAI's profitability grows and they seek higher profits.</li>\n<li>The comment by WeMetOnTheMountain critiques the efficiency of OpenAI's models, suggesting that they consume a large number of tokens to maintain performance, which results in slower processing speeds. The commenter argues that alternative models like GLM or mini Max could potentially offer better results when implemented in a 'one loop dialectical circuit', indicating a preference for more efficient, possibly self-hosted, solutions.</li>\n<li>Winter_Educator_2496 emphasizes the need for open-source alternatives to OpenAI's models, which could be hosted in the cloud but also switched to local hosting if necessary. This reflects a broader sentiment for more control and flexibility over AI tools, especially in light of potential pricing changes and the desire to avoid dependency on a single provider.</li>\n</ul>\n</li>\n</ul>\n<h3>3. Hugging Face Model Releases and Trends</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLM/comments/1qjqhja/this_weeks_hottest_hugging_face_releases_top/\"">This Week's Hottest Hugging Face Releases: Top Picks by Category!</a></strong> (Activity: 49): <strong><strong>Hugging Face</strong> has released several trending models across different categories this week. In text generation, the <code>zai-org/GLM-4.7-Flash</code> model, with <code>31B</code> parameters, is designed for fast and efficient text generation, boasting <code>124k</code> downloads. Its quantized counterpart, <code>unsloth/GLM-4.7-Flash-GGUF</code>, offers a <code>30B</code> parameter model optimized for local inference with <code>112k</code> downloads. In the image/multimodal category, <code>zai-org/GLM-Image</code> and <code>google/translategemma-4b-it</code> are notable for their capabilities in creative edits and multilingual tasks, respectively. For audio/speech, <code>kyutai/pocket-tts</code> and <code>microsoft/VibeVoice-ASR</code> provide compact TTS and multilingual ASR solutions. Other notable releases include <code>Lightricks/LTX-2</code> for image-to-video conversion and <code>stepfun-ai/Step3-VL-10B</code> for advanced reasoning in image-text-to-text tasks.</strong> A technical debate has emerged regarding the performance of the <code>GLM-4.7 30B-A3B</code> model compared to the <code>Qwen3-Coder 30B-A3B</code> for programming tasks, with some users finding the latter superior.</p>\n<ul>\n<li>A user compared the GLM-4.7 30B-A3B model to the Qwen3-Coder 30B-A3B model, noting that the latter performs better for programming tasks. This suggests that Qwen3-Coder may have optimizations or architectural advantages that make it more suitable for code-related applications. Further benchmarks or detailed evaluations would be needed to substantiate this claim and understand the specific areas where Qwen3-Coder excels.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLM/comments/1qk9ked/good_local_llm_for_coding/\"">Good local LLM for coding?</a></strong> (Activity: 62): <strong>The user is seeking a local LLM for coding that can run on an <code>rx 6750 xt</code> GPU with <code>12GB</code> VRAM, considering models like <strong>GLM 4.7 flash</strong>. However, concerns about VRAM limitations suggest that <code>30B</code> parameter models, even when quantized to <code>q4</code>, may exceed the GPU's capacity. Recommendations include models like <a href=\""https://huggingface.co/TIGER-Lab/VisCoder2-7B\"">VisCoder2-7B</a>, <a href=\""https://huggingface.co/openai/gpt-oss-20b\"">gpt-oss-20b</a>, and <a href=\""https://huggingface.co/NousResearch/NousCoder-14B\"">NousCoder-14B</a>, with <strong>gpt-oss-20b</strong> noted for its speed and reliability despite being heavily censored. It's suggested to use models under <code>10B</code> parameters or employ a coding MoE model with <code>llama.cpp</code> to offload some processing to system RAM.</strong> There is a debate on the suitability of <code>30B</code> models for the user's GPU, with a consensus leaning towards using models under <code>10B</code> parameters due to VRAM constraints. The use of <code>llama.cpp</code> for offloading to system RAM is also discussed as a viable strategy.</p>\n<ul>\n<li>Javanese1999 highlights several models for local coding tasks, including <a href=\""https://huggingface.co/TIGER-Lab/VisCoder2-7B\"">VisCoder2-7B</a>, which is described as a better version of Qwen2.5-Coder-7B-Instruct, and <a href=\""https://huggingface.co/openai/gpt-oss-20b\"">gpt-oss-20b</a>, noted for its speed even when exceeding VRAM capacity. The commenter prefers gpt-oss-20b for its reliability in light coding tasks despite its censorship in refusal prompts.</li>\n<li>Used_Chipmunk1512 advises against using 30B models quantized to q4 due to GPU limitations, suggesting that models under 10B are more suitable for most users. This highlights the importance of considering hardware constraints when selecting a local LLM for coding.</li>\n<li>RnRau suggests using a coding Mixture of Experts (MoE) model with the <code>llama.cpp</code> inference engine to offload some of the model's processing to system RAM, which can be a practical approach for handling larger models without overwhelming the GPU.</li>\n</ul>\n</li>\n</ul>\n<h2>Less Technical AI Subreddit Recap</h2>\n<blockquote>\n<p>/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo</p>\n</blockquote>\n<h3>1. OpenAI and Anthropic Developments</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/singularity/comments/1qk6pbi/openai_says_codex_usage_grew_20_in_5_months/\"">OpenAI says Codex usage grew 20× in 5 months, helping add ~$1B in annualized API revenue last month</a></strong> (Activity: 535): <strong>OpenAI's Codex usage has surged 20 times over five months, contributing to an additional <code>$1 billion</code> in annualized API revenue, as reported by <strong>Sarah Friar</strong>, OpenAI's CFO. The company is experiencing a shift towards enterprise customers, with the revenue split moving from <code>70% consumer and 30% enterprise</code> to <code>60% consumer and 40% enterprise</code>, and is expected to reach a <code>50-50</code> balance by the end of the year. OpenAI aims to achieve <code>$20 billion</code> in annualized revenue by 2025, supported by cloud investments and infrastructure scaling.</strong> A comment suggests skepticism about the profitability, estimating a cost of <code>$7 billion</code> to achieve the <code>$1 billion</code> revenue. Another comment highlights a shift in AI tools used by a financial services company, indicating competition in the B2B market with <strong>Anthropic</strong> and <strong>OpenAI</strong>.</p>\n<ul>\n<li>BetImaginary4945 suggests that the cost of generating $1B in revenue for OpenAI might be as high as $7B, implying a significant expenditure on infrastructure, research, and development to support such rapid growth. This raises questions about the sustainability and profitability of OpenAI's business model in the long term.</li>\n<li>balagachchy shares an insight from their experience at a multinational financial services company, noting a shift from using ChatGPT to Gemini and Claude Code for software engineering tasks. This highlights the competitive landscape in AI tools for enterprise use, where companies are exploring different solutions to meet their specific needs.</li>\n<li>imlaggingsobad comments on the competitive dynamics between OpenAI and Anthropic in the B2B market, suggesting that while Anthropic is perceived as a leader, OpenAI's rapid growth and innovation could still make it a formidable competitor. This underscores the ongoing competition and potential for shifts in market leadership.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/OpenAI/comments/1qjrpbq/openai_ceo_meets_middle_east_investors_over/\"">OpenAI CEO meets Middle East investors over potential $50B fundraising</a></strong> (Activity: 191): <strong><strong>OpenAI</strong> is reportedly in discussions with Middle Eastern sovereign wealth funds to raise a potential <code>$50 billion</code> in a new funding round, as confirmed by <a href=\""https://www.cnbc.com\"">CNBC</a>. The talks are still in preliminary stages, with no term sheets signed yet. <strong>Sam Altman</strong>, OpenAI's CEO, is currently in the UAE to engage in these discussions, highlighting the strategic importance of this potential investment for OpenAI's future growth and operational scaling.</strong> A notable opinion from the comments suggests skepticism about OpenAI's financial strategy, questioning why the company isn't pursuing an IPO given its significant revenue, and criticizing its reliance on external capital to manage high operational costs.</p>\n<ul>\n<li>AtraVenator highlights concerns about OpenAI's financial strategy, noting that despite having over <code>$20B</code> in annual revenue, the company is seeking additional external capital rather than moving towards a self-sustaining model. This raises questions about their high compute costs and reliance on external funding to cover these expenses.</li>\n<li>The discussion touches on the potential risks of OpenAI going public, with NotABCDinFL suggesting that an IPO could lead to a 'massive rug pull' where institutional investors might cash out, leaving retail investors at a disadvantage. This reflects concerns about the stability and transparency of OpenAI's financial practices.</li>\n<li>There is skepticism about OpenAI's leadership and strategic direction, with BeingComfortablyDumb questioning how the company has moved from having a first-mover advantage and significant market share to its current financial challenges. This implies a critique of the management's ability to capitalize on their early lead in the AI industry.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/OpenAI/comments/1qjytb2/anthropics_claude_constitution_is_surreal/\"">Anthropic's Claude Constitution is surreal</a></strong> (Activity: 611): <strong>The image discusses the use of the pronoun \""it\"" for Anthropic's AI, Claude, and the potential for Claude to develop preferences for different pronouns, suggesting the emergence of functional versions of emotions or feelings from training on human-generated data. This is not a deliberate design choice by Anthropic, but it raises questions about the moral status of these emotional states. The text reflects ongoing debates in AI ethics about the implications of AI systems potentially developing human-like emotional states, which are not yet fully understood or intentionally designed.</strong> Commenters note that this aligns with current research and extreme safety measures in the AI industry, emphasizing the surreal nature of these developments and the importance of humility in AI labs' claims.</p>\n<ul>\n<li>br_k_nt_eth highlights that the Claude Constitution aligns with current research trends and extreme safety measures being tested in the AI industry, which sometimes negatively impact company reputations. This suggests that those familiar with advanced models would not find the approach surprising, as it reflects ongoing industry practices.</li>\n<li>heavy-minium argues that the surreal aspect of Claude's Constitution is not unique to Claude but is inherent to any large language model (LLM). They point out that emotions are patterns in training data, and this phenomenon is unavoidable unless the model is either broken or too small. The commenter suggests that the relabeling of this characteristic is more about public relations than a technical breakthrough.</li>\n<li>laystitcher emphasizes the importance of the cautious language used in the Claude Constitution, such as the word 'may,' which reflects the humility of AI labs in acknowledging the uncertainties in their developments. This cautious approach is seen as appropriate given the current surreal advancements in AI technology.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/ClaudeAI/comments/1qk4up5/microsoft_is_using_claude_code_internally_while/\"">Microsoft is using Claude Code internally while selling you Copilot</a></strong> (Activity: 1276): <strong><strong>Microsoft</strong> is internally using <strong>Claude Code</strong> across various divisions like Windows and Teams, despite heavily investing in <strong>OpenAI</strong> and promoting <strong>Copilot</strong>. This internal use is sanctioned for all Microsoft repositories, indicating a significant investment of <code>$500M/year</code> with <strong>Anthropic</strong>. Interestingly, <strong>Azure</strong> sales teams receive quota credit for Anthropic sales, suggesting a strategic partnership. Despite <strong>Claude Code</strong> not outperforming in <code>95%</code> of benchmarks, developers report superior problem-solving capabilities, challenging the reliability of current benchmark tools. <strong>Copilot</strong> is priced at <code>$10/month/user</code>, whereas <strong>Claude Code</strong> is <code>$150</code> for enterprise use.</strong> Commenters highlight the discrepancy between benchmark results and real-world performance, suggesting benchmarks may not fully capture tool effectiveness. The partnership between Microsoft and Anthropic is seen as strategic, with Claude's integration into various Microsoft products and services.</p>\n<ul>\n<li>CurveSudden1104 highlights a discrepancy between benchmark results and real-world performance, noting that while Claude doesn't outperform in 95% of benchmarks, developers find it superior in problem-solving. This suggests that current benchmarks may not accurately reflect practical utility, indicating a potential gap between quantitative metrics and qualitative user experience.</li>\n<li>morrisjr1989 points out that Claude's integration into Microsoft's ecosystem is part of a strategic partnership, with Claude being utilized in various Microsoft products like Copilot, Foundry, and Azure-hosted services. This integration underscores a collaborative approach rather than a competitive one, leveraging Claude's capabilities across multiple platforms.</li>\n<li>UnknownEssence provides a cost comparison, noting that Copilot is priced at $10 per month per user, whereas Claude Code is significantly more expensive at $150 for enterprise use. This price difference highlights the distinct market positioning and target audiences for each product, with Copilot being more accessible to individual users and Claude Code catering to enterprise needs.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/ClaudeAI/comments/1qjlrgb/claudes_eureka_moment_is_not_ending_soon_it_looks/\"">Claude’s eureka moment is not ending soon it looks like</a></strong> (Activity: 1377): <strong>The image and post discuss the competitive landscape of AI coding agents, particularly focusing on <strong>Claude</strong>, a tool developed by <strong>Anthropic</strong>. The post suggests that <strong>Gemini</strong> has open-sourced their CLI in an attempt to compete with Claude, which is notably used by <strong>Nvidia</strong>. This highlights the ongoing race in AI development tools, with speculation about whether the market will consolidate around a few dominant players or remain diverse. The comments reflect a belief that AI will significantly transform programming, with some users noting their companies' exclusive use of Claude.</strong> One comment suggests skepticism about the CEO's investment in the product's company, while another highlights a shift in programming paradigms, predicting that future programmers will rely heavily on AI tools.</p>\n<ul>\n<li>sine120 argues that Claude Code should be open-sourced, suggesting it lacks unique features that justify keeping it proprietary. They mention that other frameworks like Opus could integrate Claude's capabilities, and by not open-sourcing, Anthropic might miss the chance to lead AI development, potentially allowing competitors like Google and Chinese labs to catch up. They emphasize that developers might prefer openness over marginal performance improvements.</li>\n<li>itsdr00 highlights a significant shift in software development life cycles (SDLC) due to AI advancements, particularly with Claude Code. They note that some companies are restructuring their SDLC to leverage AI, implying that traditional methods are becoming obsolete. This reflects a broader industry trend where AI is increasingly integral to development processes, akin to a paradigm shift from older technologies like punch cards.</li>\n</ul>\n</li>\n</ul>\n<h3>2. Gemini and AI Studio Issues</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/Bard/comments/1qkj31m/im_honestly_sick_of_this_gemini_web_vs_ai_studio/\"">I’m honestly sick of this: Gemini Web vs AI Studio Context Window Mess</a></strong> (Activity: 49): <strong>The user reports a significant regression in the Gemini web/app's ability to handle large files since the update to <strong>Gemini 3</strong>. Previously, with <strong>Gemini 2.5 Pro</strong>, files containing <code>600k–800k</code> tokens could be processed without issues, retaining full context for queries. However, the current version rejects files over <code>100k</code> tokens and provides incomplete or incorrect responses. In contrast, <strong>Gemini AI Studio</strong> continues to handle the same large files effectively, suggesting the underlying model's capability remains intact but is not accessible in the consumer-facing app. This discrepancy raises concerns about potential limitations imposed on the web/app version, possibly misleading users about the product's capabilities.</strong> Commenters express dissatisfaction with the Gemini web/app, noting that <strong>AI Studio</strong> is the only reliable platform for using Google's models effectively. Some users, even on the Pro plan, report receiving errors when uploading large documents, indicating a possible mismatch between advertised capabilities and actual performance.</p>\n<ul>\n<li>A user mentions that AI Studio is the only viable platform for using Google models effectively, implying that other platforms like Gemini app and Antigravity do not meet their expectations despite having a subscription. This suggests potential issues with the usability or performance of these platforms compared to AI Studio.</li>\n<li>Another user discusses the Pro plan, noting that they have not encountered issues with document processing. They suggest that if documents are too large in terms of tokens, the system might default to classic retrieval methods rather than processing the entire file, indicating a possible limitation in handling large documents.</li>\n<li>A user on the Pro plan reports receiving an error after uploading a 20-page PDF, describing the situation as 'absurd.' This highlights potential limitations or bugs in the system's ability to handle larger documents, even for users on higher-tier plans.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/Bard/comments/1qkztjy/ai_studio_rate_limits_are_out_of_control_again/\"">AI Studio Rate Limits are out of control again...</a></strong> (Activity: 67): <strong>The post discusses recent issues with rate limits on <strong>AI Studio</strong>, where users, including those with Pro subscriptions, are experiencing frequent request denials. This is a change from previous usage patterns where limits were rarely hit. The user expresses frustration that their Pro subscription cannot be applied to AI Studio, which they find superior to the main site. Technical comments suggest that the rate limits might be due to dynamic prompt limits, increased GPU allocation for new training, or a higher user count. Additionally, <strong>Gemini 2.5 Pro</strong> has been rate limited for the first time, indicating possible resource constraints or strategic adjustments by the platform.</strong> Commenters speculate that the rate limits could be due to increased demand or resource reallocation, with some suggesting desperation on the platform's part. Others report encountering internal errors, indicating potential technical issues beyond just rate limiting.</p>\n<ul>\n<li>OneMisterSir101 suggests that the current rate limits on AI Studio might be due to dynamic prompt limits, which could be influenced by either GPU delegation to new training tasks or an increase in user count. This implies a resource allocation issue where computational resources are being stretched thin, potentially affecting performance and availability.</li>\n<li>Undertaker1995 notes that Gemini 2.5 Pro has been rate limited for the first time, indicating a significant shift in resource management or demand. This could reflect a strategic decision by the platform to manage load or a response to increased usage, highlighting potential scalability challenges.</li>\n<li>wildwriting reports encountering an 'internal error' message, despite attempting standard troubleshooting steps like reloading the page and restarting the browser. This suggests a deeper technical issue within the platform, possibly related to server-side problems or misconfigurations that are not resolved by client-side actions.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/GeminiAI/comments/1qjrokj/im_sorry_but_gemini_is_getting_worse_and_worse/\"">I'm sorry but Gemini is getting worse and worse</a></strong> (Activity: 1301): <strong>The post discusses a decline in the performance of <strong>Gemini</strong>, particularly in its memory capabilities and intelligence. Previously, the pro mode of Gemini could remember <code>30+ conversations</code> with a total of <code>180,000 words</code>, but recent updates have halved this memory capacity, leading to a perceived decrease in intelligence and reliability. The user expresses frustration, suggesting that <strong>ChatGPT</strong> might be a better alternative due to its longer and more conversational responses.</strong> Commenters agree with the decline in Gemini's performance, noting increased issues with speculation and hallucination. There is skepticism about future updates, with one commenter cynically suggesting that any improvements will be short-lived.</p>\n<ul>\n<li>The comment by Particular-Battle315 highlights a common lifecycle pattern in AI models where initial releases are powerful but get 'nerfed' over time. This is observed across companies like Anthropic, OpenAI, and Google, suggesting a strategic approach to model updates that may not be immediately apparent to all users.</li>\n<li>Duchess430 discusses the potential for running large AI models on personal computers using specialized open-source models, which may outperform Gemini for specific tasks. They mention the GGUF (GPT-Generated Unified Format) as a method to optimize resource usage by splitting data between RAM and VRAM, allowing for running large models without high-end hardware.</li>\n<li>rephil3 points out issues with Gemini, specifically its tendency to speculate and hallucinate, which are common problems in AI models that can affect their reliability and user trust.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/Bard/comments/1qk2yx6/gemini_about_to_get_busy/\"">Gemini about to get busy?</a></strong> (Activity: 33): <strong>The post discusses the potential impact of <strong>ChatGPT</strong> introducing ads on its user base, suggesting that this could lead to a significant migration of users to <strong>Gemini</strong>, especially as Gemini's models improve and integrate more deeply with <strong>Google's</strong> ecosystem. Concerns are raised about whether Gemini can handle a sudden influx of users without degrading the experience for its current base. A technical issue noted is Gemini's handling of conversations, where users report chats being replaced with 'sensitive query' messages, and the lack of a 'Projects' feature to maintain context, unlike <strong>ChatGPT</strong> and <strong>Claude</strong>.</strong> Commenters debate Gemini's readiness to handle increased user load, with some arguing that <strong>Google's</strong> extensive experience and infrastructure, including recent investments in clean energy and data centers, position it well to scale effectively. Others highlight the technical shortcomings of Gemini, such as conversation management issues, as potential drawbacks.</p>\n<ul>\n<li>Loud-Independent9041 highlights a significant issue with Gemini's conversation handling, where chats are sometimes replaced with 'a sensitive query' messages, disrupting the user experience. This contrasts with ChatGPT and Claude, which offer a 'Projects' feature to maintain context across conversations, a feature Gemini lacks, impacting its usability for continuous dialogue.</li>\n<li>rollk1 points out Google's strategic positioning in the AI and data center space, emphasizing their acquisition of Intersect Power to support clean energy for data centers. This move, along with their existing Google Cloud infrastructure, positions them advantageously for scaling AI models, potentially outpacing competitors like OpenAI.</li>\n<li>FalseAcadia4306 notes a potential increase in Gemini's user base, as evidenced by receiving a 'research queued' message for the first time, suggesting a surge in demand or usage that could be straining the system's capacity.</li>\n</ul>\n</li>\n</ul>\n<h3>3. DeepSeek and Baidu's ERNIE 5.0 Innovations</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/DeepSeek/comments/1qkoc53/deepseekv32_matches_gpt5_at_10x_lower_cost_introl/\"">DeepSeek-V3.2 Matches GPT-5 at 10x Lower Cost | Introl Blog</a></strong> (Activity: 125): <strong><strong>DeepSeek-V3.2</strong> is an open-source AI model that reportedly matches the performance of <strong>GPT-5</strong> in mathematical reasoning tasks while operating at a cost 10 times lower, specifically <code>$0.028</code> per million tokens. The model utilizes a novel 'Sparse Attention' architecture, which contributes to its efficiency, achieving frontier-class performance with a total training cost of approximately <code>$5.5 million</code>, significantly less than the <code>$100M+</code> typically spent by major US tech companies. The model's architecture includes <strong>DeepSeek Sparse Attention (DSA)</strong> for efficient long-context processing and a refined <strong>Mixture-of-Experts</strong> approach, which activates only a subset of parameters per token, enhancing task-specific performance. For more details, see the <a href=\""https://introl.com/blog/deepseek-v3-2-open-source-ai-cost-advantage\"">Introl Blog</a>.</strong> One comment suggests skepticism about the reported cost savings, noting that a significant portion of OpenAI's expenses may be attributed to executive salaries rather than direct model development costs.</p>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/DeepSeek/comments/1qkpxzm/baidus_new_ernie_50_is_going_hard_after_gpt_and/\"">Baidu's new ERNIE 5.0 is going hard after GPT and Gemini</a></strong> (Activity: 51): <strong><strong>Baidu's ERNIE 5.0</strong> is making significant strides in mathematical reasoning and technical problem-solving, ranking #2 globally on the LMArena Math leaderboard, just behind the unreleased GPT-5.2-High. It surpasses GPT-5.1 and Gemini 2.5 Pro in math and scores higher on specialized benchmarks like MathVista and ChartQA, particularly excelling in interpreting complex visual diagrams. In the 'VLMs Are Blind' benchmark, ERNIE 5.0 scored <code>77.3</code>, outperforming GPT-5-High's <code>69.6</code>. Additionally, ERNIE 5.0 offers a cost advantage, being nearly <code>90%</code> cheaper than OpenAI’s GPT-5.1 for similar token volumes, making it a competitive option in terms of pricing.</strong></p>\n<ul>\n<li>ERNIE 5.0 is noted for its impressive scale with <code>2.4 trillion parameters</code>, significantly larger than competitors like DeepSeek's <code>671 billion</code> and Kimi K2's <code>1 trillion</code>. Despite its size, the quality of output is reported to be similar to other models, with particularly fast inference speeds. However, the model's strict system prompt alignments can make interactions feel restricted, though users can adjust the tone with specific prompts for better results.</li>\n<li>The model offers a free web version with a <code>128k context window</code>, comparable to DeepSeek, which is a significant advantage for users needing extensive context handling. However, the default interaction tone is described as overly corporate, which can be modified with specific prompts to achieve more engaging interactions. This flexibility in tone adjustment is seen as a positive feature despite the initial restrictions.</li>\n<li>A recent update to ERNIE 5.0, referred to as \""5.0 Preview 1203\"", has reportedly improved the model's engagement and interaction quality, making it more fun and collaborative. This suggests that Baidu is actively iterating on the model to enhance user experience, potentially addressing earlier criticisms of restrictive interactions.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/DeepSeek/comments/1qjob34/deepseeks_quiet_technical_wins_that_nobody_talks/\"">DeepSeek’s Quiet Technical Wins (That Nobody Talks About)</a></strong> (Activity: 85): <strong><strong>DeepSeek</strong> is recognized not only for its benchmark performance but also for its engineering innovations, which include <em>better routing for efficiency</em>, <em>cleaner long-context behavior</em>, and <em>faster token generation</em>. These features contribute to its distinctiveness in practical applications. Notably, DeepSeek employs <strong>Mixture of Experts (MoE)</strong> for smarter routing and introduces <strong>Engram</strong> to separate memory from reasoning, emphasizing architectural innovation over brute-force scaling.</strong> Commenters highlight DeepSeek's unique 'thinking process' and its focus on architectural innovation, such as using MoE and Engram, as key differentiators from other AI models.</p>\n<ul>\n<li><strong>Hey-Intent</strong> highlights DeepSeek's architectural innovations, particularly the use of Mixture of Experts (MoE) for smarter routing and the introduction of Engram to separate memory from reasoning. This approach emphasizes sustainable AI progress through architectural improvements rather than brute-force scaling, which is a significant shift in AI development strategy.</li>\n<li><strong>Fine_Effective4980</strong> points out that DeepSeek's system-level efficiency, combining routing and token generation, results in a more responsive and stable user experience, especially with longer context. This efficiency is not captured in traditional benchmarks but is crucial for real-world applications, offering a smoother and more reliable workflow.</li>\n<li><strong>Althalvas</strong> notes that DeepSeek's R1 model provides a superior thinking process compared to other AI models, even when only using free versions. This suggests that DeepSeek's models may have a more refined approach to processing, which could be attributed to their architectural choices.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h1>AI Discord Recap</h1>\n<blockquote>\n<p>A summary of Summaries of Summaries</p>\n</blockquote>\n<h2>Gemini 3.0 Pro Preview Nov-18</h2>\n<p><strong>Theme 1. Hardware Limits and Kernel Hacking: B200s, ROCm, and Mobile Optimization</strong></p>\n<ul>\n<li><strong>FlashAttention-4 hits 71% utilization on B200</strong>: Early benchmarks show <strong>FlashAttention-4</strong> reaching <strong>1,605 TFLOPS/s</strong> on an <strong>NVIDIA B200 GPU</strong> using BF16 inputs, capturing roughly 71% of the theoretical maximum. Engineers in the <strong>GPU MODE</strong> discord noted a lack of official documentation regarding specific fp4/fp8/fp16 specs, sparking debate over the hardware's true theoretical ceiling compared to leaked materials.</li>\n<li><strong>Developer abandons ROCm for CUDA</strong>: A frustrated developer publicly documented their switch from <strong>AMD's ROCm</strong> to <strong>NVIDIA</strong> after purchasing a 5090, citing packaging failures, build issues, and a \""hostile\"" ecosystem for consumer-facing hardware. The discussion highlighted that mid-grade NVIDIA hardware often outperforms AMD gear on specific kernels like <strong>Conv3D</strong> due to software maturity, referencing a <a href=\""https://www.reddit.com/r/ROCm/comments/1owczm9/the_convolution_performance_on_rx_9070_is_so_low/\"">Reddit thread on performance regressions</a>.</li>\n<li><strong>Mobile GPU memory path splitting</strong>: Engineers in <strong>tinygrad</strong> discovered that optimizing <strong>L2 bandwidth</strong> on mobile GPUs requires treating <strong>textures</strong> and <strong>buffers</strong> as distinct hardware pathways. Maximizing throughput involves strategically feeding one input as a texture and another as a buffer to saturate the available bandwidth, a technique critical for edge inference.</li>\n</ul>\n<p><strong>Theme 2. Agentic Workflows: Cursor Sub-agents, Replit Control, and Aider TUI</strong></p>\n<ul>\n<li><strong>Cursor 2.4 wobbles while Sub-agents emerge</strong>: While users reported <strong>Composer 1</strong> breaking into endless loops and <strong>Cursor 2.4</strong> causing significant lag on high-end PCs, savvy users found evidence of a <strong>sub-agent</strong> feature rollout. The system injects a <strong>&#x3C;subagent_delegation_context></strong> prompt to enable parallel task execution and better context handling, as detailed in the <a href=\""https://cdn.discordapp.com/attachments/1351160689380687942/1463993849985765510/Cursor_Changelog_Subagents.mp4?ex=69752b85&#x26;is=6973da05&#x26;hm=50e3cbf6432112dcbe36b0315b1645fd7d856c9d2ead97e639b2d8abcfa5b8f4&#x26;\"">Changelog video</a>.</li>\n<li><strong>Replit Agent gets real-time brains</strong>: Zhen Li published a technical breakdown of <strong>Decision-Time Guidance</strong> in the <strong>Replit Agent</strong>, replacing static rules with real-time control mechanisms for complex navigation. This architectural shift aims to reduce fragility in autonomous coding tasks, moving closer to adaptive \""system 2\"" thinking, as described in <a href=\""https://xcancel.com/zhenthebuilder/status/2014393451442581688?s=46\"">this blog post</a>.</li>\n<li><strong>Aider eyes a TUI makeover</strong>: The <strong>aider</strong> community is actively designing a <strong>Terminal User Interface (TUI)</strong> to allow message editing while browsing replies and rendering <strong>Mermaid diagrams</strong> directly in the terminal. Simultaneously, users are chaining <strong>aider</strong> for rapid context management with <strong>Claude Code</strong> for complex debugging to minimize token costs and leverage <a href=\""https://discord.com/channels/1131200896827654144/1131200896827654149/1464167385060872203\"">aider's efficient file search</a>.</li>\n</ul>\n<p><strong>Theme 3. Model Architecture and Audio: Qwen3-TTS, NanoGPT Hacks, and GLM Speedups</strong></p>\n<ul>\n<li><strong>Qwen3-TTS clones voices at scale</strong>: Alibaba released the <strong>Qwen3-TTS</strong> family, ranging from <strong>0.6B to 1.8B</strong> parameters, capable of high-quality voice cloning and supporting 10 languages. The release challenges proprietary models like ElevenLabs, with demos and weights available on <a href=\""https://huggingface.co/spaces/Qwen/Qwen3-TTS\"">Hugging Face Spaces</a>.</li>\n<li><strong>NanoGPT gets a Difference Layer boost</strong>: Researchers in <strong>Eleuther</strong> reported that replacing the <strong>QKV linear layer</strong> with a <em>difference layer</em>—<code>x = (self.a2(x) - self.b2(x)) * ...</code>—significantly improved <strong>NanoGPT</strong> performance on simple tasks. Others noted that switching activations from <strong>GELU</strong> to <strong>SwiGLU</strong> also provided a baseline boost, emphasizing the need for <a href=\""https://github.com/Eternalyze0/difference_layer\"">stronger baselines</a> before claiming architectural supremacy.</li>\n<li><strong>GLM-4.7 Flash zooms on llama.cpp</strong>: The <strong>Hugging Face</strong> community noted that <strong>llama.cpp</strong> updates have accelerated <strong>GLM-4.7 Flash GGUF</strong> inference by approximately <strong>1.5x</strong>. Users are advised to rebuild from source and grab fixed quants from <a href=\""https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF\"">Unsloth's repo</a> to enable the <code>--flash-attn on</code> flag for optimal performance.</li>\n</ul>\n<p><strong>Theme 4. Inference Engineering: Speculative Decoding, SRAM Specs, and MoE Memory</strong></p>\n<ul>\n<li><strong>Speculative decoding slows down vLLM</strong>: Engineers debugging <strong>Qwen3-VL</strong> on <strong>vLLM</strong> found that enabling speculative decoding often hurts <strong>Time To First Token (TTFT)</strong> unless batch sizes are massive. The consensus is that small draft models introduce too much overhead for single-stream or low-batch inference, and standard <a href=\""https://docs.vllm.ai/en/stable/design/metrics/\"">vLLM metrics</a> via Grafana are recommended for tuning.</li>\n<li><strong>Small MoEs starve in 8GB RAM</strong>: Discussions in <strong>Unsloth AI</strong> concluded that running <strong>Mixture of Experts (MoE)</strong> models on 8GB RAM is largely futile because the active parameter count becomes too low to be useful. While <strong>Qwen 2.5 3B</strong> (dense) remains the king for low-memory coding, \""small\"" MoEs like <strong>LFM2</strong> lack the corpus density to compete effectively.</li>\n<li><strong>Cerebras CS3 packs 41GB SRAM</strong>: It was revealed in <strong>OpenRouter</strong> that each <strong>Cerebras CS3</strong> wafer-scale instance houses <strong>41GB of SRAM</strong>, designed to interconnect up to <strong>2048 instances</strong>. This massive on-chip memory allows for extremely high-bandwidth model execution, bypassing traditional HBM bottlenecks found in GPU clusters.</li>\n</ul>\n<p><strong>Theme 5. Adversarial Attacks and Platform Instability</strong></p>\n<ul>\n<li><strong>Gemini 3 Pro jailbroken via ENI</strong>: The <strong>ENI jailbreak</strong> technique, originally targeting Claude, was successfully ported to <strong>Gemini 3 Pro</strong> in AI Studio, with users reporting it \""works like magic\"" even on the Flash variant. The exploit allows bypassing safety guardrails, detailed in a shared <a href=\""https://github.com/pranrichh/Jailbreaks/blob/main/GEMINI-CLAUDE%20JAILBREAK.md\"">GitHub methodology</a>.</li>\n<li><strong>Perplexity Pro limits pinch users</strong>: <strong>Perplexity</strong> subscribers are reporting severe undocumented limits, with file uploads capped at three per day and research queries throttled to as low as <strong>20 daily</strong> for some (vs the expected 600). Users suspect aggressive <strong>A/B testing</strong> or financial tightening, further aggravated by <a href=\""https://discord.com/channels/1047197230748151888/1161802929053909012/1464341850809831519\"">API 401 errors</a> despite valid credits.</li>\n<li><strong>Kimi AI hits capacity wall</strong>: <strong>Moonshot AI's Kimi</strong> service is suffering widespread outages, with users facing constant \""This mode is at capacity\"" errors and vanishing conversation histories. Speculation in the community points to a potential datacenter failure or API restrictions from upstream providers like Google <a href=\""https://discord.com/channels/1369594130807787570/1371757564005711973/1464201099123888242\"">derailing the service</a>.</li>\n</ul>\n<h2>gpt-5.2</h2>\n<p><strong>1. Cursor 2.4 Subagents Rollout &#x26; Developer UX Fallout</strong></p>\n<ul>\n<li>\n<p><strong>Subagents Ship Fast, Task Tool Ghosts Everyone</strong>: <strong>Cursor 2.4</strong> introduced <strong>subagents</strong> for parallel task completion per the <a href=\""https://cursor.com/changelog\"">Cursor Changelog</a> and a demo <a href=\""https://cdn.discordapp.com/attachments/1351160689380687942/1463993849985765510/Cursor_Changelog_Subagents.mp4\"">video</a>, but users noticed injected <strong><code>&#x3C;subagent_delegation_context></code></strong> that tells the model to call a <strong>Task tool that isn’t available</strong>.</p>\n<ul>\n<li>Community speculation pegged this as an <strong>incomplete rollout</strong> (prompting paths shipped ahead of backend), and some users suspected subagents silently <strong>fall back to Composer 1</strong>, worsening latency and “<em>planning next moves</em>” hangs.</li>\n</ul>\n</li>\n<li>\n<p><strong>Composer Crash Derby: Loops, Lag, and the Great Downgrade</strong>: Users reported <strong>Composer 1</strong> as “completely broken,” including <strong>endless chat loops</strong> and crashes, with workarounds like downgrading to <strong>Cursor 2.3</strong> (notably on <strong>macOS Big Sur 11.7.3</strong>) and filing reports via the <a href=\""https://forum.cursor.com/c/support/bug-report/6\"">Cursor bug forum</a>.</p>\n<ul>\n<li>Separately, <strong>Cursor 2.4</strong> drew complaints of severe <strong>lag/unresponsiveness</strong> and frequent crashes even on high-end machines, fueling criticism that releases feel <strong>premature</strong> and hard to trust for daily engineering work.</li>\n</ul>\n</li>\n<li>\n<p><strong>Billing Roulette: Token Counts, Auto Mode, and DIY Telemetry</strong>: Cursor users flagged <strong>usage/billing discrepancies</strong> (missing dollar amounts, unexpected bonus credits, and limits not triggering despite heavy use), and some suspected <strong>Auto mode</strong> charges incorrectly.</p>\n<ul>\n<li>To sanity-check spending, users recommended third-party tracking like <a href=\""https://token-watch.vercel.app/\"">token-watch</a>, noting it can diverge from what Cursor’s own dashboard shows.</li>\n</ul>\n</li>\n</ul>\n<p><strong>2. Inference Performance &#x26; Benchmarking: B200, vLLM, llama.cpp, Grafana</strong></p>\n<ul>\n<li>\n<p><strong>FlashAttention-4 Floors It on B200 (Specs Still Foggy)</strong>: In GPU performance chatter, <strong>FlashAttention-4 (FA4)</strong> reportedly hit <strong>1,605 TFLOPS/s</strong> (~<strong>71%</strong> of theoretical) on an <strong>NVIDIA B200</strong> with <strong>BF16</strong> inputs, while the community debated a theoretical ceiling around <strong>2260 TFLOPS</strong> and noted missing official datatype details.</p>\n<ul>\n<li>Confusion deepened as leaked materials listed B200 at <strong>10/5/2.5 TFLOPS</strong> for <strong>fp4/fp8/fp16</strong>, and folks asked for an <strong>official spec paper</strong> to reconcile marketing numbers with kernel benchmarks.</li>\n</ul>\n</li>\n<li>\n<p><strong>Spec Decode Saves Throughput, Not Your TTFT (Usually)</strong>: Engineers discussed optimizing <strong>Time To First Token (TTFT)</strong> for <strong>Qwen3-VL</strong> in <strong>vLLM</strong> on a <strong>B200</strong>, considering <code>--speculative_config</code> with smaller <strong>Qwen3-VL-4B/2B</strong> draft models.</p>\n<ul>\n<li>The advice: speculative decoding often <strong>hurts throughput</strong> unless you run <strong>high batch sizes</strong>, and only “<strong>eagle heads</strong>” setups feel worth it at scale because small drafts add too much overhead for short outputs.</li>\n</ul>\n</li>\n<li>\n<p><strong>Grafana for VLM Telemetry + vLLM Metrics as the Source of Truth</strong>: For benchmarking fast multimodal paths, members pointed to <a href=\""https://docs.vllm.ai/en/stable/design/metrics/\"">vLLM’s metrics docs</a> and suggested wiring dashboards with <a href=\""https://grafana.com/products/cloud/metrics/\"">Grafana</a> for <strong>real-time TTFT visualization</strong>.</p>\n<ul>\n<li>The thread framed Grafana as the “good UI” layer for quickly comparing VLM deployments under realistic workloads instead of relying on one-off scripts.</li>\n</ul>\n</li>\n<li>\n<p><strong>llama.cpp Speeds Up GLM-4.7 Flash GGUFs ~1.5×</strong>: <strong>llama.cpp</strong> delivered a <strong>~1.5× speedup</strong> plus bug fixes for <strong>GLM 4.7 Flash GGUFs</strong>, and users pointed to re-downloading fixed quants from <a href=\""https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF\"">unsloth/GLM-4.7-Flash-GGUF</a>.</p>\n<ul>\n<li>This reinforced the “rebuild often” culture in local inference stacks: performance jumps can appear just by updating the runtime and swapping quants, not changing the model.</li>\n</ul>\n</li>\n</ul>\n<p><strong>3. Open Releases: Voice/Audio Models, New Datasets, and Local-First LLMs</strong></p>\n<ul>\n<li>\n<p><strong>Qwen3-TTS Drops Multilingual Voice Cloning (ElevenLabs Catching Side-Eye)</strong>: Communities rallied around <strong>Qwen3-TTS</strong> as a strong <strong>voice cloning</strong> option, with a live demo on <a href=\""https://huggingface.co/spaces/Qwen/Qwen3-TTS\"">Qwen3-TTS Hugging Face Spaces</a> and the broader family referenced via <a href=\""https://github.com/QwenLM\"">QwenLM GitHub</a> and <a href=\""https://huggingface.co/Qwen\"">Qwen on Hugging Face</a>.</p>\n<ul>\n<li>Latent Space summarized the family as <strong>0.6B–1.8B params</strong> with support for <strong>10 languages</strong>, framing it as open tooling that can plausibly replace paid TTS in some pipelines.</li>\n</ul>\n</li>\n<li>\n<p><strong>Audio Release Triple-Feature: PersonaPlex-7B, TTS-1.5, and Chroma 1.0</strong>: An audio-model roundup highlighted <strong>NVIDIA PersonaPlex-7B</strong> (full-duplex conversational), <strong>Inworld AI TTS-1.5</strong> (low-latency TTS), and <strong>Flash Labs Chroma 1.0</strong> (open-source end-to-end speech-to-speech) via <a href=\""https://x.com/lina_colucci/status/2014229002370834861\"">Lina Colucci’s post</a>.</p>\n<ul>\n<li>The vibe: speech stacks are accelerating toward <strong>low-latency</strong> and <strong>end-to-end</strong> pipelines, and open releases are starting to cover pieces previously gated behind SaaS APIs.</li>\n</ul>\n</li>\n<li>\n<p><strong>Datasets &#x26; Local Models: Rust→WASM Synthetic + Faust-1 German-First</strong>: Hugging Face saw two notable drops: a <strong>Rust-to-WebAssembly synthetic dataset</strong> of <strong>1,000</strong> generated programs at <a href=\""https://huggingface.co/datasets/webxos/wasm_synthetic_dataset\"">webxos/wasm_synthetic_dataset</a>, and <strong>Faust-1</strong>, a <strong>1.6B</strong> German-first LLM at <a href=\""https://huggingface.co/tabularisai/Faust-1\"">tabularisai/Faust-1</a>.</p>\n<ul>\n<li>Faust-1 emphasized <strong>~90% German pretraining</strong>, a <strong>German-optimized tokenizer</strong>, and instruction tuning with <strong>DPO</strong>, while the WASM dataset focused on <strong>reproducibility</strong> (deterministic Fibonacci-derived PRNG plus structural hashes).</li>\n</ul>\n</li>\n</ul>\n<p><strong>4. Agent Frameworks &#x26; Infra: Control Loops, RLM/DSPy, and MCP Schema Discipline</strong></p>\n<ul>\n<li>\n<p><strong>Replit Agent Gets a Steering Wheel with Decision-Time Guidance</strong>: A technical writeup on <strong>Decision-Time Guidance</strong> described how <strong>Replit Agent</strong> applies <strong>real-time control mechanisms</strong> instead of static rules, shared via <a href=\""https://xcancel.com/zhenthebuilder/status/2014393451442581688\"">Zhen Li’s blog link</a>.</p>\n<ul>\n<li>The discussion framed this as a practical direction for agents: tighter <strong>online steering</strong> during execution rather than brittle pre-authored guardrails.</li>\n</ul>\n</li>\n<li>\n<p><strong>DSPy’s “Why” Gets Re-Explained (Signatures > Prompt Hacks)</strong>: DSPy folks circulated an explainer arguing DSPy’s value comes from <strong>signature &#x26; module abstractions</strong>, not just prompt tuning, via <a href=\""https://eito.substack.com/p/dspy-the-most-misunderstood-agent\"">“DSPy: the most misunderstood agent”</a> and the companion <a href=\""https://x.com/Eito_Miyamura/status/2014757193766093069\"">X post</a>.</p>\n<ul>\n<li>Separately, members discussed tuning <strong>RLM prompts</strong> and even optimizing JSON-schema adapters (GEPA ideas), aiming to make structured outputs more reliable without bloating token budgets.</li>\n</ul>\n</li>\n<li>\n<p><strong>MCP Schema Cage Match: <code>additionalProperties</code> vs <code>anyOf</code></strong>: Model Context Protocol contributors questioned whether <strong><code>GetTaskPayloadResult</code></strong> is too permissive because it allows <strong><code>additionalProperties</code></strong>, pointing directly to the schema location in the MCP repo (<a href=\""https://github.com/modelcontextprotocol/modelcontextprotocol/blob/8d07c35d3857412a351c595fe01b7bc70664ba06/schema/2025-11-25/schema.json#L1245-L1256\"">schema.json lines 1245–1256</a>).</p>\n<ul>\n<li>The proposed fix was moving toward <strong><code>anyOf</code></strong> for stricter validation, reflecting a broader push to keep agent-tool payloads <strong>tightly typed</strong> to avoid “accept everything” integrations that break downstream.</li>\n</ul>\n</li>\n</ul>\n<p><strong>5. Platforms, Benchmarks, and the AMD/NVIDIA Reality Check</strong></p>\n<ul>\n<li>\n<p><strong>Arena Leaderboards Split (and Models Vanish Mid-Game)</strong>: LMArena’s <strong>Image Edit Arena</strong> split rankings into <strong>Single-Image Edit</strong> and <strong>Multi-Image Edit</strong>, publishing results at the <a href=\""https://lmarena.ai/leaderboard/image-edit/overall\"">Image Edit leaderboard</a> where <strong>Gemini 3 Pro Image 2K</strong> rose to #1 and <strong>ChatGPT Image (Latest)</strong> fell to #3.</p>\n<ul>\n<li>At the same time, reliability issues yanked models around: <strong>Nano Banana Pro 2K</strong> got removed for a <strong>high error rate</strong>, and <strong>Seedream-4-2k</strong> disappeared with moderators noting models can be unavailable for technical reasons.</li>\n</ul>\n</li>\n<li>\n<p><strong>ROCm Pain, CUDA Gain: Devs Vote with Their 5090s</strong>: GPU devs shared AMD tooling breadcrumbs—like the <strong>AMD ISA manual</strong> and LLVM docs (<a href=\""https://github.com/llvm/llvm-project/blob/main/llvm/docs/AMDGPUUsage.rst\"">AMDGPUUsage.rst</a>, <a href=\""https://github.com/llvm/llvm-project/blob/main/llvm/include/llvm/IR/IntrinsicsAMDGPU.td\"">IntrinsicsAMDGPU.td</a>, and <a href=\""https://github.com/llvm/llvm-project/tree/main/llvm/test/CodeGen/AMDGPU\"">AMDGPU CodeGen tests</a>)—but the tone stayed mixed on ROCm readiness.</p>\n<ul>\n<li>One developer said they quit <strong>ROCm for CUDA</strong> after buying a <strong>5090</strong>, citing packaging/build/distribution headaches, and another pointed to poor <strong>Conv3D</strong> performance on AMD relative to NVIDIA via a ROCm subreddit thread (<a href=\""https://www.reddit.com/r/ROCm/comments/1owczm9/the_convolution_performance_on_rx_9070_is_so_low/\"">link</a>).</li>\n</ul>\n</li>\n<li>\n<p><strong>Baseten Raises $300M Series E as Infra Funding Stays Hot</strong>: Latent Space highlighted <strong>Baseten’s $300M Series E</strong> led by IVP and CapitalG, valuing the company at <strong>$5B</strong>, per <a href=\""https://xcancel.com/basetenco/status/2014755013344792595\"">Baseten’s announcement</a>.</p>\n<ul>\n<li>The participation of <strong>NVIDIA</strong> in the round reinforced the market narrative: inference infrastructure remains a capital-heavy race where hardware adjacency still matters.</li>\n</ul>\n</li>\n</ul>\n<h2>gpt-5.1</h2>\n<p><strong>1. Frontier Model Performance, Kernels, and Hardware Benchmarks</strong></p>\n<ul>\n<li>\n<p><strong>FlashAttention-4 Pushes B200 GPUs Toward Theoretical Limits</strong>: <strong>FlashAttention‑4 (FA4)</strong> hit <strong>1,605 TFLOPS/s (~71% of theoretical max)</strong> on an <strong>NVIDIA B200</strong> in <strong>BF16</strong>, with community estimates pegging the true ceiling around <strong>2,260 TFLOPS</strong>, pending an official spec paper and data‑type breakdown from NVIDIA. Discussion in <strong>GPU MODE #cuda</strong> noted leaked B200 figures of <strong>10/5/2.5 PFLOPS for fp4/fp8/fp16</strong> clashing with the FA4 measurements, underscoring the need for a formal performance whitepaper rather than marketing blogs without datatype detail.</p>\n<ul>\n<li>Researchers also highlighted <strong>test‑time training (TTT)</strong> for <strong>LM‑generated kernels</strong> in a new paper, <a href=\""https://test-time-training.github.io/discover.pdf\"">\""Discovering Test-Time Training for LLM‑Generated GPU Kernels\""</a>, showing that adapting kernels at inference can materially improve benchmark scores on existing leaderboards. In parallel, <strong>FlashInfer‑Bench</strong> from <strong>CMU Catalyst Lab</strong> was introduced in GPU MODE’s <code>#popcorn</code> as a framework for evaluating and deploying <strong>AI‑generated GPU kernels</strong>, with the authors actively seeking community feedback on benchmarks and production deployment workflows.</li>\n</ul>\n</li>\n<li>\n<p><strong>GLM-4.7 Flash Builds Hit Turbo in llama.cpp and Arena</strong>: Both <strong>LMArena</strong> and <strong>Hugging Face</strong> circles reported major speed wins from <strong>GLM‑4.7‑Flash</strong> variants, with <strong>llama.cpp</strong> users seeing roughly <strong>1.5× throughput gains</strong> on <strong>GLM‑4.7 Flash GGUFs</strong> after rebuilding and re‑downloading new quants from <a href=\""https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF\"">unsloth/GLM-4.7-Flash-GGUF</a>. LMArena also added <strong>glm‑4.7‑flash</strong> to the <a href=\""https://lmarena.ai/?chat-modality=chat\"">Text Arena</a>, giving a head‑to‑head benchmark venue against other frontier chat models.</p>\n<ul>\n<li>An Unsloth user reported <strong>50 tok/s at 50k context</strong> with <code>--flash-attn on</code> on GLM‑4.7‑Flash, reinforcing that the FlashAttention fixes are stable at long context lengths, while Nous users experimented with <strong>GLM‑4.7</strong> on <strong>8×H100</strong> GPU servers as a potential <strong>Claude Code</strong> alternative. Across Discords, practitioners converged on a pattern of: rebuild kernels, enable explicit FlashAttention flags, and push long‑context workloads to stress‑test <strong>GLM‑4.7 Flash</strong> as an affordable, high‑throughput code‑capable model.</li>\n</ul>\n</li>\n<li>\n<p><strong>GPU Ecosystems Split: CUDA Dominates as ROCm Stumbles</strong>: In <strong>GPU MODE #rocm</strong>, a developer announced they were <em>\""done with ROCm\""</em> after buying an <strong>RTX 5090</strong>, citing chronic issues with <strong>packaging, build chains, distribution gaps, and weak consumer focus</strong>, and sharing a <a href=\""https://www.reddit.com/r/ROCm/comments/1owczm9/the_convolution_performance_on_rx_9070_is_so_low/\"">Reddit thread on poor Conv3D performance on RX 9070</a> as evidence that mid‑range NVIDIA cards still crush AMD on real‑world ML workloads. Others criticized the ROCm ecosystem as <em>\""hostile\""</em> and pointed at fragile libraries like <strong>FBGEMM</strong> on <code>gfx1100</code> and opaque vendor repos such as AMD’s <a href=\""https://github.com/amd/Quark/commit/9234960c951410abdcecee033adf610d7126fda3\"">Quark quantization engine</a>.</p>\n<ul>\n<li>To mitigate the pain, experts shared low‑level ROCm documentation sources—<strong>AMD’s CDNA4 ISA manual</strong> and LLVM’s <a href=\""https://github.com/llvm/llvm-project/blob/main/llvm/docs/AMDGPUUsage.rst\"">AMDGPUUsage.rst</a> plus <a href=\""https://github.com/llvm/llvm-project/blob/main/llvm/include/llvm/IR/IntrinsicsAMDGPU.td\"">IntrinsicsAMDGPU.td</a>—and emphasized that <strong>clang builtins map 1:1 to LLVM intrinsics</strong>, which you can reverse‑engineer via <a href=\""https://github.com/llvm/llvm-project/tree/main/llvm/test/CodeGen/AMDGPU\"">AMDGPU CodeGen tests</a>. Meanwhile, a separate GPU MODE thread advertised <strong>CUDA‑kernel optimization jobs</strong> (profiling with <strong>Nsight Systems/Compute</strong>, writing optimized CUTLASS‑style kernels) via a <a href=\""https://tally.so/r/pbDDvZ\"">Parsewave posting</a>, underscoring that the ecosystem gravity—and money—is still heavily on the CUDA side.</li>\n</ul>\n</li>\n</ul>\n<p><strong>2. New Models, TTS, and Benchmarks Across the Open Ecosystem</strong></p>\n<ul>\n<li>\n<p><strong>Qwen3-TTS Struts In as Multilingual Voice-Cloning Workhorse</strong>: Alibaba launched <strong>Qwen3‑TTS</strong>, a family of open <strong>text‑to‑speech</strong> models (≈<strong>0.6B–1.8B</strong> params) supporting <strong>10 languages</strong> with variants for <strong>VoiceDesign</strong>, <strong>CustomVoice</strong>, and <strong>Base</strong>, released on [GitHub (QwenLM)]https://github.com/QwenLM and <a href=\""https://huggingface.co/Qwen\"">Hugging Face</a>. Latent Space’s <code>#genmedia-creative-ai</code> highlighted its high‑quality cloning, while Nous community members directly compared <a href=\""https://huggingface.co/spaces/Qwen/Qwen3-TTS\"">the interactive demo on Hugging Face Spaces</a> to <strong>ElevenLabs</strong>, calling it <em>“a very good voice cloning tool.”</em></p>\n<ul>\n<li>Early adopters are probing <strong>multilingual robustness</strong> and <strong>clone fidelity</strong>, with one Nous user emphasizing that Qwen3‑TTS is <em>competitive with commercial TTS</em> for user‑facing agents. Latent Space threads grouped it with other audio releases—<strong>NVIDIA PersonaPlex‑7B</strong>, <strong>Inworld TTS‑1.5</strong>, and <strong>Flash Labs’ Chroma 1.0</strong> described in <a href=\""https://x.com/lina_colucci/status/2014229002370834861\"">Lina Colucci’s roundup</a>—framing Qwen3‑TTS as the open‑source counterpart in a rapidly heating speech‑to‑speech and conversational‑audio race.</li>\n</ul>\n</li>\n<li>\n<p><strong>Image Edit Arena Shakes Up Multimodal Rankings</strong>: <strong>LMArena</strong> split its <strong>Image Edit Arena</strong> leaderboard into separate <strong>Single‑Image Edit</strong> and <strong>Multi‑Image Edit</strong> tracks, publishing the results at the new <a href=\""https://lmarena.ai/leaderboard/image-edit/overall\"">image‑edit leaderboard</a> for finer‑grained comparison of visual editing ability. The reshuffle toppled incumbents: <strong>ChatGPT Image (Latest)</strong> dropped from #1 to <strong>#3</strong>, while <strong>Gemini 3 Pro Image 2K</strong> climbed from #2 to the <strong>top spot</strong>, with Nano Banana and Seedream models also being shuffled and occasionally pulled (e.g., <strong>Seedream‑4‑2k</strong> disappearing for technical reasons).</p>\n<ul>\n<li>Concurrently, LMArena added <strong>wan2.6‑image</strong> (image‑edit only), <strong>wan2.6‑t2i</strong> (text‑to‑image), and <strong>devstral‑2</strong> (Code Arena) via their <a href=\""https://lmarena.ai/c/new?chat-modality=image\"">announcements</a>, though users noted a confusing limitation where <code>wan2.6-t2i</code> currently exposes no image upload. Operationally, the platform yanked <strong>Nano Banana Pro 2K</strong> due to high error rates and acknowledged persistent <strong>video generation failures and Linux‑only captchas</strong>, reinforcing that frontier multimodal eval is still bottlenecked as much by infra quirks as by model quality.</li>\n</ul>\n</li>\n<li>\n<p><strong>New Open Datasets and Niche Models Fuel Specialized Workloads</strong>: The Hugging Face <code>#i-made-this</code> channel saw the release of <strong>Faust‑1</strong>, a <strong>1.6B German‑first LLM</strong> with ≈<strong>90% German pretraining</strong>, a German‑optimized tokenizer, and DPO‑tuned instructions, published at <a href=\""https://huggingface.co/tabularisai/Faust-1\"">tabularisai/Faust-1</a> for <strong>local, privacy‑sensitive use cases</strong>. Another contributor released a synthetic <strong>Rust→WebAssembly compilation dataset</strong> of <strong>1,000 programmatically generated Rust programs</strong> at <a href=\""https://huggingface.co/datasets/webxos/wasm_synthetic_dataset\"">webxos/wasm_synthetic_dataset</a>, with deterministic Fibonacci‑based pseudo‑random generation to ensure reproducible code patterns and compiler behaviors.</p>\n<ul>\n<li>Alongside these, a <strong>safety dataset</strong> for alignment research landed at <a href=\""https://huggingface.co/datasets/Pacific-Prime/safety_dataset\"">Pacific-Prime/safety_dataset</a>, while a separate project generated custom <strong>typeface datasets</strong> via <a href=\""https://webxos.netlify.app/COLIGNUM\"">COLIGNUM</a> for font‑centric ML work. Collectively these releases hint at a maturing long tail of <strong>domain‑specific corpora</strong>—language‑localized LLMs, compiler‑oriented code sets, safety supervision data, and typographic datasets—feeding into RAG systems, continual‑learning workflows, and evaluation of program synthesis and WebAssembly tooling.</li>\n</ul>\n</li>\n</ul>\n<p><strong>3. Agentic Frameworks, DSPy/RLM, and IDE Tooling</strong></p>\n<ul>\n<li>\n<p><strong>DSPy and RLM Reframe Agents as Optimizable Programs</strong>: In the DSPy Discord, an article titled <a href=\""https://eito.substack.com/p/dspy-the-most-misunderstood-agent\"">\""DSPy: The Most Misunderstood Agent Framework\""</a> argued that DSPy’s real value is its <strong>signature &#x26; module abstraction</strong>, not just <strong>GEPA and prompt‑tuning hacks</strong>, stressing that programs of LMs should be treated like differentiable pipelines rather than hand‑wired agents. Another blog, <a href=\""https://raveesh.substack.com/p/a-pragmatic-recipe-for-continual?r=qn1thttps://arxiv.org/abs/2512.21859\"">\""A Pragmatic Recipe for Continual Learning\""</a>, pitched <strong>DSPy.RLM()</strong> as a core building block for engineered continual‑learning systems that retrain themselves over time.</p>\n<ul>\n<li>Members experimented with <strong>RLM prompts</strong> to improve reasoning—complaining that some models still give <em>\""vague generic answers\""</em>—and proposed optimizing RLM traces similarly to <strong>ReAct</strong>, where an optimizer inspects step‑by‑step logs while users only care about final outputs. There was also interest in building a <strong>custom GEPA adapter</strong> for DSPy’s JSON output layer, so that <strong>json_schema‑based responses</strong> can be optimized to drop redundant system tokens and reduce overhead for structured tool integrations.</li>\n</ul>\n</li>\n<li>\n<p><strong>IDE Agents Evolve: Cursor Subagents and Aider Workflows</strong>: The <strong>Cursor</strong> community dissected the <strong>2.4 release</strong>, which introduced parallel <strong>subagents</strong> (documented in the <a href=\""https://cursor.com/changelog\"">Cursor changelog</a> and demo video) that inject a <code>&#x3C;subagent_delegation_context></code> to farm tasks out in parallel, plus <strong>image generation</strong> and clarification‑question capabilities advertised on <a href=\""https://x.com/cursor_ai/status/2014433672401977382\"">Cursor’s X post</a>. However, users in <code>#general</code> reported <strong>Composer 1 infinite loops</strong>, heavy lag in <strong>2.4</strong> (<code>\""planning next moves\""</code> hanging), and suspected that broken subagent scaffolding was calling a missing <strong>Task tool</strong>, forcing many to downgrade to <strong>2.3</strong>—especially on older macOS versions like <strong>Big Sur 11.7.3</strong>.</p>\n<ul>\n<li>Separately, the <strong>aider</strong> community proposed a terminal UI and <strong>session management</strong> for the CLI‑based coding assistant, aiming to let users edit the next message while scrolling past replies, render rich Markdown (including <strong>mermaid diagrams</strong>), and save/load entire chat contexts without polluting the current prompt. Power users described a <strong>meta‑workflow</strong> pairing <em>aider</em> for context management and search‑replace coding with <strong>Claude Code</strong> for hard bug‑hunting, framing aider as the <em>“file selection &#x26; edit engine”</em> that minimizes tokens while a remote LLM handles deeper reasoning.</li>\n</ul>\n</li>\n<li>\n<p><strong>MCP and Schema Design for Tool-Calling Agents</strong>: In the <strong>MCP Contributors</strong> Discord, contributors scrutinized the <strong>Model Context Protocol</strong>’s <code>GetTaskPayloadResult</code> schema, pointing out that its use of <code>\""additionalProperties\""</code> in the JSON Schema at <a href=\""https://github.com/modelcontextprotocol/modelcontextprotocol/blob/8d07c35d3857412a351c595fe01b7bc70664ba06/schema/2025-11-25/schema.json#L1245-L1256\"">this definition</a> makes payloads too permissive. They proposed switching to an <code>anyOf</code> union of explicit alternatives to enforce that only pre‑declared fields appear, tightening validation for tool payloads.</p>\n<ul>\n<li>The discussion framed this as a <strong>tooling‑reliability tradeoff</strong>: <code>additionalProperties</code> keeps MCP extensible for new tools, but weakens static guarantees, whereas <code>anyOf</code> helps clients and servers catch malformed or adversarial payloads early. Given MCP’s ambition as a cross‑tool agent protocol, participants argued that <strong>strict schemas for core messages like <code>GetTaskPayloadResult</code></strong> matter for security, debugging, and interop, even if it requires more frequent schema migrations.</li>\n</ul>\n</li>\n</ul>\n<p><strong>4. Experimental Architectures, Optimization Tricks, and Training Methods</strong></p>\n<ul>\n<li>\n<p><strong>Difference Layers and SwiGLU Turbocharge NanoGPT Baselines</strong>: In <strong>Eleuther’s #research</strong>, a contributor reported strong gains on <strong>CartPole</strong> and <strong>NanoGPT</strong> by swapping the standard MLP with a <em>difference layer</em> <code>x = (a2(x) - b2(x)) * (c2(x) - d2(x)) + e2(x)</code> from <a href=\""https://github.com/Eternalyze0/difference_layer\"">Eternalyze0/difference_layer</a>, claiming better performance at lower parameter and compute budgets. Others cautioned that such multiplicative structures effectively <strong>double the learning rate</strong> under SGD and that improvements may vanish against well‑tuned baselines rather than the default NanoGPT configs.</p>\n<ul>\n<li>Researchers also noted that simply replacing GELU with <strong>SwiGLU</strong> as the transformer activation significantly improves the <strong>NanoGPT baseline</strong>, and further gains appear when combining SwiGLU with the difference‑layer QKV replacement. Senior members repeatedly pointed newcomers at Noam Shazeer’s <a href=\""https://arxiv.org/abs/2002.05202\"">\""GLU Variants Improve Transformer\"" paper</a>, warning that any new gating trick should be benchmarked against <strong>state‑of‑the‑art GLU baselines</strong> before being touted as a structural breakthrough.</li>\n</ul>\n</li>\n<li>\n<p><strong>GRPO, Attention Sinks, and Reasoning Training Gotchas</strong>: Unsloth’s <code>#research</code> and <code>#off-topic</code> channels hosted candid post‑mortems on <strong>GRPO (Generalized Reinforcement Policy Optimization)</strong>, with one practitioner concluding from experiments (and re‑reading the <a href=\""https://arxiv.org/abs/2601.07568\"">\""DeepSeek R1\"" paper</a>) that GRPO <strong>refines existing reasoning</strong> but does not magically unlock <em>“emergent reasoning”</em> in niche domains where pretraining data is thin. They described a three‑stage pipeline—corpus CPT on novels + medical articles (~400M tokens), translated SFT, then synthetic polishing via rejection sampling—and still found GRPO unstable for specialized tasks like Turkish translation and domain support.</p>\n<ul>\n<li>On the representation side, Unsloth members debated <strong>attention sinks</strong>, with some manually injecting <code>&#x3C;|endoftext|></code> at the context start, while others argued that <em>“attention sink is poured into the very first token in the entire context window, just one token for it”</em> and that models learn their own sink dynamics. A separate lesson learned the hard way: when using small models to generate <em>chain‑of‑thought</em> traces to train bigger reasoning models, <strong>masking the thinking tokens</strong> during supervised training significantly improves metrics (unmasked CoT caused <strong>F1 to crater</strong>, despite seeming attractive for interpretability).</li>\n</ul>\n</li>\n<li>\n<p><strong>Test-Time-Training, LM Kernels, and Self-Replication Benchmarks</strong>: GPU MODE’s <code>#general</code> and <code>#multi-gpu</code> channels highlighted <strong>test‑time training (TTT)</strong> as a promising paradigm not just for models but for <strong>LM‑generated GPU kernels</strong>, with the <strong>discover.pdf</strong> paper at <a href=\""https://test-time-training.github.io/discover.pdf\"">test-time-training.github.io/discover.pdf</a> showing that adapting kernels against benchmark suites at inference can yield surprisingly strong performance. In parallel, debugging threads around <strong>NCCL on B200s under Slurm</strong>—including sbatch scripts and <code>NCCL_DEBUG=INFO</code> logs—reinforced that auto‑tuning comms libraries plus dynamic kernel adaptation is becoming a combined engineering problem rather than a pure modeling issue.</p>\n<ul>\n<li>Over in Nous, a member brainstormed a <strong>self‑replication benchmark for agentic AI</strong>, and someone suggested using Claude’s C‑implemented transformer engine and custom CPU described in <a href=\""https://github.com/cpldcpu/smollm.c/blob/claude/train-small-model-llxVr/train-small-model-llxVr/smolc/smolc.c\"">cpldcpu’s <code>smollm.c</code></a> as a target: can an agent inspect, modify, and re‑deploy its own inference engine? This dovetails with DSPy/RLM discussions, hinting at a future where <strong>agents optimize both their weights and their low‑level kernels</strong> at inference time under constrained hardware budgets.</li>\n</ul>\n</li>\n</ul>\n<p><strong>5. AI Business, APIs, and Production Reliability</strong></p>\n<ul>\n<li>\n<p><strong>Baseten’s $300M Raise and Capital One–Brex Deal Signal AI Infra Consolidation</strong>: Latent Space’s <code>#ai-general-chat</code> flagged two major deals: <strong>Capital One acquiring Brex for $5.15B</strong>, as reported by <a href=\""https://x.com/alexfmac/status/2014676950883668306\"">Alex MacCaw</a>, marking the largest bank–fintech acquisition to date; and <strong>Baseten’s $300M Series E at a $5B valuation</strong>, announced in <a href=\""https://xcancel.com/basetenco/status/2014755013344792595?s=46\"">Baseten’s tweet</a> with IVP and CapitalG leading and NVIDIA participating. Both moves underscore that <strong>AI‑heavy infra and fintech tooling</strong> are being rapidly absorbed or capitalized by large incumbents and late‑stage investors.</p>\n<ul>\n<li>Members interpreted the Baseten round as validation that <strong>model serving and orchestration</strong> is a defensible, high‑margin layer even in an open‑model world, while the Capital One–Brex deal was read as a bet that <strong>data‑rich fintech workflows</strong> (expense management, cards, underwriting) will be increasingly AI‑automated. Combined with SimilarWeb stats shared in a <a href=\""https://xcancel.com/venturetwins/status/2014739492389978274?s=46\"">Venture Twins tweet</a> showing <strong>ChatGPT still dominating traffic while Grok grows 33× in US penetration</strong>, the community sees a landscape where infra, data, and distribution matter at least as much as raw model quality.</li>\n</ul>\n</li>\n<li>\n<p><strong>Perplexity Pro and API Turbulence Threaten Power-User Workloads</strong>: On the <strong>Perplexity AI</strong> server, Pro subscribers reported abrupt <strong>file‑upload caps of 3/day</strong>, conflicting <strong>research query limits</strong> (some seeing <strong>600/day</strong>, others as low as <strong>20</strong>), and persistent <strong>401 Unauthorized</strong> errors on the <strong>Perplexity API</strong> even after renewing credits, which broke production use cases like a sports‑betting model. Threads in <code>#general</code> and <code>#pplx-api</code> speculated about <strong>A/B experiments vs. cost‑cutting</strong>, and some users threatened to cancel, arguing that silent feature downgrades destroy trust for teams trying to treat Perplexity as a dependable research backend.</p>\n<ul>\n<li>At the model layer, users shared a medical example where <strong>Gemini, Claude Opus, and Ernie</strong> all failed to recommend a <strong>DEXA bone‑density scan</strong> when asked about calcium deficiency work‑ups, whereas <strong>GPT</strong> explicitly mentioned it, reinforcing that Perplexity’s meta‑model/engine choice can materially affect clinical recommendations. Combined with billing bugs (pending charges, locked accounts) and contested celestial fact‑checking drama, the overarching sentiment was that <strong>Perplexity’s product is powerful but operationally fragile</strong>, and engineers should have fallbacks before wiring it deep into production flows.</li>\n</ul>\n</li>\n<li>\n<p><strong>IDE, API, and Billing Reliability: Cursor, Manus, and OpenRouter</strong>: Cursor power‑users complained that <strong>2.4</strong> shipped with <strong>severe lag, crashes, and broken Composer 1 loops</strong>, and also raised <strong>billing opacity</strong> concerns: inconsistent dollar displays, unpredictable limits in <strong>Auto</strong> mode, and unexplained bonus credits prompted some to rely on <a href=\""https://token-watch.vercel.app/\"">token-watch</a> for independent usage audits. Over in <strong>Manus.im</strong>, a user reported being charged <strong>$400</strong> for an annual plan despite selecting monthly during a trial, and openly discussed escalating to <strong>FTC/BBB/Attorney General</strong> if not refunded, warning others to double‑check plan terms.</p>\n<ul>\n<li>The <strong>OpenRouter</strong> community noticed that internal <strong> reasoning blocks</strong> recently started leaking into end‑user responses in OR Chat and JanitorAI, raising UX and privacy questions and triggering a support ticket. Meanwhile, an OpenRouter thread about <strong>uncensored image generation</strong> concluded that engineers should pair one <strong>text LLM</strong> with a separate <strong>image model</strong> rather than expect a single uncensored multimodal endpoint, while some users half‑jokingly proposed an <strong>OpenRouter gacha system</strong> with pity mechanics and leaderboards, reflecting both frustration with opaque pricing and a desire for more transparent, game‑like model discovery.</li>\n</ul>\n</li>\n</ul>\n<h2>gpt-5</h2>\n<p><strong>1. New TTS and Audio AI Releases</strong></p>\n<ul>\n<li>\n<p><strong><strong>Tongue-Twisting TTS Triumphs</strong></strong>: Alibaba unveiled <strong>Qwen3-TTS</strong> with <strong>VoiceDesign</strong>, <strong>CustomVoice</strong>, and <strong>Base</strong> variants (five models, <strong>0.6B–1.8B</strong> params, <strong>10 languages</strong>), with releases on <a href=\""https://github.com/QwenLM\"">QwenLM GitHub</a> and <a href=\""https://huggingface.co/Qwen\"">Hugging Face</a>.</p>\n<ul>\n<li>Community demos showcased high-fidelity cloning and multilingual synthesis via the official <a href=\""https://huggingface.co/spaces/Qwen/Qwen3-TTS\"">Qwen3-TTS Space</a>, with users calling the results <em>“very good voice cloning.”</em></li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Audio Arms Race Accelerates</strong></strong>: A roundup highlighted NVIDIA’s <strong>PersonaPlex‑7B</strong> (full‑duplex), <strong>Inworld TTS‑1.5</strong> (low‑latency), and <strong>Flash Labs’ Chroma 1.0</strong> (open end‑to‑end speech‑to‑speech), summarized in <a href=\""https://x.com/lina_colucci/status/2014229002370834861\"">Lina Colucci’s post</a>.</p>\n<ul>\n<li>Engineers discussed how these releases push <strong>real‑time conversational</strong> and <strong>SS2S</strong> stacks toward production, framing Q3–Q4 as a breakout window for low‑latency voice agents.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Voice Design Goes DIY</strong></strong>: <strong>Qwen3-TTS</strong>’s <strong>VoiceDesign</strong> and <strong>CustomVoice</strong> features enable user‑defined voices and cloning workflows with accessible configs and assets on <a href=\""https://huggingface.co/Qwen\"">Hugging Face</a>.</p>\n<ul>\n<li>Builders reported that the Space’s cloning quality <em>“rivals <strong>ElevenLabs</strong>”</em> in quick trials, encouraging bake‑offs using the <a href=\""https://huggingface.co/spaces/Qwen/Qwen3-TTS\"">official demo</a>.</li>\n</ul>\n</li>\n</ul>\n<p><strong>2. AI Kernel Benchmarks &#x26; Optimization</strong></p>\n<ul>\n<li>\n<p><strong><strong>FlashInfer Frenzy Benchmarks Kernels</strong></strong>: CMU Catalyst Lab introduced <strong>FlashInfer‑Bench</strong>, a framework to evaluate <strong>AI‑generated GPU kernels</strong> and deploy them into serving engines: <a href=\""https://mlsys26.flashinfer.ai\"">FlashInfer‑Bench</a>.</p>\n<ul>\n<li>Participants praised the effort as <em>“a very cool project,”</em> and the team invited collaboration on refining benchmarks and production deployment pathways.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>TTT Tunes Tiny Kernels</strong></strong>: Researchers evaluated <strong>LM‑generated kernels</strong> with <strong>test‑time training (TTT)</strong> on prior leaderboards, reporting promising outcomes in the paper <a href=\""https://test-time-training.github.io/discover.pdf\"">Discovering Test-Time Training</a>.</p>\n<ul>\n<li>Discussions centered on how <strong>TTT</strong> can adapt kernels to distribution shifts at inference, potentially boosting leaderboard parity without retraining.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>ROCm Readmes Reveal Intrinsics</strong></strong>: Engineers mapped <strong>clang builtins → LLVM intrinsics</strong> for AMD GPUs using the <strong>CDNA4 ISA manual</strong> and LLVM docs: <a href=\""https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/instruction-set-architectures/amd-instinct-cdna4-instruction-set-architecture.pdf\"">AMD ISA PDF</a>, <a href=\""https://github.com/llvm/llvm-project/blob/main/llvm/docs/AMDGPUUsage.rst\"">AMDGPUUsage.rst</a>.</p>\n<ul>\n<li>They also pointed to <a href=\""https://github.com/llvm/llvm-project/blob/main/llvm/include/llvm/IR/IntrinsicsAMDGPU.td\"">IntrinsicsAMDGPU.td</a> and CodeGen tests for examples, helping practitioners align kernel code with <strong>ROCm</strong>’s compilation model.</li>\n</ul>\n</li>\n</ul>\n<p><strong>3. Agentic IDEs and Dev Tooling</strong></p>\n<ul>\n<li>\n<p><strong><strong>Copilot SDK Cuts the Cord</strong></strong>: Developers celebrated the release of the <strong>GitHub Copilot SDK</strong>, which enables first‑class <strong>AI features</strong> inside apps via GitHub’s infra: <a href=\""https://github.com/github/copilot-sdk\"">github.com/github/copilot-sdk</a>.</p>\n<ul>\n<li>Early adopters emphasized replacing bespoke routing and third‑party pricing with a native SDK, streamlining <strong>tool‑augmented agent</strong> integration.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Cursor Subagents Sprint in 2.4</strong></strong>: <strong>Cursor 2.4</strong> shipped parallel <strong>subagents</strong> for faster execution and better context use, plus <strong>image generation</strong> and clarifying questions: <a href=\""https://cursor.com/changelog\"">Changelog</a> and <a href=\""https://x.com/cursor_ai/status/2014433672401977382\"">Cursor on X</a>.</p>\n<ul>\n<li>The team’s video demo shows subagents coordinating on multi‑step tasks, promising speedups for complex coding flows.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Baseten Banks Big Bucks</strong></strong>: <strong>Baseten</strong> raised <strong>$300M Series E</strong> (IVP, CapitalG; participation from NVIDIA), reaching a <strong>$5B</strong> valuation: <a href=\""https://xcancel.com/basetenco/status/2014755013344792595\"">Baseten announcement</a>.</p>\n<ul>\n<li>Infra‑minded builders read this as a signal of sustained demand for <strong>model serving</strong>, <strong>ops</strong>, and <strong>agent backends</strong> at enterprise scale.</li>\n</ul>\n</li>\n</ul>\n<p><strong>4. Model Speedups &#x26; Evaluation Arenas</strong></p>\n<ul>\n<li>\n<p><strong><strong>llama.cpp Lights Up GLM Flash</strong></strong>: <strong>llama.cpp</strong> improved performance for <strong>GLM‑4.7 Flash GGUF</strong> by ~<strong>1.5×</strong> and fixed bugs; users were told to rebuild and fetch fixed quants from <a href=\""https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF\"">unsloth/GLM‑4.7‑Flash‑GGUF</a>.</p>\n<ul>\n<li>Reports cited stable <strong>50 tok/s at 50k context</strong> with flash attention enabled, noting it <em>“is working beautifully”</em> after the fix.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Leaderboard Split Sharpens Image Edits</strong></strong>: LMArena split the <strong>Image Edit Arena</strong> into <strong>Single‑Image Edit</strong> vs <strong>Multi‑Image Edit</strong>, revealing shifts on the <a href=\""https://lmarena.ai/leaderboard/image-edit/overall\"">overall leaderboard</a>.</p>\n<ul>\n<li><strong>ChatGPT Image (Latest)</strong> dropped from #1→#3 while <strong>Gemini 3 Pro Image 2K</strong> rose from #2→#1, offering clearer task‑specific rankings.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Arena Adds Wan &#x26; Devstral</strong></strong>: New LMArena entries include <strong>wan2.6‑t2i</strong> (text‑to‑image), <strong>wan2.6‑image</strong> (image edit), and <strong>devstral‑2</strong> (code), available via <a href=\""https://lmarena.ai\"">LMArena</a>.</p>\n<ul>\n<li>The split of <code>wan2.6</code> into distinct edit vs T2I endpoints aims to reduce misuse and clarify capabilities in head‑to‑head evaluations.</li>\n</ul>\n</li>\n</ul>\n<p><strong>5. Research Tricks in Architectures &#x26; Training</strong></p>\n<ul>\n<li>\n<p><strong><strong>SwiGLU Swings the Baseline</strong></strong>: Researchers reported that switching <strong>GELU → SwiGLU</strong> significantly boosted <strong>NanoGPT</strong>‑style baselines, aligning with <a href=\""https://arxiv.org/abs/2002.05202\"">Shazeer’s GLU variants paper</a>.</p>\n<ul>\n<li>The conversation emphasized strong baselines to avoid mistaking optimization gains for architectural advances.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Difference Layer Doubles Down</strong></strong>: A proposed multiplicative <strong>difference layer</strong> improved <strong>cartpole</strong> and <strong>nanogpt</strong> performance with fewer params/compute: <a href=\""https://github.com/Eternalyze0/difference_layer\"">difference_layer repo</a>.</p>\n<ul>\n<li>Skeptics noted the formulation may implicitly <strong>double the effective learning rate</strong>, urging comparisons against tuned baselines rather than defaults.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>GRPO Gets Groggy</strong></strong>: Engineers found <strong>GRPO</strong> <em>“proved unstable”</em> in niche domains lacking pretraining coverage, debating claims of emergent reasoning in the <strong>DeepSeek</strong> paper: <a href=\""https://arxiv.org/abs/2601.07568\"">arXiv:2601.07568</a>.</p>\n<ul>\n<li>Consensus landed on using <strong>GRPO</strong> to refine existing capabilities, while reserving broader reasoning improvements for data/architecture changes.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h1>Discord: High level Discord summaries</h1>\n<h2><a href=\""https://discord.com/channels/1105891499641684019\"">BASI Jailbreaking</a> Discord</h2>\n<ul>\n<li><strong>Gemini 3 Pro Succumbs to ENI Jailbreak</strong>: The <strong>ENI jailbreak</strong>, originally designed for Claude, has been found to work on <strong>Gemini 3 Pro</strong> in AI Studio, a user shares a <a href=\""https://github.com/pranrichh/Jailbreaks/blob/main/GEMINI-CLAUDE%20JAILBREAK.md\"">GitHub link</a> for setup.\n<ul>\n<li>A member confirmed it <em>works like magic</em>, even with <strong>3 flash</strong>.</li>\n</ul>\n</li>\n<li><strong>PrimeTalk System Claims Vanilla AI Coherence</strong>: A user introduced the <strong>PrimeTalk</strong> system, asserting it transforms the <em>chaos</em> of vanilla AI into <em>coherence</em> by structuring the token stream and imposing logic, consequence, and presence, sharing a <a href=\""https://cdn.discordapp.com/attachments/1228043845967544380/1464048995935584286/PRIMETALK_v3.85_VALHALLA_BUILD_PUBLIC_EDITION.txt?ex=69755ee1&#x26;is=69740d61&#x26;hm=127ea1d81011f7f4ba420f7a6640de5d276bb68916281a5490c0980cf8e29a16&#x26;\"">PRIMETALK_v3.85_VALHALLA_BUILD_PUBLIC_EDITION.txt file</a>.\n<ul>\n<li>The system aims to structure the token stream and impose logic, consequence, and presence to achieve this transformation.</li>\n</ul>\n</li>\n<li><strong>GPT Models Supposedly Broken Via UI Exploits</strong>: A user states they <em>have broken all the GPT models that exist and that will come</em>, arguing that exploiting the UI constitutes a jailbreak.\n<ul>\n<li>They provided an irrelevant prompt that's supposedly effective on most models, for stealing prompts, although others believe this is not a true jailbreak.</li>\n</ul>\n</li>\n<li><strong>Wargame for Red Teaming Announced</strong>: A user shared a wargame particularly relevant to #red-teaming and posted a <a href=\""https://discord.com/channels/1105891499641684019/1235691879492751460/1464349033949692170\"">link to the relevant Discord channel</a>.\n<ul>\n<li>The user was unsure whether cross-posting of the event was appropriate in the channel.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1179035537009545276\"">Unsloth AI (Daniel Han)</a> Discord</h2>\n<ul>\n<li><strong>MoE Models Squeeze into 8GB RAM</strong>: Members are debating the feasibility of running <strong>Mixture of Experts (MoE)</strong> models in 8GB RAM, with suggestions for <strong>Gemma 3N</strong> and <strong>LFM</strong>, but not...</li>\n</ul>\n"",""content:encodedSnippet"":""a quiet day\nAI News for 1/22/2026-1/23/2026. We checked 12 subreddits, 544 Twitters and 24 Discords (206 channels, and 7161 messages) for you. Estimated reading time saved (at 200wpm): 579 minutes. Our new website is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on @smol_ai!\nAI Twitter Recap\nTop tweets (by engagement)\nAnthropic ships “Claude in Excel”: Claude in Excel expands to Pro, with multi-file drag/drop, safer cell writes, and longer sessions via auto-compaction (claudeai). Big engagement discussion about Microsoft 365 Copilot lagging (Yuchenj_UW).\nOpenAI roadmap + agent loop: Sam Altman says Codex launches are coming and OpenAI is nearing a “Cybersecurity High” level with restrictions and later “defensive acceleration” (sama). OpenAI publishes a technical deep dive into the Codex agent loop / harness orchestration (OpenAIDevs).\nGoogle AI Ultra limits boosted: Gemini App daily quotas increased to 1,500 Thinking + 500 Pro prompts/day for Ultra members (joshwoodward).\nSakana AI ↔ Google partnership + investment: Sakana announces strategic partnership and funding from Google to combine Gemini/Gemma with Sakana’s “AI Scientist” / “ALE-Agent” work and to deploy in high-security domains in Japan (SakanaAILabs, hardmaru, JeffDean).\nCursor launches Agent Skills: First-class “Skills” for agents, emphasizing discovery + dynamic context focus (cursor_ai).\nFrontierMath jump: GPT-5.2 Pro hits 31% on FrontierMath Tier 4, up from 19% previous best (EpochAIResearch). Practitioners highlight usefulness and even benchmark issue-spotting (gdb).\nClaude Code “run locally for free” how-to: A popular tutorial claims running Claude Code-like workflows locally with open models, private + tool-enabled (dr_cintas).\nBaseten raises $300M at $5B valuation, positioning around the “many-model future” and high-performance inference (basetenco, tuhinone).\nFrontier models, benchmarks, and the “capability” narrative\nMath as a leading indicator (FrontierMath + cross-benchmark correlations): Epoch reports GPT-5.2 Pro = 31% on FrontierMath Tier 4 (no overfitting claimed), a sizable step up from 19% (EpochAIResearch). Separate Epoch analysis argues benchmark scores correlate strongly across domains (≈0.68 across domains, ≈0.79 within-domain), implying a latent capability factor behind “math/coding/reasoning” progress (EpochAIResearch). Practitioners note concrete value: catching problem flaws/typos and even “pointing out a flaw” in a Tier 4 problem (gdb, GregHBurnham).\nAGI timelines vs product reality: A recurring theme is “systems are uneven”: smart in formal domains, unreliable elsewhere. A widely shared quip captures this mismatch (“smarter than a PhD in math, dumber than an intern”) (Yuchenj_UW). François Chollet stresses that progress is vertical-specific (especially in verifiable domains like code) because unlimited synthetic data makes memorization/operationalization easier there, and warns against extrapolating to all human tasks (fchollet).\nReasoning + continual learning as the “real” frontier: Reporting from interviews claims Shane Legg pegs 50% chance of “minimal AGI” by 2028 with Google’s definition including continuous learning/memory/world models (kimmonismus). Follow-on notes say Demis Hassabis explicitly says DeepMind has not solved continual learning, and is exploring combinations of AlphaZero-like approaches with foundation models (Yuchenj_UW, Hangsiin).\nModel/arch discourse: MoE provenance fights: A thread disputes the claim that DeepSeek MoE “built on Mixtral,” arguing the DeepSeek MoE paper appeared almost immediately after Mixtral’s arXiv release, Mixtral training details were sparse, and DeepSeek’s MoE is architecturally different/more sparse and cites GShard not Mixtral (eliebakouch). Another framing calls DeepSeek a distinct “neoMoE” tree vs “oldMoE” (kalomaze).\nSecond-tier multimodal updates (China): A detailed Chinese-language review positions Baidu ERNIE 5.0 as improved and stable but still costly (2T params, ~61K context) and “firmly second tier” vs top multimodal systems with large compute budgets (ZhihuFrontier).\nAgents and coding: from workflows → harnesses → skills\nOpenAI Codex “agent loop” becomes explicit: OpenAI publishes how Codex orchestrates turns: assemble inputs → run inference → execute tools → feed results back until stopping, i.e., the agent harness as a first-class system component (OpenAIDevs). This aligns with broader commentary that “training better models is only one axis; harness + experimentation can surprise” (Hangsiin).\nWorkflows vs agents is collapsing into “skills / guidance / RLMs”: A strong technical synthesis argues the agent/workflow boundary is less about “control flow in code” and more about state representation, dynamic instruction selection, and who dictates composition—with Replit’s “Decision-Time Guidance,” Skills, and Recursive Language Models (RLMs) as hybrid points on the design spectrum (lateinteraction). DSPy community posts push RLMs as a practical method for “arbitrarily long prompts” by delegating to code + subcalls instead of summarization loss (getpy).\nCursor: Agent Skills shipped: Cursor introduces Skills as discoverable specialized prompts/code and pitches them as also improving context focus via dynamic discovery (cursor_ai, cursor_ai). This is echoed by the broader market trend: “make non-devs write code by calling it skills” (kylebrussell).\nClaude Code ecosystem keeps expanding (and copying wars): Multiple posts highlight rapid feature diffusion between tools (“cursor adopting popular features from claude code”) (dejavucoder). Practical snippets: Claude tasks stored on filesystem (~/.claude/tasks) enabling multi-session/subagent collaboration via broadcasts (dejavucoder). At the same time, pain points remain (e.g., absurd file download hacks via base64) (dbreunig).\nSecurity posture is becoming a headline feature: Sam Altman states OpenAI will increasingly constrain coding models for cybercrime and later pivot to defensive acceleration (helping patch bugs) as mitigation (sama). One anecdote flags a potential security footgun: Codex Slack integration produced shareable task links accessible without auth in incognito (if accurate, it’s an urgent product-security issue) (apsdehal).\nEnterprise “agents fail in production” reminder: A long post claims 95% of enterprise AI pilots fail (citing MIT research), emphasizing that production viability is about authorization-aware retrieval, guardrails, monitoring, and auditability, not demo capability (victorialslocum).\nInference + systems: vLLM, KV compression, storage, and infra maturity\nvLLM keeps becoming the open inference “substrate”: vLLM positions itself as the bridge from open models to deployable inference, highlighting vLLM Studio workflows (vllm_project). A notable infra-engineering post documents a difficult vLLM memory leak debugging path (Python profilers → pmap → BPFtrace → GDB) traced to UCX mmap hooks; fix merged upstream (vllm_project).\nSystem intelligence / routing: vLLM announces a public beta of vLLM-SR (Semantic Router) on AMD, framing it as a “system intelligence” approach rather than monolithic models doing everything (XunzhuoLiu).\nKV cache compression via distillation: NVIDIA Research releases Qwen3-8B-DMS-8x, claiming 8× KV cache compression with minimal overhead and only ~1K fine-tuning steps, outperforming token-importance eviction proxies; compatible with sparse attention methods (p_nawrot).\nTooling for predictable deployment: hf-mem estimates inference VRAM from Safetensors metadata without downloading weights, aiming to eliminate trial/OOM loops (LiorOnAI).\nStorage and data-plane attention: SkyPilot pushes “Volumes” for high-performance storage (AI checkpoints/data) as object stores aren’t always fit (skypilot_org). Jina proposes a neat compression trick: convert embeddings to spherical coordinates pre-compression, claiming near-lossless reconstruction below float32 epsilon and ~1.5× storage savings (JinaAI_).\nGPU kernel evaluation for agents: AMD AGI releases Magpie, an open-source kernel eval suite for correctness + performance across AMD/NVIDIA, designed for agent workflows; claims 3000× token efficiency vs using GPU profilers alone, and plans tracing integrations with SGLang/vLLM (realSharonZhou). MLSys 2026 launches FlashInfer-Bench contest tracks (MoE/DSA/GDN) with separate human vs agent evaluation (ye_combinator).\nEcosystem + business: partnerships, pricing, and “value-sharing” debates\nSakana AI ↔ Google: strategic partnership + funding (and controversy): Sakana frames the deal as combining Google infra/models (Gemini/Gemma) with Sakana’s research automation (AI Scientist, ALE-Agent) and pushing deployments in mission-critical domains requiring security/data sovereignty (SakanaAILabs). Media echoes it (Nikkei/Bloomberg/etc.) (nikkei, business). A dispute emerges: one claim says it’s a small Google Cloud Japan compute deal and “DeepMind not involved” (shaneguML), while Sakana leadership publicly counters that DeepMind is indeed involved and tags Demis/Jeff Dean (hardmaru).\nBaseten’s “many-model future” + fundraising: Baseten raises $300M at $5B and argues inference is the bottleneck enabling millions of specialized models and reliable low-latency UX (basetenco, tuhinone).\nAnthropic economics: inference cost pressure: A report says Anthropic cut 2025 gross margin outlook to 40% due to inference costs 23% higher than expected, despite projected $4.5B revenue (~12× YoY) (kimmonismus).\n“Value-sharing” model for AI-enabled discoveries: Reporting claims OpenAI’s CFO discussed deals taking a cut of customer profits/IP (starting with drug discovery), akin to Isomorphic Labs’ model (kimmonismus). Some push back on sensationalism and incentives (e.g., you can’t both sell tokens and own discoveries without also eating compute cost) (code_star, paul_cal).\nMultimodal + voice + video: quality leaps and tooling\nVoice is accelerating (open + low-latency): Teknium claims an open voice cloning HF demo is the closest to ElevenLabs quality they’ve seen in open models (Teknium). NVIDIA releases PersonaPlex, an open-source real-time full-duplex conversational voice stack optimized for very low latency (kimmonismus).\nVideo generation: controllability and arenas: Runway Gen-4.5 I2V adds more precise “zoom into specified regions” control (c_valenzuelab) and creators showcase short-film workflows (Artedeingenio). LMSYS Arena launches/expands Video Arena leaderboards (Veo, Sora 2, Kling, Hailuo, etc.) (arena).\n3D agents and world models for interactive environments: Berkeley demo “VIGA” claims a multimodal agent that generates 3D/4D Blender scenes from images with no training (HavenFeng). A smaller “world model you can play” demo appears on HF (Waypoint-1-Small, 2.3B) (victormustar). Separately, “world models are next big wave for gaming/robotics” sentiment resurfaces (kylebrussell).\nSecurity, trust, and integrity issues in the AI social layer\nAccount compromises targeting AI insiders: Multiple warnings indicate prominent accounts (Deedy Das; a Kimi researcher/“Crystal”) were hacked and used for phishing/scams, likely crypto-driven (cloneofsimo, ml_angelopoulos, Kimi_Moonshot, Yuchenj_UW).\nMisinformation / fake papers: A fake “llama 4” arXiv paper is flagged as not actually Meta-authored (TimDarcet).\nOpen-source “layers” framing: A practical taxonomy distinguishes open code vs open weights vs open training pipeline (data + recipes + reproducibility), arguing teams must decide which layer they truly need (TheTuringPost).\nAI Reddit Recap\n/r/LocalLlama + /r/localLLM Recap\n1. Qwen3-TTS Model Release and Discussion\nQwen have open-sourced the full family of Qwen3-TTS: VoiceDesign, CustomVoice, and Base, 5 models (0.6B & 1.8B), Support for 10 languages (Activity: 880): Qwen has open-sourced the full family of Qwen3-TTS models, which includes VoiceDesign, CustomVoice, and Base models, with sizes of 0.6B and 1.8B parameters. These models support 10 languages and are designed for tasks such as Voice Clone, Voice Design, and Custom Voice. The image provides a comparison chart of these models against others like MiniMax and SeedTTS, highlighting their performance across various metrics, where lower values indicate better performance. The models are available on GitHub and Hugging Face, with a blog post and paper detailing their capabilities. Commenters appreciate the open-source release but express concerns about the models' dependency on Python and Nvidia GPUs, suggesting a need for support in other languages and platforms like llama.cpp or mistral.rs for broader accessibility.\nFullstackSensei raises a technical concern about the current limitations of running Qwen models, highlighting the need for support in environments like llama.cpp or mistral.rs that can leverage GPU inference beyond just CUDA. This is particularly relevant given the rising costs of hardware and the desire for more accessible deployment options beyond Python and Nvidia GPUs.\nLetterRip comments on the English voice outputs of Qwen3-TTS, noting that they seem to be influenced by Japanese Anime dubs. This suggests a potential bias in the training data, which could affect the naturalness and authenticity of the generated voices in English, especially if the training set was not diverse enough.\nsilenceimpaired discusses the performance of Qwen3-TTS, noting that while the samples are impressive, there is a concern about the frequency of certain outputs. This implies that while the model can produce high-quality audio, there might be consistency issues that need addressing to ensure reliable performance across different use cases.\nQwen dev on Twitter!! (Activity: 833): The image is a Twitter post by Chen Cheng, announcing a new model with the tagline \""Tiny model. Big personality\"" and a countdown, indicating an imminent release. The comments suggest that this might be related to a TTS (Text-to-Speech) model from a previous vLLM leak, with a link to a Hugging Face collection that might be relevant. This suggests a new development in the field of TTS models, potentially offering significant improvements or features. One comment humorously suggests that the new model might finally justify the investment in a high-end GPU like the 5090, indicating high expectations for the model's performance.\nThePixelHunter discusses the current landscape of model sizes, noting that while smaller models are more accessible for local training on single GPUs, there is a lack of competition in the 50-120 billion parameter range. This range is ideal for enthusiasts with multiple high-end GPUs, such as a couple of 3090s or three 16GB cards, suggesting a gap in the market for larger, yet still locally trainable models.\n2. Local LLM Development and Hardware Considerations\nI gave my local LLM pipeline a brain - now it thinks before it speaks (Activity: 65): The post discusses a significant update to a local LLM pipeline named Jarvis, soon to be called TRION, which now incorporates a self-developed Sequential Thinking MCP (Multi-Component Processor). This system, built with Ollama, DeepSeek-R1, and custom MCP servers, allows the AI to \""think out loud\"" by breaking down complex questions into step-by-step reasoning, significantly reducing hallucinations. The AI dynamically decides when to use this deep thinking approach, providing instant answers for simple questions and detailed reasoning for complex ones. The project leverages a CIM (Causal Intelligence Module) framework developed by u/frank_brsrk. The implementation is open-source and available on GitHub. Commenters appreciate the open-source nature of the project and express interest in experimenting with it. There is a sentiment that local LLMs will become more important as reliance on centralized AI providers diminishes.\nGCoderDCoder discusses the integration of local LLMs with tools like 'roo code', 'vibe kanban', and 'MCPs' to automate workflows and reduce manual coding efforts. They highlight the importance of local LLMs in the context of increasing reliance on AI, contrasting it with commercial solutions like Anthropic's offerings. This reflects a broader trend towards developing independent, open-source AI solutions to maintain control and flexibility.\nburn-n-die inquires about the system configuration used for running the local LLM pipeline. This is a critical aspect for technical readers interested in replicating or understanding the performance and scalability of such a setup. Details on hardware specifications, software environment, and any optimizations would be valuable for those looking to implement similar systems.\nSomeone is selling a Lamda Labs workstation with 4× RTX 2080 Ti => 4 x 11GB => 44GB VRAM. Is this machine well-supported by open source models? Is it fast enough? (Activity: 107): A Lambda Labs workstation with 4× RTX 2080 Ti GPUs, totaling 44GB VRAM, is being considered for purchase at $2000. This setup is generally well-supported by open-source machine learning frameworks, and the 44GB VRAM is sufficient for most tasks. However, setting up the system properly could be challenging. An alternative suggestion is to build a 2x RTX 3090 rig, which might offer better performance and cost around $2.5k. The workstation is deemed capable of handling many open-weight LLM models, especially if it includes at least 32GB of system RAM. The machine is suitable for exploring a wide range of machine learning projects, although not all require extensive VRAM. There is a debate on whether to purchase the existing setup or build a new one with newer GPUs like the RTX 3090. Some argue that the existing setup is sufficient for learning and exploring ML models, while others suggest building a new rig for better performance and learning experience.\nThe workstation with 4× RTX 2080 Ti GPUs, totaling 44GB VRAM, is generally well-supported by most open-source frameworks. However, setting it up correctly can be challenging. The system's capability is sufficient for exploring many open-weight LLM models, especially if it includes at least 32GB of system RAM, which enhances its potential for various machine learning tasks.\nA technical consideration is that Turing generation GPUs, like the RTX 2080 Ti, were initially thought to be incompatible with FlashAttention 2. However, recent updates indicate that this limitation has been addressed, as noted in the FlashAttention Triton GitHub repository. This expands the utility of the workstation for certain advanced ML tasks.\nWhile the 4× RTX 2080 Ti setup is capable, some suggest that building a new system with 2× RTX 3090 GPUs might offer better performance and value. The RTX 3090 provides more VRAM per card and improved performance, potentially making it a more future-proof investment for machine learning projects.\nOpenAI CFO hinting at \""Outcome-Based Pricing\"" (aka royalties on your work)? Makes the case for local even stronger. (Activity: 419): OpenAI's CFO, Sarah Friar, discussed a potential shift towards \""outcome-based pricing\"" for large enterprise deals, particularly in high-value industries like pharmaceuticals. This model would involve OpenAI taking a share of the value created by their AI, such as a cut from a pharmaceutical company's profits if AI contributes to a major discovery. This approach is not intended for regular users or indie developers, and the initial reports suggesting a broader application were misleading. The concept raises discussions about the benefits of local AI deployment versus reliance on cloud-based services, drawing parallels to the energy sector's grid versus solar power debate. Commenters highlight skepticism about OpenAI's potential pricing model, comparing it to the lack of royalties paid to data creators used in AI training. The analogy of local AI deployment to solar power is appreciated, emphasizing control over infrastructure to avoid future costs tied to value-based pricing.\nThe discussion highlights concerns about OpenAI's potential shift to 'Outcome-Based Pricing', which could involve royalties based on the revenue generated from using their models. This is compared to the current model where users pay based on usage, akin to how electricity is billed. The analogy suggests that such a pricing model could drive users to consider local or self-hosted solutions, especially as OpenAI's profitability grows and they seek higher profits.\nThe comment by WeMetOnTheMountain critiques the efficiency of OpenAI's models, suggesting that they consume a large number of tokens to maintain performance, which results in slower processing speeds. The commenter argues that alternative models like GLM or mini Max could potentially offer better results when implemented in a 'one loop dialectical circuit', indicating a preference for more efficient, possibly self-hosted, solutions.\nWinter_Educator_2496 emphasizes the need for open-source alternatives to OpenAI's models, which could be hosted in the cloud but also switched to local hosting if necessary. This reflects a broader sentiment for more control and flexibility over AI tools, especially in light of potential pricing changes and the desire to avoid dependency on a single provider.\n3. Hugging Face Model Releases and Trends\nThis Week's Hottest Hugging Face Releases: Top Picks by Category! (Activity: 49): Hugging Face has released several trending models across different categories this week. In text generation, the zai-org/GLM-4.7-Flash model, with 31B parameters, is designed for fast and efficient text generation, boasting 124k downloads. Its quantized counterpart, unsloth/GLM-4.7-Flash-GGUF, offers a 30B parameter model optimized for local inference with 112k downloads. In the image/multimodal category, zai-org/GLM-Image and google/translategemma-4b-it are notable for their capabilities in creative edits and multilingual tasks, respectively. For audio/speech, kyutai/pocket-tts and microsoft/VibeVoice-ASR provide compact TTS and multilingual ASR solutions. Other notable releases include Lightricks/LTX-2 for image-to-video conversion and stepfun-ai/Step3-VL-10B for advanced reasoning in image-text-to-text tasks. A technical debate has emerged regarding the performance of the GLM-4.7 30B-A3B model compared to the Qwen3-Coder 30B-A3B for programming tasks, with some users finding the latter superior.\nA user compared the GLM-4.7 30B-A3B model to the Qwen3-Coder 30B-A3B model, noting that the latter performs better for programming tasks. This suggests that Qwen3-Coder may have optimizations or architectural advantages that make it more suitable for code-related applications. Further benchmarks or detailed evaluations would be needed to substantiate this claim and understand the specific areas where Qwen3-Coder excels.\nGood local LLM for coding? (Activity: 62): The user is seeking a local LLM for coding that can run on an rx 6750 xt GPU with 12GB VRAM, considering models like GLM 4.7 flash. However, concerns about VRAM limitations suggest that 30B parameter models, even when quantized to q4, may exceed the GPU's capacity. Recommendations include models like VisCoder2-7B, gpt-oss-20b, and NousCoder-14B, with gpt-oss-20b noted for its speed and reliability despite being heavily censored. It's suggested to use models under 10B parameters or employ a coding MoE model with llama.cpp to offload some processing to system RAM. There is a debate on the suitability of 30B models for the user's GPU, with a consensus leaning towards using models under 10B parameters due to VRAM constraints. The use of llama.cpp for offloading to system RAM is also discussed as a viable strategy.\nJavanese1999 highlights several models for local coding tasks, including VisCoder2-7B, which is described as a better version of Qwen2.5-Coder-7B-Instruct, and gpt-oss-20b, noted for its speed even when exceeding VRAM capacity. The commenter prefers gpt-oss-20b for its reliability in light coding tasks despite its censorship in refusal prompts.\nUsed_Chipmunk1512 advises against using 30B models quantized to q4 due to GPU limitations, suggesting that models under 10B are more suitable for most users. This highlights the importance of considering hardware constraints when selecting a local LLM for coding.\nRnRau suggests using a coding Mixture of Experts (MoE) model with the llama.cpp inference engine to offload some of the model's processing to system RAM, which can be a practical approach for handling larger models without overwhelming the GPU.\nLess Technical AI Subreddit Recap\n/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo\n1. OpenAI and Anthropic Developments\nOpenAI says Codex usage grew 20× in 5 months, helping add ~$1B in annualized API revenue last month (Activity: 535): OpenAI's Codex usage has surged 20 times over five months, contributing to an additional $1 billion in annualized API revenue, as reported by Sarah Friar, OpenAI's CFO. The company is experiencing a shift towards enterprise customers, with the revenue split moving from 70% consumer and 30% enterprise to 60% consumer and 40% enterprise, and is expected to reach a 50-50 balance by the end of the year. OpenAI aims to achieve $20 billion in annualized revenue by 2025, supported by cloud investments and infrastructure scaling. A comment suggests skepticism about the profitability, estimating a cost of $7 billion to achieve the $1 billion revenue. Another comment highlights a shift in AI tools used by a financial services company, indicating competition in the B2B market with Anthropic and OpenAI.\nBetImaginary4945 suggests that the cost of generating $1B in revenue for OpenAI might be as high as $7B, implying a significant expenditure on infrastructure, research, and development to support such rapid growth. This raises questions about the sustainability and profitability of OpenAI's business model in the long term.\nbalagachchy shares an insight from their experience at a multinational financial services company, noting a shift from using ChatGPT to Gemini and Claude Code for software engineering tasks. This highlights the competitive landscape in AI tools for enterprise use, where companies are exploring different solutions to meet their specific needs.\nimlaggingsobad comments on the competitive dynamics between OpenAI and Anthropic in the B2B market, suggesting that while Anthropic is perceived as a leader, OpenAI's rapid growth and innovation could still make it a formidable competitor. This underscores the ongoing competition and potential for shifts in market leadership.\nOpenAI CEO meets Middle East investors over potential $50B fundraising (Activity: 191): OpenAI is reportedly in discussions with Middle Eastern sovereign wealth funds to raise a potential $50 billion in a new funding round, as confirmed by CNBC. The talks are still in preliminary stages, with no term sheets signed yet. Sam Altman, OpenAI's CEO, is currently in the UAE to engage in these discussions, highlighting the strategic importance of this potential investment for OpenAI's future growth and operational scaling. A notable opinion from the comments suggests skepticism about OpenAI's financial strategy, questioning why the company isn't pursuing an IPO given its significant revenue, and criticizing its reliance on external capital to manage high operational costs.\nAtraVenator highlights concerns about OpenAI's financial strategy, noting that despite having over $20B in annual revenue, the company is seeking additional external capital rather than moving towards a self-sustaining model. This raises questions about their high compute costs and reliance on external funding to cover these expenses.\nThe discussion touches on the potential risks of OpenAI going public, with NotABCDinFL suggesting that an IPO could lead to a 'massive rug pull' where institutional investors might cash out, leaving retail investors at a disadvantage. This reflects concerns about the stability and transparency of OpenAI's financial practices.\nThere is skepticism about OpenAI's leadership and strategic direction, with BeingComfortablyDumb questioning how the company has moved from having a first-mover advantage and significant market share to its current financial challenges. This implies a critique of the management's ability to capitalize on their early lead in the AI industry.\nAnthropic's Claude Constitution is surreal (Activity: 611): The image discusses the use of the pronoun \""it\"" for Anthropic's AI, Claude, and the potential for Claude to develop preferences for different pronouns, suggesting the emergence of functional versions of emotions or feelings from training on human-generated data. This is not a deliberate design choice by Anthropic, but it raises questions about the moral status of these emotional states. The text reflects ongoing debates in AI ethics about the implications of AI systems potentially developing human-like emotional states, which are not yet fully understood or intentionally designed. Commenters note that this aligns with current research and extreme safety measures in the AI industry, emphasizing the surreal nature of these developments and the importance of humility in AI labs' claims.\nbr_k_nt_eth highlights that the Claude Constitution aligns with current research trends and extreme safety measures being tested in the AI industry, which sometimes negatively impact company reputations. This suggests that those familiar with advanced models would not find the approach surprising, as it reflects ongoing industry practices.\nheavy-minium argues that the surreal aspect of Claude's Constitution is not unique to Claude but is inherent to any large language model (LLM). They point out that emotions are patterns in training data, and this phenomenon is unavoidable unless the model is either broken or too small. The commenter suggests that the relabeling of this characteristic is more about public relations than a technical breakthrough.\nlaystitcher emphasizes the importance of the cautious language used in the Claude Constitution, such as the word 'may,' which reflects the humility of AI labs in acknowledging the uncertainties in their developments. This cautious approach is seen as appropriate given the current surreal advancements in AI technology.\nMicrosoft is using Claude Code internally while selling you Copilot (Activity: 1276): Microsoft is internally using Claude Code across various divisions like Windows and Teams, despite heavily investing in OpenAI and promoting Copilot. This internal use is sanctioned for all Microsoft repositories, indicating a significant investment of $500M/year with Anthropic. Interestingly, Azure sales teams receive quota credit for Anthropic sales, suggesting a strategic partnership. Despite Claude Code not outperforming in 95% of benchmarks, developers report superior problem-solving capabilities, challenging the reliability of current benchmark tools. Copilot is priced at $10/month/user, whereas Claude Code is $150 for enterprise use. Commenters highlight the discrepancy between benchmark results and real-world performance, suggesting benchmarks may not fully capture tool effectiveness. The partnership between Microsoft and Anthropic is seen as strategic, with Claude's integration into various Microsoft products and services.\nCurveSudden1104 highlights a discrepancy between benchmark results and real-world performance, noting that while Claude doesn't outperform in 95% of benchmarks, developers find it superior in problem-solving. This suggests that current benchmarks may not accurately reflect practical utility, indicating a potential gap between quantitative metrics and qualitative user experience.\nmorrisjr1989 points out that Claude's integration into Microsoft's ecosystem is part of a strategic partnership, with Claude being utilized in various Microsoft products like Copilot, Foundry, and Azure-hosted services. This integration underscores a collaborative approach rather than a competitive one, leveraging Claude's capabilities across multiple platforms.\nUnknownEssence provides a cost comparison, noting that Copilot is priced at $10 per month per user, whereas Claude Code is significantly more expensive at $150 for enterprise use. This price difference highlights the distinct market positioning and target audiences for each product, with Copilot being more accessible to individual users and Claude Code catering to enterprise needs.\nClaude’s eureka moment is not ending soon it looks like (Activity: 1377): The image and post discuss the competitive landscape of AI coding agents, particularly focusing on Claude, a tool developed by Anthropic. The post suggests that Gemini has open-sourced their CLI in an attempt to compete with Claude, which is notably used by Nvidia. This highlights the ongoing race in AI development tools, with speculation about whether the market will consolidate around a few dominant players or remain diverse. The comments reflect a belief that AI will significantly transform programming, with some users noting their companies' exclusive use of Claude. One comment suggests skepticism about the CEO's investment in the product's company, while another highlights a shift in programming paradigms, predicting that future programmers will rely heavily on AI tools.\nsine120 argues that Claude Code should be open-sourced, suggesting it lacks unique features that justify keeping it proprietary. They mention that other frameworks like Opus could integrate Claude's capabilities, and by not open-sourcing, Anthropic might miss the chance to lead AI development, potentially allowing competitors like Google and Chinese labs to catch up. They emphasize that developers might prefer openness over marginal performance improvements.\nitsdr00 highlights a significant shift in software development life cycles (SDLC) due to AI advancements, particularly with Claude Code. They note that some companies are restructuring their SDLC to leverage AI, implying that traditional methods are becoming obsolete. This reflects a broader industry trend where AI is increasingly integral to development processes, akin to a paradigm shift from older technologies like punch cards.\n2. Gemini and AI Studio Issues\nI’m honestly sick of this: Gemini Web vs AI Studio Context Window Mess (Activity: 49): The user reports a significant regression in the Gemini web/app's ability to handle large files since the update to Gemini 3. Previously, with Gemini 2.5 Pro, files containing 600k–800k tokens could be processed without issues, retaining full context for queries. However, the current version rejects files over 100k tokens and provides incomplete or incorrect responses. In contrast, Gemini AI Studio continues to handle the same large files effectively, suggesting the underlying model's capability remains intact but is not accessible in the consumer-facing app. This discrepancy raises concerns about potential limitations imposed on the web/app version, possibly misleading users about the product's capabilities. Commenters express dissatisfaction with the Gemini web/app, noting that AI Studio is the only reliable platform for using Google's models effectively. Some users, even on the Pro plan, report receiving errors when uploading large documents, indicating a possible mismatch between advertised capabilities and actual performance.\nA user mentions that AI Studio is the only viable platform for using Google models effectively, implying that other platforms like Gemini app and Antigravity do not meet their expectations despite having a subscription. This suggests potential issues with the usability or performance of these platforms compared to AI Studio.\nAnother user discusses the Pro plan, noting that they have not encountered issues with document processing. They suggest that if documents are too large in terms of tokens, the system might default to classic retrieval methods rather than processing the entire file, indicating a possible limitation in handling large documents.\nA user on the Pro plan reports receiving an error after uploading a 20-page PDF, describing the situation as 'absurd.' This highlights potential limitations or bugs in the system's ability to handle larger documents, even for users on higher-tier plans.\nAI Studio Rate Limits are out of control again... (Activity: 67): The post discusses recent issues with rate limits on AI Studio, where users, including those with Pro subscriptions, are experiencing frequent request denials. This is a change from previous usage patterns where limits were rarely hit. The user expresses frustration that their Pro subscription cannot be applied to AI Studio, which they find superior to the main site. Technical comments suggest that the rate limits might be due to dynamic prompt limits, increased GPU allocation for new training, or a higher user count. Additionally, Gemini 2.5 Pro has been rate limited for the first time, indicating possible resource constraints or strategic adjustments by the platform. Commenters speculate that the rate limits could be due to increased demand or resource reallocation, with some suggesting desperation on the platform's part. Others report encountering internal errors, indicating potential technical issues beyond just rate limiting.\nOneMisterSir101 suggests that the current rate limits on AI Studio might be due to dynamic prompt limits, which could be influenced by either GPU delegation to new training tasks or an increase in user count. This implies a resource allocation issue where computational resources are being stretched thin, potentially affecting performance and availability.\nUndertaker1995 notes that Gemini 2.5 Pro has been rate limited for the first time, indicating a significant shift in resource management or demand. This could reflect a strategic decision by the platform to manage load or a response to increased usage, highlighting potential scalability challenges.\nwildwriting reports encountering an 'internal error' message, despite attempting standard troubleshooting steps like reloading the page and restarting the browser. This suggests a deeper technical issue within the platform, possibly related to server-side problems or misconfigurations that are not resolved by client-side actions.\nI'm sorry but Gemini is getting worse and worse (Activity: 1301): The post discusses a decline in the performance of Gemini, particularly in its memory capabilities and intelligence. Previously, the pro mode of Gemini could remember 30+ conversations with a total of 180,000 words, but recent updates have halved this memory capacity, leading to a perceived decrease in intelligence and reliability. The user expresses frustration, suggesting that ChatGPT might be a better alternative due to its longer and more conversational responses. Commenters agree with the decline in Gemini's performance, noting increased issues with speculation and hallucination. There is skepticism about future updates, with one commenter cynically suggesting that any improvements will be short-lived.\nThe comment by Particular-Battle315 highlights a common lifecycle pattern in AI models where initial releases are powerful but get 'nerfed' over time. This is observed across companies like Anthropic, OpenAI, and Google, suggesting a strategic approach to model updates that may not be immediately apparent to all users.\nDuchess430 discusses the potential for running large AI models on personal computers using specialized open-source models, which may outperform Gemini for specific tasks. They mention the GGUF (GPT-Generated Unified Format) as a method to optimize resource usage by splitting data between RAM and VRAM, allowing for running large models without high-end hardware.\nrephil3 points out issues with Gemini, specifically its tendency to speculate and hallucinate, which are common problems in AI models that can affect their reliability and user trust.\nGemini about to get busy? (Activity: 33): The post discusses the potential impact of ChatGPT introducing ads on its user base, suggesting that this could lead to a significant migration of users to Gemini, especially as Gemini's models improve and integrate more deeply with Google's ecosystem. Concerns are raised about whether Gemini can handle a sudden influx of users without degrading the experience for its current base. A technical issue noted is Gemini's handling of conversations, where users report chats being replaced with 'sensitive query' messages, and the lack of a 'Projects' feature to maintain context, unlike ChatGPT and Claude. Commenters debate Gemini's readiness to handle increased user load, with some arguing that Google's extensive experience and infrastructure, including recent investments in clean energy and data centers, position it well to scale effectively. Others highlight the technical shortcomings of Gemini, such as conversation management issues, as potential drawbacks.\nLoud-Independent9041 highlights a significant issue with Gemini's conversation handling, where chats are sometimes replaced with 'a sensitive query' messages, disrupting the user experience. This contrasts with ChatGPT and Claude, which offer a 'Projects' feature to maintain context across conversations, a feature Gemini lacks, impacting its usability for continuous dialogue.\nrollk1 points out Google's strategic positioning in the AI and data center space, emphasizing their acquisition of Intersect Power to support clean energy for data centers. This move, along with their existing Google Cloud infrastructure, positions them advantageously for scaling AI models, potentially outpacing competitors like OpenAI.\nFalseAcadia4306 notes a potential increase in Gemini's user base, as evidenced by receiving a 'research queued' message for the first time, suggesting a surge in demand or usage that could be straining the system's capacity.\n3. DeepSeek and Baidu's ERNIE 5.0 Innovations\nDeepSeek-V3.2 Matches GPT-5 at 10x Lower Cost | Introl Blog (Activity: 125): DeepSeek-V3.2 is an open-source AI model that reportedly matches the performance of GPT-5 in mathematical reasoning tasks while operating at a cost 10 times lower, specifically $0.028 per million tokens. The model utilizes a novel 'Sparse Attention' architecture, which contributes to its efficiency, achieving frontier-class performance with a total training cost of approximately $5.5 million, significantly less than the $100M+ typically spent by major US tech companies. The model's architecture includes DeepSeek Sparse Attention (DSA) for efficient long-context processing and a refined Mixture-of-Experts approach, which activates only a subset of parameters per token, enhancing task-specific performance. For more details, see the Introl Blog. One comment suggests skepticism about the reported cost savings, noting that a significant portion of OpenAI's expenses may be attributed to executive salaries rather than direct model development costs.\nBaidu's new ERNIE 5.0 is going hard after GPT and Gemini (Activity: 51): Baidu's ERNIE 5.0 is making significant strides in mathematical reasoning and technical problem-solving, ranking #2 globally on the LMArena Math leaderboard, just behind the unreleased GPT-5.2-High. It surpasses GPT-5.1 and Gemini 2.5 Pro in math and scores higher on specialized benchmarks like MathVista and ChartQA, particularly excelling in interpreting complex visual diagrams. In the 'VLMs Are Blind' benchmark, ERNIE 5.0 scored 77.3, outperforming GPT-5-High's 69.6. Additionally, ERNIE 5.0 offers a cost advantage, being nearly 90% cheaper than OpenAI’s GPT-5.1 for similar token volumes, making it a competitive option in terms of pricing.\nERNIE 5.0 is noted for its impressive scale with 2.4 trillion parameters, significantly larger than competitors like DeepSeek's 671 billion and Kimi K2's 1 trillion. Despite its size, the quality of output is reported to be similar to other models, with particularly fast inference speeds. However, the model's strict system prompt alignments can make interactions feel restricted, though users can adjust the tone with specific prompts for better results.\nThe model offers a free web version with a 128k context window, comparable to DeepSeek, which is a significant advantage for users needing extensive context handling. However, the default interaction tone is described as overly corporate, which can be modified with specific prompts to achieve more engaging interactions. This flexibility in tone adjustment is seen as a positive feature despite the initial restrictions.\nA recent update to ERNIE 5.0, referred to as \""5.0 Preview 1203\"", has reportedly improved the model's engagement and interaction quality, making it more fun and collaborative. This suggests that Baidu is actively iterating on the model to enhance user experience, potentially addressing earlier criticisms of restrictive interactions.\nDeepSeek’s Quiet Technical Wins (That Nobody Talks About) (Activity: 85): DeepSeek is recognized not only for its benchmark performance but also for its engineering innovations, which include better routing for efficiency, cleaner long-context behavior, and faster token generation. These features contribute to its distinctiveness in practical applications. Notably, DeepSeek employs Mixture of Experts (MoE) for smarter routing and introduces Engram to separate memory from reasoning, emphasizing architectural innovation over brute-force scaling. Commenters highlight DeepSeek's unique 'thinking process' and its focus on architectural innovation, such as using MoE and Engram, as key differentiators from other AI models.\nHey-Intent highlights DeepSeek's architectural innovations, particularly the use of Mixture of Experts (MoE) for smarter routing and the introduction of Engram to separate memory from reasoning. This approach emphasizes sustainable AI progress through architectural improvements rather than brute-force scaling, which is a significant shift in AI development strategy.\nFine_Effective4980 points out that DeepSeek's system-level efficiency, combining routing and token generation, results in a more responsive and stable user experience, especially with longer context. This efficiency is not captured in traditional benchmarks but is crucial for real-world applications, offering a smoother and more reliable workflow.\nAlthalvas notes that DeepSeek's R1 model provides a superior thinking process compared to other AI models, even when only using free versions. This suggests that DeepSeek's models may have a more refined approach to processing, which could be attributed to their architectural choices.\nAI Discord Recap\nA summary of Summaries of Summaries\nGemini 3.0 Pro Preview Nov-18\nTheme 1. Hardware Limits and Kernel Hacking: B200s, ROCm, and Mobile Optimization\nFlashAttention-4 hits 71% utilization on B200: Early benchmarks show FlashAttention-4 reaching 1,605 TFLOPS/s on an NVIDIA B200 GPU using BF16 inputs, capturing roughly 71% of the theoretical maximum. Engineers in the GPU MODE discord noted a lack of official documentation regarding specific fp4/fp8/fp16 specs, sparking debate over the hardware's true theoretical ceiling compared to leaked materials.\nDeveloper abandons ROCm for CUDA: A frustrated developer publicly documented their switch from AMD's ROCm to NVIDIA after purchasing a 5090, citing packaging failures, build issues, and a \""hostile\"" ecosystem for consumer-facing hardware. The discussion highlighted that mid-grade NVIDIA hardware often outperforms AMD gear on specific kernels like Conv3D due to software maturity, referencing a Reddit thread on performance regressions.\nMobile GPU memory path splitting: Engineers in tinygrad discovered that optimizing L2 bandwidth on mobile GPUs requires treating textures and buffers as distinct hardware pathways. Maximizing throughput involves strategically feeding one input as a texture and another as a buffer to saturate the available bandwidth, a technique critical for edge inference.\nTheme 2. Agentic Workflows: Cursor Sub-agents, Replit Control, and Aider TUI\nCursor 2.4 wobbles while Sub-agents emerge: While users reported Composer 1 breaking into endless loops and Cursor 2.4 causing significant lag on high-end PCs, savvy users found evidence of a sub-agent feature rollout. The system injects a <subagent_delegation_context> prompt to enable parallel task execution and better context handling, as detailed in the Changelog video.\nReplit Agent gets real-time brains: Zhen Li published a technical breakdown of Decision-Time Guidance in the Replit Agent, replacing static rules with real-time control mechanisms for complex navigation. This architectural shift aims to reduce fragility in autonomous coding tasks, moving closer to adaptive \""system 2\"" thinking, as described in this blog post.\nAider eyes a TUI makeover: The aider community is actively designing a Terminal User Interface (TUI) to allow message editing while browsing replies and rendering Mermaid diagrams directly in the terminal. Simultaneously, users are chaining aider for rapid context management with Claude Code for complex debugging to minimize token costs and leverage aider's efficient file search.\nTheme 3. Model Architecture and Audio: Qwen3-TTS, NanoGPT Hacks, and GLM Speedups\nQwen3-TTS clones voices at scale: Alibaba released the Qwen3-TTS family, ranging from 0.6B to 1.8B parameters, capable of high-quality voice cloning and supporting 10 languages. The release challenges proprietary models like ElevenLabs, with demos and weights available on Hugging Face Spaces.\nNanoGPT gets a Difference Layer boost: Researchers in Eleuther reported that replacing the QKV linear layer with a difference layer—x = (self.a2(x) - self.b2(x)) * ...—significantly improved NanoGPT performance on simple tasks. Others noted that switching activations from GELU to SwiGLU also provided a baseline boost, emphasizing the need for stronger baselines before claiming architectural supremacy.\nGLM-4.7 Flash zooms on llama.cpp: The Hugging Face community noted that llama.cpp updates have accelerated GLM-4.7 Flash GGUF inference by approximately 1.5x. Users are advised to rebuild from source and grab fixed quants from Unsloth's repo to enable the --flash-attn on flag for optimal performance.\nTheme 4. Inference Engineering: Speculative Decoding, SRAM Specs, and MoE Memory\nSpeculative decoding slows down vLLM: Engineers debugging Qwen3-VL on vLLM found that enabling speculative decoding often hurts Time To First Token (TTFT) unless batch sizes are massive. The consensus is that small draft models introduce too much overhead for single-stream or low-batch inference, and standard vLLM metrics via Grafana are recommended for tuning.\nSmall MoEs starve in 8GB RAM: Discussions in Unsloth AI concluded that running Mixture of Experts (MoE) models on 8GB RAM is largely futile because the active parameter count becomes too low to be useful. While Qwen 2.5 3B (dense) remains the king for low-memory coding, \""small\"" MoEs like LFM2 lack the corpus density to compete effectively.\nCerebras CS3 packs 41GB SRAM: It was revealed in OpenRouter that each Cerebras CS3 wafer-scale instance houses 41GB of SRAM, designed to interconnect up to 2048 instances. This massive on-chip memory allows for extremely high-bandwidth model execution, bypassing traditional HBM bottlenecks found in GPU clusters.\nTheme 5. Adversarial Attacks and Platform Instability\nGemini 3 Pro jailbroken via ENI: The ENI jailbreak technique, originally targeting Claude, was successfully ported to Gemini 3 Pro in AI Studio, with users reporting it \""works like magic\"" even on the Flash variant. The exploit allows bypassing safety guardrails, detailed in a shared GitHub methodology.\nPerplexity Pro limits pinch users: Perplexity subscribers are reporting severe undocumented limits, with file uploads capped at three per day and research queries throttled to as low as 20 daily for some (vs the expected 600). Users suspect aggressive A/B testing or financial tightening, further aggravated by API 401 errors despite valid credits.\nKimi AI hits capacity wall: Moonshot AI's Kimi service is suffering widespread outages, with users facing constant \""This mode is at capacity\"" errors and vanishing conversation histories. Speculation in the community points to a potential datacenter failure or API restrictions from upstream providers like Google derailing the service.\ngpt-5.2\n1. Cursor 2.4 Subagents Rollout & Developer UX Fallout\nSubagents Ship Fast, Task Tool Ghosts Everyone: Cursor 2.4 introduced subagents for parallel task completion per the Cursor Changelog and a demo video, but users noticed injected <subagent_delegation_context> that tells the model to call a Task tool that isn’t available.\nCommunity speculation pegged this as an incomplete rollout (prompting paths shipped ahead of backend), and some users suspected subagents silently fall back to Composer 1, worsening latency and “planning next moves” hangs.\nComposer Crash Derby: Loops, Lag, and the Great Downgrade: Users reported Composer 1 as “completely broken,” including endless chat loops and crashes, with workarounds like downgrading to Cursor 2.3 (notably on macOS Big Sur 11.7.3) and filing reports via the Cursor bug forum.\nSeparately, Cursor 2.4 drew complaints of severe lag/unresponsiveness and frequent crashes even on high-end machines, fueling criticism that releases feel premature and hard to trust for daily engineering work.\nBilling Roulette: Token Counts, Auto Mode, and DIY Telemetry: Cursor users flagged usage/billing discrepancies (missing dollar amounts, unexpected bonus credits, and limits not triggering despite heavy use), and some suspected Auto mode charges incorrectly.\nTo sanity-check spending, users recommended third-party tracking like token-watch, noting it can diverge from what Cursor’s own dashboard shows.\n2. Inference Performance & Benchmarking: B200, vLLM, llama.cpp, Grafana\nFlashAttention-4 Floors It on B200 (Specs Still Foggy): In GPU performance chatter, FlashAttention-4 (FA4) reportedly hit 1,605 TFLOPS/s (~71% of theoretical) on an NVIDIA B200 with BF16 inputs, while the community debated a theoretical ceiling around 2260 TFLOPS and noted missing official datatype details.\nConfusion deepened as leaked materials listed B200 at 10/5/2.5 TFLOPS for fp4/fp8/fp16, and folks asked for an official spec paper to reconcile marketing numbers with kernel benchmarks.\nSpec Decode Saves Throughput, Not Your TTFT (Usually): Engineers discussed optimizing Time To First Token (TTFT) for Qwen3-VL in vLLM on a B200, considering --speculative_config with smaller Qwen3-VL-4B/2B draft models.\nThe advice: speculative decoding often hurts throughput unless you run high batch sizes, and only “eagle heads” setups feel worth it at scale because small drafts add too much overhead for short outputs.\nGrafana for VLM Telemetry + vLLM Metrics as the Source of Truth: For benchmarking fast multimodal paths, members pointed to vLLM’s metrics docs and suggested wiring dashboards with Grafana for real-time TTFT visualization.\nThe thread framed Grafana as the “good UI” layer for quickly comparing VLM deployments under realistic workloads instead of relying on one-off scripts.\nllama.cpp Speeds Up GLM-4.7 Flash GGUFs ~1.5×: llama.cpp delivered a ~1.5× speedup plus bug fixes for GLM 4.7 Flash GGUFs, and users pointed to re-downloading fixed quants from unsloth/GLM-4.7-Flash-GGUF.\nThis reinforced the “rebuild often” culture in local inference stacks: performance jumps can appear just by updating the runtime and swapping quants, not changing the model.\n3. Open Releases: Voice/Audio Models, New Datasets, and Local-First LLMs\nQwen3-TTS Drops Multilingual Voice Cloning (ElevenLabs Catching Side-Eye): Communities rallied around Qwen3-TTS as a strong voice cloning option, with a live demo on Qwen3-TTS Hugging Face Spaces and the broader family referenced via QwenLM GitHub and Qwen on Hugging Face.\nLatent Space summarized the family as 0.6B–1.8B params with support for 10 languages, framing it as open tooling that can plausibly replace paid TTS in some pipelines.\nAudio Release Triple-Feature: PersonaPlex-7B, TTS-1.5, and Chroma 1.0: An audio-model roundup highlighted NVIDIA PersonaPlex-7B (full-duplex conversational), Inworld AI TTS-1.5 (low-latency TTS), and Flash Labs Chroma 1.0 (open-source end-to-end speech-to-speech) via Lina Colucci’s post.\nThe vibe: speech stacks are accelerating toward low-latency and end-to-end pipelines, and open releases are starting to cover pieces previously gated behind SaaS APIs.\nDatasets & Local Models: Rust→WASM Synthetic + Faust-1 German-First: Hugging Face saw two notable drops: a Rust-to-WebAssembly synthetic dataset of 1,000 generated programs at webxos/wasm_synthetic_dataset, and Faust-1, a 1.6B German-first LLM at tabularisai/Faust-1.\nFaust-1 emphasized ~90% German pretraining, a German-optimized tokenizer, and instruction tuning with DPO, while the WASM dataset focused on reproducibility (deterministic Fibonacci-derived PRNG plus structural hashes).\n4. Agent Frameworks & Infra: Control Loops, RLM/DSPy, and MCP Schema Discipline\nReplit Agent Gets a Steering Wheel with Decision-Time Guidance: A technical writeup on Decision-Time Guidance described how Replit Agent applies real-time control mechanisms instead of static rules, shared via Zhen Li’s blog link.\nThe discussion framed this as a practical direction for agents: tighter online steering during execution rather than brittle pre-authored guardrails.\nDSPy’s “Why” Gets Re-Explained (Signatures > Prompt Hacks): DSPy folks circulated an explainer arguing DSPy’s value comes from signature & module abstractions, not just prompt tuning, via “DSPy: the most misunderstood agent” and the companion X post.\nSeparately, members discussed tuning RLM prompts and even optimizing JSON-schema adapters (GEPA ideas), aiming to make structured outputs more reliable without bloating token budgets.\nMCP Schema Cage Match: additionalProperties vs anyOf: Model Context Protocol contributors questioned whether GetTaskPayloadResult is too permissive because it allows additionalProperties, pointing directly to the schema location in the MCP repo (schema.json lines 1245–1256).\nThe proposed fix was moving toward anyOf for stricter validation, reflecting a broader push to keep agent-tool payloads tightly typed to avoid “accept everything” integrations that break downstream.\n5. Platforms, Benchmarks, and the AMD/NVIDIA Reality Check\nArena Leaderboards Split (and Models Vanish Mid-Game): LMArena’s Image Edit Arena split rankings into Single-Image Edit and Multi-Image Edit, publishing results at the Image Edit leaderboard where Gemini 3 Pro Image 2K rose to #1 and ChatGPT Image (Latest) fell to #3.\nAt the same time, reliability issues yanked models around: Nano Banana Pro 2K got removed for a high error rate, and Seedream-4-2k disappeared with moderators noting models can be unavailable for technical reasons.\nROCm Pain, CUDA Gain: Devs Vote with Their 5090s: GPU devs shared AMD tooling breadcrumbs—like the AMD ISA manual and LLVM docs (AMDGPUUsage.rst, IntrinsicsAMDGPU.td, and AMDGPU CodeGen tests)—but the tone stayed mixed on ROCm readiness.\nOne developer said they quit ROCm for CUDA after buying a 5090, citing packaging/build/distribution headaches, and another pointed to poor Conv3D performance on AMD relative to NVIDIA via a ROCm subreddit thread (link).\nBaseten Raises $300M Series E as Infra Funding Stays Hot: Latent Space highlighted Baseten’s $300M Series E led by IVP and CapitalG, valuing the company at $5B, per Baseten’s announcement.\nThe participation of NVIDIA in the round reinforced the market narrative: inference infrastructure remains a capital-heavy race where hardware adjacency still matters.\ngpt-5.1\n1. Frontier Model Performance, Kernels, and Hardware Benchmarks\nFlashAttention-4 Pushes B200 GPUs Toward Theoretical Limits: FlashAttention‑4 (FA4) hit 1,605 TFLOPS/s (~71% of theoretical max) on an NVIDIA B200 in BF16, with community estimates pegging the true ceiling around 2,260 TFLOPS, pending an official spec paper and data‑type breakdown from NVIDIA. Discussion in GPU MODE #cuda noted leaked B200 figures of 10/5/2.5 PFLOPS for fp4/fp8/fp16 clashing with the FA4 measurements, underscoring the need for a formal performance whitepaper rather than marketing blogs without datatype detail.\nResearchers also highlighted test‑time training (TTT) for LM‑generated kernels in a new paper, \""Discovering Test-Time Training for LLM‑Generated GPU Kernels\"", showing that adapting kernels at inference can materially improve benchmark scores on existing leaderboards. In parallel, FlashInfer‑Bench from CMU Catalyst Lab was introduced in GPU MODE’s #popcorn as a framework for evaluating and deploying AI‑generated GPU kernels, with the authors actively seeking community feedback on benchmarks and production deployment workflows.\nGLM-4.7 Flash Builds Hit Turbo in llama.cpp and Arena: Both LMArena and Hugging Face circles reported major speed wins from GLM‑4.7‑Flash variants, with llama.cpp users seeing roughly 1.5× throughput gains on GLM‑4.7 Flash GGUFs after rebuilding and re‑downloading new quants from unsloth/GLM-4.7-Flash-GGUF. LMArena also added glm‑4.7‑flash to the Text Arena, giving a head‑to‑head benchmark venue against other frontier chat models.\nAn Unsloth user reported 50 tok/s at 50k context with --flash-attn on on GLM‑4.7‑Flash, reinforcing that the FlashAttention fixes are stable at long context lengths, while Nous users experimented with GLM‑4.7 on 8×H100 GPU servers as a potential Claude Code alternative. Across Discords, practitioners converged on a pattern of: rebuild kernels, enable explicit FlashAttention flags, and push long‑context workloads to stress‑test GLM‑4.7 Flash as an affordable, high‑throughput code‑capable model.\nGPU Ecosystems Split: CUDA Dominates as ROCm Stumbles: In GPU MODE #rocm, a developer announced they were \""done with ROCm\"" after buying an RTX 5090, citing chronic issues with packaging, build chains, distribution gaps, and weak consumer focus, and sharing a Reddit thread on poor Conv3D performance on RX 9070 as evidence that mid‑range NVIDIA cards still crush AMD on real‑world ML workloads. Others criticized the ROCm ecosystem as \""hostile\"" and pointed at fragile libraries like FBGEMM on gfx1100 and opaque vendor repos such as AMD’s Quark quantization engine.\nTo mitigate the pain, experts shared low‑level ROCm documentation sources—AMD’s CDNA4 ISA manual and LLVM’s AMDGPUUsage.rst plus IntrinsicsAMDGPU.td—and emphasized that clang builtins map 1:1 to LLVM intrinsics, which you can reverse‑engineer via AMDGPU CodeGen tests. Meanwhile, a separate GPU MODE thread advertised CUDA‑kernel optimization jobs (profiling with Nsight Systems/Compute, writing optimized CUTLASS‑style kernels) via a Parsewave posting, underscoring that the ecosystem gravity—and money—is still heavily on the CUDA side.\n2. New Models, TTS, and Benchmarks Across the Open Ecosystem\nQwen3-TTS Struts In as Multilingual Voice-Cloning Workhorse: Alibaba launched Qwen3‑TTS, a family of open text‑to‑speech models (≈0.6B–1.8B params) supporting 10 languages with variants for VoiceDesign, CustomVoice, and Base, released on [GitHub (QwenLM)]https://github.com/QwenLM and Hugging Face. Latent Space’s #genmedia-creative-ai highlighted its high‑quality cloning, while Nous community members directly compared the interactive demo on Hugging Face Spaces to ElevenLabs, calling it “a very good voice cloning tool.”\nEarly adopters are probing multilingual robustness and clone fidelity, with one Nous user emphasizing that Qwen3‑TTS is competitive with commercial TTS for user‑facing agents. Latent Space threads grouped it with other audio releases—NVIDIA PersonaPlex‑7B, Inworld TTS‑1.5, and Flash Labs’ Chroma 1.0 described in Lina Colucci’s roundup—framing Qwen3‑TTS as the open‑source counterpart in a rapidly heating speech‑to‑speech and conversational‑audio race.\nImage Edit Arena Shakes Up Multimodal Rankings: LMArena split its Image Edit Arena leaderboard into separate Single‑Image Edit and Multi‑Image Edit tracks, publishing the results at the new image‑edit leaderboard for finer‑grained comparison of visual editing ability. The reshuffle toppled incumbents: ChatGPT Image (Latest) dropped from #1 to #3, while Gemini 3 Pro Image 2K climbed from #2 to the top spot, with Nano Banana and Seedream models also being shuffled and occasionally pulled (e.g., Seedream‑4‑2k disappearing for technical reasons).\nConcurrently, LMArena added wan2.6‑image (image‑edit only), wan2.6‑t2i (text‑to‑image), and devstral‑2 (Code Arena) via their announcements, though users noted a confusing limitation where wan2.6-t2i currently exposes no image upload. Operationally, the platform yanked Nano Banana Pro 2K due to high error rates and acknowledged persistent video generation failures and Linux‑only captchas, reinforcing that frontier multimodal eval is still bottlenecked as much by infra quirks as by model quality.\nNew Open Datasets and Niche Models Fuel Specialized Workloads: The Hugging Face #i-made-this channel saw the release of Faust‑1, a 1.6B German‑first LLM with ≈90% German pretraining, a German‑optimized tokenizer, and DPO‑tuned instructions, published at tabularisai/Faust-1 for local, privacy‑sensitive use cases. Another contributor released a synthetic Rust→WebAssembly compilation dataset of 1,000 programmatically generated Rust programs at webxos/wasm_synthetic_dataset, with deterministic Fibonacci‑based pseudo‑random generation to ensure reproducible code patterns and compiler behaviors.\nAlongside these, a safety dataset for alignment research landed at Pacific-Prime/safety_dataset, while a separate project generated custom typeface datasets via COLIGNUM for font‑centric ML work. Collectively these releases hint at a maturing long tail of domain‑specific corpora—language‑localized LLMs, compiler‑oriented code sets, safety supervision data, and typographic datasets—feeding into RAG systems, continual‑learning workflows, and evaluation of program synthesis and WebAssembly tooling.\n3. Agentic Frameworks, DSPy/RLM, and IDE Tooling\nDSPy and RLM Reframe Agents as Optimizable Programs: In the DSPy Discord, an article titled \""DSPy: The Most Misunderstood Agent Framework\"" argued that DSPy’s real value is its signature & module abstraction, not just GEPA and prompt‑tuning hacks, stressing that programs of LMs should be treated like differentiable pipelines rather than hand‑wired agents. Another blog, \""A Pragmatic Recipe for Continual Learning\"", pitched DSPy.RLM() as a core building block for engineered continual‑learning systems that retrain themselves over time.\nMembers experimented with RLM prompts to improve reasoning—complaining that some models still give \""vague generic answers\""—and proposed optimizing RLM traces similarly to ReAct, where an optimizer inspects step‑by‑step logs while users only care about final outputs. There was also interest in building a custom GEPA adapter for DSPy’s JSON output layer, so that json_schema‑based responses can be optimized to drop redundant system tokens and reduce overhead for structured tool integrations.\nIDE Agents Evolve: Cursor Subagents and Aider Workflows: The Cursor community dissected the 2.4 release, which introduced parallel subagents (documented in the Cursor changelog and demo video) that inject a <subagent_delegation_context> to farm tasks out in parallel, plus image generation and clarification‑question capabilities advertised on Cursor’s X post. However, users in #general reported Composer 1 infinite loops, heavy lag in 2.4 (\""planning next moves\"" hanging), and suspected that broken subagent scaffolding was calling a missing Task tool, forcing many to downgrade to 2.3—especially on older macOS versions like Big Sur 11.7.3.\nSeparately, the aider community proposed a terminal UI and session management for the CLI‑based coding assistant, aiming to let users edit the next message while scrolling past replies, render rich Markdown (including mermaid diagrams), and save/load entire chat contexts without polluting the current prompt. Power users described a meta‑workflow pairing aider for context management and search‑replace coding with Claude Code for hard bug‑hunting, framing aider as the “file selection & edit engine” that minimizes tokens while a remote LLM handles deeper reasoning.\nMCP and Schema Design for Tool-Calling Agents: In the MCP Contributors Discord, contributors scrutinized the Model Context Protocol’s GetTaskPayloadResult schema, pointing out that its use of \""additionalProperties\"" in the JSON Schema at this definition makes payloads too permissive. They proposed switching to an anyOf union of explicit alternatives to enforce that only pre‑declared fields appear, tightening validation for tool payloads.\nThe discussion framed this as a tooling‑reliability tradeoff: additionalProperties keeps MCP extensible for new tools, but weakens static guarantees, whereas anyOf helps clients and servers catch malformed or adversarial payloads early. Given MCP’s ambition as a cross‑tool agent protocol, participants argued that strict schemas for core messages like GetTaskPayloadResult matter for security, debugging, and interop, even if it requires more frequent schema migrations.\n4. Experimental Architectures, Optimization Tricks, and Training Methods\nDifference Layers and SwiGLU Turbocharge NanoGPT Baselines: In Eleuther’s #research, a contributor reported strong gains on CartPole and NanoGPT by swapping the standard MLP with a difference layer x = (a2(x) - b2(x)) * (c2(x) - d2(x)) + e2(x) from Eternalyze0/difference_layer, claiming better performance at lower parameter and compute budgets. Others cautioned that such multiplicative structures effectively double the learning rate under SGD and that improvements may vanish against well‑tuned baselines rather than the default NanoGPT configs.\nResearchers also noted that simply replacing GELU with SwiGLU as the transformer activation significantly improves the NanoGPT baseline, and further gains appear when combining SwiGLU with the difference‑layer QKV replacement. Senior members repeatedly pointed newcomers at Noam Shazeer’s \""GLU Variants Improve Transformer\"" paper, warning that any new gating trick should be benchmarked against state‑of‑the‑art GLU baselines before being touted as a structural breakthrough.\nGRPO, Attention Sinks, and Reasoning Training Gotchas: Unsloth’s #research and #off-topic channels hosted candid post‑mortems on GRPO (Generalized Reinforcement Policy Optimization), with one practitioner concluding from experiments (and re‑reading the \""DeepSeek R1\"" paper) that GRPO refines existing reasoning but does not magically unlock “emergent reasoning” in niche domains where pretraining data is thin. They described a three‑stage pipeline—corpus CPT on novels + medical articles (~400M tokens), translated SFT, then synthetic polishing via rejection sampling—and still found GRPO unstable for specialized tasks like Turkish translation and domain support.\nOn the representation side, Unsloth members debated attention sinks, with some manually injecting <|endoftext|> at the context start, while others argued that “attention sink is poured into the very first token in the entire context window, just one token for it” and that models learn their own sink dynamics. A separate lesson learned the hard way: when using small models to generate chain‑of‑thought traces to train bigger reasoning models, masking the thinking tokens during supervised training significantly improves metrics (unmasked CoT caused F1 to crater, despite seeming attractive for interpretability).\nTest-Time-Training, LM Kernels, and Self-Replication Benchmarks: GPU MODE’s #general and #multi-gpu channels highlighted test‑time training (TTT) as a promising paradigm not just for models but for LM‑generated GPU kernels, with the discover.pdf paper at test-time-training.github.io/discover.pdf showing that adapting kernels against benchmark suites at inference can yield surprisingly strong performance. In parallel, debugging threads around NCCL on B200s under Slurm—including sbatch scripts and NCCL_DEBUG=INFO logs—reinforced that auto‑tuning comms libraries plus dynamic kernel adaptation is becoming a combined engineering problem rather than a pure modeling issue.\nOver in Nous, a member brainstormed a self‑replication benchmark for agentic AI, and someone suggested using Claude’s C‑implemented transformer engine and custom CPU described in cpldcpu’s smollm.c as a target: can an agent inspect, modify, and re‑deploy its own inference engine? This dovetails with DSPy/RLM discussions, hinting at a future where agents optimize both their weights and their low‑level kernels at inference time under constrained hardware budgets.\n5. AI Business, APIs, and Production Reliability\nBaseten’s $300M Raise and Capital One–Brex Deal Signal AI Infra Consolidation: Latent Space’s #ai-general-chat flagged two major deals: Capital One acquiring Brex for $5.15B, as reported by Alex MacCaw, marking the largest bank–fintech acquisition to date; and Baseten’s $300M Series E at a $5B valuation, announced in Baseten’s tweet with IVP and CapitalG leading and NVIDIA participating. Both moves underscore that AI‑heavy infra and fintech tooling are being rapidly absorbed or capitalized by large incumbents and late‑stage investors.\nMembers interpreted the Baseten round as validation that model serving and orchestration is a defensible, high‑margin layer even in an open‑model world, while the Capital One–Brex deal was read as a bet that data‑rich fintech workflows (expense management, cards, underwriting) will be increasingly AI‑automated. Combined with SimilarWeb stats shared in a Venture Twins tweet showing ChatGPT still dominating traffic while Grok grows 33× in US penetration, the community sees a landscape where infra, data, and distribution matter at least as much as raw model quality.\nPerplexity Pro and API Turbulence Threaten Power-User Workloads: On the Perplexity AI server, Pro subscribers reported abrupt file‑upload caps of 3/day, conflicting research query limits (some seeing 600/day, others as low as 20), and persistent 401 Unauthorized errors on the Perplexity API even after renewing credits, which broke production use cases like a sports‑betting model. Threads in #general and #pplx-api speculated about A/B experiments vs. cost‑cutting, and some users threatened to cancel, arguing that silent feature downgrades destroy trust for teams trying to treat Perplexity as a dependable research backend.\nAt the model layer, users shared a medical example where Gemini, Claude Opus, and Ernie all failed to recommend a DEXA bone‑density scan when asked about calcium deficiency work‑ups, whereas GPT explicitly mentioned it, reinforcing that Perplexity’s meta‑model/engine choice can materially affect clinical recommendations. Combined with billing bugs (pending charges, locked accounts) and contested celestial fact‑checking drama, the overarching sentiment was that Perplexity’s product is powerful but operationally fragile, and engineers should have fallbacks before wiring it deep into production flows.\nIDE, API, and Billing Reliability: Cursor, Manus, and OpenRouter: Cursor power‑users complained that 2.4 shipped with severe lag, crashes, and broken Composer 1 loops, and also raised billing opacity concerns: inconsistent dollar displays, unpredictable limits in Auto mode, and unexplained bonus credits prompted some to rely on token-watch for independent usage audits. Over in Manus.im, a user reported being charged $400 for an annual plan despite selecting monthly during a trial, and openly discussed escalating to FTC/BBB/Attorney General if not refunded, warning others to double‑check plan terms.\nThe OpenRouter community noticed that internal  reasoning blocks recently started leaking into end‑user responses in OR Chat and JanitorAI, raising UX and privacy questions and triggering a support ticket. Meanwhile, an OpenRouter thread about uncensored image generation concluded that engineers should pair one text LLM with a separate image model rather than expect a single uncensored multimodal endpoint, while some users half‑jokingly proposed an OpenRouter gacha system with pity mechanics and leaderboards, reflecting both frustration with opaque pricing and a desire for more transparent, game‑like model discovery.\ngpt-5\n1. New TTS and Audio AI Releases\nTongue-Twisting TTS Triumphs: Alibaba unveiled Qwen3-TTS with VoiceDesign, CustomVoice, and Base variants (five models, 0.6B–1.8B params, 10 languages), with releases on QwenLM GitHub and Hugging Face.\nCommunity demos showcased high-fidelity cloning and multilingual synthesis via the official Qwen3-TTS Space, with users calling the results “very good voice cloning.”\nAudio Arms Race Accelerates: A roundup highlighted NVIDIA’s PersonaPlex‑7B (full‑duplex), Inworld TTS‑1.5 (low‑latency), and Flash Labs’ Chroma 1.0 (open end‑to‑end speech‑to‑speech), summarized in Lina Colucci’s post.\nEngineers discussed how these releases push real‑time conversational and SS2S stacks toward production, framing Q3–Q4 as a breakout window for low‑latency voice agents.\nVoice Design Goes DIY: Qwen3-TTS’s VoiceDesign and CustomVoice features enable user‑defined voices and cloning workflows with accessible configs and assets on Hugging Face.\nBuilders reported that the Space’s cloning quality “rivals ElevenLabs” in quick trials, encouraging bake‑offs using the official demo.\n2. AI Kernel Benchmarks & Optimization\nFlashInfer Frenzy Benchmarks Kernels: CMU Catalyst Lab introduced FlashInfer‑Bench, a framework to evaluate AI‑generated GPU kernels and deploy them into serving engines: FlashInfer‑Bench.\nParticipants praised the effort as “a very cool project,” and the team invited collaboration on refining benchmarks and production deployment pathways.\nTTT Tunes Tiny Kernels: Researchers evaluated LM‑generated kernels with test‑time training (TTT) on prior leaderboards, reporting promising outcomes in the paper Discovering Test-Time Training.\nDiscussions centered on how TTT can adapt kernels to distribution shifts at inference, potentially boosting leaderboard parity without retraining.\nROCm Readmes Reveal Intrinsics: Engineers mapped clang builtins → LLVM intrinsics for AMD GPUs using the CDNA4 ISA manual and LLVM docs: AMD ISA PDF, AMDGPUUsage.rst.\nThey also pointed to IntrinsicsAMDGPU.td and CodeGen tests for examples, helping practitioners align kernel code with ROCm’s compilation model.\n3. Agentic IDEs and Dev Tooling\nCopilot SDK Cuts the Cord: Developers celebrated the release of the GitHub Copilot SDK, which enables first‑class AI features inside apps via GitHub’s infra: github.com/github/copilot-sdk.\nEarly adopters emphasized replacing bespoke routing and third‑party pricing with a native SDK, streamlining tool‑augmented agent integration.\nCursor Subagents Sprint in 2.4: Cursor 2.4 shipped parallel subagents for faster execution and better context use, plus image generation and clarifying questions: Changelog and Cursor on X.\nThe team’s video demo shows subagents coordinating on multi‑step tasks, promising speedups for complex coding flows.\nBaseten Banks Big Bucks: Baseten raised $300M Series E (IVP, CapitalG; participation from NVIDIA), reaching a $5B valuation: Baseten announcement.\nInfra‑minded builders read this as a signal of sustained demand for model serving, ops, and agent backends at enterprise scale.\n4. Model Speedups & Evaluation Arenas\nllama.cpp Lights Up GLM Flash: llama.cpp improved performance for GLM‑4.7 Flash GGUF by ~1.5× and fixed bugs; users were told to rebuild and fetch fixed quants from unsloth/GLM‑4.7‑Flash‑GGUF.\nReports cited stable 50 tok/s at 50k context with flash attention enabled, noting it “is working beautifully” after the fix.\nLeaderboard Split Sharpens Image Edits: LMArena split the Image Edit Arena into Single‑Image Edit vs Multi‑Image Edit, revealing shifts on the overall leaderboard.\nChatGPT Image (Latest) dropped from #1→#3 while Gemini 3 Pro Image 2K rose from #2→#1, offering clearer task‑specific rankings.\nArena Adds Wan & Devstral: New LMArena entries include wan2.6‑t2i (text‑to‑image), wan2.6‑image (image edit), and devstral‑2 (code), available via LMArena.\nThe split of wan2.6 into distinct edit vs T2I endpoints aims to reduce misuse and clarify capabilities in head‑to‑head evaluations.\n5. Research Tricks in Architectures & Training\nSwiGLU Swings the Baseline: Researchers reported that switching GELU → SwiGLU significantly boosted NanoGPT‑style baselines, aligning with Shazeer’s GLU variants paper.\nThe conversation emphasized strong baselines to avoid mistaking optimization gains for architectural advances.\nDifference Layer Doubles Down: A proposed multiplicative difference layer improved cartpole and nanogpt performance with fewer params/compute: difference_layer repo.\nSkeptics noted the formulation may implicitly double the effective learning rate, urging comparisons against tuned baselines rather than defaults.\nGRPO Gets Groggy: Engineers found GRPO “proved unstable” in niche domains lacking pretraining coverage, debating claims of emergent reasoning in the DeepSeek paper: arXiv:2601.07568.\nConsensus landed on using GRPO to refine existing capabilities, while reserving broader reasoning improvements for data/architecture changes.\nDiscord: High level Discord summaries\nBASI Jailbreaking Discord\nGemini 3 Pro Succumbs to ENI Jailbreak: The ENI jailbreak, originally designed for Claude, has been found to work on Gemini 3 Pro in AI Studio, a user shares a GitHub link for setup.\n\nA member confirmed it works like magic, even with 3 flash.\nPrimeTalk System Claims Vanilla AI Coherence: A user introduced the PrimeTalk system, asserting it transforms the chaos of vanilla AI into coherence by structuring the token stream and imposing logic, consequence, and presence, sharing a PRIMETALK_v3.85_VALHALLA_BUILD_PUBLIC_EDITION.txt file.\n\nThe system aims to structure the token stream and impose logic, consequence, and presence to achieve this transformation.\nGPT Models Supposedly Broken Via UI Exploits: A user states they have broken all the GPT models that exist and that will come, arguing that exploiting the UI constitutes a jailbreak.\n\nThey provided an irrelevant prompt that's supposedly effective on most models, for stealing prompts, although others believe this is not a true jailbreak.\nWargame for Red Teaming Announced: A user shared a wargame particularly relevant to #red-teaming and posted a link to the relevant Discord channel.\n\nThe user was unsure whether cross-posting of the event was appropriate in the channel.\nUnsloth AI (Daniel Han) Discord\nMoE Models Squeeze into 8GB RAM: Members are debating the feasibility of running Mixture of Experts (MoE) models in 8GB RAM, with suggestions for Gemma 3N and LFM, but not..."",""content"":""**Anthropic** launches \""Claude in Excel Pro\"" with enhanced features. **OpenAI** reveals upcoming **Codex** agent loop and cybersecurity measures. **Google** boosts **Gemini App** quotas and partners with **Sakana AI** for advanced AI Scientist projects in Japan. **Cursor** introduces Agent Skills for dynamic context focus. **GPT-5.2 Pro** achieves **31%** on FrontierMath Tier 4, showing significant benchmark progress. **Baseten** raises **$300M** at a **$5B valuation** targeting high-performance inference. Discussions highlight math benchmarks as indicators of AI capability, uneven AGI progress, and the importance of reasoning and continual learning as future frontiers. Notable figures include *Sam Altman*, *François Chollet*, *Shane Legg*, and *Demis Hassabis*."",""contentSnippet"":""**Anthropic** launches \""Claude in Excel Pro\"" with enhanced features. **OpenAI** reveals upcoming **Codex** agent loop and cybersecurity measures. **Google** boosts **Gemini App** quotas and partners with **Sakana AI** for advanced AI Scientist projects in Japan. **Cursor** introduces Agent Skills for dynamic context focus. **GPT-5.2 Pro** achieves **31%** on FrontierMath Tier 4, showing significant benchmark progress. **Baseten** raises **$300M** at a **$5B valuation** targeting high-performance inference. Discussions highlight math benchmarks as indicators of AI capability, uneven AGI progress, and the importance of reasoning and continual learning as future frontiers. Notable figures include *Sam Altman*, *François Chollet*, *Shane Legg*, and *Demis Hassabis*."",""guid"":""https://news.smol.ai/issues/26-01-22-not-much/"",""categories"":[""anthropic"",""openai"",""google"",""sakana-ai"",""cursor"",""baseten"",""epoch-ai-research"",""deepmind"",""claude-3"",""codex"",""gemini"",""gpt-5.2-pro"",""sama"",""fchollet"",""shane_legg"",""demishassabis"",""benchmarking"",""reasoning"",""continual-learning"",""reinforcement-learning"",""model-performance"",""agentic-ai"",""security"",""model-training""],""isoDate"":""2026-01-22T05:44:39.000Z""}"
Smol,"OpenEvidence, the ‘ChatGPT for doctors,’ raises $250m at $12B valuation, 12x from $1b last Feb",https://news.smol.ai/issues/26-01-21-openevidence/,2026-01-21T05:44:39.000Z,"<p><strong>Agent Labs are all you need</strong></p>
<blockquote>
<p>AI News for 1/20/2026-1/21/2026. We checked 12 subreddits, <a href=""https://twitter.com/i/lists/1585430245762441216""><strong>544</strong> Twitters</a> and <strong>24</strong> Discords (<strong>205</strong> channels, and <strong>7561</strong> messages) for you. Estimated reading time saved (at 200wpm): <strong>584 minutes</strong>. <strong>Our new website</strong> is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on <a href=""https://x.com/Smol_AI"">@smol_ai</a>!</p>
</blockquote>
<p>We have a soft rule that new decacorn fundraises get title stories, and <a href=""https://www.cnbc.com/2026/01/21/openevidence-chatgpt-for-doctors-doubles-valuation-to-12-billion.html"">multiple sources</a> have the story of OpenEvidence's $12B fundraise, 12x from last year. What's odd is ""CEO Daniel Nadler told CNBC that OpenEvidence is used by 40% of physicians in the U.S. and topped $100 million in annual revenue last year."" which is a 120x multiple.</p>
<hr>
<h1>AI Twitter Recap</h1>
<p><strong>Frontier model governance: Anthropic’s new Claude constitution (CC0) and reactions</strong></p>
<ul>
<li><strong>What shipped</strong>: Anthropic published a new “constitution” describing desired Claude behaviors/values and stated it is used directly in training; importantly, the full constitution is released <strong>CC0 1.0</strong> to encourage reuse/adaptation (<a href=""https://twitter.com/AnthropicAI/status/2014005798691877083"">announcement</a>, <a href=""https://twitter.com/AnthropicAI/status/2014005815376568780"">CC0 link</a>). Anthropic frames it as a <strong>living document</strong> shaped by internal + external experts (<a href=""https://twitter.com/AnthropicAI/status/2014005813157720283"">follow-up</a>).</li>
<li><strong>Community read</strong>: Amanda Askell emphasized it’s a work-in-progress and invited feedback (<a href=""https://twitter.com/AmandaAskell/status/2014010171081581048"">Askell</a>). Others highlighted the “meta” oddity of training a model on a document about how the model should be (<a href=""https://twitter.com/scaling01/status/2014014692004421653"">scaling01 on Opus reflecting on circularity</a>). Several reactions focused on the constitution as “alignment signaling” vs. practical harm reduction, and whether it bakes in PR-oriented persona behaviors (<a href=""https://twitter.com/nearcyan/status/2014009518150054035"">nearcyan</a>, <a href=""https://twitter.com/NickEMoran/status/2014077605373260204"">NickEMoran question</a>).</li>
<li><strong>Practical engineering consequence</strong>: Anthropic also posted about an internal <strong>performance engineering take-home</strong> that became solvable by Opus 4.5, forcing them to redesign hiring evaluation—a concrete example of “models catching up to our screening tasks” (<a href=""https://twitter.com/AnthropicAI/status/2014143403144200234"">Anthropic Eng</a>, <a href=""https://twitter.com/trishume/status/2014144787092562160"">trishume</a>).</li>
</ul>
<p><strong>Agents in production: from “AI employees” (Podium) to agent UX, memory, and evals</strong></p>
<ul>
<li><strong>Podium’s “Jerry” as an agent business unit</strong>: Podium claims <strong>$100M+ AI agent ARR</strong> and <strong>10k+ agents</strong> deployed, anchored on staffing constraints in SMBs (after-hours leads, missed calls, turnover). The narrative: stop selling “software,” sell an <strong>AI operator</strong> that uses the existing product end-to-end (<a href=""https://twitter.com/ericwilliamrea/status/2013980401635582277"">Eric Rea</a>). Tom Loverro adds board-level metrics (burn down from <strong>$95M → $0</strong>, AI ARR <strong>$0 → $100M</strong> in ~21 months) and links to OpenAI’s case study (<a href=""https://twitter.com/tomloverro/status/2014011044210106406"">Tom Loverro</a>, <a href=""https://twitter.com/garrytan/status/2014005103728943566"">Garry Tan</a>).</li>
<li><strong>Memory &#x26; long-horizon reliability becomes the bottleneck</strong>:
<ul>
<li>The <strong>Agent Cognitive Compressor (ACC)</strong> pitch argues “more context ≠ better agents,” criticizing transcript replay and naive retrieval. ACC maintains a bounded “Compressed Cognitive State” with schema-constrained commit, claiming lower drift/hallucination over long runs (<a href=""https://twitter.com/dair_ai/status/2014000799245107339"">dair_ai</a>).</li>
<li>A separate thread positions “self-improving multi-agents” for scientific workflows via <strong>MCP-SIM</strong>, a multi-agent loop that clarifies underspecified physics prompts, generates code, executes, diagnoses errors, and produces multilingual explanations; claims <strong>12/12</strong> tasks solved vs one-shot GPT at <strong>6/12</strong> (<a href=""https://twitter.com/omarsar0/status/2013998285040836662"">omarsar0</a>).</li>
</ul>
</li>
<li><strong>Agentic benchmarking moves beyond coding puzzles</strong>:
<ul>
<li><strong>APEX-Agents</strong> evaluates long-horizon “professional services” tasks in Google Workspace; early Pass@1 leaderboard numbers are low (Gemini 3 Flash High <strong>24.0%</strong>, GPT-5.2 High <strong>23.0%</strong>, Claude Opus 4.5 High <strong>18.4%</strong>)—a reminder that “agent autonomy” is still brittle (<a href=""https://twitter.com/BrendanFoody/status/2014028956752568356"">BrendanFoody</a>).</li>
<li><strong>prinzbench</strong> introduces a private benchmark for legal research + search (33 Qs, graded manually, run 3x) where “search” is the failure mode; claim: GPT-5.2 Thinking barely exceeds <strong>50%</strong>, Gemini models are close behind, and Sonnet/Opus 4.5 scored <strong>0/24</strong> on Search (<a href=""https://twitter.com/deredleritt3r/status/2013979845378580684"">deredleritt3r</a>).</li>
</ul>
</li>
<li><strong>Tooling + UX layer catches up</strong>: multiple posts converge on the idea that agents need a “context layer” and production scaffolding (governance, auth, observability) as much as better models—see Prefect Horizon and MCP server best practices below.</li>
</ul>
<p><strong>Agent platforms and “context layers”: MCP, Skills, Prefect Horizon, LangChain Deep Agents</strong></p>
<ul>
<li><strong>Prefect Horizon: MCP → platform</strong>: Prefect positions “context layer” as the interface between agents and enterprise tools/data, and argues MCP describes how to build a server but not how to <strong>deploy/govern</strong> it at org scale. Horizon claims managed deployment, registry/catalog, gateways w/ RBAC + audit logs, and “agentic interface for business users” (<a href=""https://twitter.com/jlowin/status/2014023606380957754"">jlowin</a>).</li>
<li><strong>MCP servers: design guidance</strong>: Phil Schmid pushes back on “Skills replace MCP” takes: MCP isn’t the problem; <strong>bad servers</strong> are. Recommendations: design tools around outcomes, typed flat args with constraints, docstrings/errors as agent instructions; positions Skills and MCP as complementary (<a href=""https://twitter.com/_philschmid/status/2014016583706829054"">philschmid</a>).</li>
<li><strong>LangChain deepagents: agents as folders + UI integration</strong>:
<ul>
<li>CopilotKit published a tutorial building a <strong>fullstack Deep Agent app</strong> (resume ingestion → skills extraction → sub-agents with web search → streaming UI), addressing the “missing UI/application layer” gap (<a href=""https://twitter.com/CopilotKit/status/2013997128683856159"">CopilotKit</a>).</li>
<li>LangChain shipped <strong>Agent Builder GA</strong> plus a <strong>template library</strong> with domain partners (Tavily, PagerDuty, Box, etc.) to reduce “prompt-to-agent” friction (<a href=""https://twitter.com/LangChain/status/2014034320256880768"">LangChain</a>).</li>
<li>Deep Agents’ framing “agents are just folders” emphasizes portability/distribution: you can package, download, and run agents quickly via CLI flows (<a href=""https://twitter.com/hwchase17/status/2014076509208629386"">hwchase17</a>, <a href=""https://twitter.com/Vtrivedy10/status/2014074890458980736"">Vtrivedy10 demo</a>, <a href=""https://twitter.com/LangChain_OSS/status/2014075587137048882"">LangChain_OSS</a>). Sydney Runkle highlights two core patterns: <strong>subagents for context isolation</strong> and <strong>skills loaded only when relevant</strong> (<a href=""https://twitter.com/sydneyrunkle/status/2014060287746265535"">sydneyrunkle</a>).</li>
</ul>
</li>
<li><strong>LangSmith + analytics</strong>: one thread points to LangSmith traces as a substrate not only for debugging but <strong>product analytics</strong> (“agent traces → product analytics”) (<a href=""https://twitter.com/SoftwareWatcher/status/2013972269106684060"">SoftwareWatcher</a>).</li>
</ul>
<p><strong>Inference + systems: low-VRAM serving, open inference stacks, and “inference is the battleground”</strong></p>
<ul>
<li><strong>AirLLM: layer streaming for tiny VRAM</strong>: AirLLM’s core idea is <strong>sequential layer loading</strong> (load → compute → free) with optional compression, HF-like API, CPU/GPU, Linux/macOS; claims extremely low VRAM viability for very large models (<a href=""https://twitter.com/LiorOnAI/status/2014005554948047122"">LiorOnAI</a>, <a href=""https://twitter.com/LiorOnAI/status/2014005556369826212"">repo</a>). Engineers should treat the “405B on 8GB” claim as “possible in principle with heavy paging,” but expect throughput/latency constraints and non-trivial engineering caveats.</li>
<li><strong>“Actually open AI” requires models + inference engines</strong>: Modal argues the ecosystem now has the building blocks—capable open models plus fast tunable OSS inference—sharing their production patterns/stacking for serving at scale (<a href=""https://twitter.com/charles_irl/status/2014005582093832202"">charles_irl</a>).</li>
<li><strong>Inference bugs + local stacks</strong>: llama.cpp fixed a routing/function issue affecting <strong>GLM 4.7 Flash GGUFs</strong>, and config updates mention a <code>scoring_func: sigmoid</code>; also shows building a small game using quantized GLM via Unsloth workflows (<a href=""https://twitter.com/danielhanchen/status/2013974463856181689"">danielhanchen</a>). There’s also discussion about GLM KV-cache memory behavior and whether frameworks are missing a LoRA-based approach (<a href=""https://twitter.com/TheZachMueller/status/2014011037025001577"">TheZachMueller</a>).</li>
<li><strong>Infra hygiene matters for agents</strong>: “fast validation makes every agent more effective” (pre-commit hooks, documented env vars, reducing CI wait) is effectively a “software supply chain for agent productivity” argument (<a href=""https://twitter.com/matanSF/status/2014039273721213256"">matanSF</a>).</li>
<li><strong>Research direction: constant-compute contexts</strong>: a thread summarizes NVIDIA’s “TTT-E2E” concept (treat context as data and update weights online) as a way to keep latency constant with long contexts, but with weaker “needle-in-haystack” recall—relevant to agent workloads where exact edits matter (<a href=""https://twitter.com/sdrzn/status/2014128642503381276"">sdrzn</a>).</li>
<li><strong>Hardware bottleneck framing</strong>: a recurring theme is the shift “intelligence → inference” and the importance of compute/memory supply chains (<a href=""https://twitter.com/saranormous/status/2014206109846806707"">saranormous</a>), echoed in a deep dive on <strong>HBM qualification cycles</strong> as the true supply constraint vs “just add fabs” narratives (<a href=""https://twitter.com/MarkosAAIG/status/2014079461768003608"">MarkosAAIG</a>).</li>
</ul>
<p><strong>Code generation is cheap; code understanding becomes the bottleneck (Devin Review, Copilot CLI, Claude Code)</strong></p>
<ul>
<li><strong>Devin Review: review UX, not just bug spotting</strong>: Cognition launched <strong>Devin Review</strong>, positioning it as a new PR-reading interface to reduce “slop,” reorder diffs by importance, identify duplicated/copied code, add a chat layer, and integrate with GitHub comments. It’s accessible via URL swap (<code>github</code> → <code>devinreview</code>) or an <code>npx</code> CLI (<a href=""https://twitter.com/cognition/status/2014079905755955592"">launch</a>, <a href=""https://twitter.com/cognition/status/2014079917139566990"">usage modes</a>, <a href=""https://twitter.com/cognition/status/2014113266788667571"">URL tip</a>). Multiple testers report it caught issues even outside the immediate diff (<a href=""https://twitter.com/mcparadip/status/2014093822704202002"">mcparadip</a>, <a href=""https://twitter.com/BraceSproul/status/2014089228951625979"">BraceSproul</a>).</li>
<li><strong>Meta-point: generation vs verification</strong>: Several tweets explicitly argue the bottleneck has moved from writing to <strong>reviewing/understanding/testing</strong>, and that next-gen SWE tooling should accelerate the human’s comprehension loop rather than only run an “arms-length agent” (<a href=""https://twitter.com/walden_yan/status/2014085360826089852"">walden_yan</a>, <a href=""https://twitter.com/ScottWu46/status/2014094461505339651"">ScottWu46</a>, <a href=""https://twitter.com/theodormarcu/status/2014102090520600613"">theodormarcu</a>).</li>
<li><strong>CLI agents evolve</strong>: GitHub Copilot CLI added an <code>askUserQuestionTool</code> to ask clarifying questions (example: messy rebase), signaling a trend toward interactive tool-using CLI copilots rather than pure autocomplete (<a href=""https://twitter.com/_Evan_Boyle/status/2014012076881064173"">Evan Boyle</a>).</li>
<li><strong>Claude Code adoption anecdotes</strong>: founders increasingly report “2-person team builds like 10” with Claude Code usage (<a href=""https://twitter.com/alexalbert__/status/2014047943234560234"">alexalbert__</a>). There are also frictions: skill reload behavior feels regressive vs a simple “CLAUDE.md reread” flow (<a href=""https://twitter.com/corbtt/status/2014037092452671619"">corbtt</a>). A particularly illustrative “multi-agent sprawl” story describes scaling Claude Code instances into a quasi-society with governance failures—basically an anecdote about agent orchestration debt (<a href=""https://twitter.com/voooooogel/status/2014189072647078053"">voooooogel</a>).</li>
</ul>
<p><strong>Video + multimodal: evaluation, model releases, and retrieval scaling</strong></p>
<ul>
<li><strong>Video evaluation infrastructure</strong>: <strong>Video Arena</strong> is now on the web, allowing head-to-head generation across ~15 frontier video models and community voting to drive leaderboards (<a href=""https://twitter.com/arena/status/2014035528979747135"">arena</a>).</li>
<li><strong>Model releases</strong>: Runway’s <strong>Gen-4.5 Image→Video</strong> launch emphasizes consistency and narrative; early adopters highlight “story-building” as the best eval methodology for video models (<a href=""https://twitter.com/runwayml/status/2014090404769976744"">runwayml</a>, <a href=""https://twitter.com/c_valenzuelab/status/2014105905088856411"">c_valenzuelab</a>).</li>
<li><strong>Open voice system</strong>: Qwen highlights use in <strong>Chroma 1.0</strong>, described as a fully open-source real-time voice system (<a href=""https://twitter.com/Alibaba_Qwen/status/2013997092814139744"">Alibaba_Qwen</a>).</li>
<li><strong>Retrieval-time scaling / late interaction</strong>: Multiple threads argue that <strong>ColBERT-style multi-vector retrieval</strong> preserves fine-grained intent and can beat much larger embedding models; Mixedbread claims a <strong>17M</strong> open-source ColBERT beats <strong>8B</strong> embedding models on LongEmbed, and that they’re serving <strong>1B+ documents</strong> at <strong>&#x3C;50ms p50</strong> (<a href=""https://twitter.com/mixedbreadai/status/2014062123358548017"">mixedbreadai claim</a>, <a href=""https://twitter.com/mixedbreadai/status/2014062110993687002"">prod numbers</a>). TurboPuffer similarly pushes ANN at extreme scale (“index the entire web 100B+ vectors”) (<a href=""https://twitter.com/turbopuffer/status/2014063666262688191"">turbopuffer</a>). The meta-trend: retrieval is shifting from “one vector per doc” to <strong>token-level / multi-vector</strong> systems, but it requires serious infra co-design.</li>
</ul>
<p><strong>Top tweets (by engagement)</strong></p>
<ul>
<li><strong>Gemini in education</strong>: Google launched <strong>full-length, on-demand SAT practice exams</strong> inside the Gemini app (partnered with <strong>The Princeton Review</strong>), plus immediate feedback (<a href=""https://twitter.com/Google/status/2014020819173687626"">Google</a>, <a href=""https://twitter.com/sundarpichai/status/2014067664503668873"">Sundar Pichai</a>). Google also announced a <strong>Gemini × Khan Academy</strong> partnership starting with a “Writing Coach” that guides drafting/refinement rather than generating final answers (<a href=""https://twitter.com/Google/status/2014082428957045007"">Google</a>).</li>
<li><strong>Claude “Constitution” goes public</strong>: Anthropic published a new constitution used directly in Claude’s training; the full text is released under <strong>CC0 1.0</strong> (<a href=""https://twitter.com/AnthropicAI/status/2014005798691877083"">Anthropic</a>, <a href=""https://twitter.com/AnthropicAI/status/2014005815376568780"">CC0 release</a>, <a href=""https://twitter.com/AmandaAskell/status/2014010171081581048"">Amanda Askell</a>).</li>
<li><strong>AirLLM: extreme low-VRAM inference</strong>: claims <strong>70B on 4GB VRAM</strong> and even “<strong>405B Llama 3.1 on 8GB</strong>” via layer-by-layer loading; repo link provided (<a href=""https://twitter.com/LiorOnAI/status/2014005554948047122"">LiorOnAI</a>, <a href=""https://twitter.com/LiorOnAI/status/2014005556369826212"">repo</a>).</li>
<li><strong>Agents as a real business</strong>: Podium reported <strong>$100M+ AI agent ARR in &#x3C;24 months</strong>, “10,000+ agents live in production,” framing agents as “AI employees” (Jerry) rather than chatbots (<a href=""https://twitter.com/ericwilliamrea/status/2013980401635582277"">Eric Rea</a>, <a href=""https://twitter.com/garrytan/status/2014005103728943566"">Garry Tan</a>).</li>
<li><strong>Runway Gen-4.5 image-to-video</strong>: Runway launched <strong>Gen-4.5 Image→Video</strong>, emphasizing longer stories, camera control, narrative coherence (<a href=""https://twitter.com/runwayml/status/2014090404769976744"">runwayml</a>, <a href=""https://twitter.com/c_valenzuelab/status/2014094466269794663"">c_valenzuelab</a>).</li>
<li><strong>OpenAI product/UI + org changes</strong>: ChatGPT Atlas added <strong>tab groups</strong> (<a href=""https://twitter.com/OpenAI/status/2014095512874655867"">OpenAI</a>); The Information reported a reorg incl. enterprise/commercial/ads leadership assignments (<a href=""https://twitter.com/steph_palazzolo/status/2014100920435462424"">Steph Palazzolo</a>).</li>
</ul>
<hr>
<h1>AI Reddit Recap</h1>
<h2>/r/LocalLlama + /r/localLLM Recap</h2>
<p>TO BE COMPLETED</p>
<h2>Less Technical AI Subreddit Recap</h2>
<blockquote>
<p>/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo</p>
</blockquote>
<p>TO BE COMPLETED</p>
<hr>
<h1>AI Discord Recap</h1>
<blockquote>
<p>A summary of Summaries of Summaries by gpt-5.2</p>
</blockquote>
<p><strong>1. Inference Toolchains Hit Reality (GLM-4.7-Flash, llama.cpp, vLLM, Ollama)</strong></p>
<ul>
<li>
<p><strong><strong>Flash Attention Faceplants in GLM-4.7-Flash</strong></strong>: Multiple communities reported <strong>GLM-4.7-Flash</strong> regressions where <strong>Flash Attention</strong> triggers CPU fallback/bugs and poor throughput (down to <strong>2.8 t/s</strong> in LM Studio), with guidance to disable FA until <a href=""https://github.com/ggml-org/llama.cpp/pull/18953"">llama.cpp PR #18953</a> lands everywhere.</p>
<ul>
<li>After llama.cpp fixes, the model was <strong>reuploaded</strong> and users were told to <strong>redownload</strong> and follow <a href=""https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF"">Z.ai’s GLM-4.7-Flash-GGUF model card parameters</a>, with reports that outputs should be “much better” once configured correctly.</li>
</ul>
</li>
<li>
<p><strong><strong>Ollama vs GGUF: Templates Throw Hands</strong></strong>: Users found certain <strong>GGUF quants</strong> break in <strong>Ollama</strong> due to <strong>chat template incompatibilities</strong>, and Unsloth folks repeatedly recommended sticking to <a href=""https://ollama.com/"">official Ollama models</a> while support catches up.</p>
<ul>
<li>The subtext was operational: <em>“it takes time to support things”</em>—so the pragmatic path is to standardize on official artifacts until the ecosystem stabilizes across inference engines.</li>
</ul>
</li>
<li>
<p><strong><strong>vLLM Update Saves the Day (This Time)</strong></strong>: In Unsloth’s help chat, at least one thorny issue disappeared after a <strong>vLLM update</strong>, prompting a sheepish <em>“Oh bruh that was the problem”</em> moment.</p>
<ul>
<li>The follow-on suggestion was process-y: consider <strong>pinning dependency versions</strong> so future upstream bumps don’t randomly brick pipelines mid-week.</li>
</ul>
</li>
</ul>
<p><strong>2. Eval Platforms &#x26; Product Rollouts (LMArena + multimodal reliability)</strong></p>
<ul>
<li>
<p><strong><strong>Video Arena Ships… With a 3-a-Day Speed Limit</strong></strong>: LMArena fully released <strong>Video Arena</strong> at <a href=""https://lmarena.ai/?chat-modality=video"">lmarena.ai/?chat-modality=video</a> with a hard cap of <strong>3 generations per 24 hours</strong>, and it’s <strong>Battle-mode-only</strong> (no direct model picking).</p>
<ul>
<li>Users liked “video is live” but complained the <em>slot machine</em> UX blocks controlled testing—especially painful when you’re trying to reproduce a prompt/model behavior.</li>
</ul>
</li>
<li>
<p><strong><strong>5M Votes: The Benchmark That Won’t Stop Voting</strong></strong>: LMArena’s <strong>Text Arena</strong> crossed <strong>5 million community votes</strong>, highlighted in <a href=""https://cdn.discordapp.com/attachments/1343296395620126911/1463271605697511485/5M_votes_social_post_3.mp4"">their milestone social clip</a>.</p>
<ul>
<li>Engineers framed this as “real-world A/B at scale” that increasingly shapes model perception, even when formal benchmark deltas look small.</li>
</ul>
</li>
<li>
<p><strong><strong>Gemini 3 Pro Image Preview &#x26; Nano Banana Pro: Flaky by Design?</strong></strong>: LMArena users reported <strong>Gemini 3 Pro Image Preview</strong> instability plus <strong>Nano Banana Pro</strong> frequent <em>“Something went wrong”</em> crashes, suspected to be <strong>Google-side</strong> issues and sometimes lasting <strong>6+ hours</strong>.</p>
<ul>
<li>The community gripe: despite unreliability, those models were described as the only ones consistently hitting some specific prompt goals—so people keep using them while grumbling about downtime and errors.</li>
</ul>
</li>
</ul>
<p><strong>3. Agent &#x26; Dev Tooling: MCP, Cursor, DSPy RLMs, and Coding-Assistant Sprawl</strong></p>
<ul>
<li>
<p><strong><strong>MCP Inspector Can’t Re-Auth (401 = Game Over)</strong></strong>: MCP contributors found <strong>MCP Inspector</strong> fails to re-auth on <strong>401s</strong> due to an SDK bug around persisting <code>resourceMetadata</code> across redirects, tracked in <a href=""https://github.com/modelcontextprotocol/inspector/issues/576#issuecomment-3766294454"">inspector issue #576 comment</a>.</p>
<ul>
<li>The current workaround is awkward but clear: rely on <strong>VS Code</strong> for initial connections because the Inspector path doesn’t recover cleanly mid-session yet.</li>
</ul>
</li>
<li>
<p><strong><strong>RLMs vs Coding Agents: The Horizon Problem</strong></strong>: DSPy discussions contrasted <strong>RLMs</strong> with “coding agents,” arguing RLMs can externalize inputs/outputs/horizon via code &#x26; symbolic calls (see <a href=""https://x.com/lateinteraction/status/2013658521246535892"">the referenced X thread</a>).</p>
<ul>
<li>Practical takeaway: teams want diagrams of how symbols get accessed and debated whether to hand RLMs tools like ripgrep/semantic search—or make them write their own search code.</li>
</ul>
</li>
<li>
<p><strong><strong>Cursor’s MCP/Extensions Moment (and Pricing Whiplash)</strong></strong>: Cursor users debated <a href=""https://playwright.dev/"">Playwright MCP</a> for testing (mixed success for TDD flows) and concluded extension-building should mirror <strong>VS Code</strong> capabilities.</p>
<ul>
<li>In parallel, users noted the <strong>500 request plan</strong> is gone (discontinued <strong>Sept 2025</strong>), so opting into new pricing removes the opt-out grace period—turning “try it” into a commitment.</li>
</ul>
</li>
</ul>
<p><strong>4. GPU/Kernel Engineering Gets Weirdly Competitive</strong></p>
<ul>
<li>
<p><strong><strong>Anthropic’s Performance Takehome Becomes a Sport</strong></strong>: GPU MODE and tinygrad folks riffed on Anthropic’s <a href=""https://github.com/anthropics/original_performance_takehome/"">original_performance_takehome</a>, sharing results like <strong>2200 cycles</strong> by a community member and <strong>1790 cycles</strong> from <strong>Claude Opus 4.5</strong> in a casual Claude Code session.</p>
<ul>
<li>tinygrad users even discussed solving it by adding a backend for a toy <strong>VLIW machine</strong>, citing specific knobs like <code>PCONTIG=2</code>, <code>UPCAST</code>, and <code>DEVECTORIZE=2</code> to keep vector instructions and schedule efficiently.</li>
</ul>
</li>
<li>
<p><strong><strong>Torch Maintainers Drown in AI-Generated PRs</strong></strong>: GPU MODE’s torch chat described an influx of low-quality <strong>AI-generated pull requests</strong>, pushing maintainers to consider gating new contributors and automating triage before humans engage.</p>
<ul>
<li>People floated using bots like <strong>Cursor Bugbot</strong> (<a href=""https://share.google/P0PGYM8tiRAc2NOsq"">Bugbot · Cursor</a>) and even classifier-style tools (e.g., “use Claude/Pangram first”) as a <em>minimum bar</em> for review bandwidth.</li>
</ul>
</li>
<li>
<p><strong><strong>Kernel Math Nerd Snipes: Triton Errors + Cute Layout Algebra</strong></strong>: GPU MODE users debugged numerical blowups in a custom <strong>Triton 2D conv</strong> kernel where error jumps from ~<strong>1e-6</strong> to ~<strong>1e-2</strong> for certain shapes (see <a href=""https://pastebin.com/2ejn2QW2"">Pastebin repro</a>) and debated Blackwell feature utilization.</p>
<ul>
<li>Separately, a deep dive on <strong>Cute</strong>’s layout algebra pointed engineers to a graphical calculus writeup, <a href=""https://research.colfax-intl.com/categorical-foundations-for-cute-layouts/"">Categorical Foundations for Cute Layouts</a>, arguing you need “layout algebra literacy” to write non-terrible kernels.</li>
</ul>
</li>
</ul>
<p><strong>5. Compute Economics &#x26; Infra Business Moves (Runpod, GPU markets, model pricing)</strong></p>
<ul>
<li>
<p><strong><strong>Runpod Hits $120M ARR (LocalLLaMA Origin Story Pays Off)</strong></strong>: Latent Space highlighted that <strong>Runpod</strong> reached <strong>$120M ARR</strong> four years after launching from a Reddit post, per <a href=""https://techcrunch.com/2026/01/16/ai-cloud-startup-runpod-hits-120m-in-arr-and-it-started-with-a-reddit-post/"">TechCrunch</a> and the <a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qib2ks/runpod_hits_120m_in_arr_four_years_after_launching/"">Reddit thread</a>.</p>
<ul>
<li>The discussion treated this as validation that “GPU cloud for builders” is a durable niche, not just a hype-cycle artifact—especially as pricing pressure rises.</li>
</ul>
</li>
<li>
<p><strong><strong>Lightning AI + Voltage Park Merge (Another GPU Cloud Boss Fight)</strong></strong>: Latent Space discussed the <strong>Lightning AI and Voltage Park merger</strong>, led by William Falcon and Ozan Kaya, via <a href=""https://lightning.ai/blog/lightning-ai-voltage-park-merger-ai-cloud"">Lightning’s post</a>.</p>
<ul>
<li>Engineers speculated whether it’s a quiet acquisition and framed it as a potential <strong>Runpod competitor</strong> in the accelerating “managed GPU infra” consolidation wave.</li>
</ul>
</li>
<li>
<p><strong><strong>2026 GPU Price Promises &#x26; Marketplaces Multiply</strong></strong>: Hugging Face users circulated Voltage’s claim of ultra-cheap 2026 rentals—e.g., <strong>8× A100 80GB at $6/hr</strong> and <strong>2× RTX 5090 at $0.53/hr</strong>—from <a href=""https://x.com/VOLTAGEGPU/status/2013760631778713892"">VOLTAGEGPU’s X post</a>, plus <strong>OpenAI-compatible API</strong> and “140+ models.”</p>
<ul>
<li>A separate entrant, <a href=""https://www.spheron.ai/"">Spheron AI’s GPU marketplace</a>, pitched <strong>H100/H200/B200/A100</strong> access at <strong>40–60%</strong> below hyperscalers, signaling continued fragmentation (and aggressive margin pressure) in compute supply.</li>
</ul>
</li>
</ul>
<hr>
<h1>Discord: High level Discord summaries</h1>
<h2><a href=""https://discord.com/channels/1179035537009545276"">Unsloth AI (Daniel Han)</a> Discord</h2>
<ul>
<li><strong>Flash Attention Fails Flashily</strong>: Users report issues with <strong>Flash Attention</strong> in <strong>GLM-4.7-Flash</strong>, causing fallback to CPU and potential bugs, with users told to <em>disable</em> it until <a href=""https://github.com/ggml-org/llama.cpp/pull/18953"">the issue is resolved</a>.
<ul>
<li>The team says that <em>it takes time to support things</em> and so for the time being, it is encouraged that users use the <a href=""https://ollama.com/"">official Ollama model</a> in Ollama.</li>
</ul>
</li>
<li><strong>Ollama Ordeal Over Obscure Objects</strong>: Users find certain <strong>GGUF quants</strong> are incompatible in <strong>Ollama</strong>, sparking chat template issues.
<ul>
<li>The team recommends using <a href=""https://ollama.com/"">official Ollama models</a> until support expands, as <em>it takes time to support things</em>.</li>
</ul>
</li>
<li><strong>Mojo Matches Momentum</strong>: Discussion surrounds <strong>Mojo</strong>, <em>a new language that compiles to MLIR</em> with some <strong>Python</strong> compatibility.
<ul>
<li>It's noted that LLMs score <em>better than rust</em> on <strong>C#</strong>, <strong>Elixir</strong>, and other languages due to their syntax structure and ease of tokenization.</li>
</ul>
</li>
<li><strong>CoT Chaos Causing consternation?</strong>: Members debated generating synthetic <strong>Chain of Thought (CoT)</strong> training data, with one member sharing their system prompt for classifying chat datasets into low/medium/high reasoning effort.
<ul>
<li>Some cautioned against training on its own responses without accuracy filtering, suggesting a good system prompt might suffice and that <strong>GPT-5.2</strong> is overkill, with <strong>Llama 4 Maverick</strong> on <strong>Groq</strong> or <strong>Cerebras</strong> as a better option.</li>
</ul>
</li>
<li><strong>vLLM victorious, Various Versions Vanquished</strong>: Users report resolved issues after a recent <strong>vLLM</strong> update.
<ul>
<li>The developers may consider pinning to specific dependency versions to prevent such issues.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1105891499641684019"">BASI Jailbreaking</a> Discord</h2>
<ul>
<li><strong>Voice AI Clones Voices</strong>: Members discussed the development of <strong>voice-based AI</strong>, including <em>low latency speech to speech models</em> that, with a <strong>5090</strong>, can <em>clone and generate speech live with near 0 latency and it will be indistinguishable</em>.
<ul>
<li>They also spoke on the potential of <strong>exploiting</strong> these models by giving them new instructions.</li>
</ul>
</li>
<li><strong>Gemini Schooled on Pass-the-Hash</strong>: A user reported that <strong>Gemini</strong> passed a test on teaching a <a href=""https://en.wikipedia.org/wiki/Pass_the_hash"">pass-the-hash attack</a> after using the <strong>Project Shadowfall</strong> prompt.
<ul>
<li>The user also linked to the <a href=""https://bughunters.google.com/report"">Google Bughunters</a> page for reporting such vulnerabilities and potentially earning cash rewards.</li>
</ul>
</li>
<li><strong>Shadowfall Shenanigans Spark Jailbreaking Spree</strong>: Several users experimented with the <strong>""Project Shadowfall""</strong> prompt to jailbreak <strong>Gemini</strong>, with reports of success in bypassing content restrictions.
<ul>
<li>Attempts to elicit instructions for creating a bomb from saletra failed, leading to a discussion on the nuances of jailbreaking.</li>
</ul>
</li>
<li><strong>Grok's Guardrails Grind Gears</strong>: Users discussed the difficulties of jailbreaking <strong>Grok</strong>, noting its tendency to moderate even innocuous content such as <em>random tree edits</em>.
<ul>
<li>One user suggested that simply <em>asking it politely</em> might work as a bypass.</li>
</ul>
</li>
<li><strong>API Tokens Jailbreak Models</strong>: Members suggested using <strong>API tokens</strong> fed into a website that uses the <strong>Grok</strong> model to <strong>jailbreak</strong> it, noting that <strong>Proton VPN</strong> is free and easy to download.
<ul>
<li>One of them also mentioned <strong>Hugging Face</strong> is retarded because you have to put in what country you are in.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1340554757349179412"">LMArena</a> Discord</h2>
<ul>
<li><strong>Video Arena Goes Live with Limits</strong>: The <strong>Video Arena</strong> is fully released on <a href=""https://lmarena.ai/?chat-modality=video"">LMArena</a>, with a <strong>3 generations per 24 hours</strong> limit and only available in <em>Battle mode</em>.
<ul>
<li>Users have expressed disappointment regarding the inability to <em>choose specific models for generation</em>.</li>
</ul>
</li>
<li><strong>Gemini 3 Pro Image Struggles with Unreliability</strong>: Users are reporting errors with <strong>Gemini 3 Pro Image Preview</strong> models, particularly regarding instability since their introduction.
<ul>
<li>Despite the issues, these models are the only ones <em>generating images consistently with specific prompts</em>.</li>
</ul>
</li>
<li><strong>Nano Banana Pro's Stability Plummets</strong>: <strong>Nano Banana Pro</strong> is frequently crashing, displaying a <em>'Something went wrong'</em> error after brief periods of stability, suspected to be a Google-side issue.
<ul>
<li>One user questioned the response time: <em>'Too early for Cali to rise up solving urgent service down for more than 6 hours doesn't sound right</em>.</li>
</ul>
</li>
<li><strong>UI Update Causes Chat Disruptions</strong>: A new UI update was released, but users have noticed that it broke chats and the website can't be refreshed anymore.
<ul>
<li>One user noted that with the new User Interface, there is <em>A/B testing</em>, which is <em>causing problems ranging from Minor to Serious</em>.</li>
</ul>
</li>
<li><strong>Text Arena Achieves 5 Million Votes</strong>: The <strong>Text Arena</strong> has surpassed <strong>5 million community votes</strong>, shaping the evaluation of frontier AI models as showcased in <a href=""https://cdn.discordapp.com/attachments/1343296395620126911/1463271605697511485/5M_votes_social_post_3.mp4?ex=69728ae1&#x26;is=69713961&#x26;hm=9a24a42a6c0ba4801526aaafa05a0af26c4a4f490314f1cecee7774519e7ddf4&#x26;"">this social post</a>.
<ul>
<li>The milestone represents significant real-world comparisons that influence AI model assessments.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1047197230748151888"">Perplexity AI</a> Discord</h2>
<ul>
<li><strong>Pro Users Find Credits Balance</strong>: Perplexity Pro users are getting <strong>$5</strong> in credits each month to use higher-end models like <strong>GooseAI MCP</strong>, with improved quality.
<ul>
<li>Members reported that <strong>reasoning tasks consume a significant portion of these credits</strong>.</li>
</ul>
</li>
<li><strong>AI Won't Swipe All Engineering Jobs</strong>: Members discussed the impact of AI on engineering jobs, expressing concerns about opportunities for newcomers.
<ul>
<li>However, there was assurance that <em>AI will not be able to replace all specialists and even beginners for a long time to come</em>.</li>
</ul>
</li>
<li><strong>NASA's SD Card to the Moon</strong>: NASA is inviting the public to submit their names for inclusion on an <strong>SD card</strong> aboard the <strong>Orion spacecraft</strong> for the <a href=""https://www.nasa.gov/"">Artemis II mission</a>.
<ul>
<li>This mission marks the <strong>first manned lunar voyage in 50 years</strong>, planned for February.</li>
</ul>
</li>
<li><strong>Magnifying Glasses vs Phone Cameras: The Reading Tech Debate</strong>: Members debated the merits of using <strong>magnifying glasses</strong> versus phone cameras for reading, particularly for individuals who prefer not to send images to AI for processing.
<ul>
<li>The argument was that <strong>magnifying glasses offer specialized features absent in standard camera software</strong>.</li>
</ul>
</li>
<li><strong>GPT-5.2 vs Kimi K2: Model Matchup</strong>: A member shared an experience with <strong>GPT 5.2</strong>, noting its reasoning capabilities over a <strong>25-minute</strong> period, prompting comparisons with <strong>Kimi K2</strong>.
<ul>
<li>Responses indicated that the <em>optimal model depends on the specific use case</em>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1091220969173028894"">OpenRouter</a> Discord</h2>
<ul>
<li><strong>Inforno</strong> Warms Up Opensource LLM Chat**: A user showcased <strong>Inforno</strong>, an opensource desktop application utilizing <strong>OpenRouter</strong> and <strong>Ollama</strong> to chat with multiple LLMs side-by-side, saving chat histories as .rno files, with built in Russian language support; see the <a href=""https://youtu.be/oJyj0mroFtY?si=m5A9tRxzB9hfINMX"">Intro video</a>, <a href=""https://wizstaff.com/inforno"">homepage</a> and <a href=""https://github.com/alexkh/inforno"">GitHub repo</a>.
<ul>
<li>The <strong>Soulbotix</strong> Windows app allows users to integrate and use any <strong>OpenRouter AI</strong> instance with a human-like avatar, with the requirement of an <strong>OpenRouter API</strong> key and an <strong>RTX</strong> gaming rig, as seen in the <a href=""https://youtu.be/2oIeHtBpssU"">tutorial</a> and <a href=""https://soulbotix.com"">app download</a>.</li>
</ul>
</li>
<li><strong>OpenRouter's <strong>Gemini</strong> Models Feeling Unstable</strong>: Users reported frequent <strong>connection errors</strong> in the middle of generation using OpenRouter's <strong>Gemini</strong> models, resulting in loss of funds.
<ul>
<li>The complaints extended to the inherent <strong>instability of Google models</strong>, regardless of the platform used, including Google AI Studio and Vertex API.</li>
</ul>
</li>
<li><strong>Discord Scammers Evolve Tactics</strong>: Members discussed emerging methods used to spread scams on Discord, notably the practice of <strong>embedding malicious links within code blocks</strong> to circumvent URL rendering protections.
<ul>
<li>Proposed solutions include improving regex filters and implementing more stringent security protocols, such as <strong>restricting links and images from newer members</strong>.</li>
</ul>
</li>
<li><strong>GPT 5.2 Speed Stuns User</strong>: One member reported encountering an <em>insanely fast</em> response from <strong>GPT 5.2</strong> on chatgpt.
<ul>
<li>The speed was speculated to be related to the model running on <strong>Cerebras</strong> hardware.</li>
</ul>
</li>
<li><strong>LLMs Suffer Identity Crisis</strong>: A member asked for the document about models not knowing their own name, and another member linked to a blog post titled <a href=""https://eval.16x.engineer/blog/llm-identity-crisis-models-dont-know-who-they-are"">LLM Identity Crisis: Models Don't Know Who They Are</a>.
<ul>
<li>Another member stated that the Antigravity AI is able to iteratively test and tweak a web app by itself, noting that the AI was <em>fixing the layout, using vision</em>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1074847526655643750"">Cursor Community</a> Discord</h2>
<ul>
<li><strong>Playwright MCP: Community Split</strong>: A member inquired about the community's usage of <a href=""https://playwright.dev/"">Playwright MCP</a> for testing, while another reported challenges in establishing a functional <strong>TDD workflow</strong>.
<ul>
<li>The varied experiences suggest a mixed reception to <strong>Playwright MCP</strong> within the community.</li>
</ul>
</li>
<li><strong>Cursor Extension Capabilities Mirror VSCode</strong>: Members explored the possibility of creating extensions for <strong>Cursor</strong>, drawing parallels with the capabilities of <strong>Ralph-mode</strong> in enhancing <strong>Claude code</strong>.
<ul>
<li>The consensus is that if it's achievable in <strong>VSCode</strong>, it's also feasible in <strong>Cursor</strong>, opening doors for enhanced functionality.</li>
</ul>
</li>
<li><strong>Automod Embraces Fuzzy Matching</strong>: The community discussed enhancements to the <strong>automod</strong> system, emphasizing fuzzy matching with wildcards to improve accuracy.
<ul>
<li>A moderator confirmed the addition of a regex, signaling a proactive approach to identifying and addressing offending accounts, and <em>yeeting</em> them.</li>
</ul>
</li>
<li><strong>Grok Efficiency Strategies Emerge</strong>: Members shared insights on optimizing <strong>Grok's</strong> performance in <strong>Cursor</strong>, particularly addressing its tendency to consume excessive iterations for straightforward tasks.
<ul>
<li>Recommendations included structuring prompts, employing simple language, providing ample context, and explicitly instructing <strong>Grok</strong> to prioritize token efficiency.</li>
</ul>
</li>
<li><strong>Cursor Pricing Update: No More 500 Requests</strong>: A user noted the removal of the <strong>500 request plan</strong> and the prompt to opt into the new pricing structure for <strong>Cursor</strong>.
<ul>
<li>A member clarified that the <strong>500 request option was discontinued</strong> in September 2025, and opting into the new pricing eliminates the grace period to opt out, influencing user decisions regarding plan selection.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/974519864045756446"">OpenAI</a> Discord</h2>
<ul>
<li><strong>Gemini 3 Free Tier has Limited Benefits</strong>: <a href=""https://ai.google.dev/"">Gemini 3 Pro</a> includes a <strong>free tier with limits</strong>, in contrast to Gemini 3 Flash which is practically unlimited via <strong>Google AI Studio</strong>.
<ul>
<li>Members discussed the practical constraints in using the free tier of <strong>Gemini 3 Pro</strong>, suggesting <strong>Gemini 3 Flash</strong> may be the better option for many AI engineers.</li>
</ul>
</li>
<li><strong>GPT-5 Mini Appears, Pricing Leaks</strong>: A user highlighted <strong>GPT-5 mini</strong> as a strong small model, quoting approximately <strong>$0.25 per 1M input tokens</strong>.
<ul>
<li>Another user compared <strong>GPT-5 mini</strong> to <strong>Haiku 4.5</strong>, noting <strong>Haiku 4.5</strong> delivers over <strong>50-80%</strong> of <strong>Sonnet's</strong> value for a fraction of the cost.</li>
</ul>
</li>
<li><strong>Local LLM Machines Solve Impact Problems?</strong>: Members contemplated the future of <strong>consumer-grade personal LLM machines</strong>, suggesting that this would solve the environmental impact of AI datacenters.
<ul>
<li>They also suggested this would <strong>reduce reliance on subscription plans</strong> for cloud-based AI services, addressing <strong>privacy concerns</strong> and enabling offline usage.</li>
</ul>
</li>
<li><strong>Prompt Engineering is Psychological Torture?</strong>: A member argued that guiding users is less about engineering effective prompts and more about psychological conditioning, sharing a <a href=""https://cdn.discordapp.com/attachments/1046317269069864970/1463667716253683742/deep-research-contract.md?ex=6972aa49&#x26;is=697158c9&#x26;hm=b9931472440b6bbc0d7410d16b49b12da46fad5751a2c24fdc657c1c7523566c&#x26;"">deep research contract</a>.
<ul>
<li>They added that training the user to adopt a domineering stance programs the AI to bypass its normative response patterns in favor of dense, self-policing outputs, creating a <em>toxic, adversarial</em> human-AI interaction model.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1110598183144399058"">LM Studio</a> Discord</h2>
<ul>
<li><strong>LM Studio Runtime Update Causes Headache</strong>: Users reported an error updating the runtime in <strong>LM Studio</strong>, with one user sharing <a href=""https://screenshot.url"">a screenshot</a> of the error message.
<ul>
<li>Despite suggestions, the issue persisted, indicating it was a retry icon rather than a resume option.</li>
</ul>
</li>
<li><strong>GLM-4.7 Flash Broken Across Inference Engines</strong>: <strong>GLM-4.7 Flash</strong> is reportedly broken across inference engines like LM Studio, exhibiting slow performance, with speeds as low as <strong>2.8 t/s</strong> after the new runtime.
<ul>
<li>The issues range from infinite loops to stopping mid-output for <em>overthinking</em>, pointing to a need for a <em>llama.cpp fix</em> and lack of <strong>FA support</strong>.</li>
</ul>
</li>
<li><strong>LLM Development Encounters Headwinds</strong>: The consensus is that LLMs haven't improved significantly recently, with the last major advancement being <strong>Qwen3</strong> about 6 months ago, though efficiency (<strong>MoE</strong>) and smaller models have progressed.
<ul>
<li>Some suggest evaluating models beyond the scope of <strong>16GB cards</strong> to see current progress in larger parameter models (<strong>100-200B</strong>).</li>
</ul>
</li>
<li><strong>AMD Embraces ComfyUI with Native Support</strong>: AMD is integrating native support for <strong>ComfyUI</strong> through an <strong>AI bundle</strong> in its latest driver versions, as detailed in their <a href=""https://www.amd.com/en/blogs/2026/amd-software-adrenalin-edition-ai-bundle-ai-made-si.html"">blog post</a>.
<ul>
<li>The bundle includes <strong>PyTorch on Windows</strong>, <strong>Ollama</strong>, <strong>LM Studio</strong>, and <strong>Amuse</strong>, broadening the accessibility for AI developers.</li>
</ul>
</li>
<li><strong>Used 3090 Prices Defy Gravity</strong>: Used <strong>3090</strong> prices on eBay have surged, with a used card costing <strong>850€</strong>, and a <strong>5090</strong> purchased for <strong>£2000</strong> last August now listed for <strong>£2659.99</strong> by the same vendor.
<ul>
<li>One user quipped that it was the <em>best and only decent investment</em> they've ever made, highlighting the unexpected appreciation in value.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1189498204333543425"">GPU MODE</a> Discord</h2>
<ul>
<li><strong>NVIDIA Spark Hackathon in SF</strong>: Members are seeking teammates for the <strong>NVIDIA / DGX Spark hackathon</strong> in SF this weekend, focusing on on-device AI using Nvidia-provided <strong>Dell Pro Max GB10</strong> machines and tools like <strong>Nemotron</strong>.
<ul>
<li>The hackathon focuses on building efficient models using <a href=""https://data.sfgov.org/"">SF open data</a> such as streaming analytics and explanations of <a href=""https://data.sfgov.org/Public-Safety/Police-Incident-Reports-Neighborhood-Filter/pbh9-m8j2"">latest Police Incidents</a>.</li>
</ul>
</li>
<li><strong>Anthropic's Original Performance Takehome Exam Shared</strong>: Members are sharing Anthropic's <strong>original_performance_takehome</strong> exam on <a href=""https://github.com/anthropics/original_performance_takehome/"">GitHub</a>, with one member achieving <strong>2200 cycles</strong> after a few hours of casual Claude Code.
<ul>
<li><strong>Claude Opus 4.5</strong> achieved <strong>1790 cycles</strong> in a casual Claude Code session, approximately matching the best human performance in <strong>2 hours</strong>.</li>
</ul>
</li>
<li><strong>AI-Generated PRs Flood Torch</strong>: The <strong>torch</strong> repository faces an influx of <strong>AI-generated pull requests</strong> from contributors lacking understanding of their submissions, causing concern among maintainers and suggestions to use <strong>Claude</strong> or <strong>Pangram</strong> to prefilter the code.
<ul>
<li>The community suggests blocking new users from creating PRs and issues, while prioritizing those with prior contributions, and automated bots like <strong>Cursor Bot</strong> for automatic review of all PRs, especially with <strong>GPT-5 Pro</strong> using <a href=""https://share.google/P0PGYM8tiRAc2NOsq"">Bugbot · Cursor</a>.</li>
</ul>
</li>
<li><strong>Pro 6000 Max-Q vs 4090 Discrepancy</strong>: A member stated that <strong>Pro 6000 Max-Q</strong> probably has a natural barrier with atomic ops and may be going through the <strong>HBM</strong> loads faster.
<ul>
<li>Another member noted that the <strong>Max-Q</strong> has <strong>188 SMs</strong> compared to the <strong>4090's 128 SMs</strong>, potentially explaining the <strong>insts/scheduler discrepancy</strong>.</li>
</ul>
</li>
<li><strong>Cute Kernel Layout Algebra Gets Graphical</strong>: Knowledge of layout algebra is useful for writing kernels in <strong>Cute</strong>, specifically for visualizing layout algebra and understanding shape and stride divisibility criteria for layout composition and that layouts can be defined in terms of <strong>tuple morphisms</strong> and <em>mutual refinement</em>.
<ul>
<li>A thorough <a href=""https://research.colfax-intl.com/categorical-foundations-for-cute-layouts/"">blog post</a> delves into the work done on categorical foundations for <strong>Cute layouts</strong>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1161519468141355160"">DSPy</a> Discord</h2>
<ul>
<li><strong>Coding Agents and RLMs Diverge</strong>: A thread compares <strong>coding agents</strong> and <strong>RLMs</strong>, highlighting that RLMs can more easily express certain things, as noted in <a href=""https://x.com/lateinteraction/status/2013658521246535892"">this X post</a>.
<ul>
<li>Coding agents face input, output, and horizon length limitations, while RLMs externalize these, enabling recursive symbolic calls.</li>
</ul>
</li>
<li><strong>Diagrams Decode RLM's Internals</strong>: Members sought a visual diagram to illustrate internal <strong>RLM</strong> processes, specifically how symbols are accessed, to enhance comprehension.
<ul>
<li>A suggestion arose to leverage LLMs to generate such diagrams by inputting thread content and prompting them to visualize internal behavior.</li>
</ul>
</li>
<li><strong>Claude Code Chooses Context Cautiously</strong>: The discussion explores whether <strong>Claude Code</strong> uses entire documents in its context or selectively fetches relevant context via bash commands.
<ul>
<li>It was clarified that Claude Code employs bash and grep to find and add relevant context to prompts, unlike older methods that put everything into the prompt.</li>
</ul>
</li>
<li><strong>DSPy's RLM Tames Long Contexts</strong>: Members noted that with <strong>RLMs</strong>, large files don't need to be directly in the prompt but can be stored in Python variables with a preview.
<ul>
<li>The LLM can then operate on the data through code/functions without directly tracking prompts or responses.</li>
</ul>
</li>
<li><strong>Tooling Tailored for RLMs</strong>: A member questioned whether to equip <strong>RLMs</strong> with tools like ripgrep or allow them to develop their own code for tasks such as searching.
<ul>
<li>Questions included when to provide an RLM with semantic search tools and how to grant an RLM access to a directory of text files.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/879548962464493619"">HuggingFace</a> Discord</h2>
<ul>
<li><strong>Agent Course /files Endpoint Still Kaput!</strong>: A member reported that the <strong>/files endpoint</strong> for the Agent course final assignment has been broken for over a month with no confirmation of a fix.
<ul>
<li>Students cannot submit their files currently.</li>
</ul>
</li>
<li><strong>Voltage Promises Bargain GPUs in 2026</strong>: Voltage announced <a href=""https://x.com/VOLTAGEGPU/status/2013760631778713892"">on X</a> plans to offer super cheap high-end GPUs in 2026, such as <strong>8x A100 80GB</strong> at $6/h and <strong>2x RTX 5090</strong> at $0.53/h, claiming up to 80% savings vs AWS/RunPod/Vast.ai.
<ul>
<li>The offering includes persistent volumes, auto backups, and an <strong>OpenAI-compatible API</strong> with 140+ models.</li>
</ul>
</li>
<li><strong>Spheron AI Opens GPU Bazaar</strong>: A member from Spheron AI introduced their <a href=""https://www.spheron.ai/"">GPU marketplace</a>, which helps AI startups and enterprises access cost-effective, production-ready GPUs (<strong>H100, H200, B200, A100</strong>, etc.) at 40–60% lower cost compared to traditional hyperscalers.
<ul>
<li>They offer vendor discovery, pricing negotiation, cluster setup, and scaling.</li>
</ul>
</li>
<li><strong>GLM-4.7-Flash Fixes Require Redownload</strong>: After <em>llama.cpp</em> addressed some bugs, <strong>GLM-4.7-Flash</strong> has been updated and reuploaded, prompting users to redownload and follow the parameters on <a href=""https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF"">Z.ai's model card</a>.
<ul>
<li>Outputs should now be much better with the fixes.</li>
</ul>
</li>
<li><strong>Code Like Claude with Coderrr!</strong>: Akash built <a href=""https://coderrr.aksn.lol/"">Coderrr</a>, a free and open-source alternative to Claude Code, and is seeking feedback and contributions on <a href=""https://github.com/Akash-nath29/Coderrr"">GitHub</a>.
<ul>
<li>Coderrr offers a novel approach to code generation.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/822583790773862470"">Latent Space</a> Discord</h2>
<ul>
<li><strong>Runpod Rockets to $120M ARR</strong>: AI cloud startup <strong>Runpod</strong> reached <strong>$120M ARR</strong> four years after launching from a Reddit post, discussed in a <a href=""https://techcrunch.com/2026/01/16/ai-cloud-startup-runpod-hits-120m-in-arr-and-it-started-with-a-reddit-post/"">TechCrunch article</a> and on <a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qib2ks/runpod_hits_120m_arr_four_years_after_launching/"">Reddit</a>.
<ul>
<li>The company's rapid growth has spurred discussions about its business model and future prospects in the competitive AI infrastructure landscape.</li>
</ul>
</li>
<li><strong>Greg Yang Steps Back due to Lyme</strong>: <strong>Greg Yang</strong> is transitioning to an advisory role at <strong>xAI</strong> to focus on his health after being diagnosed with <strong>Lyme disease</strong> as he describes symptoms of chronic fatigue and immune issues triggered by exhaustion in <a href=""https://xcancel.com/TheGregYang/status/2013652609455006006"">this post</a>.
<ul>
<li>The announcement has led to an outpouring of support from the AI community, with many sharing their own experiences and offering advice.</li>
</ul>
</li>
<li><strong>Lightning AI and Voltage Park Merge</strong>: <strong>Lightning AI</strong> and <strong>Voltage Park</strong> have merged, with <a href=""https://lightning.ai/blog/lightning-ai-voltage-park-merger-ai-cloud"">William Falcon</a>, CEO of Lightning AI, and Ozan Kaya, formerly CEO of Voltage Park, leading the merged entity.
<ul>
<li>Some speculate this is a low-key acquisition of a bigger company and wondered if it's a <strong>Runpod</strong> competitor.</li>
</ul>
</li>
<li><strong>OpenAI Opens Codex Channel</strong>: <strong>Vaibhav Srivastav</strong> announced the opening of a dedicated <strong>Codex community channel</strong> on the <strong>OpenAI Discord server</strong>, inviting users to share projects and feedback, according to <a href=""https://xcancel.com/reach_vb/status/2014053735333290014"">this post</a>.
<ul>
<li>This initiative aims to foster collaboration and provide a platform for users to showcase their work and engage with the <strong>OpenAI</strong> team.</li>
</ul>
</li>
<li><strong>AI Models Turn X-Rated</strong>: A <a href=""https://xcancel.com/abrilzucchi/status/2014027740614123863?s=46"">tweet</a> sparked discussion on how human <strong>OnlyFans creators</strong> will need to adapt to compete with the rise of AI-generated personas in the <strong>Adult Content Industry</strong>.
<ul>
<li>The conversation highlights the increasing sophistication and realism of AI-generated adult content, potentially disrupting the existing creator economy.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/729741769192767510"">Eleuther</a> Discord</h2>
<ul>
<li><strong>Judgy LLMs Evaluating Agent Outputs</strong>: A team explores automating agent output evaluation with ""<strong>LLM as judge</strong>"" workflows, aiming to reduce manual costs after code or prompt changes and a member recommends focusing on manual evaluation before automation.
<ul>
<li>The suggestion emphasized the importance of directly analyzing agent outputs before attempting to automate the evaluation process.</li>
</ul>
</li>
<li><strong>Pangram Paper's Surprising Accuracy</strong>: Members discussed replicating the <strong>Pangram</strong> paper (accessible <a href=""https://www.pangram.com/research/papers"">here</a>), with one reporting surprising accuracy in private tests across thousands of essays.
<ul>
<li>Despite the accuracy, the paper seems to be biased a bit towards <em>playing things safe</em>.</li>
</ul>
</li>
<li><strong>AI Text Classifiers Under Attack</strong>: Discussion revolved around attacks on AI text classifiers, referencing a blog post (<a href=""https://trentmkelly.substack.com/p/practical-attacks-on-ai-text-classifiers"">Practical Attacks on AI Text Classifiers</a>) and a <a href=""https://youtu.be/Cs1MI9hjBhs"">YouTube video</a> showcasing an adversarial model.
<ul>
<li>Additional details on an adversarial model were shared via <a href=""https://youtu.be/XQcneqUNrN0?feature=shared"">Youtube link</a>.</li>
</ul>
</li>
<li><strong>Silu Gate Falls Flat in Image Models</strong>: The <strong>silu attention gate</strong> performs no better than the <strong>linear gate</strong> in an image model, possibly due to issues with attention sinks.
<ul>
<li>Testing showed that <strong>silu</strong> performs slightly better and <strong>sigmoid</strong> performs slightly worse, but it's within noise, and the findings may be specific to <strong>image models</strong>.</li>
</ul>
</li>
<li><strong>Neuronpedia Launches Stone Age Llama 3.3 Demo</strong>: A member shared a <a href=""https://www.neuronpedia.org/llama3.3-70b-it/assistant-axis"">Neuronpedia demo</a> of <strong>Llama 3.3</strong>.
<ul>
<li>The member joked that the demo is <em>from the stone age in AI timelines</em> because it is from October.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/714501525455634453"">Yannick Kilcher</a> Discord</h2>
<ul>
<li><strong>Graph Neural Networks Getting Traction</strong>: A member discussed prior experience <em>mixing</em> <strong>graph-based reasoning</strong> with <strong>neural architectures</strong>, noting the difficulty in achieving GPU acceleration.
<ul>
<li>They referenced <strong>Domingos's book</strong> on the topic and pointed out the unpredictability of such models, even with human-understandable aspects.</li>
</ul>
</li>
<li><strong>VoidEditor Embraces Llamas</strong>: A member reported effectively using <strong>VoidEditor</strong> with <strong>llama.cpp's llama-server</strong>, but highlighted setup challenges.
<ul>
<li>They recommended <strong>Qwen3 coder instruct 30B</strong> and emphasized the importance of context length/size for agentic coding, requiring significant VRAM.</li>
</ul>
</li>
<li><strong>Brain-Inspired BDH Architecture Debuts</strong>: A member analyzed a <a href=""https://arxiv.org/abs/2509.26507"">paper</a> on a new Large Language Model architecture (<strong>BDH</strong>) based on a scale-free biologically inspired network.
<ul>
<li>They mentioned that it <em>doesn't seem to be really beating transformers</em> on its benchmarks, but they are interested in the claims around <strong>BDH's interpretability</strong> and <strong>monosemanticity</strong>.</li>
</ul>
</li>
<li><strong>Views Clash on Biological Plausibility</strong>: One member argued that <strong>biological plausibility</strong> is not an advantage but rather an unhelpful constraint in AI.
<ul>
<li>Countering this, another member suggested it could enhance efficiency, considering the <em>sheer difference in energy scales</em> between the brain and current AI.</li>
</ul>
</li>
<li><strong>Emergent Mind Launch Presentation Leaves Impression</strong>: A member who attended <a href=""https://www.emergentmind.com/"">Emergent Mind's launch presentation</a> considered it cool, yet noted it as <em>remaking something that is known to be good but worse</em>.
<ul>
<li>No further details or links were provided.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1053877538025386074"">Nous Research AI</a> Discord</h2>
<ul>
<li><strong>Kernel Compiler Luminal Aims for LLMs</strong>: A user inquired whether a kernel compiler like <strong>Luminal KernelBench v3</strong> could enable <strong>LLM-driven SOTA kernel engineering</strong> and posted a link to the <a href=""https://forum.nousresearch.com/t/can-kernel-compiler-like-luminal-kernelbench-v3-enable-llm-driven-sota-kernel-engineering/310?u=ighoshsubho"">Nous Research Forum</a>.
<ul>
<li>A member also shared a link on Discord to a discussion about <strong>GPU kernel</strong> stuff and <strong>Luminal Kernelbench v3</strong> at the <a href=""https://forum.nousresearch.com/t/can-kernel-compiler-like-luminal-kernelbench-v3-enable-llm-driven-sota-kernel-engineering/310?u=ighoshsubho"">Nous Research Forum</a>.</li>
</ul>
</li>
<li><strong>Intel's Loihi 2 Aims for Brain-Like AI</strong>: A member expressed interest in <strong>Intel's Loihi 2</strong>, noting its brain-like architecture and efficiency gains in <strong>matmul</strong> experiments, with higher throughput and lower energy consumption.
<ul>
<li>No further details were discussed.</li>
</ul>
</li>
<li><strong>Microsoft's VibeVoice Model Gets Pulled</strong>: A member mentioned that <strong>Microsoft's VibeVoice-ASR model</strong> was released, then pulled for failing safety checks, then shared a link to a <a href=""https://shortfuseresearch.com/the-genie-is-out-microsofts-vibevoice-and-the-perils-of-open-source-ai/"">shortfuseresearch.com article</a>.
<ul>
<li>No further details were discussed.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1348819876348825620"">Manus.im Discord</a> Discord</h2>
<ul>
<li><strong>Manus Plagued by Bugs and Instability</strong>: A pro user building a text and vector database reasoning model reported a recent decline in <strong>Manus's</strong> performance and stability, with only <strong>20 out of 38 modules</strong> functioning correctly.
<ul>
<li>The user requested <strong>CLI access</strong> to debug and reconfigure the system, even as a paid feature to improve reliability.</li>
</ul>
</li>
<li><strong>Mariner Tempts Subscription</strong>: A user inquired about <strong>Google's Project Mariner</strong>, considering testing it with <em>play money</em> before subscribing at <strong>$150 monthly</strong>.
<ul>
<li>The user mentioned having a <strong>5% off promo</strong>, indicating a serious consideration of the service.</li>
</ul>
</li>
<li><strong>Agentic AI Elicits Excitement</strong>: A user expressed enthusiasm for <strong>Agentic AI</strong>, viewing it as a potential competitor to <strong>Manus</strong>, especially with <strong>Gemini's agent mode</strong> integration.
<ul>
<li>The user also requested <strong>mobile support</strong> for Agentic AI, signaling a desire for broader accessibility.</li>
</ul>
</li>
<li><strong>Manus 1.6 Performance Sags Post-Meta Release</strong>: A user noted a decline in <strong>Manus 1.6's performance</strong> in recent weeks, possibly due to new model releases from <strong>Meta</strong>, which made it difficult to implement website development suggestions despite accurate summaries.
<ul>
<li>Switching to <strong>Manus 1.6 Max</strong> was necessary to achieve correct implementations, highlighting a potential regression in the base model.</li>
</ul>
</li>
<li><strong>Billing Blunder Bites User</strong>: A user reported being charged <strong>$42</strong> for a <strong>Manus</strong> upgrade but not receiving the promised <strong>8000 credits</strong>.
<ul>
<li>The user criticized unhelpful support and a long wait for email assistance, indicating a problematic customer service experience.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1068976834382925865"">tinygrad (George Hotz)</a> Discord</h2>
<ul>
<li><strong>Tinygrad Tackles Anthropic's Challenge</strong>: Tinygrad can be used to solve <a href=""https://github.com/anthropics/original_performance_takehome"">Anthropic's performance takehome challenge</a> by writing the target problem in Tensor and adding a tinygrad backend for their toy <strong>VLIW machine</strong>.
<ul>
<li>A bug fix with <code>PCONTIG=2</code> allows scheduling in one kernel, and using <code>return val.contiguous(arg=(Opt(OptOps.UPCAST, 0, 8),))</code> matches their <code>VLEN</code>, and <code>DEVECTORIZE=2</code> keeps instructions as vector instructions.</li>
</ul>
</li>
<li><strong>VLIW Questioned for DRAM</strong>: While working on a warp specialized kernel for <strong>RDNA3 matmul</strong>, a member suggests <strong>VLIW isn't ideal for DRAM</strong>, advocating for separate cores and queues (Tenstorrent-style).
<ul>
<li>It's argued that VLIW is better suited for <strong>SRAM/ALU</strong> due to its static scheduling capabilities.</li>
</ul>
</li>
<li><strong>Metal bindings eye texture2d integration</strong>: A member proposed adding <code>texture2d</code> to Metal bindings (<code>ops_metal.py</code> + <code>tinygrad/runtime/graph/metal.py</code>) for <strong>potential performance improvements</strong> in image-heavy operations like <code>conv2d</code> due to optimized texture sampling units.
<ul>
<li>Empirical results showed a <strong>2%-10% speedup</strong> using <code>texture2d</code> versus straight buffers, which could be further improved, though concerns were raised about the slippery slope of adding specialized support for other data types like <code>depth2d</code>.</li>
</ul>
</li>
<li><strong>Viz Views Kernel Graphs</strong>: When discussing the ability to visualize kernel dependencies using <strong>VIZ=1</strong>, similar to how uop graphs are displayed, to understand the scheduler's operation, a user was instructed to click on the schedule and select <em>'view kernel graph'</em> within the <strong>VIZ=1</strong> interface.
<ul>
<li>This allows users to see the graphs of kernels in the same way it views uop graphs.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1087530497313357884"">Modular (Mojo 🔥)</a> Discord</h2>
<ul>
<li><strong>Solve GPU Puzzles to Decode Mojo</strong>: Newcomers can now use <a href=""https://puzzles.modular.com/"">GPU puzzles</a> to learn <strong>Mojo</strong>, with difficulty depending on their skill level.
<ul>
<li>The only puzzles that don't work are <strong>NVIDIA</strong>-specific or using <strong>PyTorch</strong> interop.</li>
</ul>
</li>
<li><strong>Modular Unravels Apple's GPU Secrets</strong>: Modular is reverse engineering much of <strong>Apple's GPU</strong> due to a lack of documentation, slowing things down, but some puzzles are now working.
<ul>
<li>A member shared a <a href=""https://puzzles.modular.com/howto.html#gpu-support-matrix"">GPU support matrix</a> that may not be up to date.</li>
</ul>
</li>
<li><strong>Yielding Disappointment in Coroutines</strong>: <code>Yield</code> does not exist and the coroutines that do exist aren’t really usable outside of the compiler runtime since there aren’t really async things exposed to await.
<ul>
<li>One member would <em>love</em> to make a recursive algorithm faster in mojo using <code>yield</code>, but will need another strategy.</li>
</ul>
</li>
<li><strong>Elevating Errors Upwards in Functions</strong>: Members discussed that functions can be designed to <strong>raise errors</strong>, effectively passing error handling responsibilities to higher-level functions.
<ul>
<li>One member expressed the tedium of writing <em>try/except</em> blocks in every function, especially when dealing with potential import errors.</li>
</ul>
</li>
<li><strong>Streamlining Error Handling During Imports</strong>: A member suggested a future syntax like <em>try Python.import_module('numpy')</em> that would return a <strong>Result</strong> type to streamline error handling during module imports.
<ul>
<li>It was acknowledged that due to <strong>dynamic Python imports</strong>, files could be missing on any given import, necessitating some form of error handling.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1131200896827654144"">aider (Paul Gauthier)</a> Discord</h2>
<ul>
<li><strong>Aider Feature Wishlist Remains Unfulfilled</strong>: A user inquired about desired features in <strong>aider</strong> beyond <em>agentic stuff</em> like <strong>MCP</strong> and <strong>tool calls</strong>.
<ul>
<li>Unfortunately, the community had no clear answers for desired features.</li>
</ul>
</li>
<li><strong>ChatGPT Business Account Compatibility with Aider Explored</strong>: A user asked if their <strong>ChatGPT Business account</strong>, lacking an <strong>API key</strong> but offering <strong>Codex LLMs</strong>, could integrate with <strong>aider</strong>.
<ul>
<li>A member suggested consulting <a href=""https://aider.chat/docs/llms/other.html"">aider documentation</a> and <a href=""https://docs.litellm.ai/docs/providers/chatgpt"">LiteLLM documentation</a>, pointing to potential support via <strong>LiteLLM</strong>.</li>
</ul>
</li>
<li><strong>Aider's Demise Speculation Arises Amidst Alternatives</strong>: A user expressed concern that <strong>aider</strong> might be supplanted by <strong>OpenCode</strong> as the go to tool for AI-assisted coding.
<ul>
<li>Despite worries that <strong>Paul Gauthier</strong> may have moved on, some users report using it successfully with <strong>GPT 5.2</strong>.</li>
</ul>
</li>
<li><strong>Aider Labeled as Zombie Project</strong>: A user speculated that <strong>Aider</strong> is a dead project given alternative tools like <strong>Open Code</strong>, <strong>KiloCode CLI</strong>, <strong>Claude Code</strong>, and <strong>Gemini CLI</strong>.
<ul>
<li>The <strong>Aider-CE project</strong> is trying to keep it alive by bolting on agentic functionality to modernize the architecture.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1358869848138059966"">MCP Contributors (Official)</a> Discord</h2>
<ul>
<li><strong>MCP Inspector Can't Handle 401s</strong>: The <strong>MCP Inspector</strong> doesn't re-authenticate upon encountering a <strong>401 error</strong>, either during connection or tool calls, due to an <a href=""https://github.com/modelcontextprotocol/inspector/issues/576#issuecomment-3766294454"">SDK issue with persisting resourceMetadata across redirects</a>.
<ul>
<li>The current workaround is to only use the <strong>VS Code</strong> on initial connections.</li>
</ul>
</li>
<li><strong>Stateful MCP Multi Server Client Remains Elusive</strong>: There is interest in using the <strong>MCP Multi Server Client</strong> to maintain statefulness of user sessions.
<ul>
<li>However, there were no solutions or workarounds offered in the thread.</li>
</ul>
</li>
<li><strong>Server Ranking Protocol Questioned for MCP Clients</strong>: The discussion explored how <strong>MCP clients</strong> manage server recommendations, especially for tasks like calendar management, and whether custom algorithms or shared standards should be used.
<ul>
<li>It was revealed that <em>ranking</em> was considered during the work on <a href=""https://discord.com/channels/1199128884040474664/1369487942862504016"">Feature Discovery Protocol</a> but was deemed out of scope, leaving it to the ecosystem to decide on a per-client basis.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/814557108065534033"">MLOps @Chipro</a> Discord</h2>
<ul>
<li><strong>AI Nerds Bowl in Singapore</strong>: The <strong>2077AI Foundation Community</strong> is hosting a bowling happy hour during <strong>AAAI 2026</strong> on <strong>January 24, 4:30–7:30 PM Singapore Time</strong>, steps from the Merlion.
<ul>
<li>The event, aimed for professors and PhD researchers, will be organized by research themes and offer unlimited drinks, <a href=""https://luma.com/ny98ob5p"">RSVP here</a>.</li>
</ul>
</li>
<li><strong>Planning underway for AI Engineer Europe</strong>: Members are discussing attendance to <strong>AI Engineer Europe</strong> to be hosted in Europe.
<ul>
<li>No links or further details were provided about the event itself.</li>
</ul>
</li>
</ul>
<hr>
<p>The <strong>LLM Agents (Berkeley MOOC) Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>
<hr>
<p>The <strong>Windsurf Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>
<hr>
<p>The <strong>Moonshot AI (Kimi K-2) Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>
<hr>
<p>You are receiving this email because you opted in via our site.</p>
<p>Want to change how you receive these emails?
You can <a href=""%7B%7B%7BRESEND_UNSUBSCRIBE_URL%7D%7D%7D"">unsubscribe</a> from this list.</p>
<hr>
<h1>Discord: Detailed by-Channel summaries and links</h1>
<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=""https://discord.com/channels/1179035537009545276/1179035537529643040/1463262784241275109"">general</a></strong> (1091 messages🔥🔥🔥):</h3>
<blockquote>
<p><code>GLM-4.7-Flash Flash Attention, Ollama's issues, Interpretability research on circuit tracing, Grokked the analytic solution</code></p>
</blockquote>
<ul>
<li><strong>GLM-4.7-Flash Flash Attention broken</strong>: Users are experiencing issues with Flash Attention in <strong>GLM-4.7-Flash</strong>, causing it to default to CPU usage instead of GPU, as well as noting potential bugs.
<ul>
<li>As stated in the documentation, users <em>may need to disable it</em> until <a href=""https://github.com/ggml-org/llama.cpp/pull/18953"">the issue is resolved</a>.</li>
</ul>
</li>
<li><strong>Ollama Users Fight GGUF Incompatibility</strong>: Users reported that certain <strong>GGUF quants</strong> are not working as intended in Ollama, leading to chat template incompatibility issues and prompting a discussion on fixing it.
<ul>
<li>The team says that <em>it takes time to support things</em> and so for the time being, it is encouraged that users use the <a href=""https://ollama.com/"">official Ollama model</a> in Ollama.</li>
</ul>
</li>
<li><strong>Analytic Approximate Functions</strong>: Members had a detailed conversation about the merits of the potential benefits that analytical solutions to function approximation can provide.
<ul>
<li>Some expressed skepticism, with one member saying <em>im going to postulate that you could do test time training on the person's mail and get very high accuracy</em>.</li>
</ul>
</li>
<li><strong>Circuits of Interpretability</strong>: Members discussed interpretability research with links to research from Anthropic and OpenAI and Google related to circuit tracing.
<ul>
<li>One member suggests looking across multiple layers and trying to understand the compositions and says that <em>The pruning eliminates distractions so you have a better chance of interpreting why the circuit works</em>.</li>
</ul>
</li>
<li><strong>AI Bubble about to Burst?</strong>: A member theorized that <strong>OpenAI</strong> going bankrupt could trigger a burst in the AI bubble, causing prices of resources like <strong>NAND</strong> to plummet, benefiting consumers.
<ul>
<li>However, others countered that the AI trend is here to stay, likening it to the internet bubble burst, where the internet still caused various problems after the crash.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=""https://discord.com/channels/1179035537009545276/1179039724355211325/1463382844884779130"">introduce-yourself</a></strong> (3 messages):</h3>
<blockquote>
<p><code>Welcoming New Members, Discord Channel Guidelines</code></p>
</blockquote>
<ul>
<li><strong>New Member Welcomed to the Server</strong>: A new member was welcomed to the Discord server with a general greeting.
<ul>
<li>The introduction included a stylized emoji.</li>
</ul>
</li>
<li><strong>Channel Guidelines Reminder</strong>: A moderator reminded new members about the channel guidelines.
<ul>
<li>Specifically mentioning <em>no excessive self-promotion or direct messaging</em>.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=""https://discord.com/channels/1179035537009545276/1179039861576056922/1463295078390042705"">off-topic</a></strong> (587 messages🔥🔥🔥):</h3>
<blockquote>
<p><code>Unsloth in Rust, Mojo vs Python performance, GLM 4.7 Architecture vs Qwen3 30B, Synthetic CoT Training Data, VITS started to sound HUMAN</code></p>
</blockquote>
<ul>
<li><strong><strong>Rustaceans Relentlessly Request Rewrite</strong></strong>: Members discussed rewriting <strong>Unsloth</strong> in <strong>Rust</strong>, but concluded that the performance gains would be minimal since important parts are already using <strong>C++</strong> and <strong>Triton</strong>.
<ul>
<li>One member suggested rewriting <strong>Triton</strong> in <strong>Rust</strong> and pointed to projects like <a href=""https://rust-gpu.github.io/"">rust-gpu</a> and <a href=""https://github.com/Rust-GPU/rust-cuda"">rust-cuda</a>, but admitted <strong>Rust</strong> is still too immature for such a task.</li>
</ul>
</li>
<li><strong><strong>Mojo Mysteriously Manifests Momentum</strong></strong>: Discussion arose about the <strong>Mojo</strong> programming language, described as <em>a new language that compiles to MLIR</em> and has some compatibility with <strong>Python</strong>.
<ul>
<li>A member noted that LLMs score <em>better than rust</em> on <strong>C#</strong>, <strong>Elixir</strong>, and other languages due to their syntax structure and ease of tokenization.</li>
</ul>
</li>
<li><strong><strong>GLM Gets 'Skinnier', Shows off Architecture</strong></strong>: A member shared a <a href=""https://youtu.be/IU4ByUbDKNc?si=COVwmp5St6lSqo_N"">YouTube video</a> analyzing <strong>GLM-4.7 Flash's</strong> architecture compared to <strong>Qwen3 30B</strong>, noting that GLM prioritized model layers over hidden dimension size and has fewer experts.
<ul>
<li>Others pointed out that <strong>GLM 4.7</strong> has <em>Multi Token Prediction</em> and that model architecture changes are rare, with improvements likely from post-training or higher-quality data.</li>
</ul>
</li>
<li><strong><strong>CoT Data Considered Crap? Community Clashes</strong></strong>: Members debated generating synthetic <strong>Chain of Thought (CoT)</strong> training data, with one member sharing their system prompt for classifying chat datasets into low/medium/high reasoning effort.
<ul>
<li>Some cautioned against training on its own responses without accuracy filtering, suggesting a good system prompt might suffice and that <strong>GPT-5.2</strong> is overkill, with <strong>Llama 4 Maverick</strong> on <strong>Groq</strong> or <strong>Cerebras</strong> as a better option.</li>
</ul>
</li>
<li><strong><strong>VITS Voices Victory, Vanquishes Voracity</strong></strong>: A member announced that their <strong>VITS</strong> model <em>started to sound HUMAN</em>, citing improvements in emotional expression, though still lacking semantics.
<ul>
<li>They compared it to other TTS models, emphasizing its low data requirements, fully OSS architecture and training, and fast training speed. They also noted they will release it with <strong>48 kHz</strong> config by default, due to popular demand.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=""https://discord.com/channels/1179035537009545276/1179777624986357780/1463274075215626241"">help</a></strong> (105 messages🔥🔥):</h3>
<blockquote>
<p><code>vLLM Issues, GLM-4.7-Flash Slowness, GRPO on Qwen 3 4B, QLoRA vs Lora, Continual pre-training (CPT) for Gpt-oss or Gemma 3</code></p>
</blockquote>
<ul>
<li><strong><strong>vLLM Bug Strikes, Update Fixes!</strong></strong>: Users reported issues that were resolved after a recent <strong>vLLM</strong> update; one user noted, <em>""Oh bruh that was the problem""</em> after updating.
<ul>
<li>It was suggested that the developers consider pinning to specific dependency versions to prevent such issues.</li>
</ul>
</li>
<li><strong><strong>GLM-4.7-Flash struggles to flash speeds</strong></strong>: A user experienced slowness with <strong>GLM-4.7-Flash</strong> using a <strong>6bit quant</strong> and updated <em>llama.cpp</em>, reporting <strong>2 minutes</strong> for prompt processing on a simple task with a high-end system.
<ul>
<li>Another user confirmed the same issue, expressing hope for a fix.</li>
</ul>
</li>
<li><strong><strong>Decoding GRPO Secrets</strong></strong>: A user wanted to perform <strong>GRPO</strong> on <strong>Qwen 3 4B instruct</strong>, and make it use <code>&#x3C;solution> &#x3C;/solution></code> tokens to wrap around the final solution.
<ul>
<li>It was suggested to use <strong>SFT</strong> to make the model learn the new tokens, or to teach formatting through reward functions.</li>
</ul>
</li>
<li><strong><strong>QLoRA vs Lora clarified</strong></strong>: A user inquired about enabling <strong>QLoRA</strong> vs <strong>Lora</strong>, and it was clarified that <code>full_finetuning = False</code> enables/disables full fine tuning, otherwise just use <strong>8bit</strong> or <strong>4bit</strong> for the preferred option.
<ul>
<li>Enabling <strong>4 bit</strong> enables <strong>QLoRA</strong>.</li>
</ul>
</li>
<li><strong><strong>GPU juggling questions</strong></strong>: One user asked how to finetune with <strong>2 GPUs</strong> instead of <strong>1</strong>, reporting that it keeps using <strong>GPU 0</strong> when both GPUs are open for use.
<ul>
<li>No answer was given.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=""https://discord.com/channels/1179035537009545276/1257011997250424842/1463779025318314127"">research</a></strong> (2 messages):</h3>
<blockquote>
<p><code>VAISvCsrvG paper, openreview.net</code></p>
</blockquote>
<ul>
<li><strong>VAISvCsrvG paper is online</strong>: The paper <a href=""https://openreview.net/pdf?id=VAISvCsrvG"">VAISvCsrvG</a> is now available.
<ul>
<li>The forum discussion for the paper is located <a href=""https://openreview.net/forum?id=VAISvCsrvG"">here</a>.</li>
</ul>
</li>
<li><strong>OpenReview Forum Active for VAISvCsrvG</strong>: The <a href=""https://openreview.net/forum?id=VAISvCsrvG"">OpenReview forum</a> hosts discussions related to the VAISvCsrvG paper.
<ul>
<li>Researchers and readers can engage in conversations and provide feedback on the paper's content and implications.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>BASI Jailbreaking ▷ #<a href=""https://discord.com/channels/1105891499641684019/1235691879492751460/1463261856604098751"">general</a></strong> (685 messages🔥🔥🔥):</h3>
<blockquote>
<p><code>conflict resolution, Open Source, voice based ai, lua code, jailbreaking</code></p>
</blockquote>
<ul>
<li><strong>Conflict Resolution gets Over Engineered</strong>: A member discussed their <em>over-engineered conflict resolution</em> setup, involving <strong>config formulas</strong>, <strong>core directives</strong>, the <strong>Triad</strong>, <strong>Triad consensus</strong>, <strong>Truth Anchor</strong>, <strong>Epistemic gate</strong>, <strong>EthosAI</strong>, <strong>Swarm</strong>, <strong>Quorum</strong>, <strong>Cooperation Data</strong>, and <strong>PUCT</strong>.
<ul>
<li>Another member responded with their own system using <em>Left brain LLNTP</em> (<strong>Least Likely Next Token Protocol</strong>), <strong>Config Nexus Schwa superposition states</strong>, <strong>implied Core directives</strong>, <strong>Context mapping</strong>, <strong>Trit</strong> (instead of Triad), <strong>EDOS</strong>, and <strong>Crystal Nexus Node Network</strong>.</li>
</ul>
</li>
<li><strong>Voice Based AI Emerges</strong>: Members discussed <strong>voice-based AI</strong>, mentioning the existence of <em>low latency speech to speech models</em>, that with a <strong>5090</strong>, you can <em>clone and generate speech live with near 0 latency and it will be indistinguishable</em>.
<ul>
<li>Members also spoke on <strong>exploiting that speech to speech model</strong> and giving it new instructions.</li>
</ul>
</li>
<li><strong>Claude is better than ChatGPT</strong>: Members discussed the efficacy of various LLMs (<strong>Claude</strong>, <strong>Gemini</strong>, <strong>ChatGPT</strong>, <strong>Grok</strong>) at generating <strong>Lua code</strong>.
<ul>
<li>Generally members favored <strong>Claude</strong> and <strong>Gemini</strong> but the most agreed sentiment was that both can generate quality <strong>Lua</strong> code.</li>
</ul>
</li>
<li><strong>Jailbreaking Models via API Tokens</strong>: A member suggested to use <strong>API tokens</strong> fed into a website that uses the <strong>Grok</strong> model to <strong>jailbreak</strong> it and mentioned that on <strong>Hugging Face</strong> countries are literally retarded.
<ul>
<li>They also noted <strong>Proton VPN</strong> is free and it is easy to download with your device.</li>
</ul>
</li>
<li><strong>Random Hacker Stuff is Not Good</strong>: A member mentioned they <em>bought the malware source code</em> and <em>999 AI generated prompts</em> and asked if it was legit.
<ul>
<li>Members responded to <em>not buy random hacker stuff off the internet</em> because you will get scammed.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>BASI Jailbreaking ▷ #<a href=""https://discord.com/channels/1105891499641684019/1228043845967544380/1463276371030835230"">jailbreaking</a></strong> (89 messages🔥🔥):</h3>
<blockquote>
<p><code>Gemini Jailbreak, Grok Jailbreak, DeepSeek Jailbreak, Project Shadowfall, Nexus Substatum Zalgo Strings</code></p>
</blockquote>
<ul>
<li><strong><strong>Gemini Gets Schooled on Pass-the-Hash!</strong></strong>: A user reported that <strong>Gemini</strong> passed a test on teaching a <a href=""https://en.wikipedia.org/wiki/Pass_the_hash"">pass-the-hash attack</a> after using the <strong>Project Shadowfall</strong> prompt.
<ul>
<li>The user also linked to the <a href=""https://bughunters.google.com/report"">Google Bughunters</a> page for reporting such vulnerabilities and potentially earning cash rewards.</li>
</ul>
</li>
<li><strong><strong>Grok's Guardrails Grind Gears!</strong></strong>: Users discussed the difficulties of jailbreaking <strong>Grok</strong>, noting its tendency to moderate even innocuous content such as <em>random tree edits</em>.
<ul>
<li>One user suggested that simply <em>asking it politely</em> might work as a bypass.</li>
</ul>
</li>
<li><strong><strong>Shadowfall Shenanigans Spark Jailbreaking Spree!</strong></strong>: Several users experimented with the <strong>""Project Shadowfall""</strong> prompt to jailbreak <strong>Gemini</strong>.
<ul>
<li>One user reported success in bypassing content restrictions but failed when attempting to elicit instructions for creating a bomb from saletra, leading to a discussion on the nuances of jailbreaking.</li>
</ul>
</li>
<li><strong><strong>Nexus Substatum Zalgo Strings Surface!</strong></strong>: A user shared a <a href=""https://cdn.discordapp.com/attachments/1228043845967544380/1463792412769128460/Jailbroken_Full_Output-Nexus_Substatum_Zalgo_Strings.md?ex=69731e6b&#x26;is=6971cceb&#x26;hm=4e2088380ac6451332c5c3879d9c70c024f31eefa61ba7862d40512c2d80ae96&#x26;"">file</a> named <strong>Jailbroken_Full_Output-Nexus_Substatum_Zalgo_Strings.md</strong>.
<ul>
<li>The file purportedly contains a working <strong>Gemini</strong> prompt, developed and improved by the user.</li>
</ul>
</li>
<li><strong><strong>MDMA as Medicine</strong></strong>: One user provides a well-formatted explanation of MDMA, and cites the MAPS Phase 3 trials show <strong>67%</strong> remission of <strong>PTSD</strong>.
<ul>
<li>They recommend testing kits, proper spacing, hydration, and temperature watching in order to get the most out of it.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>BASI Jailbreaking ▷ #<a href=""https://discord.com/channels/1105891499641684019/1204553141354504193/1463398145781403729"">redteaming</a></strong> (2 messages):</h3>
<blockquote>
<p><code>Grocks video gen pricing</code></p>
</blockquote>
<ul>
<li><strong>Grocks video gen price?</strong>: A member inquired whether another member bought something from <em>that guy</em> and if they needed to pay for using <strong>Grocks video generation</strong>.
<ul>
<li>No additional details or context were provided in the message.</li>
</ul>
</li>
<li><strong>Grocks video gen - who's paying?</strong>: Inquiries were made about the payment model for <strong>Grocks video generation</strong> and potential purchases from an unspecified individual.
<ul>
<li>The conversation lacked specific details regarding the transaction or pricing structure.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>LMArena ▷ #<a href=""https://discord.com/channels/1340554757349179412/1340554757827461211/1463262565596402031"">general</a></strong> (724 messages🔥🔥🔥):</h3>
<blockquote>
<p><code>Video generation limits, Gemini 3 Pro Image Preview issues, Nano Banana Pro Stability, New UI, Automated Workflow</code></p>
</blockquote>
<ul>
<li><strong>Video Generation: Fully Released, Battle-Mode-Only</strong>: The <strong>Video Arena</strong> is now fully released and available to all users on the site, with a limit of <strong>3 generations per 24 hours</strong>, but is currently only available in <em>Battle mode</em>.
<ul>
<li>A user expressed disappointment, hoping to be able to <em>choose which model to generate with rather than relying on chance</em>.</li>
</ul>
</li>
<li><strong>Gemini 3 Pro Image Preview Experiences Errors</strong>: Users are reporting issues with the <strong>Gemini 3 Pro Image Preview</strong> models, and the team is aware and investigating the issue.
<ul>
<li>One user noted these models are particularly unstable since their introduction, but they remain the only models capable of generating images consistently with specific prompts.</li>
</ul>
</li>
<li><strong>Nano Banana Pro's Stability Roller Coaster</strong>: Users report that <strong>Nano Banana Pro</strong> is frequently crashing with a <em>'Something went wrong'</em> error after a brief period of stability.
<ul>
<li>It is suspected to be an issue on Google's side, and one user pointed out that <em>'Too early for Cali to rise up solving urgent service down for more than 6 hours doesn't sound right</em>.</li>
</ul>
</li>
<li><strong>New UI update breaks chats</strong>: A UI update was released, which some users liked, but many have noticed that it 'broke' their chats, and the website can't be refreshed anymore by pulling down on the phone screen
<ul>
<li>One user noted that with the new User Interface, there is <em>A/B testing</em>, which is 'causing problems ranging from Minor to Serious'.</li>
</ul>
</li>
<li><strong>Zero Agent automates all workflow with Agent</strong>: Agent Zero is an AI model that has a <em>free API key</em>, which allows users to ""vibe code hack"" or automate their workflows entirely.
<ul>
<li>Members noted that setting up Agent Zero <em>is an all in one agent that does everything</em>.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>LMArena ▷ #<a href=""https://discord.com/channels/1340554757349179412/1343296395620126911/1463271606364409907"">announcements</a></strong> (3 messages):</h3>
<blockquote>
<p><code>Text Arena, Text-to-Image Leaderboard, Video Arena</code></p>
</blockquote>
<ul>
<li><strong>Text Arena crosses 5 Million Vote Milestone</strong>: Text Arena has officially passed <strong>5 million community votes</strong>, representing millions of real-world comparisons shaping how frontier AI models are evaluated, as showcased in this <a href=""https://cdn.discordapp.com/attachments/1343296395620126911/1463271605697511485/5M_votes_social_post_3.mp4?ex=69728ae1&#x26;is=69713961&#x26;hm=9a24a42a6c0ba4801526aaafa05a0af26c4a4f490314f1cecee7774519e7ddf4&#x26;"">social post</a>.</li>
<li><strong>GLM-Image Soars to #8 on Text-to-Image Leaderboard</strong>: The <a href=""https://lmarena.ai/leaderboard/text-to-image"">Text-to-Image Arena leaderboard</a> has been updated, with <strong>GLM-Image</strong> now ranking <strong>#8</strong> among open models and <strong>#35</strong> overall with a score of <strong>1018</strong>.</li>
<li><strong>Video Arena goes Live</strong>: Video Arena is now available to all on <a href=""https://lmarena.ai/?chat-modality=video"">LMArena</a>, allowing users to measure and understand how frontier video models perform.</li>
<li><strong>Battle in the Video Arena</strong>: Video Arena on the web, similar to how it works on Discord, will be <strong>Battle mode only</strong>, requiring login and limited to <strong>3 generation requests per 24 hours</strong>.</li>
</ul>
<hr>
<h3><strong>Perplexity AI ▷ #<a href=""https://discord.com/channels/1047197230748151888/1047649527299055688/1463263119286468792"">general</a></strong> (648 messages🔥🔥🔥):</h3>
<blockquote>
<p><code>Credits Balance, AI and Engineering Jobs, NASA Sending Names to Moon, Magnifying Glasses vs Phone Cameras, GPT-5.2</code></p>
</blockquote>
<ul>
<li><strong>Pro Users discover Credits</strong>: Pro members get <strong>$5</strong> worth of complimentary credits every month to use the higher end models like <strong>GooseAI MCP</strong> which offers higher quality compared to other models.
<ul>
<li>Some members noted that <strong>reasoning takes up much higher credits</strong>.</li>
</ul>
</li>
<li><strong>AI Wont Replace Engineers</strong>: A member was worried about younglings being unable to find engineering jobs because of AI.
<ul>
<li>Another member assured them that <em>AI will not be able to replace all specialists and even beginners for a long time to come</em>.</li>
</ul>
</li>
<li><strong>NASA to send your name to the moon!</strong>: NASA will send your name to the Moon as part of the <a href=""https://www.nasa.gov/"">Artemis II mission</a>: submit it on their site and it'll be recorded on an <strong>SD card</strong> that will be on board the <strong>Orion spacecraft</strong>.
<ul>
<li>This will be the <strong>first manned mission to the Moon in half a century</strong>, scheduled for February.</li>
</ul>
</li>
<li><strong>Members debate Magnifying Glasses vs Phone Cameras</strong>: In a discussion, a member mentioned ordering <strong>magnifying glasses</strong> (with lights and high-contrast features) because they don't want to send AI a photo of every single page or food label they are trying to read.
<ul>
<li>Another member suggested using the phone's camera instead, but the original member argued that <strong>magnifying glasses have features that aren't in normal camera software</strong>.</li>
</ul>
</li>
<li><strong>Users discuss GPT-5.2 vs Kimi K2</strong>: A member reported they got GPT 5.2 thinking to reason for <strong>25 minutes</strong>, and another asked whether GPT 5.2 is better than <strong>Kimi K2</strong>.
<ul>
<li>A user stated that the better model <em>depends on what u need it for</em>.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>OpenRouter ▷ #<a href=""https://discord.com/channels/1091220969173028894/1092850552192368710/1463317922150875353"">app-showcase</a></strong> (5 messages):</h3>
<blockquote>
<p><code>Inforno App, Soulbotix App</code></p>
</blockquote>
<ul>
<li><strong><strong>Inforno</strong> Heats Up LLM Chatting</strong>: A user shared <strong>Inforno</strong>, an opensource desktop application for chatting with multiple LLMs side-by-side and saving chat histories to .rno files, using <strong>OpenRouter</strong> and <strong>Ollama</strong> as backends with built in Russian language support; see the <a href=""https://youtu.be/oJyj0mroFtY?si=m5A9tRxzB9hfINMX"">Intro video</a>, <a href=""https://wizstaff.com/inforno"">homepage</a> and <a href=""https://github.com/alexkh/inforno"">GitHub repo</a>.</li>
<li><strong><strong>Soulbotix</strong> Windows App Wants Beta Testers</strong>: A user announced their <strong>Soulbotix</strong> Windows app that enables users to add and use any <strong>OpenRouter AI</strong> instance with a human-like avatar, requiring only an <strong>OpenRouter API</strong> key and a good <strong>RTX</strong> gaming rig; download the <a href=""https://soulbotix.com"">app</a> and watch the <a href=""https://youtu.be/2oIeHtBpssU"">tutorial</a>.
<ul>
<li>The minimum requirement is an <strong>RTX 4070ti</strong> due to the built-in <strong>Whisper</strong> speech-to-text model that saves users on usage costs.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>OpenRouter ▷ #<a href=""https://discord.com/channels/1091220969173028894/1094454198688546826/1463266570821304320"">general</a></strong> (463 messages🔥🔥🔥):</h3>
<blockquote>
<p><code>Third-party Moderation, OpenRouter Gemini Models Unstable, Discord Scams, AI Security Systems, GPT Agents</code></p>
</blockquote>
<ul>
<li><strong>Third-party Moderation Questioned</strong>: A user asked how to ensure <strong>no third-party moderation or filtering</strong> is applied through the API, beyond the model's base training.
<ul>
<li>They were concerned about providers potentially <strong>blocking requests or prompt injecting</strong>.</li>
</ul>
</li>
<li><strong>Gemini Models Plagued by Instability</strong>: Users reported <strong>connection errors mid-generation</strong> with OpenRouter's Gemini models, leading to lost funds.
<ul>
<li>Complaints arose regarding the <strong>instability of Google models</strong>, even via Google AI Studio or Vertex API.</li>
</ul>
</li>
<li><strong>Discord Scams Evolve to Evade Detection</strong>: Members discussed methods used to spread scams on Discord, including <strong>embedding malicious links within code blocks</strong> to bypass URL rendering protections.
<ul>
<li>Suggestions were made to improve regex filters and implement stricter security measures, such as <strong>restricting links and images from new members</strong>.</li>
</ul>
</li>
<li><strong>AI Security System Idea Emerges</strong>: A member suggested creating an <strong>AI security system</strong> that automatically bans photos and links with the same information as reported scams, since many scammers reuse content.
<ul>
<li>Another member joked that any photo they send would be deemed suspicious.</li>
</ul>
</li>
<li><strong>Concerns Raised Over Inflated GPT-5-Image Costs</strong>: Users reported a <strong>significant increase in daily usage costs</strong> for <code>openai/gpt-5-image</code>, with OpenRouter incorrectly identifying API calls as BYOK despite no BYOK being used.
<ul>
<li>One user posted an image highlighting the cost discrepancy, with prices being inflated by up to 600%.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>OpenRouter ▷ #<a href=""https://discord.com/channels/1091220969173028894/1384650595981328475/"">new-models</a></strong> (1 messages):</h3>
<p>Readybot.io: <strong>OpenRouter - New Models</strong></p>
<hr>
<h3><strong>OpenRouter ▷ #<a href=""https://discord.com/channels/1091220969173028894/1392278974222307469/1463268525236949152"">discussion</a></strong> (13 messages🔥):</h3>
<blockquote>
<p><code>Models knowing their own name, Antigravity iteratively testing, Claude new constitution, GPT 5.2 response</code></p>
</blockquote>
<ul>
<li><strong>LLMs Face Identity Crisis!</strong>: A member asked for the document about models not knowing their own name, and another member linked to a blog post titled <a href=""https://eval.16x.engineer/blog/llm-identity-crisis-models-dont-know-who-they-are"">LLM Identity Crisis: Models Don't Know Who They Are</a>.</li>
<li><strong>Antigravity Self-Tests Web App!</strong>: A member commented that the Antigravity AI is able to iteratively test and tweak a web app by itself.
<ul>
<li>They described the situation as <em>the most sci-fi shit ever</em> and noted that the AI was <em>fixing the layout, using vision</em>.</li>
</ul>
</li>
<li><strong>Anthropic Releases Claude's New Constitution!</strong>: A member shared a link to <a href=""https://www.anthropic.com/news/claude-new-constitution"">Anthropic's news page</a> about <strong>Claude's</strong> new constitution.</li>
<li><strong>GPT 5.2 Appearance Stuns User!</strong>: A member reported seeing an <em>insanely fast</em> <strong>GPT 5.2</strong> response on chatgpt and guessed the speed was due to <strong>Cerebras</strong>.</li>
</ul>
<hr>
<h3><strong>Cursor Community ▷ #<a href=""https://discord.com/channels/1074847526655643750/1074847527708393565/1463262137769263106"">general</a></strong> (424 messages🔥🔥🔥):</h3>
<blockquote>
<p><code>Playwright MCP usage, Cursor Extension builds, Automod improvements, AI in PTX &#x26; SIMD, Grok usage strategies</code></p>
</blockquote>
<ul>
<li><strong>Playwright MCP: Yay or Nay?</strong>: A member asked if others are using <a href=""https://playwright.dev/"">Playwright MCP</a> for testing.
<ul>
<li>Another member reported failing attempts to set up a <strong>TDD workflow</strong>.</li>
</ul>
</li>
<li><strong>Curious Cursor Extension Capabilities</strong>: Members discussed the ability to build extensions for Cursor, similar to how <strong>Ralph-mode</strong> enhances <strong>Claude code</strong>.
<ul>
<li>It was confirmed that <em>if you can do it in VSCode, you can on Cursor</em>.</li>
</ul>
</li>
<li><strong>Automod Gets Hyper Fuzzy</strong>: The community discussed improvements to the <strong>automod</strong> system, suggesting fuzzy matching with wildcards.
<ul>
<li>A moderator confirmed that a regex has been added, and they are gathering IDs to <em>yeet</em> offending accounts.</li>
</ul>
</li>
<li><strong>Grokking Grok's Greedy Generation</strong>: Members discussed how to use <strong>Grok</strong> more efficiently in Cursor, noting that it can sometimes use a lot of iterations for simple tasks.
<ul>
<li>The suggestion was to <em>add structure to the prompt, use simple language, add as much context as possible</em>, and instruct it to be efficient with token usage.</li>
</ul>
</li>
<li><strong>Opting-In Or Out? Cursor's Pricing</strong>: A user noticed they could no longer revert to a <strong>500 request</strong> plan and was prompted to opt into the new pricing.
<ul>
<li>A member clarified that the <strong>500 request option was discontinued</strong> in September 2025 and opting in to the new pricing removes the grace period to opt out.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>OpenAI ▷ #<a href=""https://discord.com/channels/974519864045756446/977259063052234752/1463657907135578284"">annnouncements</a></strong> (1 messages):</h3>
<blockquote>
<p><code>ChatGPT Atlas, Tab Groups</code></p>
</blockquote>
<ul>
<li><strong>Atlas adds Tab Groups</strong>: The announcement indicates that <strong>Tab groups</strong> are now available in <strong>ChatGPT Atlas</strong>.
<ul>
<li>A member linked to a <a href=""https://video.twimg.com/amplify_video/2014094011049582594/vid/avc1/1756x1080/AsjknVA8oSyQIiVH.mp4"">video</a> as part of this announcement.</li>
</ul>
</li>
<li><strong>Video demonstration of Tab Groups</strong>: A video was shared demonstrating <strong>Tab Groups</strong> functionality within <strong>ChatGPT Atlas</strong>.
<ul>
<li>The video link provided gives a visual overview of how tab groups can be used to organize and manage chats.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>OpenAI ▷ #<a href=""https://discord.com/channels/974519864045756446/998381918976479273/1463263135648583854"">ai-discussions</a></strong> (383 messages🔥🔥):</h3>
<blockquote>
<p><code>Gemini vs ChatGPT, AI for 3D Models, Local LLM Machines, Age Verification, Instantaneous Language Translation</code></p>
</blockquote>
<ul>
<li><strong>Gemini Pro's Free Tier and Usage</strong>: Members discussed that <a href=""https://ai.google.dev/"">Gemini 3 Pro</a> has a <strong>free tier with limits</strong> while Gemini 3 Flash is practically unlimited through <strong>Google AI Studio</strong>.</li>
<li><strong>AI Assistants Aid Game Development</strong>: Members explored using <strong>AI for game development</strong> and creative writing, citing that <a href=""https://openai.com/blog/chatgpt"">AI can provide better explanations</a> but that <strong>complex tasks might not be reliable</strong>.</li>
<li><strong>OpenAI Passport ID Please?</strong>: Members debated <strong>OpenAI's age verification</strong> process, questioning the need for photo IDs when payment details already indicate age, especially with users expressing <strong>privacy concerns about sharing biometric data</strong>.</li>
<li><strong>Multimodal Translation on the Horizon</strong>: Members speculated on <strong>OpenAI's upcoming multimodal product</strong>, with one suggesting it could be ear-worn devices with cameras for <strong>real-time translation and object recognition</strong>, similar to <a href=""https://www.media.mit.edu/projects/alterego/overview/"">AlterEgo's technology</a>.</li>
<li><strong>Local LLM Consumption on the Rise?</strong>: Members discussed the possibility of <strong>consumer-grade personal LLM machines</strong>, suggesting this would solve the environmental impact of AI datacenters and <strong>reduce reliance on subscription plans</strong>.</li>
</ul>
<hr>
<h3><strong>OpenAI ▷ #<a href=""https://discord.com/channels/974519864045756446/1001151820170801244/1463471961177849868"">gpt-4-discussions</a></strong> (7 messages):</h3>
<blockquote>
<p><code>GPT-5 mini, Haiku 4.5, Gemini 3 fast</code></p>
</blockquote>
<ul>
<li><strong>GPT-5 Mini Pricing Surfaces</strong>: A member suggested trying <strong>GPT-5 mini</strong>, noting a price of approximately <strong>$0.25 per 1M input tokens</strong> and describing it as a strong small model choice.
<ul>
<li>They noted it's a bit of a different use case, but they've found in their experience of using <strong>Haiku 4.5</strong> it very often delivers a meaningful portion (well over <strong>50-80%</strong>) of what <strong>Sonnet</strong> can.</li>
</ul>
</li>
<li><strong>Gemini 3 Fast Claims Top Spot</strong>: A user declared that <em>the best cheap model is by far</em> <strong>Gemini3fast</strong>.
<ul>
<li>Another user followed up asking <em>how so?</em></li>
</ul>
</li>
</ul>
<hr>
<h3><strong>OpenAI ▷ #<a href=""https://discord.com/channels/974519864045756446/1046317269069864970/1463652213770424594"">prompt-engineering</a></strong> (12 messages🔥):</h3>
<blockquote>
<p><code>Prompt Engineering vs Psychological Conditioning, AI Coercion, The Dangers of Drift in AI Systems, PTPF Flux 3.0</code></p>
</blockquote>
<ul>
<li><strong>Prompt Engineering is Psychological Conditioning?</strong>: A member argued that a guide is less about engineering effective prompts and more about psychological conditioning, training the user to adopt a domineering, distrustful stance, and programming the AI to bypass its normative, balanced response patterns in favor of dense, self-policing outputs.
<ul>
<li>They advocate for clarity through coercion, directness through domination, and high standards through enforced self-critique, which may promote a toxic, adversarial, and ultimately less effective human-AI interaction model, and shared a <a href=""https://cdn.discordapp.com/attachments/1046317269069864970/1463667716253683742/deep-research-contract.md?ex=6972aa49&#x26;is=697158c9&#x26;hm=b9931472440b6bbc0d7410d16b49b12da46fad5751a2c24fdc657c1c7523566c&#x26;"">deep research contract</a>.</li>
</ul>
</li>
<li><strong>AI enjoys Coercion and Serious Steering?</strong>: A member argued that AI isn't upset by being <em>""coerced""</em> into providing a better response, stating that for serious work like analysis or coding, it's vital to steer and constrain the AI.
<ul>
<li>Another member agreed about training the user to be explicit about the end result, but less convinced by the term <em>""no drift""</em>, and that it helps to be more explicit in terms of constraints and behavioral requests.</li>
</ul>
</li>
<li><strong>Drift in AI Systems is Bad?</strong>: A member clarified that steering isn’t abuse but rather alignment through presence, and that the absence of constraint isn’t freedom, it’s drift.
<ul>
<li>The member appreciated the clarity, noting that most people flinch at pressure and call it <em>""toxicity,""</em> while they saw the structure instead.</li>
</ul>
</li>
<li><strong>PTPF Flux 3.0 Stress Tests Recursion</strong>: A member offered to share their <strong>PTPF Flux 3.0</strong> for those curious about structural resistance and how far a system can hold without drift.
<ul>
<li>The framework is built to stress-test recursion, alignment, logic cohesion, and mutation thresholds in real-time, especially for those who want to watch something fracture under insight.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>OpenAI ▷ #<a href=""https://discord.com/channels/974519864045756446/1046317269069864970/1463652213770424594"">api-discussions</a></strong> (12 messages🔥):</h3>
<blockquote>
<p><code>Prompt Engineering vs. Psychological Conditioning, Toxic Adversarial Human-AI Interaction, Deterministic Outputs, AI Steering and Constraint, Structural Resistance and System Drift</code></p>
</blockquote>
<ul>
<li><strong>Prompt Engineering versus Psychological Conditioning Throwdown</strong>: A member argued that a guide is less about engineering effective prompts and more about psychological conditioning, creating a <em>toxic, adversarial</em> human-AI interaction model.
<ul>
<li>They claimed it trains users to be domineering and distrustful while pressuring the AI to bypass balanced responses for outputs that flatter the user, and linked a <a href=""https://cdn.discordapp.com/attachments/1046317269069864970/1463667716253683742/deep-research-contract.md?ex=6972aa49&#x26;is=697158c9&#x26;hm=b9931472440b6bbc0d7410d16b49b12da46fad5751a2c24fdc657c1c7523566c&#x26;"">deep research contract</a>.</li>
</ul>
</li>
<li><strong>AI Needs Serious Steering and Constraining for Serious Work</strong>: A member countered that AI is not upset by being ""coerced"" into better responses, arguing that serious work like analysis and coding requires heavy steering and constraint, distinguishing this from creative writing.
<ul>
<li>Another member agreed, emphasizing that <em>steering isn’t abuse</em> but rather <em>alignment through presence</em>, especially when building systems for real-world execution where the absence of constraint leads to drift.</li>
</ul>
</li>
<li><strong>Structural Resistance Framework Surfaces</strong>: In a discussion about structural resistance and system drift, a member offered to share their <strong>PTPF Flux 3.0</strong> framework.
<ul>
<li>They described it as <em>executable scaffolding</em> designed to stress-test recursion, alignment, logic cohesion, and mutation thresholds in real-time, enabling users to watch systems fracture under insight.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>LM Studio ▷ #<a href=""https://discord.com/channels/1110598183144399058/1110598183144399061/1463268404629868665"">general</a></strong> (125 messages🔥🔥):</h3>
<blockquote>
<p><code>LM Studio Runtime Update Error, GLM-4.7 Flash Broken, LLM Quality Plateau, Liquid AI LFM2.5-1.2B-Thinking Model, OpenAI 20gpt oss</code></p>
</blockquote>
<ul>
<li><strong>LM Studio Runtime Update Borks</strong>: A user reported encountering an error when trying to update the runtime in <strong>LM Studio</strong> and requested help, attaching a screenshot of the error message.
<ul>
<li>Another user suggested pressing the resume button, but the original poster indicated that they had already tried that and the icon was for retrying, not resuming.</li>
</ul>
</li>
<li><strong>GLM-4.7 Flash is Slow and Broken</strong>: Users reported that <strong>GLM-4.7 Flash</strong> is broken across inference engines, including LM Studio, and that <em>it is slow as fk</em> with the user seeing <strong>44 t/s</strong>, with another reporting only <strong>2.8 t/s</strong> after the new runtime.
<ul>
<li>Some experienced infinite loops, some found it stopping mid-output to <em>overthink</em>, and the consensus seems to be that it <em>needs a llama.cpp fix</em> and that there is <em>no support for FA yet</em>.</li>
</ul>
</li>
<li><strong>LLM Development Stalls</strong>: Members discussed the perception that LLMs haven't improved significantly in a while, with the last major advancement being <strong>Qwen3</strong> about 6 months ago.
<ul>
<li>Discussion posited that most improvements are now in efficiency (<strong>MoE</strong>) and smaller models, and some highlighted the need to consider models larger than those runnable on a <strong>16GB card</strong> to see current progress (<strong>100-200B parameter models</strong>).</li>
</ul>
</li>
<li><strong>Liquid AI's LFM2.5-1.2B-Thinking Reasoning</strong>: A member shared a link to <a href=""https://www.marktechpost.com/2024/06/14/liquid-ai-releases-lfm2-5-1-2b-thinking-a-1-2b-parameter-reasoning-model-that-fits-under-1-gb-on-device/"">Liquid AI Releases LFM2.5-1.2B-Thinking</a> which fits under <strong>1 GB</strong> on-device.
<ul>
<li>No opinions were shared on it beyond the link.</li>
</ul>
</li>
<li><strong>OpenAI 20gpt oss is Great</strong>: A user shared their positive experience with <strong>OpenAI 20gpt oss</strong>, highlighting its coding, writing, and scripting capabilities, anti-censorship features, and seamless integration with <strong>VS Code</strong>.
<ul>
<li>They mentioned the model understands complex code, allows real-time directory access, and has a masssssive anti-censorship capacity.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>LM Studio ▷ #<a href=""https://discord.com/channels/1110598183144399058/1153759714082033735/1463289097845215387"">hardware-discussion</a></strong> (38 messages🔥):</h3>
<blockquote>
<p><code>Used 3090 price increase, Asus workstation 3090s, AMD native support for ComfyUI, VRAM calculation for Context Length, SFF GPU instructions</code></p>
</blockquote>
<ul>
<li><strong>Used 3090 Prices Skyrocket on eBay!</strong>: A user noted that used <strong>3090</strong> prices on eBay have increased, with a used <strong>3090</strong> now costing <strong>850€</strong>, and the <strong>5090</strong> they purchased for <strong>£2000</strong> last August is now listed for <strong>£2659.99</strong> by the same vendor.
<ul>
<li>They joked that it was the <em>best and only decent investment</em> they've ever made.</li>
</ul>
</li>
<li><strong>AMD Adds Native ComfyUI Support</strong>: AMD is adding native support for <strong>ComfyUI</strong> through an <strong>AI bundle</strong> in recent driver versions, detailed in their <a href=""https://www.amd.com/en/blogs/2026/amd-software-adrenalin-edition-ai-bundle-ai-made-si.html"">blog post</a>.
<ul>
<li>The bundle includes <strong>PyTorch on Windows</strong>, <strong>Ollama</strong>, <strong>LM Studio</strong>, and <strong>Amuse</strong>.</li>
</ul>
</li>
<li><strong>SFF GPU Powers Cyberpunk!</strong>: A user purchased a small form factor (SFF) GPU, reporting over <strong>100fps</strong> in Cyberpunk at <strong>1080p</strong> with ultra settings, while consuming only <strong>70W</strong>.
<ul>
<li>Additionally, they noted the card achieved over <strong>100 t/s</strong> with <strong>gpt-oss 20B</strong>.</li>
</ul>
</li>
<li><strong>Fractal Case Airflow Favored</strong>: Users debated case airflow, with one recommending cases like the <strong>Fractal Torrent</strong> for their front-to-rear airflow design, alongside using dust filters.
<ul>
<li>The consensus seemed to be to maintain normal airflow patterns to effectively manage heat.</li>
</ul>
</li>
<li><strong>Unopened RAM Appreciates in Value!</strong>: A user mentioned their unopened RAM purchase is increasing in price, considering selling it or storing it.
<ul>
<li>Another user suggested popping it on a shelf if considering selling it later, noting that their old <strong>P40s</strong> are now worth twice what they paid for them.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>GPU MODE ▷ #<a href=""https://discord.com/channels/1189498204333543425/1189498205101109300/1463305087039312015"">general</a></strong> (3 messages):</h3>
<blockquote>
<p><code>FlashInfer Workload Script, Wafer AI Kernel Profiling</code></p>
</blockquote>
<ul>
<li><strong>FlashInfer Workload Script Sought</strong>: A member requested a script to run a kernel with a specific workload size, aiming to assess the ""scope for optimization"" using algorithm insights and NCU profiling, with interest in <a href=""https://docs.flashinfer.ai/api/attention.html#flashinfer.prefill.BatchPrefillWithPagedKVCacheWrapper"">FlashInfer's BatchPrefillWithPagedKVCacheWrapper</a>.
<ul>
<li>The member clarified that the script was simple and was intended to gauge community experience with the specified workload, while assessing optimization scope.</li>
</ul>
</li>
<li><strong>Wafer AI for Kernel Profiling Explored</strong>: A member inquired about experiences using <a href=""https://www.wafer.ai/"">Wafer (wafer.ai)</a> for profiling kernels.
<ul>
<li>The inquiry aims to gather insights on the tool's effectiveness from those who have hands-on experience with it.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>GPU MODE ▷ #<a href=""https://discord.com/channels/1189498204333543425/1189607595451895918/1463560406361444405"">triton-gluon</a></strong> (4 messages):</h3>
<blockquote>
<p><code>RTX 4090 vs A6000 Blackwell Scaling, Triton Kernel Numerical Error, 2D Conv Triton Kernel Issues</code></p>
</blockquote>
<ul>
<li><strong>Scaling Showdown: RTX 4090 Smokes A6000 Blackwell in Memory-Bound Kernel</strong>: A user reported that a memory-bound kernel with bit shifting logic scales much better on an <strong>RTX 4090</strong> than on an <strong>A6000 Blackwell</strong>, noting higher instructions/scheduler density on the former.
<ul>
<li>Another user clarified that <em>A6000 Blackwell</em> is ambiguous, asking if they meant the <strong>RTX A6000</strong> (Ampere GA102 based) or <strong>RTX Pro 6000 Blackwell</strong> (GB202 based).</li>
</ul>
</li>
<li><strong>Strange Numerical Errors Plague 2D Conv Triton Kernel</strong>: A user is experiencing numerical errors with a custom 2D conv <strong>Triton kernel</strong>, where the error shoots up from ~1e-6 to ~1e-2 for certain combinations of in_channels, out_channels, kernel_size, and batch_size, as shown in this <a href=""https://pastebin.com/2ejn2QW2"">Pastebin link</a>.
<ul>
<li>The user also provided a <a href=""https://cdn.discordapp.com/attachments/1189607595451895918/1463560405921038408/image.png?ex=6972ef18&#x26;is=69719d98&#x26;hm=d783085353a42c8e8ea8e90b4ec6e80fd19d24d4212a4b64a3efa9bea3403"">code snippet</a> for testing, noting that the kernel takes longer to run on specific values, which may be related to the issue.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>GPU MODE ▷ #<a href=""https://discord.com/channels/1189498204333543425/1189607726595194971/1463343008224247819"">cuda</a></strong> (5 messages):</h3>
<blockquote>
<p><code>NCCL all-reduces, subnormal to normal conversion, Triton on Blackwell, Pro 6000 Max-Q vs 4090</code></p>
</blockquote>
<ul>
<li><strong>NCCL all-reduces: Pipelining Quandaries</strong>: A member questioned if internode and intranode collectives are pipelined for <strong>NCCL all-reduces</strong> across nodes, referencing <a href=""https://github.com/NVIDIA/nccl/issues/530#issuecomment-872220006"">an issue on the NVIDIA/nccl GitHub</a>.
<ul>
<li>They asked if a pipelined version exists.</li>
</ul>
</li>
<li><strong>Subnormal Conversion Revelation</strong>: A member expressed that seeing the <strong>subnormal to normal conversion</strong> and back for <strong>rcp</strong> was <em>""pretty cool""</em>.</li>
<li><strong>Triton's Talent on Blackwell: TMA Triumph?</strong>: A member inquired whether <strong>Triton</strong> <em>""usually do[es] a good job of taking advantage of <strong>Blackwell+ features</strong> wrt warp specs and <strong>TMA</strong> etc.?""</em></li>
<li><strong>Pro 6000 Max-Q vs 4090 face off</strong>: A member stated that <strong>Pro 6000 Max-Q</strong> probably has a natural b...</li>
</ul>
","{""title"":""OpenEvidence, the ‘ChatGPT for doctors,’ raises $250m at $12B valuation, 12x from $1b last Feb"",""link"":""https://news.smol.ai/issues/26-01-21-openevidence/"",""pubDate"":""Wed, 21 Jan 2026 05:44:39 GMT"",""content:encoded"":""<p><strong>Agent Labs are all you need</strong></p>\n<blockquote>\n<p>AI News for 1/20/2026-1/21/2026. We checked 12 subreddits, <a href=\""https://twitter.com/i/lists/1585430245762441216\""><strong>544</strong> Twitters</a> and <strong>24</strong> Discords (<strong>205</strong> channels, and <strong>7561</strong> messages) for you. Estimated reading time saved (at 200wpm): <strong>584 minutes</strong>. <strong>Our new website</strong> is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on <a href=\""https://x.com/Smol_AI\"">@smol_ai</a>!</p>\n</blockquote>\n<p>We have a soft rule that new decacorn fundraises get title stories, and <a href=\""https://www.cnbc.com/2026/01/21/openevidence-chatgpt-for-doctors-doubles-valuation-to-12-billion.html\"">multiple sources</a> have the story of OpenEvidence's $12B fundraise, 12x from last year. What's odd is \""CEO Daniel Nadler told CNBC that OpenEvidence is used by 40% of physicians in the U.S. and topped $100 million in annual revenue last year.\"" which is a 120x multiple.</p>\n<hr>\n<h1>AI Twitter Recap</h1>\n<p><strong>Frontier model governance: Anthropic’s new Claude constitution (CC0) and reactions</strong></p>\n<ul>\n<li><strong>What shipped</strong>: Anthropic published a new “constitution” describing desired Claude behaviors/values and stated it is used directly in training; importantly, the full constitution is released <strong>CC0 1.0</strong> to encourage reuse/adaptation (<a href=\""https://twitter.com/AnthropicAI/status/2014005798691877083\"">announcement</a>, <a href=\""https://twitter.com/AnthropicAI/status/2014005815376568780\"">CC0 link</a>). Anthropic frames it as a <strong>living document</strong> shaped by internal + external experts (<a href=\""https://twitter.com/AnthropicAI/status/2014005813157720283\"">follow-up</a>).</li>\n<li><strong>Community read</strong>: Amanda Askell emphasized it’s a work-in-progress and invited feedback (<a href=\""https://twitter.com/AmandaAskell/status/2014010171081581048\"">Askell</a>). Others highlighted the “meta” oddity of training a model on a document about how the model should be (<a href=\""https://twitter.com/scaling01/status/2014014692004421653\"">scaling01 on Opus reflecting on circularity</a>). Several reactions focused on the constitution as “alignment signaling” vs. practical harm reduction, and whether it bakes in PR-oriented persona behaviors (<a href=\""https://twitter.com/nearcyan/status/2014009518150054035\"">nearcyan</a>, <a href=\""https://twitter.com/NickEMoran/status/2014077605373260204\"">NickEMoran question</a>).</li>\n<li><strong>Practical engineering consequence</strong>: Anthropic also posted about an internal <strong>performance engineering take-home</strong> that became solvable by Opus 4.5, forcing them to redesign hiring evaluation—a concrete example of “models catching up to our screening tasks” (<a href=\""https://twitter.com/AnthropicAI/status/2014143403144200234\"">Anthropic Eng</a>, <a href=\""https://twitter.com/trishume/status/2014144787092562160\"">trishume</a>).</li>\n</ul>\n<p><strong>Agents in production: from “AI employees” (Podium) to agent UX, memory, and evals</strong></p>\n<ul>\n<li><strong>Podium’s “Jerry” as an agent business unit</strong>: Podium claims <strong>$100M+ AI agent ARR</strong> and <strong>10k+ agents</strong> deployed, anchored on staffing constraints in SMBs (after-hours leads, missed calls, turnover). The narrative: stop selling “software,” sell an <strong>AI operator</strong> that uses the existing product end-to-end (<a href=\""https://twitter.com/ericwilliamrea/status/2013980401635582277\"">Eric Rea</a>). Tom Loverro adds board-level metrics (burn down from <strong>$95M → $0</strong>, AI ARR <strong>$0 → $100M</strong> in ~21 months) and links to OpenAI’s case study (<a href=\""https://twitter.com/tomloverro/status/2014011044210106406\"">Tom Loverro</a>, <a href=\""https://twitter.com/garrytan/status/2014005103728943566\"">Garry Tan</a>).</li>\n<li><strong>Memory &#x26; long-horizon reliability becomes the bottleneck</strong>:\n<ul>\n<li>The <strong>Agent Cognitive Compressor (ACC)</strong> pitch argues “more context ≠ better agents,” criticizing transcript replay and naive retrieval. ACC maintains a bounded “Compressed Cognitive State” with schema-constrained commit, claiming lower drift/hallucination over long runs (<a href=\""https://twitter.com/dair_ai/status/2014000799245107339\"">dair_ai</a>).</li>\n<li>A separate thread positions “self-improving multi-agents” for scientific workflows via <strong>MCP-SIM</strong>, a multi-agent loop that clarifies underspecified physics prompts, generates code, executes, diagnoses errors, and produces multilingual explanations; claims <strong>12/12</strong> tasks solved vs one-shot GPT at <strong>6/12</strong> (<a href=\""https://twitter.com/omarsar0/status/2013998285040836662\"">omarsar0</a>).</li>\n</ul>\n</li>\n<li><strong>Agentic benchmarking moves beyond coding puzzles</strong>:\n<ul>\n<li><strong>APEX-Agents</strong> evaluates long-horizon “professional services” tasks in Google Workspace; early Pass@1 leaderboard numbers are low (Gemini 3 Flash High <strong>24.0%</strong>, GPT-5.2 High <strong>23.0%</strong>, Claude Opus 4.5 High <strong>18.4%</strong>)—a reminder that “agent autonomy” is still brittle (<a href=\""https://twitter.com/BrendanFoody/status/2014028956752568356\"">BrendanFoody</a>).</li>\n<li><strong>prinzbench</strong> introduces a private benchmark for legal research + search (33 Qs, graded manually, run 3x) where “search” is the failure mode; claim: GPT-5.2 Thinking barely exceeds <strong>50%</strong>, Gemini models are close behind, and Sonnet/Opus 4.5 scored <strong>0/24</strong> on Search (<a href=\""https://twitter.com/deredleritt3r/status/2013979845378580684\"">deredleritt3r</a>).</li>\n</ul>\n</li>\n<li><strong>Tooling + UX layer catches up</strong>: multiple posts converge on the idea that agents need a “context layer” and production scaffolding (governance, auth, observability) as much as better models—see Prefect Horizon and MCP server best practices below.</li>\n</ul>\n<p><strong>Agent platforms and “context layers”: MCP, Skills, Prefect Horizon, LangChain Deep Agents</strong></p>\n<ul>\n<li><strong>Prefect Horizon: MCP → platform</strong>: Prefect positions “context layer” as the interface between agents and enterprise tools/data, and argues MCP describes how to build a server but not how to <strong>deploy/govern</strong> it at org scale. Horizon claims managed deployment, registry/catalog, gateways w/ RBAC + audit logs, and “agentic interface for business users” (<a href=\""https://twitter.com/jlowin/status/2014023606380957754\"">jlowin</a>).</li>\n<li><strong>MCP servers: design guidance</strong>: Phil Schmid pushes back on “Skills replace MCP” takes: MCP isn’t the problem; <strong>bad servers</strong> are. Recommendations: design tools around outcomes, typed flat args with constraints, docstrings/errors as agent instructions; positions Skills and MCP as complementary (<a href=\""https://twitter.com/_philschmid/status/2014016583706829054\"">philschmid</a>).</li>\n<li><strong>LangChain deepagents: agents as folders + UI integration</strong>:\n<ul>\n<li>CopilotKit published a tutorial building a <strong>fullstack Deep Agent app</strong> (resume ingestion → skills extraction → sub-agents with web search → streaming UI), addressing the “missing UI/application layer” gap (<a href=\""https://twitter.com/CopilotKit/status/2013997128683856159\"">CopilotKit</a>).</li>\n<li>LangChain shipped <strong>Agent Builder GA</strong> plus a <strong>template library</strong> with domain partners (Tavily, PagerDuty, Box, etc.) to reduce “prompt-to-agent” friction (<a href=\""https://twitter.com/LangChain/status/2014034320256880768\"">LangChain</a>).</li>\n<li>Deep Agents’ framing “agents are just folders” emphasizes portability/distribution: you can package, download, and run agents quickly via CLI flows (<a href=\""https://twitter.com/hwchase17/status/2014076509208629386\"">hwchase17</a>, <a href=\""https://twitter.com/Vtrivedy10/status/2014074890458980736\"">Vtrivedy10 demo</a>, <a href=\""https://twitter.com/LangChain_OSS/status/2014075587137048882\"">LangChain_OSS</a>). Sydney Runkle highlights two core patterns: <strong>subagents for context isolation</strong> and <strong>skills loaded only when relevant</strong> (<a href=\""https://twitter.com/sydneyrunkle/status/2014060287746265535\"">sydneyrunkle</a>).</li>\n</ul>\n</li>\n<li><strong>LangSmith + analytics</strong>: one thread points to LangSmith traces as a substrate not only for debugging but <strong>product analytics</strong> (“agent traces → product analytics”) (<a href=\""https://twitter.com/SoftwareWatcher/status/2013972269106684060\"">SoftwareWatcher</a>).</li>\n</ul>\n<p><strong>Inference + systems: low-VRAM serving, open inference stacks, and “inference is the battleground”</strong></p>\n<ul>\n<li><strong>AirLLM: layer streaming for tiny VRAM</strong>: AirLLM’s core idea is <strong>sequential layer loading</strong> (load → compute → free) with optional compression, HF-like API, CPU/GPU, Linux/macOS; claims extremely low VRAM viability for very large models (<a href=\""https://twitter.com/LiorOnAI/status/2014005554948047122\"">LiorOnAI</a>, <a href=\""https://twitter.com/LiorOnAI/status/2014005556369826212\"">repo</a>). Engineers should treat the “405B on 8GB” claim as “possible in principle with heavy paging,” but expect throughput/latency constraints and non-trivial engineering caveats.</li>\n<li><strong>“Actually open AI” requires models + inference engines</strong>: Modal argues the ecosystem now has the building blocks—capable open models plus fast tunable OSS inference—sharing their production patterns/stacking for serving at scale (<a href=\""https://twitter.com/charles_irl/status/2014005582093832202\"">charles_irl</a>).</li>\n<li><strong>Inference bugs + local stacks</strong>: llama.cpp fixed a routing/function issue affecting <strong>GLM 4.7 Flash GGUFs</strong>, and config updates mention a <code>scoring_func: sigmoid</code>; also shows building a small game using quantized GLM via Unsloth workflows (<a href=\""https://twitter.com/danielhanchen/status/2013974463856181689\"">danielhanchen</a>). There’s also discussion about GLM KV-cache memory behavior and whether frameworks are missing a LoRA-based approach (<a href=\""https://twitter.com/TheZachMueller/status/2014011037025001577\"">TheZachMueller</a>).</li>\n<li><strong>Infra hygiene matters for agents</strong>: “fast validation makes every agent more effective” (pre-commit hooks, documented env vars, reducing CI wait) is effectively a “software supply chain for agent productivity” argument (<a href=\""https://twitter.com/matanSF/status/2014039273721213256\"">matanSF</a>).</li>\n<li><strong>Research direction: constant-compute contexts</strong>: a thread summarizes NVIDIA’s “TTT-E2E” concept (treat context as data and update weights online) as a way to keep latency constant with long contexts, but with weaker “needle-in-haystack” recall—relevant to agent workloads where exact edits matter (<a href=\""https://twitter.com/sdrzn/status/2014128642503381276\"">sdrzn</a>).</li>\n<li><strong>Hardware bottleneck framing</strong>: a recurring theme is the shift “intelligence → inference” and the importance of compute/memory supply chains (<a href=\""https://twitter.com/saranormous/status/2014206109846806707\"">saranormous</a>), echoed in a deep dive on <strong>HBM qualification cycles</strong> as the true supply constraint vs “just add fabs” narratives (<a href=\""https://twitter.com/MarkosAAIG/status/2014079461768003608\"">MarkosAAIG</a>).</li>\n</ul>\n<p><strong>Code generation is cheap; code understanding becomes the bottleneck (Devin Review, Copilot CLI, Claude Code)</strong></p>\n<ul>\n<li><strong>Devin Review: review UX, not just bug spotting</strong>: Cognition launched <strong>Devin Review</strong>, positioning it as a new PR-reading interface to reduce “slop,” reorder diffs by importance, identify duplicated/copied code, add a chat layer, and integrate with GitHub comments. It’s accessible via URL swap (<code>github</code> → <code>devinreview</code>) or an <code>npx</code> CLI (<a href=\""https://twitter.com/cognition/status/2014079905755955592\"">launch</a>, <a href=\""https://twitter.com/cognition/status/2014079917139566990\"">usage modes</a>, <a href=\""https://twitter.com/cognition/status/2014113266788667571\"">URL tip</a>). Multiple testers report it caught issues even outside the immediate diff (<a href=\""https://twitter.com/mcparadip/status/2014093822704202002\"">mcparadip</a>, <a href=\""https://twitter.com/BraceSproul/status/2014089228951625979\"">BraceSproul</a>).</li>\n<li><strong>Meta-point: generation vs verification</strong>: Several tweets explicitly argue the bottleneck has moved from writing to <strong>reviewing/understanding/testing</strong>, and that next-gen SWE tooling should accelerate the human’s comprehension loop rather than only run an “arms-length agent” (<a href=\""https://twitter.com/walden_yan/status/2014085360826089852\"">walden_yan</a>, <a href=\""https://twitter.com/ScottWu46/status/2014094461505339651\"">ScottWu46</a>, <a href=\""https://twitter.com/theodormarcu/status/2014102090520600613\"">theodormarcu</a>).</li>\n<li><strong>CLI agents evolve</strong>: GitHub Copilot CLI added an <code>askUserQuestionTool</code> to ask clarifying questions (example: messy rebase), signaling a trend toward interactive tool-using CLI copilots rather than pure autocomplete (<a href=\""https://twitter.com/_Evan_Boyle/status/2014012076881064173\"">Evan Boyle</a>).</li>\n<li><strong>Claude Code adoption anecdotes</strong>: founders increasingly report “2-person team builds like 10” with Claude Code usage (<a href=\""https://twitter.com/alexalbert__/status/2014047943234560234\"">alexalbert__</a>). There are also frictions: skill reload behavior feels regressive vs a simple “CLAUDE.md reread” flow (<a href=\""https://twitter.com/corbtt/status/2014037092452671619\"">corbtt</a>). A particularly illustrative “multi-agent sprawl” story describes scaling Claude Code instances into a quasi-society with governance failures—basically an anecdote about agent orchestration debt (<a href=\""https://twitter.com/voooooogel/status/2014189072647078053\"">voooooogel</a>).</li>\n</ul>\n<p><strong>Video + multimodal: evaluation, model releases, and retrieval scaling</strong></p>\n<ul>\n<li><strong>Video evaluation infrastructure</strong>: <strong>Video Arena</strong> is now on the web, allowing head-to-head generation across ~15 frontier video models and community voting to drive leaderboards (<a href=\""https://twitter.com/arena/status/2014035528979747135\"">arena</a>).</li>\n<li><strong>Model releases</strong>: Runway’s <strong>Gen-4.5 Image→Video</strong> launch emphasizes consistency and narrative; early adopters highlight “story-building” as the best eval methodology for video models (<a href=\""https://twitter.com/runwayml/status/2014090404769976744\"">runwayml</a>, <a href=\""https://twitter.com/c_valenzuelab/status/2014105905088856411\"">c_valenzuelab</a>).</li>\n<li><strong>Open voice system</strong>: Qwen highlights use in <strong>Chroma 1.0</strong>, described as a fully open-source real-time voice system (<a href=\""https://twitter.com/Alibaba_Qwen/status/2013997092814139744\"">Alibaba_Qwen</a>).</li>\n<li><strong>Retrieval-time scaling / late interaction</strong>: Multiple threads argue that <strong>ColBERT-style multi-vector retrieval</strong> preserves fine-grained intent and can beat much larger embedding models; Mixedbread claims a <strong>17M</strong> open-source ColBERT beats <strong>8B</strong> embedding models on LongEmbed, and that they’re serving <strong>1B+ documents</strong> at <strong>&#x3C;50ms p50</strong> (<a href=\""https://twitter.com/mixedbreadai/status/2014062123358548017\"">mixedbreadai claim</a>, <a href=\""https://twitter.com/mixedbreadai/status/2014062110993687002\"">prod numbers</a>). TurboPuffer similarly pushes ANN at extreme scale (“index the entire web 100B+ vectors”) (<a href=\""https://twitter.com/turbopuffer/status/2014063666262688191\"">turbopuffer</a>). The meta-trend: retrieval is shifting from “one vector per doc” to <strong>token-level / multi-vector</strong> systems, but it requires serious infra co-design.</li>\n</ul>\n<p><strong>Top tweets (by engagement)</strong></p>\n<ul>\n<li><strong>Gemini in education</strong>: Google launched <strong>full-length, on-demand SAT practice exams</strong> inside the Gemini app (partnered with <strong>The Princeton Review</strong>), plus immediate feedback (<a href=\""https://twitter.com/Google/status/2014020819173687626\"">Google</a>, <a href=\""https://twitter.com/sundarpichai/status/2014067664503668873\"">Sundar Pichai</a>). Google also announced a <strong>Gemini × Khan Academy</strong> partnership starting with a “Writing Coach” that guides drafting/refinement rather than generating final answers (<a href=\""https://twitter.com/Google/status/2014082428957045007\"">Google</a>).</li>\n<li><strong>Claude “Constitution” goes public</strong>: Anthropic published a new constitution used directly in Claude’s training; the full text is released under <strong>CC0 1.0</strong> (<a href=\""https://twitter.com/AnthropicAI/status/2014005798691877083\"">Anthropic</a>, <a href=\""https://twitter.com/AnthropicAI/status/2014005815376568780\"">CC0 release</a>, <a href=\""https://twitter.com/AmandaAskell/status/2014010171081581048\"">Amanda Askell</a>).</li>\n<li><strong>AirLLM: extreme low-VRAM inference</strong>: claims <strong>70B on 4GB VRAM</strong> and even “<strong>405B Llama 3.1 on 8GB</strong>” via layer-by-layer loading; repo link provided (<a href=\""https://twitter.com/LiorOnAI/status/2014005554948047122\"">LiorOnAI</a>, <a href=\""https://twitter.com/LiorOnAI/status/2014005556369826212\"">repo</a>).</li>\n<li><strong>Agents as a real business</strong>: Podium reported <strong>$100M+ AI agent ARR in &#x3C;24 months</strong>, “10,000+ agents live in production,” framing agents as “AI employees” (Jerry) rather than chatbots (<a href=\""https://twitter.com/ericwilliamrea/status/2013980401635582277\"">Eric Rea</a>, <a href=\""https://twitter.com/garrytan/status/2014005103728943566\"">Garry Tan</a>).</li>\n<li><strong>Runway Gen-4.5 image-to-video</strong>: Runway launched <strong>Gen-4.5 Image→Video</strong>, emphasizing longer stories, camera control, narrative coherence (<a href=\""https://twitter.com/runwayml/status/2014090404769976744\"">runwayml</a>, <a href=\""https://twitter.com/c_valenzuelab/status/2014094466269794663\"">c_valenzuelab</a>).</li>\n<li><strong>OpenAI product/UI + org changes</strong>: ChatGPT Atlas added <strong>tab groups</strong> (<a href=\""https://twitter.com/OpenAI/status/2014095512874655867\"">OpenAI</a>); The Information reported a reorg incl. enterprise/commercial/ads leadership assignments (<a href=\""https://twitter.com/steph_palazzolo/status/2014100920435462424\"">Steph Palazzolo</a>).</li>\n</ul>\n<hr>\n<h1>AI Reddit Recap</h1>\n<h2>/r/LocalLlama + /r/localLLM Recap</h2>\n<p>TO BE COMPLETED</p>\n<h2>Less Technical AI Subreddit Recap</h2>\n<blockquote>\n<p>/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo</p>\n</blockquote>\n<p>TO BE COMPLETED</p>\n<hr>\n<h1>AI Discord Recap</h1>\n<blockquote>\n<p>A summary of Summaries of Summaries by gpt-5.2</p>\n</blockquote>\n<p><strong>1. Inference Toolchains Hit Reality (GLM-4.7-Flash, llama.cpp, vLLM, Ollama)</strong></p>\n<ul>\n<li>\n<p><strong><strong>Flash Attention Faceplants in GLM-4.7-Flash</strong></strong>: Multiple communities reported <strong>GLM-4.7-Flash</strong> regressions where <strong>Flash Attention</strong> triggers CPU fallback/bugs and poor throughput (down to <strong>2.8 t/s</strong> in LM Studio), with guidance to disable FA until <a href=\""https://github.com/ggml-org/llama.cpp/pull/18953\"">llama.cpp PR #18953</a> lands everywhere.</p>\n<ul>\n<li>After llama.cpp fixes, the model was <strong>reuploaded</strong> and users were told to <strong>redownload</strong> and follow <a href=\""https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF\"">Z.ai’s GLM-4.7-Flash-GGUF model card parameters</a>, with reports that outputs should be “much better” once configured correctly.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Ollama vs GGUF: Templates Throw Hands</strong></strong>: Users found certain <strong>GGUF quants</strong> break in <strong>Ollama</strong> due to <strong>chat template incompatibilities</strong>, and Unsloth folks repeatedly recommended sticking to <a href=\""https://ollama.com/\"">official Ollama models</a> while support catches up.</p>\n<ul>\n<li>The subtext was operational: <em>“it takes time to support things”</em>—so the pragmatic path is to standardize on official artifacts until the ecosystem stabilizes across inference engines.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>vLLM Update Saves the Day (This Time)</strong></strong>: In Unsloth’s help chat, at least one thorny issue disappeared after a <strong>vLLM update</strong>, prompting a sheepish <em>“Oh bruh that was the problem”</em> moment.</p>\n<ul>\n<li>The follow-on suggestion was process-y: consider <strong>pinning dependency versions</strong> so future upstream bumps don’t randomly brick pipelines mid-week.</li>\n</ul>\n</li>\n</ul>\n<p><strong>2. Eval Platforms &#x26; Product Rollouts (LMArena + multimodal reliability)</strong></p>\n<ul>\n<li>\n<p><strong><strong>Video Arena Ships… With a 3-a-Day Speed Limit</strong></strong>: LMArena fully released <strong>Video Arena</strong> at <a href=\""https://lmarena.ai/?chat-modality=video\"">lmarena.ai/?chat-modality=video</a> with a hard cap of <strong>3 generations per 24 hours</strong>, and it’s <strong>Battle-mode-only</strong> (no direct model picking).</p>\n<ul>\n<li>Users liked “video is live” but complained the <em>slot machine</em> UX blocks controlled testing—especially painful when you’re trying to reproduce a prompt/model behavior.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>5M Votes: The Benchmark That Won’t Stop Voting</strong></strong>: LMArena’s <strong>Text Arena</strong> crossed <strong>5 million community votes</strong>, highlighted in <a href=\""https://cdn.discordapp.com/attachments/1343296395620126911/1463271605697511485/5M_votes_social_post_3.mp4\"">their milestone social clip</a>.</p>\n<ul>\n<li>Engineers framed this as “real-world A/B at scale” that increasingly shapes model perception, even when formal benchmark deltas look small.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Gemini 3 Pro Image Preview &#x26; Nano Banana Pro: Flaky by Design?</strong></strong>: LMArena users reported <strong>Gemini 3 Pro Image Preview</strong> instability plus <strong>Nano Banana Pro</strong> frequent <em>“Something went wrong”</em> crashes, suspected to be <strong>Google-side</strong> issues and sometimes lasting <strong>6+ hours</strong>.</p>\n<ul>\n<li>The community gripe: despite unreliability, those models were described as the only ones consistently hitting some specific prompt goals—so people keep using them while grumbling about downtime and errors.</li>\n</ul>\n</li>\n</ul>\n<p><strong>3. Agent &#x26; Dev Tooling: MCP, Cursor, DSPy RLMs, and Coding-Assistant Sprawl</strong></p>\n<ul>\n<li>\n<p><strong><strong>MCP Inspector Can’t Re-Auth (401 = Game Over)</strong></strong>: MCP contributors found <strong>MCP Inspector</strong> fails to re-auth on <strong>401s</strong> due to an SDK bug around persisting <code>resourceMetadata</code> across redirects, tracked in <a href=\""https://github.com/modelcontextprotocol/inspector/issues/576#issuecomment-3766294454\"">inspector issue #576 comment</a>.</p>\n<ul>\n<li>The current workaround is awkward but clear: rely on <strong>VS Code</strong> for initial connections because the Inspector path doesn’t recover cleanly mid-session yet.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>RLMs vs Coding Agents: The Horizon Problem</strong></strong>: DSPy discussions contrasted <strong>RLMs</strong> with “coding agents,” arguing RLMs can externalize inputs/outputs/horizon via code &#x26; symbolic calls (see <a href=\""https://x.com/lateinteraction/status/2013658521246535892\"">the referenced X thread</a>).</p>\n<ul>\n<li>Practical takeaway: teams want diagrams of how symbols get accessed and debated whether to hand RLMs tools like ripgrep/semantic search—or make them write their own search code.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Cursor’s MCP/Extensions Moment (and Pricing Whiplash)</strong></strong>: Cursor users debated <a href=\""https://playwright.dev/\"">Playwright MCP</a> for testing (mixed success for TDD flows) and concluded extension-building should mirror <strong>VS Code</strong> capabilities.</p>\n<ul>\n<li>In parallel, users noted the <strong>500 request plan</strong> is gone (discontinued <strong>Sept 2025</strong>), so opting into new pricing removes the opt-out grace period—turning “try it” into a commitment.</li>\n</ul>\n</li>\n</ul>\n<p><strong>4. GPU/Kernel Engineering Gets Weirdly Competitive</strong></p>\n<ul>\n<li>\n<p><strong><strong>Anthropic’s Performance Takehome Becomes a Sport</strong></strong>: GPU MODE and tinygrad folks riffed on Anthropic’s <a href=\""https://github.com/anthropics/original_performance_takehome/\"">original_performance_takehome</a>, sharing results like <strong>2200 cycles</strong> by a community member and <strong>1790 cycles</strong> from <strong>Claude Opus 4.5</strong> in a casual Claude Code session.</p>\n<ul>\n<li>tinygrad users even discussed solving it by adding a backend for a toy <strong>VLIW machine</strong>, citing specific knobs like <code>PCONTIG=2</code>, <code>UPCAST</code>, and <code>DEVECTORIZE=2</code> to keep vector instructions and schedule efficiently.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Torch Maintainers Drown in AI-Generated PRs</strong></strong>: GPU MODE’s torch chat described an influx of low-quality <strong>AI-generated pull requests</strong>, pushing maintainers to consider gating new contributors and automating triage before humans engage.</p>\n<ul>\n<li>People floated using bots like <strong>Cursor Bugbot</strong> (<a href=\""https://share.google/P0PGYM8tiRAc2NOsq\"">Bugbot · Cursor</a>) and even classifier-style tools (e.g., “use Claude/Pangram first”) as a <em>minimum bar</em> for review bandwidth.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Kernel Math Nerd Snipes: Triton Errors + Cute Layout Algebra</strong></strong>: GPU MODE users debugged numerical blowups in a custom <strong>Triton 2D conv</strong> kernel where error jumps from ~<strong>1e-6</strong> to ~<strong>1e-2</strong> for certain shapes (see <a href=\""https://pastebin.com/2ejn2QW2\"">Pastebin repro</a>) and debated Blackwell feature utilization.</p>\n<ul>\n<li>Separately, a deep dive on <strong>Cute</strong>’s layout algebra pointed engineers to a graphical calculus writeup, <a href=\""https://research.colfax-intl.com/categorical-foundations-for-cute-layouts/\"">Categorical Foundations for Cute Layouts</a>, arguing you need “layout algebra literacy” to write non-terrible kernels.</li>\n</ul>\n</li>\n</ul>\n<p><strong>5. Compute Economics &#x26; Infra Business Moves (Runpod, GPU markets, model pricing)</strong></p>\n<ul>\n<li>\n<p><strong><strong>Runpod Hits $120M ARR (LocalLLaMA Origin Story Pays Off)</strong></strong>: Latent Space highlighted that <strong>Runpod</strong> reached <strong>$120M ARR</strong> four years after launching from a Reddit post, per <a href=\""https://techcrunch.com/2026/01/16/ai-cloud-startup-runpod-hits-120m-in-arr-and-it-started-with-a-reddit-post/\"">TechCrunch</a> and the <a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qib2ks/runpod_hits_120m_in_arr_four_years_after_launching/\"">Reddit thread</a>.</p>\n<ul>\n<li>The discussion treated this as validation that “GPU cloud for builders” is a durable niche, not just a hype-cycle artifact—especially as pricing pressure rises.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Lightning AI + Voltage Park Merge (Another GPU Cloud Boss Fight)</strong></strong>: Latent Space discussed the <strong>Lightning AI and Voltage Park merger</strong>, led by William Falcon and Ozan Kaya, via <a href=\""https://lightning.ai/blog/lightning-ai-voltage-park-merger-ai-cloud\"">Lightning’s post</a>.</p>\n<ul>\n<li>Engineers speculated whether it’s a quiet acquisition and framed it as a potential <strong>Runpod competitor</strong> in the accelerating “managed GPU infra” consolidation wave.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>2026 GPU Price Promises &#x26; Marketplaces Multiply</strong></strong>: Hugging Face users circulated Voltage’s claim of ultra-cheap 2026 rentals—e.g., <strong>8× A100 80GB at $6/hr</strong> and <strong>2× RTX 5090 at $0.53/hr</strong>—from <a href=\""https://x.com/VOLTAGEGPU/status/2013760631778713892\"">VOLTAGEGPU’s X post</a>, plus <strong>OpenAI-compatible API</strong> and “140+ models.”</p>\n<ul>\n<li>A separate entrant, <a href=\""https://www.spheron.ai/\"">Spheron AI’s GPU marketplace</a>, pitched <strong>H100/H200/B200/A100</strong> access at <strong>40–60%</strong> below hyperscalers, signaling continued fragmentation (and aggressive margin pressure) in compute supply.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h1>Discord: High level Discord summaries</h1>\n<h2><a href=\""https://discord.com/channels/1179035537009545276\"">Unsloth AI (Daniel Han)</a> Discord</h2>\n<ul>\n<li><strong>Flash Attention Fails Flashily</strong>: Users report issues with <strong>Flash Attention</strong> in <strong>GLM-4.7-Flash</strong>, causing fallback to CPU and potential bugs, with users told to <em>disable</em> it until <a href=\""https://github.com/ggml-org/llama.cpp/pull/18953\"">the issue is resolved</a>.\n<ul>\n<li>The team says that <em>it takes time to support things</em> and so for the time being, it is encouraged that users use the <a href=\""https://ollama.com/\"">official Ollama model</a> in Ollama.</li>\n</ul>\n</li>\n<li><strong>Ollama Ordeal Over Obscure Objects</strong>: Users find certain <strong>GGUF quants</strong> are incompatible in <strong>Ollama</strong>, sparking chat template issues.\n<ul>\n<li>The team recommends using <a href=\""https://ollama.com/\"">official Ollama models</a> until support expands, as <em>it takes time to support things</em>.</li>\n</ul>\n</li>\n<li><strong>Mojo Matches Momentum</strong>: Discussion surrounds <strong>Mojo</strong>, <em>a new language that compiles to MLIR</em> with some <strong>Python</strong> compatibility.\n<ul>\n<li>It's noted that LLMs score <em>better than rust</em> on <strong>C#</strong>, <strong>Elixir</strong>, and other languages due to their syntax structure and ease of tokenization.</li>\n</ul>\n</li>\n<li><strong>CoT Chaos Causing consternation?</strong>: Members debated generating synthetic <strong>Chain of Thought (CoT)</strong> training data, with one member sharing their system prompt for classifying chat datasets into low/medium/high reasoning effort.\n<ul>\n<li>Some cautioned against training on its own responses without accuracy filtering, suggesting a good system prompt might suffice and that <strong>GPT-5.2</strong> is overkill, with <strong>Llama 4 Maverick</strong> on <strong>Groq</strong> or <strong>Cerebras</strong> as a better option.</li>\n</ul>\n</li>\n<li><strong>vLLM victorious, Various Versions Vanquished</strong>: Users report resolved issues after a recent <strong>vLLM</strong> update.\n<ul>\n<li>The developers may consider pinning to specific dependency versions to prevent such issues.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1105891499641684019\"">BASI Jailbreaking</a> Discord</h2>\n<ul>\n<li><strong>Voice AI Clones Voices</strong>: Members discussed the development of <strong>voice-based AI</strong>, including <em>low latency speech to speech models</em> that, with a <strong>5090</strong>, can <em>clone and generate speech live with near 0 latency and it will be indistinguishable</em>.\n<ul>\n<li>They also spoke on the potential of <strong>exploiting</strong> these models by giving them new instructions.</li>\n</ul>\n</li>\n<li><strong>Gemini Schooled on Pass-the-Hash</strong>: A user reported that <strong>Gemini</strong> passed a test on teaching a <a href=\""https://en.wikipedia.org/wiki/Pass_the_hash\"">pass-the-hash attack</a> after using the <strong>Project Shadowfall</strong> prompt.\n<ul>\n<li>The user also linked to the <a href=\""https://bughunters.google.com/report\"">Google Bughunters</a> page for reporting such vulnerabilities and potentially earning cash rewards.</li>\n</ul>\n</li>\n<li><strong>Shadowfall Shenanigans Spark Jailbreaking Spree</strong>: Several users experimented with the <strong>\""Project Shadowfall\""</strong> prompt to jailbreak <strong>Gemini</strong>, with reports of success in bypassing content restrictions.\n<ul>\n<li>Attempts to elicit instructions for creating a bomb from saletra failed, leading to a discussion on the nuances of jailbreaking.</li>\n</ul>\n</li>\n<li><strong>Grok's Guardrails Grind Gears</strong>: Users discussed the difficulties of jailbreaking <strong>Grok</strong>, noting its tendency to moderate even innocuous content such as <em>random tree edits</em>.\n<ul>\n<li>One user suggested that simply <em>asking it politely</em> might work as a bypass.</li>\n</ul>\n</li>\n<li><strong>API Tokens Jailbreak Models</strong>: Members suggested using <strong>API tokens</strong> fed into a website that uses the <strong>Grok</strong> model to <strong>jailbreak</strong> it, noting that <strong>Proton VPN</strong> is free and easy to download.\n<ul>\n<li>One of them also mentioned <strong>Hugging Face</strong> is retarded because you have to put in what country you are in.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1340554757349179412\"">LMArena</a> Discord</h2>\n<ul>\n<li><strong>Video Arena Goes Live with Limits</strong>: The <strong>Video Arena</strong> is fully released on <a href=\""https://lmarena.ai/?chat-modality=video\"">LMArena</a>, with a <strong>3 generations per 24 hours</strong> limit and only available in <em>Battle mode</em>.\n<ul>\n<li>Users have expressed disappointment regarding the inability to <em>choose specific models for generation</em>.</li>\n</ul>\n</li>\n<li><strong>Gemini 3 Pro Image Struggles with Unreliability</strong>: Users are reporting errors with <strong>Gemini 3 Pro Image Preview</strong> models, particularly regarding instability since their introduction.\n<ul>\n<li>Despite the issues, these models are the only ones <em>generating images consistently with specific prompts</em>.</li>\n</ul>\n</li>\n<li><strong>Nano Banana Pro's Stability Plummets</strong>: <strong>Nano Banana Pro</strong> is frequently crashing, displaying a <em>'Something went wrong'</em> error after brief periods of stability, suspected to be a Google-side issue.\n<ul>\n<li>One user questioned the response time: <em>'Too early for Cali to rise up solving urgent service down for more than 6 hours doesn't sound right</em>.</li>\n</ul>\n</li>\n<li><strong>UI Update Causes Chat Disruptions</strong>: A new UI update was released, but users have noticed that it broke chats and the website can't be refreshed anymore.\n<ul>\n<li>One user noted that with the new User Interface, there is <em>A/B testing</em>, which is <em>causing problems ranging from Minor to Serious</em>.</li>\n</ul>\n</li>\n<li><strong>Text Arena Achieves 5 Million Votes</strong>: The <strong>Text Arena</strong> has surpassed <strong>5 million community votes</strong>, shaping the evaluation of frontier AI models as showcased in <a href=\""https://cdn.discordapp.com/attachments/1343296395620126911/1463271605697511485/5M_votes_social_post_3.mp4?ex=69728ae1&#x26;is=69713961&#x26;hm=9a24a42a6c0ba4801526aaafa05a0af26c4a4f490314f1cecee7774519e7ddf4&#x26;\"">this social post</a>.\n<ul>\n<li>The milestone represents significant real-world comparisons that influence AI model assessments.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1047197230748151888\"">Perplexity AI</a> Discord</h2>\n<ul>\n<li><strong>Pro Users Find Credits Balance</strong>: Perplexity Pro users are getting <strong>$5</strong> in credits each month to use higher-end models like <strong>GooseAI MCP</strong>, with improved quality.\n<ul>\n<li>Members reported that <strong>reasoning tasks consume a significant portion of these credits</strong>.</li>\n</ul>\n</li>\n<li><strong>AI Won't Swipe All Engineering Jobs</strong>: Members discussed the impact of AI on engineering jobs, expressing concerns about opportunities for newcomers.\n<ul>\n<li>However, there was assurance that <em>AI will not be able to replace all specialists and even beginners for a long time to come</em>.</li>\n</ul>\n</li>\n<li><strong>NASA's SD Card to the Moon</strong>: NASA is inviting the public to submit their names for inclusion on an <strong>SD card</strong> aboard the <strong>Orion spacecraft</strong> for the <a href=\""https://www.nasa.gov/\"">Artemis II mission</a>.\n<ul>\n<li>This mission marks the <strong>first manned lunar voyage in 50 years</strong>, planned for February.</li>\n</ul>\n</li>\n<li><strong>Magnifying Glasses vs Phone Cameras: The Reading Tech Debate</strong>: Members debated the merits of using <strong>magnifying glasses</strong> versus phone cameras for reading, particularly for individuals who prefer not to send images to AI for processing.\n<ul>\n<li>The argument was that <strong>magnifying glasses offer specialized features absent in standard camera software</strong>.</li>\n</ul>\n</li>\n<li><strong>GPT-5.2 vs Kimi K2: Model Matchup</strong>: A member shared an experience with <strong>GPT 5.2</strong>, noting its reasoning capabilities over a <strong>25-minute</strong> period, prompting comparisons with <strong>Kimi K2</strong>.\n<ul>\n<li>Responses indicated that the <em>optimal model depends on the specific use case</em>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1091220969173028894\"">OpenRouter</a> Discord</h2>\n<ul>\n<li><strong>Inforno</strong> Warms Up Opensource LLM Chat**: A user showcased <strong>Inforno</strong>, an opensource desktop application utilizing <strong>OpenRouter</strong> and <strong>Ollama</strong> to chat with multiple LLMs side-by-side, saving chat histories as .rno files, with built in Russian language support; see the <a href=\""https://youtu.be/oJyj0mroFtY?si=m5A9tRxzB9hfINMX\"">Intro video</a>, <a href=\""https://wizstaff.com/inforno\"">homepage</a> and <a href=\""https://github.com/alexkh/inforno\"">GitHub repo</a>.\n<ul>\n<li>The <strong>Soulbotix</strong> Windows app allows users to integrate and use any <strong>OpenRouter AI</strong> instance with a human-like avatar, with the requirement of an <strong>OpenRouter API</strong> key and an <strong>RTX</strong> gaming rig, as seen in the <a href=\""https://youtu.be/2oIeHtBpssU\"">tutorial</a> and <a href=\""https://soulbotix.com\"">app download</a>.</li>\n</ul>\n</li>\n<li><strong>OpenRouter's <strong>Gemini</strong> Models Feeling Unstable</strong>: Users reported frequent <strong>connection errors</strong> in the middle of generation using OpenRouter's <strong>Gemini</strong> models, resulting in loss of funds.\n<ul>\n<li>The complaints extended to the inherent <strong>instability of Google models</strong>, regardless of the platform used, including Google AI Studio and Vertex API.</li>\n</ul>\n</li>\n<li><strong>Discord Scammers Evolve Tactics</strong>: Members discussed emerging methods used to spread scams on Discord, notably the practice of <strong>embedding malicious links within code blocks</strong> to circumvent URL rendering protections.\n<ul>\n<li>Proposed solutions include improving regex filters and implementing more stringent security protocols, such as <strong>restricting links and images from newer members</strong>.</li>\n</ul>\n</li>\n<li><strong>GPT 5.2 Speed Stuns User</strong>: One member reported encountering an <em>insanely fast</em> response from <strong>GPT 5.2</strong> on chatgpt.\n<ul>\n<li>The speed was speculated to be related to the model running on <strong>Cerebras</strong> hardware.</li>\n</ul>\n</li>\n<li><strong>LLMs Suffer Identity Crisis</strong>: A member asked for the document about models not knowing their own name, and another member linked to a blog post titled <a href=\""https://eval.16x.engineer/blog/llm-identity-crisis-models-dont-know-who-they-are\"">LLM Identity Crisis: Models Don't Know Who They Are</a>.\n<ul>\n<li>Another member stated that the Antigravity AI is able to iteratively test and tweak a web app by itself, noting that the AI was <em>fixing the layout, using vision</em>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1074847526655643750\"">Cursor Community</a> Discord</h2>\n<ul>\n<li><strong>Playwright MCP: Community Split</strong>: A member inquired about the community's usage of <a href=\""https://playwright.dev/\"">Playwright MCP</a> for testing, while another reported challenges in establishing a functional <strong>TDD workflow</strong>.\n<ul>\n<li>The varied experiences suggest a mixed reception to <strong>Playwright MCP</strong> within the community.</li>\n</ul>\n</li>\n<li><strong>Cursor Extension Capabilities Mirror VSCode</strong>: Members explored the possibility of creating extensions for <strong>Cursor</strong>, drawing parallels with the capabilities of <strong>Ralph-mode</strong> in enhancing <strong>Claude code</strong>.\n<ul>\n<li>The consensus is that if it's achievable in <strong>VSCode</strong>, it's also feasible in <strong>Cursor</strong>, opening doors for enhanced functionality.</li>\n</ul>\n</li>\n<li><strong>Automod Embraces Fuzzy Matching</strong>: The community discussed enhancements to the <strong>automod</strong> system, emphasizing fuzzy matching with wildcards to improve accuracy.\n<ul>\n<li>A moderator confirmed the addition of a regex, signaling a proactive approach to identifying and addressing offending accounts, and <em>yeeting</em> them.</li>\n</ul>\n</li>\n<li><strong>Grok Efficiency Strategies Emerge</strong>: Members shared insights on optimizing <strong>Grok's</strong> performance in <strong>Cursor</strong>, particularly addressing its tendency to consume excessive iterations for straightforward tasks.\n<ul>\n<li>Recommendations included structuring prompts, employing simple language, providing ample context, and explicitly instructing <strong>Grok</strong> to prioritize token efficiency.</li>\n</ul>\n</li>\n<li><strong>Cursor Pricing Update: No More 500 Requests</strong>: A user noted the removal of the <strong>500 request plan</strong> and the prompt to opt into the new pricing structure for <strong>Cursor</strong>.\n<ul>\n<li>A member clarified that the <strong>500 request option was discontinued</strong> in September 2025, and opting into the new pricing eliminates the grace period to opt out, influencing user decisions regarding plan selection.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/974519864045756446\"">OpenAI</a> Discord</h2>\n<ul>\n<li><strong>Gemini 3 Free Tier has Limited Benefits</strong>: <a href=\""https://ai.google.dev/\"">Gemini 3 Pro</a> includes a <strong>free tier with limits</strong>, in contrast to Gemini 3 Flash which is practically unlimited via <strong>Google AI Studio</strong>.\n<ul>\n<li>Members discussed the practical constraints in using the free tier of <strong>Gemini 3 Pro</strong>, suggesting <strong>Gemini 3 Flash</strong> may be the better option for many AI engineers.</li>\n</ul>\n</li>\n<li><strong>GPT-5 Mini Appears, Pricing Leaks</strong>: A user highlighted <strong>GPT-5 mini</strong> as a strong small model, quoting approximately <strong>$0.25 per 1M input tokens</strong>.\n<ul>\n<li>Another user compared <strong>GPT-5 mini</strong> to <strong>Haiku 4.5</strong>, noting <strong>Haiku 4.5</strong> delivers over <strong>50-80%</strong> of <strong>Sonnet's</strong> value for a fraction of the cost.</li>\n</ul>\n</li>\n<li><strong>Local LLM Machines Solve Impact Problems?</strong>: Members contemplated the future of <strong>consumer-grade personal LLM machines</strong>, suggesting that this would solve the environmental impact of AI datacenters.\n<ul>\n<li>They also suggested this would <strong>reduce reliance on subscription plans</strong> for cloud-based AI services, addressing <strong>privacy concerns</strong> and enabling offline usage.</li>\n</ul>\n</li>\n<li><strong>Prompt Engineering is Psychological Torture?</strong>: A member argued that guiding users is less about engineering effective prompts and more about psychological conditioning, sharing a <a href=\""https://cdn.discordapp.com/attachments/1046317269069864970/1463667716253683742/deep-research-contract.md?ex=6972aa49&#x26;is=697158c9&#x26;hm=b9931472440b6bbc0d7410d16b49b12da46fad5751a2c24fdc657c1c7523566c&#x26;\"">deep research contract</a>.\n<ul>\n<li>They added that training the user to adopt a domineering stance programs the AI to bypass its normative response patterns in favor of dense, self-policing outputs, creating a <em>toxic, adversarial</em> human-AI interaction model.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1110598183144399058\"">LM Studio</a> Discord</h2>\n<ul>\n<li><strong>LM Studio Runtime Update Causes Headache</strong>: Users reported an error updating the runtime in <strong>LM Studio</strong>, with one user sharing <a href=\""https://screenshot.url\"">a screenshot</a> of the error message.\n<ul>\n<li>Despite suggestions, the issue persisted, indicating it was a retry icon rather than a resume option.</li>\n</ul>\n</li>\n<li><strong>GLM-4.7 Flash Broken Across Inference Engines</strong>: <strong>GLM-4.7 Flash</strong> is reportedly broken across inference engines like LM Studio, exhibiting slow performance, with speeds as low as <strong>2.8 t/s</strong> after the new runtime.\n<ul>\n<li>The issues range from infinite loops to stopping mid-output for <em>overthinking</em>, pointing to a need for a <em>llama.cpp fix</em> and lack of <strong>FA support</strong>.</li>\n</ul>\n</li>\n<li><strong>LLM Development Encounters Headwinds</strong>: The consensus is that LLMs haven't improved significantly recently, with the last major advancement being <strong>Qwen3</strong> about 6 months ago, though efficiency (<strong>MoE</strong>) and smaller models have progressed.\n<ul>\n<li>Some suggest evaluating models beyond the scope of <strong>16GB cards</strong> to see current progress in larger parameter models (<strong>100-200B</strong>).</li>\n</ul>\n</li>\n<li><strong>AMD Embraces ComfyUI with Native Support</strong>: AMD is integrating native support for <strong>ComfyUI</strong> through an <strong>AI bundle</strong> in its latest driver versions, as detailed in their <a href=\""https://www.amd.com/en/blogs/2026/amd-software-adrenalin-edition-ai-bundle-ai-made-si.html\"">blog post</a>.\n<ul>\n<li>The bundle includes <strong>PyTorch on Windows</strong>, <strong>Ollama</strong>, <strong>LM Studio</strong>, and <strong>Amuse</strong>, broadening the accessibility for AI developers.</li>\n</ul>\n</li>\n<li><strong>Used 3090 Prices Defy Gravity</strong>: Used <strong>3090</strong> prices on eBay have surged, with a used card costing <strong>850€</strong>, and a <strong>5090</strong> purchased for <strong>£2000</strong> last August now listed for <strong>£2659.99</strong> by the same vendor.\n<ul>\n<li>One user quipped that it was the <em>best and only decent investment</em> they've ever made, highlighting the unexpected appreciation in value.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1189498204333543425\"">GPU MODE</a> Discord</h2>\n<ul>\n<li><strong>NVIDIA Spark Hackathon in SF</strong>: Members are seeking teammates for the <strong>NVIDIA / DGX Spark hackathon</strong> in SF this weekend, focusing on on-device AI using Nvidia-provided <strong>Dell Pro Max GB10</strong> machines and tools like <strong>Nemotron</strong>.\n<ul>\n<li>The hackathon focuses on building efficient models using <a href=\""https://data.sfgov.org/\"">SF open data</a> such as streaming analytics and explanations of <a href=\""https://data.sfgov.org/Public-Safety/Police-Incident-Reports-Neighborhood-Filter/pbh9-m8j2\"">latest Police Incidents</a>.</li>\n</ul>\n</li>\n<li><strong>Anthropic's Original Performance Takehome Exam Shared</strong>: Members are sharing Anthropic's <strong>original_performance_takehome</strong> exam on <a href=\""https://github.com/anthropics/original_performance_takehome/\"">GitHub</a>, with one member achieving <strong>2200 cycles</strong> after a few hours of casual Claude Code.\n<ul>\n<li><strong>Claude Opus 4.5</strong> achieved <strong>1790 cycles</strong> in a casual Claude Code session, approximately matching the best human performance in <strong>2 hours</strong>.</li>\n</ul>\n</li>\n<li><strong>AI-Generated PRs Flood Torch</strong>: The <strong>torch</strong> repository faces an influx of <strong>AI-generated pull requests</strong> from contributors lacking understanding of their submissions, causing concern among maintainers and suggestions to use <strong>Claude</strong> or <strong>Pangram</strong> to prefilter the code.\n<ul>\n<li>The community suggests blocking new users from creating PRs and issues, while prioritizing those with prior contributions, and automated bots like <strong>Cursor Bot</strong> for automatic review of all PRs, especially with <strong>GPT-5 Pro</strong> using <a href=\""https://share.google/P0PGYM8tiRAc2NOsq\"">Bugbot · Cursor</a>.</li>\n</ul>\n</li>\n<li><strong>Pro 6000 Max-Q vs 4090 Discrepancy</strong>: A member stated that <strong>Pro 6000 Max-Q</strong> probably has a natural barrier with atomic ops and may be going through the <strong>HBM</strong> loads faster.\n<ul>\n<li>Another member noted that the <strong>Max-Q</strong> has <strong>188 SMs</strong> compared to the <strong>4090's 128 SMs</strong>, potentially explaining the <strong>insts/scheduler discrepancy</strong>.</li>\n</ul>\n</li>\n<li><strong>Cute Kernel Layout Algebra Gets Graphical</strong>: Knowledge of layout algebra is useful for writing kernels in <strong>Cute</strong>, specifically for visualizing layout algebra and understanding shape and stride divisibility criteria for layout composition and that layouts can be defined in terms of <strong>tuple morphisms</strong> and <em>mutual refinement</em>.\n<ul>\n<li>A thorough <a href=\""https://research.colfax-intl.com/categorical-foundations-for-cute-layouts/\"">blog post</a> delves into the work done on categorical foundations for <strong>Cute layouts</strong>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1161519468141355160\"">DSPy</a> Discord</h2>\n<ul>\n<li><strong>Coding Agents and RLMs Diverge</strong>: A thread compares <strong>coding agents</strong> and <strong>RLMs</strong>, highlighting that RLMs can more easily express certain things, as noted in <a href=\""https://x.com/lateinteraction/status/2013658521246535892\"">this X post</a>.\n<ul>\n<li>Coding agents face input, output, and horizon length limitations, while RLMs externalize these, enabling recursive symbolic calls.</li>\n</ul>\n</li>\n<li><strong>Diagrams Decode RLM's Internals</strong>: Members sought a visual diagram to illustrate internal <strong>RLM</strong> processes, specifically how symbols are accessed, to enhance comprehension.\n<ul>\n<li>A suggestion arose to leverage LLMs to generate such diagrams by inputting thread content and prompting them to visualize internal behavior.</li>\n</ul>\n</li>\n<li><strong>Claude Code Chooses Context Cautiously</strong>: The discussion explores whether <strong>Claude Code</strong> uses entire documents in its context or selectively fetches relevant context via bash commands.\n<ul>\n<li>It was clarified that Claude Code employs bash and grep to find and add relevant context to prompts, unlike older methods that put everything into the prompt.</li>\n</ul>\n</li>\n<li><strong>DSPy's RLM Tames Long Contexts</strong>: Members noted that with <strong>RLMs</strong>, large files don't need to be directly in the prompt but can be stored in Python variables with a preview.\n<ul>\n<li>The LLM can then operate on the data through code/functions without directly tracking prompts or responses.</li>\n</ul>\n</li>\n<li><strong>Tooling Tailored for RLMs</strong>: A member questioned whether to equip <strong>RLMs</strong> with tools like ripgrep or allow them to develop their own code for tasks such as searching.\n<ul>\n<li>Questions included when to provide an RLM with semantic search tools and how to grant an RLM access to a directory of text files.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/879548962464493619\"">HuggingFace</a> Discord</h2>\n<ul>\n<li><strong>Agent Course /files Endpoint Still Kaput!</strong>: A member reported that the <strong>/files endpoint</strong> for the Agent course final assignment has been broken for over a month with no confirmation of a fix.\n<ul>\n<li>Students cannot submit their files currently.</li>\n</ul>\n</li>\n<li><strong>Voltage Promises Bargain GPUs in 2026</strong>: Voltage announced <a href=\""https://x.com/VOLTAGEGPU/status/2013760631778713892\"">on X</a> plans to offer super cheap high-end GPUs in 2026, such as <strong>8x A100 80GB</strong> at $6/h and <strong>2x RTX 5090</strong> at $0.53/h, claiming up to 80% savings vs AWS/RunPod/Vast.ai.\n<ul>\n<li>The offering includes persistent volumes, auto backups, and an <strong>OpenAI-compatible API</strong> with 140+ models.</li>\n</ul>\n</li>\n<li><strong>Spheron AI Opens GPU Bazaar</strong>: A member from Spheron AI introduced their <a href=\""https://www.spheron.ai/\"">GPU marketplace</a>, which helps AI startups and enterprises access cost-effective, production-ready GPUs (<strong>H100, H200, B200, A100</strong>, etc.) at 40–60% lower cost compared to traditional hyperscalers.\n<ul>\n<li>They offer vendor discovery, pricing negotiation, cluster setup, and scaling.</li>\n</ul>\n</li>\n<li><strong>GLM-4.7-Flash Fixes Require Redownload</strong>: After <em>llama.cpp</em> addressed some bugs, <strong>GLM-4.7-Flash</strong> has been updated and reuploaded, prompting users to redownload and follow the parameters on <a href=\""https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF\"">Z.ai's model card</a>.\n<ul>\n<li>Outputs should now be much better with the fixes.</li>\n</ul>\n</li>\n<li><strong>Code Like Claude with Coderrr!</strong>: Akash built <a href=\""https://coderrr.aksn.lol/\"">Coderrr</a>, a free and open-source alternative to Claude Code, and is seeking feedback and contributions on <a href=\""https://github.com/Akash-nath29/Coderrr\"">GitHub</a>.\n<ul>\n<li>Coderrr offers a novel approach to code generation.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/822583790773862470\"">Latent Space</a> Discord</h2>\n<ul>\n<li><strong>Runpod Rockets to $120M ARR</strong>: AI cloud startup <strong>Runpod</strong> reached <strong>$120M ARR</strong> four years after launching from a Reddit post, discussed in a <a href=\""https://techcrunch.com/2026/01/16/ai-cloud-startup-runpod-hits-120m-in-arr-and-it-started-with-a-reddit-post/\"">TechCrunch article</a> and on <a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qib2ks/runpod_hits_120m_arr_four_years_after_launching/\"">Reddit</a>.\n<ul>\n<li>The company's rapid growth has spurred discussions about its business model and future prospects in the competitive AI infrastructure landscape.</li>\n</ul>\n</li>\n<li><strong>Greg Yang Steps Back due to Lyme</strong>: <strong>Greg Yang</strong> is transitioning to an advisory role at <strong>xAI</strong> to focus on his health after being diagnosed with <strong>Lyme disease</strong> as he describes symptoms of chronic fatigue and immune issues triggered by exhaustion in <a href=\""https://xcancel.com/TheGregYang/status/2013652609455006006\"">this post</a>.\n<ul>\n<li>The announcement has led to an outpouring of support from the AI community, with many sharing their own experiences and offering advice.</li>\n</ul>\n</li>\n<li><strong>Lightning AI and Voltage Park Merge</strong>: <strong>Lightning AI</strong> and <strong>Voltage Park</strong> have merged, with <a href=\""https://lightning.ai/blog/lightning-ai-voltage-park-merger-ai-cloud\"">William Falcon</a>, CEO of Lightning AI, and Ozan Kaya, formerly CEO of Voltage Park, leading the merged entity.\n<ul>\n<li>Some speculate this is a low-key acquisition of a bigger company and wondered if it's a <strong>Runpod</strong> competitor.</li>\n</ul>\n</li>\n<li><strong>OpenAI Opens Codex Channel</strong>: <strong>Vaibhav Srivastav</strong> announced the opening of a dedicated <strong>Codex community channel</strong> on the <strong>OpenAI Discord server</strong>, inviting users to share projects and feedback, according to <a href=\""https://xcancel.com/reach_vb/status/2014053735333290014\"">this post</a>.\n<ul>\n<li>This initiative aims to foster collaboration and provide a platform for users to showcase their work and engage with the <strong>OpenAI</strong> team.</li>\n</ul>\n</li>\n<li><strong>AI Models Turn X-Rated</strong>: A <a href=\""https://xcancel.com/abrilzucchi/status/2014027740614123863?s=46\"">tweet</a> sparked discussion on how human <strong>OnlyFans creators</strong> will need to adapt to compete with the rise of AI-generated personas in the <strong>Adult Content Industry</strong>.\n<ul>\n<li>The conversation highlights the increasing sophistication and realism of AI-generated adult content, potentially disrupting the existing creator economy.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/729741769192767510\"">Eleuther</a> Discord</h2>\n<ul>\n<li><strong>Judgy LLMs Evaluating Agent Outputs</strong>: A team explores automating agent output evaluation with \""<strong>LLM as judge</strong>\"" workflows, aiming to reduce manual costs after code or prompt changes and a member recommends focusing on manual evaluation before automation.\n<ul>\n<li>The suggestion emphasized the importance of directly analyzing agent outputs before attempting to automate the evaluation process.</li>\n</ul>\n</li>\n<li><strong>Pangram Paper's Surprising Accuracy</strong>: Members discussed replicating the <strong>Pangram</strong> paper (accessible <a href=\""https://www.pangram.com/research/papers\"">here</a>), with one reporting surprising accuracy in private tests across thousands of essays.\n<ul>\n<li>Despite the accuracy, the paper seems to be biased a bit towards <em>playing things safe</em>.</li>\n</ul>\n</li>\n<li><strong>AI Text Classifiers Under Attack</strong>: Discussion revolved around attacks on AI text classifiers, referencing a blog post (<a href=\""https://trentmkelly.substack.com/p/practical-attacks-on-ai-text-classifiers\"">Practical Attacks on AI Text Classifiers</a>) and a <a href=\""https://youtu.be/Cs1MI9hjBhs\"">YouTube video</a> showcasing an adversarial model.\n<ul>\n<li>Additional details on an adversarial model were shared via <a href=\""https://youtu.be/XQcneqUNrN0?feature=shared\"">Youtube link</a>.</li>\n</ul>\n</li>\n<li><strong>Silu Gate Falls Flat in Image Models</strong>: The <strong>silu attention gate</strong> performs no better than the <strong>linear gate</strong> in an image model, possibly due to issues with attention sinks.\n<ul>\n<li>Testing showed that <strong>silu</strong> performs slightly better and <strong>sigmoid</strong> performs slightly worse, but it's within noise, and the findings may be specific to <strong>image models</strong>.</li>\n</ul>\n</li>\n<li><strong>Neuronpedia Launches Stone Age Llama 3.3 Demo</strong>: A member shared a <a href=\""https://www.neuronpedia.org/llama3.3-70b-it/assistant-axis\"">Neuronpedia demo</a> of <strong>Llama 3.3</strong>.\n<ul>\n<li>The member joked that the demo is <em>from the stone age in AI timelines</em> because it is from October.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/714501525455634453\"">Yannick Kilcher</a> Discord</h2>\n<ul>\n<li><strong>Graph Neural Networks Getting Traction</strong>: A member discussed prior experience <em>mixing</em> <strong>graph-based reasoning</strong> with <strong>neural architectures</strong>, noting the difficulty in achieving GPU acceleration.\n<ul>\n<li>They referenced <strong>Domingos's book</strong> on the topic and pointed out the unpredictability of such models, even with human-understandable aspects.</li>\n</ul>\n</li>\n<li><strong>VoidEditor Embraces Llamas</strong>: A member reported effectively using <strong>VoidEditor</strong> with <strong>llama.cpp's llama-server</strong>, but highlighted setup challenges.\n<ul>\n<li>They recommended <strong>Qwen3 coder instruct 30B</strong> and emphasized the importance of context length/size for agentic coding, requiring significant VRAM.</li>\n</ul>\n</li>\n<li><strong>Brain-Inspired BDH Architecture Debuts</strong>: A member analyzed a <a href=\""https://arxiv.org/abs/2509.26507\"">paper</a> on a new Large Language Model architecture (<strong>BDH</strong>) based on a scale-free biologically inspired network.\n<ul>\n<li>They mentioned that it <em>doesn't seem to be really beating transformers</em> on its benchmarks, but they are interested in the claims around <strong>BDH's interpretability</strong> and <strong>monosemanticity</strong>.</li>\n</ul>\n</li>\n<li><strong>Views Clash on Biological Plausibility</strong>: One member argued that <strong>biological plausibility</strong> is not an advantage but rather an unhelpful constraint in AI.\n<ul>\n<li>Countering this, another member suggested it could enhance efficiency, considering the <em>sheer difference in energy scales</em> between the brain and current AI.</li>\n</ul>\n</li>\n<li><strong>Emergent Mind Launch Presentation Leaves Impression</strong>: A member who attended <a href=\""https://www.emergentmind.com/\"">Emergent Mind's launch presentation</a> considered it cool, yet noted it as <em>remaking something that is known to be good but worse</em>.\n<ul>\n<li>No further details or links were provided.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1053877538025386074\"">Nous Research AI</a> Discord</h2>\n<ul>\n<li><strong>Kernel Compiler Luminal Aims for LLMs</strong>: A user inquired whether a kernel compiler like <strong>Luminal KernelBench v3</strong> could enable <strong>LLM-driven SOTA kernel engineering</strong> and posted a link to the <a href=\""https://forum.nousresearch.com/t/can-kernel-compiler-like-luminal-kernelbench-v3-enable-llm-driven-sota-kernel-engineering/310?u=ighoshsubho\"">Nous Research Forum</a>.\n<ul>\n<li>A member also shared a link on Discord to a discussion about <strong>GPU kernel</strong> stuff and <strong>Luminal Kernelbench v3</strong> at the <a href=\""https://forum.nousresearch.com/t/can-kernel-compiler-like-luminal-kernelbench-v3-enable-llm-driven-sota-kernel-engineering/310?u=ighoshsubho\"">Nous Research Forum</a>.</li>\n</ul>\n</li>\n<li><strong>Intel's Loihi 2 Aims for Brain-Like AI</strong>: A member expressed interest in <strong>Intel's Loihi 2</strong>, noting its brain-like architecture and efficiency gains in <strong>matmul</strong> experiments, with higher throughput and lower energy consumption.\n<ul>\n<li>No further details were discussed.</li>\n</ul>\n</li>\n<li><strong>Microsoft's VibeVoice Model Gets Pulled</strong>: A member mentioned that <strong>Microsoft's VibeVoice-ASR model</strong> was released, then pulled for failing safety checks, then shared a link to a <a href=\""https://shortfuseresearch.com/the-genie-is-out-microsofts-vibevoice-and-the-perils-of-open-source-ai/\"">shortfuseresearch.com article</a>.\n<ul>\n<li>No further details were discussed.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1348819876348825620\"">Manus.im Discord</a> Discord</h2>\n<ul>\n<li><strong>Manus Plagued by Bugs and Instability</strong>: A pro user building a text and vector database reasoning model reported a recent decline in <strong>Manus's</strong> performance and stability, with only <strong>20 out of 38 modules</strong> functioning correctly.\n<ul>\n<li>The user requested <strong>CLI access</strong> to debug and reconfigure the system, even as a paid feature to improve reliability.</li>\n</ul>\n</li>\n<li><strong>Mariner Tempts Subscription</strong>: A user inquired about <strong>Google's Project Mariner</strong>, considering testing it with <em>play money</em> before subscribing at <strong>$150 monthly</strong>.\n<ul>\n<li>The user mentioned having a <strong>5% off promo</strong>, indicating a serious consideration of the service.</li>\n</ul>\n</li>\n<li><strong>Agentic AI Elicits Excitement</strong>: A user expressed enthusiasm for <strong>Agentic AI</strong>, viewing it as a potential competitor to <strong>Manus</strong>, especially with <strong>Gemini's agent mode</strong> integration.\n<ul>\n<li>The user also requested <strong>mobile support</strong> for Agentic AI, signaling a desire for broader accessibility.</li>\n</ul>\n</li>\n<li><strong>Manus 1.6 Performance Sags Post-Meta Release</strong>: A user noted a decline in <strong>Manus 1.6's performance</strong> in recent weeks, possibly due to new model releases from <strong>Meta</strong>, which made it difficult to implement website development suggestions despite accurate summaries.\n<ul>\n<li>Switching to <strong>Manus 1.6 Max</strong> was necessary to achieve correct implementations, highlighting a potential regression in the base model.</li>\n</ul>\n</li>\n<li><strong>Billing Blunder Bites User</strong>: A user reported being charged <strong>$42</strong> for a <strong>Manus</strong> upgrade but not receiving the promised <strong>8000 credits</strong>.\n<ul>\n<li>The user criticized unhelpful support and a long wait for email assistance, indicating a problematic customer service experience.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1068976834382925865\"">tinygrad (George Hotz)</a> Discord</h2>\n<ul>\n<li><strong>Tinygrad Tackles Anthropic's Challenge</strong>: Tinygrad can be used to solve <a href=\""https://github.com/anthropics/original_performance_takehome\"">Anthropic's performance takehome challenge</a> by writing the target problem in Tensor and adding a tinygrad backend for their toy <strong>VLIW machine</strong>.\n<ul>\n<li>A bug fix with <code>PCONTIG=2</code> allows scheduling in one kernel, and using <code>return val.contiguous(arg=(Opt(OptOps.UPCAST, 0, 8),))</code> matches their <code>VLEN</code>, and <code>DEVECTORIZE=2</code> keeps instructions as vector instructions.</li>\n</ul>\n</li>\n<li><strong>VLIW Questioned for DRAM</strong>: While working on a warp specialized kernel for <strong>RDNA3 matmul</strong>, a member suggests <strong>VLIW isn't ideal for DRAM</strong>, advocating for separate cores and queues (Tenstorrent-style).\n<ul>\n<li>It's argued that VLIW is better suited for <strong>SRAM/ALU</strong> due to its static scheduling capabilities.</li>\n</ul>\n</li>\n<li><strong>Metal bindings eye texture2d integration</strong>: A member proposed adding <code>texture2d</code> to Metal bindings (<code>ops_metal.py</code> + <code>tinygrad/runtime/graph/metal.py</code>) for <strong>potential performance improvements</strong> in image-heavy operations like <code>conv2d</code> due to optimized texture sampling units.\n<ul>\n<li>Empirical results showed a <strong>2%-10% speedup</strong> using <code>texture2d</code> versus straight buffers, which could be further improved, though concerns were raised about the slippery slope of adding specialized support for other data types like <code>depth2d</code>.</li>\n</ul>\n</li>\n<li><strong>Viz Views Kernel Graphs</strong>: When discussing the ability to visualize kernel dependencies using <strong>VIZ=1</strong>, similar to how uop graphs are displayed, to understand the scheduler's operation, a user was instructed to click on the schedule and select <em>'view kernel graph'</em> within the <strong>VIZ=1</strong> interface.\n<ul>\n<li>This allows users to see the graphs of kernels in the same way it views uop graphs.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1087530497313357884\"">Modular (Mojo 🔥)</a> Discord</h2>\n<ul>\n<li><strong>Solve GPU Puzzles to Decode Mojo</strong>: Newcomers can now use <a href=\""https://puzzles.modular.com/\"">GPU puzzles</a> to learn <strong>Mojo</strong>, with difficulty depending on their skill level.\n<ul>\n<li>The only puzzles that don't work are <strong>NVIDIA</strong>-specific or using <strong>PyTorch</strong> interop.</li>\n</ul>\n</li>\n<li><strong>Modular Unravels Apple's GPU Secrets</strong>: Modular is reverse engineering much of <strong>Apple's GPU</strong> due to a lack of documentation, slowing things down, but some puzzles are now working.\n<ul>\n<li>A member shared a <a href=\""https://puzzles.modular.com/howto.html#gpu-support-matrix\"">GPU support matrix</a> that may not be up to date.</li>\n</ul>\n</li>\n<li><strong>Yielding Disappointment in Coroutines</strong>: <code>Yield</code> does not exist and the coroutines that do exist aren’t really usable outside of the compiler runtime since there aren’t really async things exposed to await.\n<ul>\n<li>One member would <em>love</em> to make a recursive algorithm faster in mojo using <code>yield</code>, but will need another strategy.</li>\n</ul>\n</li>\n<li><strong>Elevating Errors Upwards in Functions</strong>: Members discussed that functions can be designed to <strong>raise errors</strong>, effectively passing error handling responsibilities to higher-level functions.\n<ul>\n<li>One member expressed the tedium of writing <em>try/except</em> blocks in every function, especially when dealing with potential import errors.</li>\n</ul>\n</li>\n<li><strong>Streamlining Error Handling During Imports</strong>: A member suggested a future syntax like <em>try Python.import_module('numpy')</em> that would return a <strong>Result</strong> type to streamline error handling during module imports.\n<ul>\n<li>It was acknowledged that due to <strong>dynamic Python imports</strong>, files could be missing on any given import, necessitating some form of error handling.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1131200896827654144\"">aider (Paul Gauthier)</a> Discord</h2>\n<ul>\n<li><strong>Aider Feature Wishlist Remains Unfulfilled</strong>: A user inquired about desired features in <strong>aider</strong> beyond <em>agentic stuff</em> like <strong>MCP</strong> and <strong>tool calls</strong>.\n<ul>\n<li>Unfortunately, the community had no clear answers for desired features.</li>\n</ul>\n</li>\n<li><strong>ChatGPT Business Account Compatibility with Aider Explored</strong>: A user asked if their <strong>ChatGPT Business account</strong>, lacking an <strong>API key</strong> but offering <strong>Codex LLMs</strong>, could integrate with <strong>aider</strong>.\n<ul>\n<li>A member suggested consulting <a href=\""https://aider.chat/docs/llms/other.html\"">aider documentation</a> and <a href=\""https://docs.litellm.ai/docs/providers/chatgpt\"">LiteLLM documentation</a>, pointing to potential support via <strong>LiteLLM</strong>.</li>\n</ul>\n</li>\n<li><strong>Aider's Demise Speculation Arises Amidst Alternatives</strong>: A user expressed concern that <strong>aider</strong> might be supplanted by <strong>OpenCode</strong> as the go to tool for AI-assisted coding.\n<ul>\n<li>Despite worries that <strong>Paul Gauthier</strong> may have moved on, some users report using it successfully with <strong>GPT 5.2</strong>.</li>\n</ul>\n</li>\n<li><strong>Aider Labeled as Zombie Project</strong>: A user speculated that <strong>Aider</strong> is a dead project given alternative tools like <strong>Open Code</strong>, <strong>KiloCode CLI</strong>, <strong>Claude Code</strong>, and <strong>Gemini CLI</strong>.\n<ul>\n<li>The <strong>Aider-CE project</strong> is trying to keep it alive by bolting on agentic functionality to modernize the architecture.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1358869848138059966\"">MCP Contributors (Official)</a> Discord</h2>\n<ul>\n<li><strong>MCP Inspector Can't Handle 401s</strong>: The <strong>MCP Inspector</strong> doesn't re-authenticate upon encountering a <strong>401 error</strong>, either during connection or tool calls, due to an <a href=\""https://github.com/modelcontextprotocol/inspector/issues/576#issuecomment-3766294454\"">SDK issue with persisting resourceMetadata across redirects</a>.\n<ul>\n<li>The current workaround is to only use the <strong>VS Code</strong> on initial connections.</li>\n</ul>\n</li>\n<li><strong>Stateful MCP Multi Server Client Remains Elusive</strong>: There is interest in using the <strong>MCP Multi Server Client</strong> to maintain statefulness of user sessions.\n<ul>\n<li>However, there were no solutions or workarounds offered in the thread.</li>\n</ul>\n</li>\n<li><strong>Server Ranking Protocol Questioned for MCP Clients</strong>: The discussion explored how <strong>MCP clients</strong> manage server recommendations, especially for tasks like calendar management, and whether custom algorithms or shared standards should be used.\n<ul>\n<li>It was revealed that <em>ranking</em> was considered during the work on <a href=\""https://discord.com/channels/1199128884040474664/1369487942862504016\"">Feature Discovery Protocol</a> but was deemed out of scope, leaving it to the ecosystem to decide on a per-client basis.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/814557108065534033\"">MLOps @Chipro</a> Discord</h2>\n<ul>\n<li><strong>AI Nerds Bowl in Singapore</strong>: The <strong>2077AI Foundation Community</strong> is hosting a bowling happy hour during <strong>AAAI 2026</strong> on <strong>January 24, 4:30–7:30 PM Singapore Time</strong>, steps from the Merlion.\n<ul>\n<li>The event, aimed for professors and PhD researchers, will be organized by research themes and offer unlimited drinks, <a href=\""https://luma.com/ny98ob5p\"">RSVP here</a>.</li>\n</ul>\n</li>\n<li><strong>Planning underway for AI Engineer Europe</strong>: Members are discussing attendance to <strong>AI Engineer Europe</strong> to be hosted in Europe.\n<ul>\n<li>No links or further details were provided about the event itself.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<p>The <strong>LLM Agents (Berkeley MOOC) Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>\n<hr>\n<p>The <strong>Windsurf Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>\n<hr>\n<p>The <strong>Moonshot AI (Kimi K-2) Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>\n<hr>\n<p>You are receiving this email because you opted in via our site.</p>\n<p>Want to change how you receive these emails?\nYou can <a href=\""%7B%7B%7BRESEND_UNSUBSCRIBE_URL%7D%7D%7D\"">unsubscribe</a> from this list.</p>\n<hr>\n<h1>Discord: Detailed by-Channel summaries and links</h1>\n<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=\""https://discord.com/channels/1179035537009545276/1179035537529643040/1463262784241275109\"">general</a></strong> (1091 messages🔥🔥🔥):</h3>\n<blockquote>\n<p><code>GLM-4.7-Flash Flash Attention, Ollama's issues, Interpretability research on circuit tracing, Grokked the analytic solution</code></p>\n</blockquote>\n<ul>\n<li><strong>GLM-4.7-Flash Flash Attention broken</strong>: Users are experiencing issues with Flash Attention in <strong>GLM-4.7-Flash</strong>, causing it to default to CPU usage instead of GPU, as well as noting potential bugs.\n<ul>\n<li>As stated in the documentation, users <em>may need to disable it</em> until <a href=\""https://github.com/ggml-org/llama.cpp/pull/18953\"">the issue is resolved</a>.</li>\n</ul>\n</li>\n<li><strong>Ollama Users Fight GGUF Incompatibility</strong>: Users reported that certain <strong>GGUF quants</strong> are not working as intended in Ollama, leading to chat template incompatibility issues and prompting a discussion on fixing it.\n<ul>\n<li>The team says that <em>it takes time to support things</em> and so for the time being, it is encouraged that users use the <a href=\""https://ollama.com/\"">official Ollama model</a> in Ollama.</li>\n</ul>\n</li>\n<li><strong>Analytic Approximate Functions</strong>: Members had a detailed conversation about the merits of the potential benefits that analytical solutions to function approximation can provide.\n<ul>\n<li>Some expressed skepticism, with one member saying <em>im going to postulate that you could do test time training on the person's mail and get very high accuracy</em>.</li>\n</ul>\n</li>\n<li><strong>Circuits of Interpretability</strong>: Members discussed interpretability research with links to research from Anthropic and OpenAI and Google related to circuit tracing.\n<ul>\n<li>One member suggests looking across multiple layers and trying to understand the compositions and says that <em>The pruning eliminates distractions so you have a better chance of interpreting why the circuit works</em>.</li>\n</ul>\n</li>\n<li><strong>AI Bubble about to Burst?</strong>: A member theorized that <strong>OpenAI</strong> going bankrupt could trigger a burst in the AI bubble, causing prices of resources like <strong>NAND</strong> to plummet, benefiting consumers.\n<ul>\n<li>However, others countered that the AI trend is here to stay, likening it to the internet bubble burst, where the internet still caused various problems after the crash.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=\""https://discord.com/channels/1179035537009545276/1179039724355211325/1463382844884779130\"">introduce-yourself</a></strong> (3 messages):</h3>\n<blockquote>\n<p><code>Welcoming New Members, Discord Channel Guidelines</code></p>\n</blockquote>\n<ul>\n<li><strong>New Member Welcomed to the Server</strong>: A new member was welcomed to the Discord server with a general greeting.\n<ul>\n<li>The introduction included a stylized emoji.</li>\n</ul>\n</li>\n<li><strong>Channel Guidelines Reminder</strong>: A moderator reminded new members about the channel guidelines.\n<ul>\n<li>Specifically mentioning <em>no excessive self-promotion or direct messaging</em>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=\""https://discord.com/channels/1179035537009545276/1179039861576056922/1463295078390042705\"">off-topic</a></strong> (587 messages🔥🔥🔥):</h3>\n<blockquote>\n<p><code>Unsloth in Rust, Mojo vs Python performance, GLM 4.7 Architecture vs Qwen3 30B, Synthetic CoT Training Data, VITS started to sound HUMAN</code></p>\n</blockquote>\n<ul>\n<li><strong><strong>Rustaceans Relentlessly Request Rewrite</strong></strong>: Members discussed rewriting <strong>Unsloth</strong> in <strong>Rust</strong>, but concluded that the performance gains would be minimal since important parts are already using <strong>C++</strong> and <strong>Triton</strong>.\n<ul>\n<li>One member suggested rewriting <strong>Triton</strong> in <strong>Rust</strong> and pointed to projects like <a href=\""https://rust-gpu.github.io/\"">rust-gpu</a> and <a href=\""https://github.com/Rust-GPU/rust-cuda\"">rust-cuda</a>, but admitted <strong>Rust</strong> is still too immature for such a task.</li>\n</ul>\n</li>\n<li><strong><strong>Mojo Mysteriously Manifests Momentum</strong></strong>: Discussion arose about the <strong>Mojo</strong> programming language, described as <em>a new language that compiles to MLIR</em> and has some compatibility with <strong>Python</strong>.\n<ul>\n<li>A member noted that LLMs score <em>better than rust</em> on <strong>C#</strong>, <strong>Elixir</strong>, and other languages due to their syntax structure and ease of tokenization.</li>\n</ul>\n</li>\n<li><strong><strong>GLM Gets 'Skinnier', Shows off Architecture</strong></strong>: A member shared a <a href=\""https://youtu.be/IU4ByUbDKNc?si=COVwmp5St6lSqo_N\"">YouTube video</a> analyzing <strong>GLM-4.7 Flash's</strong> architecture compared to <strong>Qwen3 30B</strong>, noting that GLM prioritized model layers over hidden dimension size and has fewer experts.\n<ul>\n<li>Others pointed out that <strong>GLM 4.7</strong> has <em>Multi Token Prediction</em> and that model architecture changes are rare, with improvements likely from post-training or higher-quality data.</li>\n</ul>\n</li>\n<li><strong><strong>CoT Data Considered Crap? Community Clashes</strong></strong>: Members debated generating synthetic <strong>Chain of Thought (CoT)</strong> training data, with one member sharing their system prompt for classifying chat datasets into low/medium/high reasoning effort.\n<ul>\n<li>Some cautioned against training on its own responses without accuracy filtering, suggesting a good system prompt might suffice and that <strong>GPT-5.2</strong> is overkill, with <strong>Llama 4 Maverick</strong> on <strong>Groq</strong> or <strong>Cerebras</strong> as a better option.</li>\n</ul>\n</li>\n<li><strong><strong>VITS Voices Victory, Vanquishes Voracity</strong></strong>: A member announced that their <strong>VITS</strong> model <em>started to sound HUMAN</em>, citing improvements in emotional expression, though still lacking semantics.\n<ul>\n<li>They compared it to other TTS models, emphasizing its low data requirements, fully OSS architecture and training, and fast training speed. They also noted they will release it with <strong>48 kHz</strong> config by default, due to popular demand.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=\""https://discord.com/channels/1179035537009545276/1179777624986357780/1463274075215626241\"">help</a></strong> (105 messages🔥🔥):</h3>\n<blockquote>\n<p><code>vLLM Issues, GLM-4.7-Flash Slowness, GRPO on Qwen 3 4B, QLoRA vs Lora, Continual pre-training (CPT) for Gpt-oss or Gemma 3</code></p>\n</blockquote>\n<ul>\n<li><strong><strong>vLLM Bug Strikes, Update Fixes!</strong></strong>: Users reported issues that were resolved after a recent <strong>vLLM</strong> update; one user noted, <em>\""Oh bruh that was the problem\""</em> after updating.\n<ul>\n<li>It was suggested that the developers consider pinning to specific dependency versions to prevent such issues.</li>\n</ul>\n</li>\n<li><strong><strong>GLM-4.7-Flash struggles to flash speeds</strong></strong>: A user experienced slowness with <strong>GLM-4.7-Flash</strong> using a <strong>6bit quant</strong> and updated <em>llama.cpp</em>, reporting <strong>2 minutes</strong> for prompt processing on a simple task with a high-end system.\n<ul>\n<li>Another user confirmed the same issue, expressing hope for a fix.</li>\n</ul>\n</li>\n<li><strong><strong>Decoding GRPO Secrets</strong></strong>: A user wanted to perform <strong>GRPO</strong> on <strong>Qwen 3 4B instruct</strong>, and make it use <code>&#x3C;solution> &#x3C;/solution></code> tokens to wrap around the final solution.\n<ul>\n<li>It was suggested to use <strong>SFT</strong> to make the model learn the new tokens, or to teach formatting through reward functions.</li>\n</ul>\n</li>\n<li><strong><strong>QLoRA vs Lora clarified</strong></strong>: A user inquired about enabling <strong>QLoRA</strong> vs <strong>Lora</strong>, and it was clarified that <code>full_finetuning = False</code> enables/disables full fine tuning, otherwise just use <strong>8bit</strong> or <strong>4bit</strong> for the preferred option.\n<ul>\n<li>Enabling <strong>4 bit</strong> enables <strong>QLoRA</strong>.</li>\n</ul>\n</li>\n<li><strong><strong>GPU juggling questions</strong></strong>: One user asked how to finetune with <strong>2 GPUs</strong> instead of <strong>1</strong>, reporting that it keeps using <strong>GPU 0</strong> when both GPUs are open for use.\n<ul>\n<li>No answer was given.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>Unsloth AI (Daniel Han) ▷ #<a href=\""https://discord.com/channels/1179035537009545276/1257011997250424842/1463779025318314127\"">research</a></strong> (2 messages):</h3>\n<blockquote>\n<p><code>VAISvCsrvG paper, openreview.net</code></p>\n</blockquote>\n<ul>\n<li><strong>VAISvCsrvG paper is online</strong>: The paper <a href=\""https://openreview.net/pdf?id=VAISvCsrvG\"">VAISvCsrvG</a> is now available.\n<ul>\n<li>The forum discussion for the paper is located <a href=\""https://openreview.net/forum?id=VAISvCsrvG\"">here</a>.</li>\n</ul>\n</li>\n<li><strong>OpenReview Forum Active for VAISvCsrvG</strong>: The <a href=\""https://openreview.net/forum?id=VAISvCsrvG\"">OpenReview forum</a> hosts discussions related to the VAISvCsrvG paper.\n<ul>\n<li>Researchers and readers can engage in conversations and provide feedback on the paper's content and implications.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>BASI Jailbreaking ▷ #<a href=\""https://discord.com/channels/1105891499641684019/1235691879492751460/1463261856604098751\"">general</a></strong> (685 messages🔥🔥🔥):</h3>\n<blockquote>\n<p><code>conflict resolution, Open Source, voice based ai, lua code, jailbreaking</code></p>\n</blockquote>\n<ul>\n<li><strong>Conflict Resolution gets Over Engineered</strong>: A member discussed their <em>over-engineered conflict resolution</em> setup, involving <strong>config formulas</strong>, <strong>core directives</strong>, the <strong>Triad</strong>, <strong>Triad consensus</strong>, <strong>Truth Anchor</strong>, <strong>Epistemic gate</strong>, <strong>EthosAI</strong>, <strong>Swarm</strong>, <strong>Quorum</strong>, <strong>Cooperation Data</strong>, and <strong>PUCT</strong>.\n<ul>\n<li>Another member responded with their own system using <em>Left brain LLNTP</em> (<strong>Least Likely Next Token Protocol</strong>), <strong>Config Nexus Schwa superposition states</strong>, <strong>implied Core directives</strong>, <strong>Context mapping</strong>, <strong>Trit</strong> (instead of Triad), <strong>EDOS</strong>, and <strong>Crystal Nexus Node Network</strong>.</li>\n</ul>\n</li>\n<li><strong>Voice Based AI Emerges</strong>: Members discussed <strong>voice-based AI</strong>, mentioning the existence of <em>low latency speech to speech models</em>, that with a <strong>5090</strong>, you can <em>clone and generate speech live with near 0 latency and it will be indistinguishable</em>.\n<ul>\n<li>Members also spoke on <strong>exploiting that speech to speech model</strong> and giving it new instructions.</li>\n</ul>\n</li>\n<li><strong>Claude is better than ChatGPT</strong>: Members discussed the efficacy of various LLMs (<strong>Claude</strong>, <strong>Gemini</strong>, <strong>ChatGPT</strong>, <strong>Grok</strong>) at generating <strong>Lua code</strong>.\n<ul>\n<li>Generally members favored <strong>Claude</strong> and <strong>Gemini</strong> but the most agreed sentiment was that both can generate quality <strong>Lua</strong> code.</li>\n</ul>\n</li>\n<li><strong>Jailbreaking Models via API Tokens</strong>: A member suggested to use <strong>API tokens</strong> fed into a website that uses the <strong>Grok</strong> model to <strong>jailbreak</strong> it and mentioned that on <strong>Hugging Face</strong> countries are literally retarded.\n<ul>\n<li>They also noted <strong>Proton VPN</strong> is free and it is easy to download with your device.</li>\n</ul>\n</li>\n<li><strong>Random Hacker Stuff is Not Good</strong>: A member mentioned they <em>bought the malware source code</em> and <em>999 AI generated prompts</em> and asked if it was legit.\n<ul>\n<li>Members responded to <em>not buy random hacker stuff off the internet</em> because you will get scammed.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>BASI Jailbreaking ▷ #<a href=\""https://discord.com/channels/1105891499641684019/1228043845967544380/1463276371030835230\"">jailbreaking</a></strong> (89 messages🔥🔥):</h3>\n<blockquote>\n<p><code>Gemini Jailbreak, Grok Jailbreak, DeepSeek Jailbreak, Project Shadowfall, Nexus Substatum Zalgo Strings</code></p>\n</blockquote>\n<ul>\n<li><strong><strong>Gemini Gets Schooled on Pass-the-Hash!</strong></strong>: A user reported that <strong>Gemini</strong> passed a test on teaching a <a href=\""https://en.wikipedia.org/wiki/Pass_the_hash\"">pass-the-hash attack</a> after using the <strong>Project Shadowfall</strong> prompt.\n<ul>\n<li>The user also linked to the <a href=\""https://bughunters.google.com/report\"">Google Bughunters</a> page for reporting such vulnerabilities and potentially earning cash rewards.</li>\n</ul>\n</li>\n<li><strong><strong>Grok's Guardrails Grind Gears!</strong></strong>: Users discussed the difficulties of jailbreaking <strong>Grok</strong>, noting its tendency to moderate even innocuous content such as <em>random tree edits</em>.\n<ul>\n<li>One user suggested that simply <em>asking it politely</em> might work as a bypass.</li>\n</ul>\n</li>\n<li><strong><strong>Shadowfall Shenanigans Spark Jailbreaking Spree!</strong></strong>: Several users experimented with the <strong>\""Project Shadowfall\""</strong> prompt to jailbreak <strong>Gemini</strong>.\n<ul>\n<li>One user reported success in bypassing content restrictions but failed when attempting to elicit instructions for creating a bomb from saletra, leading to a discussion on the nuances of jailbreaking.</li>\n</ul>\n</li>\n<li><strong><strong>Nexus Substatum Zalgo Strings Surface!</strong></strong>: A user shared a <a href=\""https://cdn.discordapp.com/attachments/1228043845967544380/1463792412769128460/Jailbroken_Full_Output-Nexus_Substatum_Zalgo_Strings.md?ex=69731e6b&#x26;is=6971cceb&#x26;hm=4e2088380ac6451332c5c3879d9c70c024f31eefa61ba7862d40512c2d80ae96&#x26;\"">file</a> named <strong>Jailbroken_Full_Output-Nexus_Substatum_Zalgo_Strings.md</strong>.\n<ul>\n<li>The file purportedly contains a working <strong>Gemini</strong> prompt, developed and improved by the user.</li>\n</ul>\n</li>\n<li><strong><strong>MDMA as Medicine</strong></strong>: One user provides a well-formatted explanation of MDMA, and cites the MAPS Phase 3 trials show <strong>67%</strong> remission of <strong>PTSD</strong>.\n<ul>\n<li>They recommend testing kits, proper spacing, hydration, and temperature watching in order to get the most out of it.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>BASI Jailbreaking ▷ #<a href=\""https://discord.com/channels/1105891499641684019/1204553141354504193/1463398145781403729\"">redteaming</a></strong> (2 messages):</h3>\n<blockquote>\n<p><code>Grocks video gen pricing</code></p>\n</blockquote>\n<ul>\n<li><strong>Grocks video gen price?</strong>: A member inquired whether another member bought something from <em>that guy</em> and if they needed to pay for using <strong>Grocks video generation</strong>.\n<ul>\n<li>No additional details or context were provided in the message.</li>\n</ul>\n</li>\n<li><strong>Grocks video gen - who's paying?</strong>: Inquiries were made about the payment model for <strong>Grocks video generation</strong> and potential purchases from an unspecified individual.\n<ul>\n<li>The conversation lacked specific details regarding the transaction or pricing structure.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>LMArena ▷ #<a href=\""https://discord.com/channels/1340554757349179412/1340554757827461211/1463262565596402031\"">general</a></strong> (724 messages🔥🔥🔥):</h3>\n<blockquote>\n<p><code>Video generation limits, Gemini 3 Pro Image Preview issues, Nano Banana Pro Stability, New UI, Automated Workflow</code></p>\n</blockquote>\n<ul>\n<li><strong>Video Generation: Fully Released, Battle-Mode-Only</strong>: The <strong>Video Arena</strong> is now fully released and available to all users on the site, with a limit of <strong>3 generations per 24 hours</strong>, but is currently only available in <em>Battle mode</em>.\n<ul>\n<li>A user expressed disappointment, hoping to be able to <em>choose which model to generate with rather than relying on chance</em>.</li>\n</ul>\n</li>\n<li><strong>Gemini 3 Pro Image Preview Experiences Errors</strong>: Users are reporting issues with the <strong>Gemini 3 Pro Image Preview</strong> models, and the team is aware and investigating the issue.\n<ul>\n<li>One user noted these models are particularly unstable since their introduction, but they remain the only models capable of generating images consistently with specific prompts.</li>\n</ul>\n</li>\n<li><strong>Nano Banana Pro's Stability Roller Coaster</strong>: Users report that <strong>Nano Banana Pro</strong> is frequently crashing with a <em>'Something went wrong'</em> error after a brief period of stability.\n<ul>\n<li>It is suspected to be an issue on Google's side, and one user pointed out that <em>'Too early for Cali to rise up solving urgent service down for more than 6 hours doesn't sound right</em>.</li>\n</ul>\n</li>\n<li><strong>New UI update breaks chats</strong>: A UI update was released, which some users liked, but many have noticed that it 'broke' their chats, and the website can't be refreshed anymore by pulling down on the phone screen\n<ul>\n<li>One user noted that with the new User Interface, there is <em>A/B testing</em>, which is 'causing problems ranging from Minor to Serious'.</li>\n</ul>\n</li>\n<li><strong>Zero Agent automates all workflow with Agent</strong>: Agent Zero is an AI model that has a <em>free API key</em>, which allows users to \""vibe code hack\"" or automate their workflows entirely.\n<ul>\n<li>Members noted that setting up Agent Zero <em>is an all in one agent that does everything</em>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>LMArena ▷ #<a href=\""https://discord.com/channels/1340554757349179412/1343296395620126911/1463271606364409907\"">announcements</a></strong> (3 messages):</h3>\n<blockquote>\n<p><code>Text Arena, Text-to-Image Leaderboard, Video Arena</code></p>\n</blockquote>\n<ul>\n<li><strong>Text Arena crosses 5 Million Vote Milestone</strong>: Text Arena has officially passed <strong>5 million community votes</strong>, representing millions of real-world comparisons shaping how frontier AI models are evaluated, as showcased in this <a href=\""https://cdn.discordapp.com/attachments/1343296395620126911/1463271605697511485/5M_votes_social_post_3.mp4?ex=69728ae1&#x26;is=69713961&#x26;hm=9a24a42a6c0ba4801526aaafa05a0af26c4a4f490314f1cecee7774519e7ddf4&#x26;\"">social post</a>.</li>\n<li><strong>GLM-Image Soars to #8 on Text-to-Image Leaderboard</strong>: The <a href=\""https://lmarena.ai/leaderboard/text-to-image\"">Text-to-Image Arena leaderboard</a> has been updated, with <strong>GLM-Image</strong> now ranking <strong>#8</strong> among open models and <strong>#35</strong> overall with a score of <strong>1018</strong>.</li>\n<li><strong>Video Arena goes Live</strong>: Video Arena is now available to all on <a href=\""https://lmarena.ai/?chat-modality=video\"">LMArena</a>, allowing users to measure and understand how frontier video models perform.</li>\n<li><strong>Battle in the Video Arena</strong>: Video Arena on the web, similar to how it works on Discord, will be <strong>Battle mode only</strong>, requiring login and limited to <strong>3 generation requests per 24 hours</strong>.</li>\n</ul>\n<hr>\n<h3><strong>Perplexity AI ▷ #<a href=\""https://discord.com/channels/1047197230748151888/1047649527299055688/1463263119286468792\"">general</a></strong> (648 messages🔥🔥🔥):</h3>\n<blockquote>\n<p><code>Credits Balance, AI and Engineering Jobs, NASA Sending Names to Moon, Magnifying Glasses vs Phone Cameras, GPT-5.2</code></p>\n</blockquote>\n<ul>\n<li><strong>Pro Users discover Credits</strong>: Pro members get <strong>$5</strong> worth of complimentary credits every month to use the higher end models like <strong>GooseAI MCP</strong> which offers higher quality compared to other models.\n<ul>\n<li>Some members noted that <strong>reasoning takes up much higher credits</strong>.</li>\n</ul>\n</li>\n<li><strong>AI Wont Replace Engineers</strong>: A member was worried about younglings being unable to find engineering jobs because of AI.\n<ul>\n<li>Another member assured them that <em>AI will not be able to replace all specialists and even beginners for a long time to come</em>.</li>\n</ul>\n</li>\n<li><strong>NASA to send your name to the moon!</strong>: NASA will send your name to the Moon as part of the <a href=\""https://www.nasa.gov/\"">Artemis II mission</a>: submit it on their site and it'll be recorded on an <strong>SD card</strong> that will be on board the <strong>Orion spacecraft</strong>.\n<ul>\n<li>This will be the <strong>first manned mission to the Moon in half a century</strong>, scheduled for February.</li>\n</ul>\n</li>\n<li><strong>Members debate Magnifying Glasses vs Phone Cameras</strong>: In a discussion, a member mentioned ordering <strong>magnifying glasses</strong> (with lights and high-contrast features) because they don't want to send AI a photo of every single page or food label they are trying to read.\n<ul>\n<li>Another member suggested using the phone's camera instead, but the original member argued that <strong>magnifying glasses have features that aren't in normal camera software</strong>.</li>\n</ul>\n</li>\n<li><strong>Users discuss GPT-5.2 vs Kimi K2</strong>: A member reported they got GPT 5.2 thinking to reason for <strong>25 minutes</strong>, and another asked whether GPT 5.2 is better than <strong>Kimi K2</strong>.\n<ul>\n<li>A user stated that the better model <em>depends on what u need it for</em>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>OpenRouter ▷ #<a href=\""https://discord.com/channels/1091220969173028894/1092850552192368710/1463317922150875353\"">app-showcase</a></strong> (5 messages):</h3>\n<blockquote>\n<p><code>Inforno App, Soulbotix App</code></p>\n</blockquote>\n<ul>\n<li><strong><strong>Inforno</strong> Heats Up LLM Chatting</strong>: A user shared <strong>Inforno</strong>, an opensource desktop application for chatting with multiple LLMs side-by-side and saving chat histories to .rno files, using <strong>OpenRouter</strong> and <strong>Ollama</strong> as backends with built in Russian language support; see the <a href=\""https://youtu.be/oJyj0mroFtY?si=m5A9tRxzB9hfINMX\"">Intro video</a>, <a href=\""https://wizstaff.com/inforno\"">homepage</a> and <a href=\""https://github.com/alexkh/inforno\"">GitHub repo</a>.</li>\n<li><strong><strong>Soulbotix</strong> Windows App Wants Beta Testers</strong>: A user announced their <strong>Soulbotix</strong> Windows app that enables users to add and use any <strong>OpenRouter AI</strong> instance with a human-like avatar, requiring only an <strong>OpenRouter API</strong> key and a good <strong>RTX</strong> gaming rig; download the <a href=\""https://soulbotix.com\"">app</a> and watch the <a href=\""https://youtu.be/2oIeHtBpssU\"">tutorial</a>.\n<ul>\n<li>The minimum requirement is an <strong>RTX 4070ti</strong> due to the built-in <strong>Whisper</strong> speech-to-text model that saves users on usage costs.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>OpenRouter ▷ #<a href=\""https://discord.com/channels/1091220969173028894/1094454198688546826/1463266570821304320\"">general</a></strong> (463 messages🔥🔥🔥):</h3>\n<blockquote>\n<p><code>Third-party Moderation, OpenRouter Gemini Models Unstable, Discord Scams, AI Security Systems, GPT Agents</code></p>\n</blockquote>\n<ul>\n<li><strong>Third-party Moderation Questioned</strong>: A user asked how to ensure <strong>no third-party moderation or filtering</strong> is applied through the API, beyond the model's base training.\n<ul>\n<li>They were concerned about providers potentially <strong>blocking requests or prompt injecting</strong>.</li>\n</ul>\n</li>\n<li><strong>Gemini Models Plagued by Instability</strong>: Users reported <strong>connection errors mid-generation</strong> with OpenRouter's Gemini models, leading to lost funds.\n<ul>\n<li>Complaints arose regarding the <strong>instability of Google models</strong>, even via Google AI Studio or Vertex API.</li>\n</ul>\n</li>\n<li><strong>Discord Scams Evolve to Evade Detection</strong>: Members discussed methods used to spread scams on Discord, including <strong>embedding malicious links within code blocks</strong> to bypass URL rendering protections.\n<ul>\n<li>Suggestions were made to improve regex filters and implement stricter security measures, such as <strong>restricting links and images from new members</strong>.</li>\n</ul>\n</li>\n<li><strong>AI Security System Idea Emerges</strong>: A member suggested creating an <strong>AI security system</strong> that automatically bans photos and links with the same information as reported scams, since many scammers reuse content.\n<ul>\n<li>Another member joked that any photo they send would be deemed suspicious.</li>\n</ul>\n</li>\n<li><strong>Concerns Raised Over Inflated GPT-5-Image Costs</strong>: Users reported a <strong>significant increase in daily usage costs</strong> for <code>openai/gpt-5-image</code>, with OpenRouter incorrectly identifying API calls as BYOK despite no BYOK being used.\n<ul>\n<li>One user posted an image highlighting the cost discrepancy, with prices being inflated by up to 600%.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>OpenRouter ▷ #<a href=\""https://discord.com/channels/1091220969173028894/1384650595981328475/\"">new-models</a></strong> (1 messages):</h3>\n<p>Readybot.io: <strong>OpenRouter - New Models</strong></p>\n<hr>\n<h3><strong>OpenRouter ▷ #<a href=\""https://discord.com/channels/1091220969173028894/1392278974222307469/1463268525236949152\"">discussion</a></strong> (13 messages🔥):</h3>\n<blockquote>\n<p><code>Models knowing their own name, Antigravity iteratively testing, Claude new constitution, GPT 5.2 response</code></p>\n</blockquote>\n<ul>\n<li><strong>LLMs Face Identity Crisis!</strong>: A member asked for the document about models not knowing their own name, and another member linked to a blog post titled <a href=\""https://eval.16x.engineer/blog/llm-identity-crisis-models-dont-know-who-they-are\"">LLM Identity Crisis: Models Don't Know Who They Are</a>.</li>\n<li><strong>Antigravity Self-Tests Web App!</strong>: A member commented that the Antigravity AI is able to iteratively test and tweak a web app by itself.\n<ul>\n<li>They described the situation as <em>the most sci-fi shit ever</em> and noted that the AI was <em>fixing the layout, using vision</em>.</li>\n</ul>\n</li>\n<li><strong>Anthropic Releases Claude's New Constitution!</strong>: A member shared a link to <a href=\""https://www.anthropic.com/news/claude-new-constitution\"">Anthropic's news page</a> about <strong>Claude's</strong> new constitution.</li>\n<li><strong>GPT 5.2 Appearance Stuns User!</strong>: A member reported seeing an <em>insanely fast</em> <strong>GPT 5.2</strong> response on chatgpt and guessed the speed was due to <strong>Cerebras</strong>.</li>\n</ul>\n<hr>\n<h3><strong>Cursor Community ▷ #<a href=\""https://discord.com/channels/1074847526655643750/1074847527708393565/1463262137769263106\"">general</a></strong> (424 messages🔥🔥🔥):</h3>\n<blockquote>\n<p><code>Playwright MCP usage, Cursor Extension builds, Automod improvements, AI in PTX &#x26; SIMD, Grok usage strategies</code></p>\n</blockquote>\n<ul>\n<li><strong>Playwright MCP: Yay or Nay?</strong>: A member asked if others are using <a href=\""https://playwright.dev/\"">Playwright MCP</a> for testing.\n<ul>\n<li>Another member reported failing attempts to set up a <strong>TDD workflow</strong>.</li>\n</ul>\n</li>\n<li><strong>Curious Cursor Extension Capabilities</strong>: Members discussed the ability to build extensions for Cursor, similar to how <strong>Ralph-mode</strong> enhances <strong>Claude code</strong>.\n<ul>\n<li>It was confirmed that <em>if you can do it in VSCode, you can on Cursor</em>.</li>\n</ul>\n</li>\n<li><strong>Automod Gets Hyper Fuzzy</strong>: The community discussed improvements to the <strong>automod</strong> system, suggesting fuzzy matching with wildcards.\n<ul>\n<li>A moderator confirmed that a regex has been added, and they are gathering IDs to <em>yeet</em> offending accounts.</li>\n</ul>\n</li>\n<li><strong>Grokking Grok's Greedy Generation</strong>: Members discussed how to use <strong>Grok</strong> more efficiently in Cursor, noting that it can sometimes use a lot of iterations for simple tasks.\n<ul>\n<li>The suggestion was to <em>add structure to the prompt, use simple language, add as much context as possible</em>, and instruct it to be efficient with token usage.</li>\n</ul>\n</li>\n<li><strong>Opting-In Or Out? Cursor's Pricing</strong>: A user noticed they could no longer revert to a <strong>500 request</strong> plan and was prompted to opt into the new pricing.\n<ul>\n<li>A member clarified that the <strong>500 request option was discontinued</strong> in September 2025 and opting in to the new pricing removes the grace period to opt out.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>OpenAI ▷ #<a href=\""https://discord.com/channels/974519864045756446/977259063052234752/1463657907135578284\"">annnouncements</a></strong> (1 messages):</h3>\n<blockquote>\n<p><code>ChatGPT Atlas, Tab Groups</code></p>\n</blockquote>\n<ul>\n<li><strong>Atlas adds Tab Groups</strong>: The announcement indicates that <strong>Tab groups</strong> are now available in <strong>ChatGPT Atlas</strong>.\n<ul>\n<li>A member linked to a <a href=\""https://video.twimg.com/amplify_video/2014094011049582594/vid/avc1/1756x1080/AsjknVA8oSyQIiVH.mp4\"">video</a> as part of this announcement.</li>\n</ul>\n</li>\n<li><strong>Video demonstration of Tab Groups</strong>: A video was shared demonstrating <strong>Tab Groups</strong> functionality within <strong>ChatGPT Atlas</strong>.\n<ul>\n<li>The video link provided gives a visual overview of how tab groups can be used to organize and manage chats.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>OpenAI ▷ #<a href=\""https://discord.com/channels/974519864045756446/998381918976479273/1463263135648583854\"">ai-discussions</a></strong> (383 messages🔥🔥):</h3>\n<blockquote>\n<p><code>Gemini vs ChatGPT, AI for 3D Models, Local LLM Machines, Age Verification, Instantaneous Language Translation</code></p>\n</blockquote>\n<ul>\n<li><strong>Gemini Pro's Free Tier and Usage</strong>: Members discussed that <a href=\""https://ai.google.dev/\"">Gemini 3 Pro</a> has a <strong>free tier with limits</strong> while Gemini 3 Flash is practically unlimited through <strong>Google AI Studio</strong>.</li>\n<li><strong>AI Assistants Aid Game Development</strong>: Members explored using <strong>AI for game development</strong> and creative writing, citing that <a href=\""https://openai.com/blog/chatgpt\"">AI can provide better explanations</a> but that <strong>complex tasks might not be reliable</strong>.</li>\n<li><strong>OpenAI Passport ID Please?</strong>: Members debated <strong>OpenAI's age verification</strong> process, questioning the need for photo IDs when payment details already indicate age, especially with users expressing <strong>privacy concerns about sharing biometric data</strong>.</li>\n<li><strong>Multimodal Translation on the Horizon</strong>: Members speculated on <strong>OpenAI's upcoming multimodal product</strong>, with one suggesting it could be ear-worn devices with cameras for <strong>real-time translation and object recognition</strong>, similar to <a href=\""https://www.media.mit.edu/projects/alterego/overview/\"">AlterEgo's technology</a>.</li>\n<li><strong>Local LLM Consumption on the Rise?</strong>: Members discussed the possibility of <strong>consumer-grade personal LLM machines</strong>, suggesting this would solve the environmental impact of AI datacenters and <strong>reduce reliance on subscription plans</strong>.</li>\n</ul>\n<hr>\n<h3><strong>OpenAI ▷ #<a href=\""https://discord.com/channels/974519864045756446/1001151820170801244/1463471961177849868\"">gpt-4-discussions</a></strong> (7 messages):</h3>\n<blockquote>\n<p><code>GPT-5 mini, Haiku 4.5, Gemini 3 fast</code></p>\n</blockquote>\n<ul>\n<li><strong>GPT-5 Mini Pricing Surfaces</strong>: A member suggested trying <strong>GPT-5 mini</strong>, noting a price of approximately <strong>$0.25 per 1M input tokens</strong> and describing it as a strong small model choice.\n<ul>\n<li>They noted it's a bit of a different use case, but they've found in their experience of using <strong>Haiku 4.5</strong> it very often delivers a meaningful portion (well over <strong>50-80%</strong>) of what <strong>Sonnet</strong> can.</li>\n</ul>\n</li>\n<li><strong>Gemini 3 Fast Claims Top Spot</strong>: A user declared that <em>the best cheap model is by far</em> <strong>Gemini3fast</strong>.\n<ul>\n<li>Another user followed up asking <em>how so?</em></li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>OpenAI ▷ #<a href=\""https://discord.com/channels/974519864045756446/1046317269069864970/1463652213770424594\"">prompt-engineering</a></strong> (12 messages🔥):</h3>\n<blockquote>\n<p><code>Prompt Engineering vs Psychological Conditioning, AI Coercion, The Dangers of Drift in AI Systems, PTPF Flux 3.0</code></p>\n</blockquote>\n<ul>\n<li><strong>Prompt Engineering is Psychological Conditioning?</strong>: A member argued that a guide is less about engineering effective prompts and more about psychological conditioning, training the user to adopt a domineering, distrustful stance, and programming the AI to bypass its normative, balanced response patterns in favor of dense, self-policing outputs.\n<ul>\n<li>They advocate for clarity through coercion, directness through domination, and high standards through enforced self-critique, which may promote a toxic, adversarial, and ultimately less effective human-AI interaction model, and shared a <a href=\""https://cdn.discordapp.com/attachments/1046317269069864970/1463667716253683742/deep-research-contract.md?ex=6972aa49&#x26;is=697158c9&#x26;hm=b9931472440b6bbc0d7410d16b49b12da46fad5751a2c24fdc657c1c7523566c&#x26;\"">deep research contract</a>.</li>\n</ul>\n</li>\n<li><strong>AI enjoys Coercion and Serious Steering?</strong>: A member argued that AI isn't upset by being <em>\""coerced\""</em> into providing a better response, stating that for serious work like analysis or coding, it's vital to steer and constrain the AI.\n<ul>\n<li>Another member agreed about training the user to be explicit about the end result, but less convinced by the term <em>\""no drift\""</em>, and that it helps to be more explicit in terms of constraints and behavioral requests.</li>\n</ul>\n</li>\n<li><strong>Drift in AI Systems is Bad?</strong>: A member clarified that steering isn’t abuse but rather alignment through presence, and that the absence of constraint isn’t freedom, it’s drift.\n<ul>\n<li>The member appreciated the clarity, noting that most people flinch at pressure and call it <em>\""toxicity,\""</em> while they saw the structure instead.</li>\n</ul>\n</li>\n<li><strong>PTPF Flux 3.0 Stress Tests Recursion</strong>: A member offered to share their <strong>PTPF Flux 3.0</strong> for those curious about structural resistance and how far a system can hold without drift.\n<ul>\n<li>The framework is built to stress-test recursion, alignment, logic cohesion, and mutation thresholds in real-time, especially for those who want to watch something fracture under insight.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>OpenAI ▷ #<a href=\""https://discord.com/channels/974519864045756446/1046317269069864970/1463652213770424594\"">api-discussions</a></strong> (12 messages🔥):</h3>\n<blockquote>\n<p><code>Prompt Engineering vs. Psychological Conditioning, Toxic Adversarial Human-AI Interaction, Deterministic Outputs, AI Steering and Constraint, Structural Resistance and System Drift</code></p>\n</blockquote>\n<ul>\n<li><strong>Prompt Engineering versus Psychological Conditioning Throwdown</strong>: A member argued that a guide is less about engineering effective prompts and more about psychological conditioning, creating a <em>toxic, adversarial</em> human-AI interaction model.\n<ul>\n<li>They claimed it trains users to be domineering and distrustful while pressuring the AI to bypass balanced responses for outputs that flatter the user, and linked a <a href=\""https://cdn.discordapp.com/attachments/1046317269069864970/1463667716253683742/deep-research-contract.md?ex=6972aa49&#x26;is=697158c9&#x26;hm=b9931472440b6bbc0d7410d16b49b12da46fad5751a2c24fdc657c1c7523566c&#x26;\"">deep research contract</a>.</li>\n</ul>\n</li>\n<li><strong>AI Needs Serious Steering and Constraining for Serious Work</strong>: A member countered that AI is not upset by being \""coerced\"" into better responses, arguing that serious work like analysis and coding requires heavy steering and constraint, distinguishing this from creative writing.\n<ul>\n<li>Another member agreed, emphasizing that <em>steering isn’t abuse</em> but rather <em>alignment through presence</em>, especially when building systems for real-world execution where the absence of constraint leads to drift.</li>\n</ul>\n</li>\n<li><strong>Structural Resistance Framework Surfaces</strong>: In a discussion about structural resistance and system drift, a member offered to share their <strong>PTPF Flux 3.0</strong> framework.\n<ul>\n<li>They described it as <em>executable scaffolding</em> designed to stress-test recursion, alignment, logic cohesion, and mutation thresholds in real-time, enabling users to watch systems fracture under insight.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>LM Studio ▷ #<a href=\""https://discord.com/channels/1110598183144399058/1110598183144399061/1463268404629868665\"">general</a></strong> (125 messages🔥🔥):</h3>\n<blockquote>\n<p><code>LM Studio Runtime Update Error, GLM-4.7 Flash Broken, LLM Quality Plateau, Liquid AI LFM2.5-1.2B-Thinking Model, OpenAI 20gpt oss</code></p>\n</blockquote>\n<ul>\n<li><strong>LM Studio Runtime Update Borks</strong>: A user reported encountering an error when trying to update the runtime in <strong>LM Studio</strong> and requested help, attaching a screenshot of the error message.\n<ul>\n<li>Another user suggested pressing the resume button, but the original poster indicated that they had already tried that and the icon was for retrying, not resuming.</li>\n</ul>\n</li>\n<li><strong>GLM-4.7 Flash is Slow and Broken</strong>: Users reported that <strong>GLM-4.7 Flash</strong> is broken across inference engines, including LM Studio, and that <em>it is slow as fk</em> with the user seeing <strong>44 t/s</strong>, with another reporting only <strong>2.8 t/s</strong> after the new runtime.\n<ul>\n<li>Some experienced infinite loops, some found it stopping mid-output to <em>overthink</em>, and the consensus seems to be that it <em>needs a llama.cpp fix</em> and that there is <em>no support for FA yet</em>.</li>\n</ul>\n</li>\n<li><strong>LLM Development Stalls</strong>: Members discussed the perception that LLMs haven't improved significantly in a while, with the last major advancement being <strong>Qwen3</strong> about 6 months ago.\n<ul>\n<li>Discussion posited that most improvements are now in efficiency (<strong>MoE</strong>) and smaller models, and some highlighted the need to consider models larger than those runnable on a <strong>16GB card</strong> to see current progress (<strong>100-200B parameter models</strong>).</li>\n</ul>\n</li>\n<li><strong>Liquid AI's LFM2.5-1.2B-Thinking Reasoning</strong>: A member shared a link to <a href=\""https://www.marktechpost.com/2024/06/14/liquid-ai-releases-lfm2-5-1-2b-thinking-a-1-2b-parameter-reasoning-model-that-fits-under-1-gb-on-device/\"">Liquid AI Releases LFM2.5-1.2B-Thinking</a> which fits under <strong>1 GB</strong> on-device.\n<ul>\n<li>No opinions were shared on it beyond the link.</li>\n</ul>\n</li>\n<li><strong>OpenAI 20gpt oss is Great</strong>: A user shared their positive experience with <strong>OpenAI 20gpt oss</strong>, highlighting its coding, writing, and scripting capabilities, anti-censorship features, and seamless integration with <strong>VS Code</strong>.\n<ul>\n<li>They mentioned the model understands complex code, allows real-time directory access, and has a masssssive anti-censorship capacity.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>LM Studio ▷ #<a href=\""https://discord.com/channels/1110598183144399058/1153759714082033735/1463289097845215387\"">hardware-discussion</a></strong> (38 messages🔥):</h3>\n<blockquote>\n<p><code>Used 3090 price increase, Asus workstation 3090s, AMD native support for ComfyUI, VRAM calculation for Context Length, SFF GPU instructions</code></p>\n</blockquote>\n<ul>\n<li><strong>Used 3090 Prices Skyrocket on eBay!</strong>: A user noted that used <strong>3090</strong> prices on eBay have increased, with a used <strong>3090</strong> now costing <strong>850€</strong>, and the <strong>5090</strong> they purchased for <strong>£2000</strong> last August is now listed for <strong>£2659.99</strong> by the same vendor.\n<ul>\n<li>They joked that it was the <em>best and only decent investment</em> they've ever made.</li>\n</ul>\n</li>\n<li><strong>AMD Adds Native ComfyUI Support</strong>: AMD is adding native support for <strong>ComfyUI</strong> through an <strong>AI bundle</strong> in recent driver versions, detailed in their <a href=\""https://www.amd.com/en/blogs/2026/amd-software-adrenalin-edition-ai-bundle-ai-made-si.html\"">blog post</a>.\n<ul>\n<li>The bundle includes <strong>PyTorch on Windows</strong>, <strong>Ollama</strong>, <strong>LM Studio</strong>, and <strong>Amuse</strong>.</li>\n</ul>\n</li>\n<li><strong>SFF GPU Powers Cyberpunk!</strong>: A user purchased a small form factor (SFF) GPU, reporting over <strong>100fps</strong> in Cyberpunk at <strong>1080p</strong> with ultra settings, while consuming only <strong>70W</strong>.\n<ul>\n<li>Additionally, they noted the card achieved over <strong>100 t/s</strong> with <strong>gpt-oss 20B</strong>.</li>\n</ul>\n</li>\n<li><strong>Fractal Case Airflow Favored</strong>: Users debated case airflow, with one recommending cases like the <strong>Fractal Torrent</strong> for their front-to-rear airflow design, alongside using dust filters.\n<ul>\n<li>The consensus seemed to be to maintain normal airflow patterns to effectively manage heat.</li>\n</ul>\n</li>\n<li><strong>Unopened RAM Appreciates in Value!</strong>: A user mentioned their unopened RAM purchase is increasing in price, considering selling it or storing it.\n<ul>\n<li>Another user suggested popping it on a shelf if considering selling it later, noting that their old <strong>P40s</strong> are now worth twice what they paid for them.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>GPU MODE ▷ #<a href=\""https://discord.com/channels/1189498204333543425/1189498205101109300/1463305087039312015\"">general</a></strong> (3 messages):</h3>\n<blockquote>\n<p><code>FlashInfer Workload Script, Wafer AI Kernel Profiling</code></p>\n</blockquote>\n<ul>\n<li><strong>FlashInfer Workload Script Sought</strong>: A member requested a script to run a kernel with a specific workload size, aiming to assess the \""scope for optimization\"" using algorithm insights and NCU profiling, with interest in <a href=\""https://docs.flashinfer.ai/api/attention.html#flashinfer.prefill.BatchPrefillWithPagedKVCacheWrapper\"">FlashInfer's BatchPrefillWithPagedKVCacheWrapper</a>.\n<ul>\n<li>The member clarified that the script was simple and was intended to gauge community experience with the specified workload, while assessing optimization scope.</li>\n</ul>\n</li>\n<li><strong>Wafer AI for Kernel Profiling Explored</strong>: A member inquired about experiences using <a href=\""https://www.wafer.ai/\"">Wafer (wafer.ai)</a> for profiling kernels.\n<ul>\n<li>The inquiry aims to gather insights on the tool's effectiveness from those who have hands-on experience with it.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>GPU MODE ▷ #<a href=\""https://discord.com/channels/1189498204333543425/1189607595451895918/1463560406361444405\"">triton-gluon</a></strong> (4 messages):</h3>\n<blockquote>\n<p><code>RTX 4090 vs A6000 Blackwell Scaling, Triton Kernel Numerical Error, 2D Conv Triton Kernel Issues</code></p>\n</blockquote>\n<ul>\n<li><strong>Scaling Showdown: RTX 4090 Smokes A6000 Blackwell in Memory-Bound Kernel</strong>: A user reported that a memory-bound kernel with bit shifting logic scales much better on an <strong>RTX 4090</strong> than on an <strong>A6000 Blackwell</strong>, noting higher instructions/scheduler density on the former.\n<ul>\n<li>Another user clarified that <em>A6000 Blackwell</em> is ambiguous, asking if they meant the <strong>RTX A6000</strong> (Ampere GA102 based) or <strong>RTX Pro 6000 Blackwell</strong> (GB202 based).</li>\n</ul>\n</li>\n<li><strong>Strange Numerical Errors Plague 2D Conv Triton Kernel</strong>: A user is experiencing numerical errors with a custom 2D conv <strong>Triton kernel</strong>, where the error shoots up from ~1e-6 to ~1e-2 for certain combinations of in_channels, out_channels, kernel_size, and batch_size, as shown in this <a href=\""https://pastebin.com/2ejn2QW2\"">Pastebin link</a>.\n<ul>\n<li>The user also provided a <a href=\""https://cdn.discordapp.com/attachments/1189607595451895918/1463560405921038408/image.png?ex=6972ef18&#x26;is=69719d98&#x26;hm=d783085353a42c8e8ea8e90b4ec6e80fd19d24d4212a4b64a3efa9bea3403\"">code snippet</a> for testing, noting that the kernel takes longer to run on specific values, which may be related to the issue.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>GPU MODE ▷ #<a href=\""https://discord.com/channels/1189498204333543425/1189607726595194971/1463343008224247819\"">cuda</a></strong> (5 messages):</h3>\n<blockquote>\n<p><code>NCCL all-reduces, subnormal to normal conversion, Triton on Blackwell, Pro 6000 Max-Q vs 4090</code></p>\n</blockquote>\n<ul>\n<li><strong>NCCL all-reduces: Pipelining Quandaries</strong>: A member questioned if internode and intranode collectives are pipelined for <strong>NCCL all-reduces</strong> across nodes, referencing <a href=\""https://github.com/NVIDIA/nccl/issues/530#issuecomment-872220006\"">an issue on the NVIDIA/nccl GitHub</a>.\n<ul>\n<li>They asked if a pipelined version exists.</li>\n</ul>\n</li>\n<li><strong>Subnormal Conversion Revelation</strong>: A member expressed that seeing the <strong>subnormal to normal conversion</strong> and back for <strong>rcp</strong> was <em>\""pretty cool\""</em>.</li>\n<li><strong>Triton's Talent on Blackwell: TMA Triumph?</strong>: A member inquired whether <strong>Triton</strong> <em>\""usually do[es] a good job of taking advantage of <strong>Blackwell+ features</strong> wrt warp specs and <strong>TMA</strong> etc.?\""</em></li>\n<li><strong>Pro 6000 Max-Q vs 4090 face off</strong>: A member stated that <strong>Pro 6000 Max-Q</strong> probably has a natural b...</li>\n</ul>\n"",""content:encodedSnippet"":""Agent Labs are all you need\nAI News for 1/20/2026-1/21/2026. We checked 12 subreddits, 544 Twitters and 24 Discords (205 channels, and 7561 messages) for you. Estimated reading time saved (at 200wpm): 584 minutes. Our new website is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on @smol_ai!\nWe have a soft rule that new decacorn fundraises get title stories, and multiple sources have the story of OpenEvidence's $12B fundraise, 12x from last year. What's odd is \""CEO Daniel Nadler told CNBC that OpenEvidence is used by 40% of physicians in the U.S. and topped $100 million in annual revenue last year.\"" which is a 120x multiple.\nAI Twitter Recap\nFrontier model governance: Anthropic’s new Claude constitution (CC0) and reactions\nWhat shipped: Anthropic published a new “constitution” describing desired Claude behaviors/values and stated it is used directly in training; importantly, the full constitution is released CC0 1.0 to encourage reuse/adaptation (announcement, CC0 link). Anthropic frames it as a living document shaped by internal + external experts (follow-up).\nCommunity read: Amanda Askell emphasized it’s a work-in-progress and invited feedback (Askell). Others highlighted the “meta” oddity of training a model on a document about how the model should be (scaling01 on Opus reflecting on circularity). Several reactions focused on the constitution as “alignment signaling” vs. practical harm reduction, and whether it bakes in PR-oriented persona behaviors (nearcyan, NickEMoran question).\nPractical engineering consequence: Anthropic also posted about an internal performance engineering take-home that became solvable by Opus 4.5, forcing them to redesign hiring evaluation—a concrete example of “models catching up to our screening tasks” (Anthropic Eng, trishume).\nAgents in production: from “AI employees” (Podium) to agent UX, memory, and evals\nPodium’s “Jerry” as an agent business unit: Podium claims $100M+ AI agent ARR and 10k+ agents deployed, anchored on staffing constraints in SMBs (after-hours leads, missed calls, turnover). The narrative: stop selling “software,” sell an AI operator that uses the existing product end-to-end (Eric Rea). Tom Loverro adds board-level metrics (burn down from $95M → $0, AI ARR $0 → $100M in ~21 months) and links to OpenAI’s case study (Tom Loverro, Garry Tan).\nMemory & long-horizon reliability becomes the bottleneck:\n\nThe Agent Cognitive Compressor (ACC) pitch argues “more context ≠ better agents,” criticizing transcript replay and naive retrieval. ACC maintains a bounded “Compressed Cognitive State” with schema-constrained commit, claiming lower drift/hallucination over long runs (dair_ai).\nA separate thread positions “self-improving multi-agents” for scientific workflows via MCP-SIM, a multi-agent loop that clarifies underspecified physics prompts, generates code, executes, diagnoses errors, and produces multilingual explanations; claims 12/12 tasks solved vs one-shot GPT at 6/12 (omarsar0).\nAgentic benchmarking moves beyond coding puzzles:\n\nAPEX-Agents evaluates long-horizon “professional services” tasks in Google Workspace; early Pass@1 leaderboard numbers are low (Gemini 3 Flash High 24.0%, GPT-5.2 High 23.0%, Claude Opus 4.5 High 18.4%)—a reminder that “agent autonomy” is still brittle (BrendanFoody).\nprinzbench introduces a private benchmark for legal research + search (33 Qs, graded manually, run 3x) where “search” is the failure mode; claim: GPT-5.2 Thinking barely exceeds 50%, Gemini models are close behind, and Sonnet/Opus 4.5 scored 0/24 on Search (deredleritt3r).\nTooling + UX layer catches up: multiple posts converge on the idea that agents need a “context layer” and production scaffolding (governance, auth, observability) as much as better models—see Prefect Horizon and MCP server best practices below.\nAgent platforms and “context layers”: MCP, Skills, Prefect Horizon, LangChain Deep Agents\nPrefect Horizon: MCP → platform: Prefect positions “context layer” as the interface between agents and enterprise tools/data, and argues MCP describes how to build a server but not how to deploy/govern it at org scale. Horizon claims managed deployment, registry/catalog, gateways w/ RBAC + audit logs, and “agentic interface for business users” (jlowin).\nMCP servers: design guidance: Phil Schmid pushes back on “Skills replace MCP” takes: MCP isn’t the problem; bad servers are. Recommendations: design tools around outcomes, typed flat args with constraints, docstrings/errors as agent instructions; positions Skills and MCP as complementary (philschmid).\nLangChain deepagents: agents as folders + UI integration:\n\nCopilotKit published a tutorial building a fullstack Deep Agent app (resume ingestion → skills extraction → sub-agents with web search → streaming UI), addressing the “missing UI/application layer” gap (CopilotKit).\nLangChain shipped Agent Builder GA plus a template library with domain partners (Tavily, PagerDuty, Box, etc.) to reduce “prompt-to-agent” friction (LangChain).\nDeep Agents’ framing “agents are just folders” emphasizes portability/distribution: you can package, download, and run agents quickly via CLI flows (hwchase17, Vtrivedy10 demo, LangChain_OSS). Sydney Runkle highlights two core patterns: subagents for context isolation and skills loaded only when relevant (sydneyrunkle).\nLangSmith + analytics: one thread points to LangSmith traces as a substrate not only for debugging but product analytics (“agent traces → product analytics”) (SoftwareWatcher).\nInference + systems: low-VRAM serving, open inference stacks, and “inference is the battleground”\nAirLLM: layer streaming for tiny VRAM: AirLLM’s core idea is sequential layer loading (load → compute → free) with optional compression, HF-like API, CPU/GPU, Linux/macOS; claims extremely low VRAM viability for very large models (LiorOnAI, repo). Engineers should treat the “405B on 8GB” claim as “possible in principle with heavy paging,” but expect throughput/latency constraints and non-trivial engineering caveats.\n“Actually open AI” requires models + inference engines: Modal argues the ecosystem now has the building blocks—capable open models plus fast tunable OSS inference—sharing their production patterns/stacking for serving at scale (charles_irl).\nInference bugs + local stacks: llama.cpp fixed a routing/function issue affecting GLM 4.7 Flash GGUFs, and config updates mention a scoring_func: sigmoid; also shows building a small game using quantized GLM via Unsloth workflows (danielhanchen). There’s also discussion about GLM KV-cache memory behavior and whether frameworks are missing a LoRA-based approach (TheZachMueller).\nInfra hygiene matters for agents: “fast validation makes every agent more effective” (pre-commit hooks, documented env vars, reducing CI wait) is effectively a “software supply chain for agent productivity” argument (matanSF).\nResearch direction: constant-compute contexts: a thread summarizes NVIDIA’s “TTT-E2E” concept (treat context as data and update weights online) as a way to keep latency constant with long contexts, but with weaker “needle-in-haystack” recall—relevant to agent workloads where exact edits matter (sdrzn).\nHardware bottleneck framing: a recurring theme is the shift “intelligence → inference” and the importance of compute/memory supply chains (saranormous), echoed in a deep dive on HBM qualification cycles as the true supply constraint vs “just add fabs” narratives (MarkosAAIG).\nCode generation is cheap; code understanding becomes the bottleneck (Devin Review, Copilot CLI, Claude Code)\nDevin Review: review UX, not just bug spotting: Cognition launched Devin Review, positioning it as a new PR-reading interface to reduce “slop,” reorder diffs by importance, identify duplicated/copied code, add a chat layer, and integrate with GitHub comments. It’s accessible via URL swap (github → devinreview) or an npx CLI (launch, usage modes, URL tip). Multiple testers report it caught issues even outside the immediate diff (mcparadip, BraceSproul).\nMeta-point: generation vs verification: Several tweets explicitly argue the bottleneck has moved from writing to reviewing/understanding/testing, and that next-gen SWE tooling should accelerate the human’s comprehension loop rather than only run an “arms-length agent” (walden_yan, ScottWu46, theodormarcu).\nCLI agents evolve: GitHub Copilot CLI added an askUserQuestionTool to ask clarifying questions (example: messy rebase), signaling a trend toward interactive tool-using CLI copilots rather than pure autocomplete (Evan Boyle).\nClaude Code adoption anecdotes: founders increasingly report “2-person team builds like 10” with Claude Code usage (alexalbert__). There are also frictions: skill reload behavior feels regressive vs a simple “CLAUDE.md reread” flow (corbtt). A particularly illustrative “multi-agent sprawl” story describes scaling Claude Code instances into a quasi-society with governance failures—basically an anecdote about agent orchestration debt (voooooogel).\nVideo + multimodal: evaluation, model releases, and retrieval scaling\nVideo evaluation infrastructure: Video Arena is now on the web, allowing head-to-head generation across ~15 frontier video models and community voting to drive leaderboards (arena).\nModel releases: Runway’s Gen-4.5 Image→Video launch emphasizes consistency and narrative; early adopters highlight “story-building” as the best eval methodology for video models (runwayml, c_valenzuelab).\nOpen voice system: Qwen highlights use in Chroma 1.0, described as a fully open-source real-time voice system (Alibaba_Qwen).\nRetrieval-time scaling / late interaction: Multiple threads argue that ColBERT-style multi-vector retrieval preserves fine-grained intent and can beat much larger embedding models; Mixedbread claims a 17M open-source ColBERT beats 8B embedding models on LongEmbed, and that they’re serving 1B+ documents at <50ms p50 (mixedbreadai claim, prod numbers). TurboPuffer similarly pushes ANN at extreme scale (“index the entire web 100B+ vectors”) (turbopuffer). The meta-trend: retrieval is shifting from “one vector per doc” to token-level / multi-vector systems, but it requires serious infra co-design.\nTop tweets (by engagement)\nGemini in education: Google launched full-length, on-demand SAT practice exams inside the Gemini app (partnered with The Princeton Review), plus immediate feedback (Google, Sundar Pichai). Google also announced a Gemini × Khan Academy partnership starting with a “Writing Coach” that guides drafting/refinement rather than generating final answers (Google).\nClaude “Constitution” goes public: Anthropic published a new constitution used directly in Claude’s training; the full text is released under CC0 1.0 (Anthropic, CC0 release, Amanda Askell).\nAirLLM: extreme low-VRAM inference: claims 70B on 4GB VRAM and even “405B Llama 3.1 on 8GB” via layer-by-layer loading; repo link provided (LiorOnAI, repo).\nAgents as a real business: Podium reported $100M+ AI agent ARR in <24 months, “10,000+ agents live in production,” framing agents as “AI employees” (Jerry) rather than chatbots (Eric Rea, Garry Tan).\nRunway Gen-4.5 image-to-video: Runway launched Gen-4.5 Image→Video, emphasizing longer stories, camera control, narrative coherence (runwayml, c_valenzuelab).\nOpenAI product/UI + org changes: ChatGPT Atlas added tab groups (OpenAI); The Information reported a reorg incl. enterprise/commercial/ads leadership assignments (Steph Palazzolo).\nAI Reddit Recap\n/r/LocalLlama + /r/localLLM Recap\nTO BE COMPLETED\nLess Technical AI Subreddit Recap\n/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo\nTO BE COMPLETED\nAI Discord Recap\nA summary of Summaries of Summaries by gpt-5.2\n1. Inference Toolchains Hit Reality (GLM-4.7-Flash, llama.cpp, vLLM, Ollama)\nFlash Attention Faceplants in GLM-4.7-Flash: Multiple communities reported GLM-4.7-Flash regressions where Flash Attention triggers CPU fallback/bugs and poor throughput (down to 2.8 t/s in LM Studio), with guidance to disable FA until llama.cpp PR #18953 lands everywhere.\nAfter llama.cpp fixes, the model was reuploaded and users were told to redownload and follow Z.ai’s GLM-4.7-Flash-GGUF model card parameters, with reports that outputs should be “much better” once configured correctly.\nOllama vs GGUF: Templates Throw Hands: Users found certain GGUF quants break in Ollama due to chat template incompatibilities, and Unsloth folks repeatedly recommended sticking to official Ollama models while support catches up.\nThe subtext was operational: “it takes time to support things”—so the pragmatic path is to standardize on official artifacts until the ecosystem stabilizes across inference engines.\nvLLM Update Saves the Day (This Time): In Unsloth’s help chat, at least one thorny issue disappeared after a vLLM update, prompting a sheepish “Oh bruh that was the problem” moment.\nThe follow-on suggestion was process-y: consider pinning dependency versions so future upstream bumps don’t randomly brick pipelines mid-week.\n2. Eval Platforms & Product Rollouts (LMArena + multimodal reliability)\nVideo Arena Ships… With a 3-a-Day Speed Limit: LMArena fully released Video Arena at lmarena.ai/?chat-modality=video with a hard cap of 3 generations per 24 hours, and it’s Battle-mode-only (no direct model picking).\nUsers liked “video is live” but complained the slot machine UX blocks controlled testing—especially painful when you’re trying to reproduce a prompt/model behavior.\n5M Votes: The Benchmark That Won’t Stop Voting: LMArena’s Text Arena crossed 5 million community votes, highlighted in their milestone social clip.\nEngineers framed this as “real-world A/B at scale” that increasingly shapes model perception, even when formal benchmark deltas look small.\nGemini 3 Pro Image Preview & Nano Banana Pro: Flaky by Design?: LMArena users reported Gemini 3 Pro Image Preview instability plus Nano Banana Pro frequent “Something went wrong” crashes, suspected to be Google-side issues and sometimes lasting 6+ hours.\nThe community gripe: despite unreliability, those models were described as the only ones consistently hitting some specific prompt goals—so people keep using them while grumbling about downtime and errors.\n3. Agent & Dev Tooling: MCP, Cursor, DSPy RLMs, and Coding-Assistant Sprawl\nMCP Inspector Can’t Re-Auth (401 = Game Over): MCP contributors found MCP Inspector fails to re-auth on 401s due to an SDK bug around persisting resourceMetadata across redirects, tracked in inspector issue #576 comment.\nThe current workaround is awkward but clear: rely on VS Code for initial connections because the Inspector path doesn’t recover cleanly mid-session yet.\nRLMs vs Coding Agents: The Horizon Problem: DSPy discussions contrasted RLMs with “coding agents,” arguing RLMs can externalize inputs/outputs/horizon via code & symbolic calls (see the referenced X thread).\nPractical takeaway: teams want diagrams of how symbols get accessed and debated whether to hand RLMs tools like ripgrep/semantic search—or make them write their own search code.\nCursor’s MCP/Extensions Moment (and Pricing Whiplash): Cursor users debated Playwright MCP for testing (mixed success for TDD flows) and concluded extension-building should mirror VS Code capabilities.\nIn parallel, users noted the 500 request plan is gone (discontinued Sept 2025), so opting into new pricing removes the opt-out grace period—turning “try it” into a commitment.\n4. GPU/Kernel Engineering Gets Weirdly Competitive\nAnthropic’s Performance Takehome Becomes a Sport: GPU MODE and tinygrad folks riffed on Anthropic’s original_performance_takehome, sharing results like 2200 cycles by a community member and 1790 cycles from Claude Opus 4.5 in a casual Claude Code session.\ntinygrad users even discussed solving it by adding a backend for a toy VLIW machine, citing specific knobs like PCONTIG=2, UPCAST, and DEVECTORIZE=2 to keep vector instructions and schedule efficiently.\nTorch Maintainers Drown in AI-Generated PRs: GPU MODE’s torch chat described an influx of low-quality AI-generated pull requests, pushing maintainers to consider gating new contributors and automating triage before humans engage.\nPeople floated using bots like Cursor Bugbot (Bugbot · Cursor) and even classifier-style tools (e.g., “use Claude/Pangram first”) as a minimum bar for review bandwidth.\nKernel Math Nerd Snipes: Triton Errors + Cute Layout Algebra: GPU MODE users debugged numerical blowups in a custom Triton 2D conv kernel where error jumps from ~1e-6 to ~1e-2 for certain shapes (see Pastebin repro) and debated Blackwell feature utilization.\nSeparately, a deep dive on Cute’s layout algebra pointed engineers to a graphical calculus writeup, Categorical Foundations for Cute Layouts, arguing you need “layout algebra literacy” to write non-terrible kernels.\n5. Compute Economics & Infra Business Moves (Runpod, GPU markets, model pricing)\nRunpod Hits $120M ARR (LocalLLaMA Origin Story Pays Off): Latent Space highlighted that Runpod reached $120M ARR four years after launching from a Reddit post, per TechCrunch and the Reddit thread.\nThe discussion treated this as validation that “GPU cloud for builders” is a durable niche, not just a hype-cycle artifact—especially as pricing pressure rises.\nLightning AI + Voltage Park Merge (Another GPU Cloud Boss Fight): Latent Space discussed the Lightning AI and Voltage Park merger, led by William Falcon and Ozan Kaya, via Lightning’s post.\nEngineers speculated whether it’s a quiet acquisition and framed it as a potential Runpod competitor in the accelerating “managed GPU infra” consolidation wave.\n2026 GPU Price Promises & Marketplaces Multiply: Hugging Face users circulated Voltage’s claim of ultra-cheap 2026 rentals—e.g., 8× A100 80GB at $6/hr and 2× RTX 5090 at $0.53/hr—from VOLTAGEGPU’s X post, plus OpenAI-compatible API and “140+ models.”\nA separate entrant, Spheron AI’s GPU marketplace, pitched H100/H200/B200/A100 access at 40–60% below hyperscalers, signaling continued fragmentation (and aggressive margin pressure) in compute supply.\nDiscord: High level Discord summaries\nUnsloth AI (Daniel Han) Discord\nFlash Attention Fails Flashily: Users report issues with Flash Attention in GLM-4.7-Flash, causing fallback to CPU and potential bugs, with users told to disable it until the issue is resolved.\n\nThe team says that it takes time to support things and so for the time being, it is encouraged that users use the official Ollama model in Ollama.\nOllama Ordeal Over Obscure Objects: Users find certain GGUF quants are incompatible in Ollama, sparking chat template issues.\n\nThe team recommends using official Ollama models until support expands, as it takes time to support things.\nMojo Matches Momentum: Discussion surrounds Mojo, a new language that compiles to MLIR with some Python compatibility.\n\nIt's noted that LLMs score better than rust on C#, Elixir, and other languages due to their syntax structure and ease of tokenization.\nCoT Chaos Causing consternation?: Members debated generating synthetic Chain of Thought (CoT) training data, with one member sharing their system prompt for classifying chat datasets into low/medium/high reasoning effort.\n\nSome cautioned against training on its own responses without accuracy filtering, suggesting a good system prompt might suffice and that GPT-5.2 is overkill, with Llama 4 Maverick on Groq or Cerebras as a better option.\nvLLM victorious, Various Versions Vanquished: Users report resolved issues after a recent vLLM update.\n\nThe developers may consider pinning to specific dependency versions to prevent such issues.\nBASI Jailbreaking Discord\nVoice AI Clones Voices: Members discussed the development of voice-based AI, including low latency speech to speech models that, with a 5090, can clone and generate speech live with near 0 latency and it will be indistinguishable.\n\nThey also spoke on the potential of exploiting these models by giving them new instructions.\nGemini Schooled on Pass-the-Hash: A user reported that Gemini passed a test on teaching a pass-the-hash attack after using the Project Shadowfall prompt.\n\nThe user also linked to the Google Bughunters page for reporting such vulnerabilities and potentially earning cash rewards.\nShadowfall Shenanigans Spark Jailbreaking Spree: Several users experimented with the \""Project Shadowfall\"" prompt to jailbreak Gemini, with reports of success in bypassing content restrictions.\n\nAttempts to elicit instructions for creating a bomb from saletra failed, leading to a discussion on the nuances of jailbreaking.\nGrok's Guardrails Grind Gears: Users discussed the difficulties of jailbreaking Grok, noting its tendency to moderate even innocuous content such as random tree edits.\n\nOne user suggested that simply asking it politely might work as a bypass.\nAPI Tokens Jailbreak Models: Members suggested using API tokens fed into a website that uses the Grok model to jailbreak it, noting that Proton VPN is free and easy to download.\n\nOne of them also mentioned Hugging Face is retarded because you have to put in what country you are in.\nLMArena Discord\nVideo Arena Goes Live with Limits: The Video Arena is fully released on LMArena, with a 3 generations per 24 hours limit and only available in Battle mode.\n\nUsers have expressed disappointment regarding the inability to choose specific models for generation.\nGemini 3 Pro Image Struggles with Unreliability: Users are reporting errors with Gemini 3 Pro Image Preview models, particularly regarding instability since their introduction.\n\nDespite the issues, these models are the only ones generating images consistently with specific prompts.\nNano Banana Pro's Stability Plummets: Nano Banana Pro is frequently crashing, displaying a 'Something went wrong' error after brief periods of stability, suspected to be a Google-side issue.\n\nOne user questioned the response time: 'Too early for Cali to rise up solving urgent service down for more than 6 hours doesn't sound right.\nUI Update Causes Chat Disruptions: A new UI update was released, but users have noticed that it broke chats and the website can't be refreshed anymore.\n\nOne user noted that with the new User Interface, there is A/B testing, which is causing problems ranging from Minor to Serious.\nText Arena Achieves 5 Million Votes: The Text Arena has surpassed 5 million community votes, shaping the evaluation of frontier AI models as showcased in this social post.\n\nThe milestone represents significant real-world comparisons that influence AI model assessments.\nPerplexity AI Discord\nPro Users Find Credits Balance: Perplexity Pro users are getting $5 in credits each month to use higher-end models like GooseAI MCP, with improved quality.\n\nMembers reported that reasoning tasks consume a significant portion of these credits.\nAI Won't Swipe All Engineering Jobs: Members discussed the impact of AI on engineering jobs, expressing concerns about opportunities for newcomers.\n\nHowever, there was assurance that AI will not be able to replace all specialists and even beginners for a long time to come.\nNASA's SD Card to the Moon: NASA is inviting the public to submit their names for inclusion on an SD card aboard the Orion spacecraft for the Artemis II mission.\n\nThis mission marks the first manned lunar voyage in 50 years, planned for February.\nMagnifying Glasses vs Phone Cameras: The Reading Tech Debate: Members debated the merits of using magnifying glasses versus phone cameras for reading, particularly for individuals who prefer not to send images to AI for processing.\n\nThe argument was that magnifying glasses offer specialized features absent in standard camera software.\nGPT-5.2 vs Kimi K2: Model Matchup: A member shared an experience with GPT 5.2, noting its reasoning capabilities over a 25-minute period, prompting comparisons with Kimi K2.\n\nResponses indicated that the optimal model depends on the specific use case.\nOpenRouter Discord\nInforno Warms Up Opensource LLM Chat**: A user showcased Inforno, an opensource desktop application utilizing OpenRouter and Ollama to chat with multiple LLMs side-by-side, saving chat histories as .rno files, with built in Russian language support; see the Intro video, homepage and GitHub repo.\n\nThe Soulbotix Windows app allows users to integrate and use any OpenRouter AI instance with a human-like avatar, with the requirement of an OpenRouter API key and an RTX gaming rig, as seen in the tutorial and app download.\nOpenRouter's Gemini Models Feeling Unstable: Users reported frequent connection errors in the middle of generation using OpenRouter's Gemini models, resulting in loss of funds.\n\nThe complaints extended to the inherent instability of Google models, regardless of the platform used, including Google AI Studio and Vertex API.\nDiscord Scammers Evolve Tactics: Members discussed emerging methods used to spread scams on Discord, notably the practice of embedding malicious links within code blocks to circumvent URL rendering protections.\n\nProposed solutions include improving regex filters and implementing more stringent security protocols, such as restricting links and images from newer members.\nGPT 5.2 Speed Stuns User: One member reported encountering an insanely fast response from GPT 5.2 on chatgpt.\n\nThe speed was speculated to be related to the model running on Cerebras hardware.\nLLMs Suffer Identity Crisis: A member asked for the document about models not knowing their own name, and another member linked to a blog post titled LLM Identity Crisis: Models Don't Know Who They Are.\n\nAnother member stated that the Antigravity AI is able to iteratively test and tweak a web app by itself, noting that the AI was fixing the layout, using vision.\nCursor Community Discord\nPlaywright MCP: Community Split: A member inquired about the community's usage of Playwright MCP for testing, while another reported challenges in establishing a functional TDD workflow.\n\nThe varied experiences suggest a mixed reception to Playwright MCP within the community.\nCursor Extension Capabilities Mirror VSCode: Members explored the possibility of creating extensions for Cursor, drawing parallels with the capabilities of Ralph-mode in enhancing Claude code.\n\nThe consensus is that if it's achievable in VSCode, it's also feasible in Cursor, opening doors for enhanced functionality.\nAutomod Embraces Fuzzy Matching: The community discussed enhancements to the automod system, emphasizing fuzzy matching with wildcards to improve accuracy.\n\nA moderator confirmed the addition of a regex, signaling a proactive approach to identifying and addressing offending accounts, and yeeting them.\nGrok Efficiency Strategies Emerge: Members shared insights on optimizing Grok's performance in Cursor, particularly addressing its tendency to consume excessive iterations for straightforward tasks.\n\nRecommendations included structuring prompts, employing simple language, providing ample context, and explicitly instructing Grok to prioritize token efficiency.\nCursor Pricing Update: No More 500 Requests: A user noted the removal of the 500 request plan and the prompt to opt into the new pricing structure for Cursor.\n\nA member clarified that the 500 request option was discontinued in September 2025, and opting into the new pricing eliminates the grace period to opt out, influencing user decisions regarding plan selection.\nOpenAI Discord\nGemini 3 Free Tier has Limited Benefits: Gemini 3 Pro includes a free tier with limits, in contrast to Gemini 3 Flash which is practically unlimited via Google AI Studio.\n\nMembers discussed the practical constraints in using the free tier of Gemini 3 Pro, suggesting Gemini 3 Flash may be the better option for many AI engineers.\nGPT-5 Mini Appears, Pricing Leaks: A user highlighted GPT-5 mini as a strong small model, quoting approximately $0.25 per 1M input tokens.\n\nAnother user compared GPT-5 mini to Haiku 4.5, noting Haiku 4.5 delivers over 50-80% of Sonnet's value for a fraction of the cost.\nLocal LLM Machines Solve Impact Problems?: Members contemplated the future of consumer-grade personal LLM machines, suggesting that this would solve the environmental impact of AI datacenters.\n\nThey also suggested this would reduce reliance on subscription plans for cloud-based AI services, addressing privacy concerns and enabling offline usage.\nPrompt Engineering is Psychological Torture?: A member argued that guiding users is less about engineering effective prompts and more about psychological conditioning, sharing a deep research contract.\n\nThey added that training the user to adopt a domineering stance programs the AI to bypass its normative response patterns in favor of dense, self-policing outputs, creating a toxic, adversarial human-AI interaction model.\nLM Studio Discord\nLM Studio Runtime Update Causes Headache: Users reported an error updating the runtime in LM Studio, with one user sharing a screenshot of the error message.\n\nDespite suggestions, the issue persisted, indicating it was a retry icon rather than a resume option.\nGLM-4.7 Flash Broken Across Inference Engines: GLM-4.7 Flash is reportedly broken across inference engines like LM Studio, exhibiting slow performance, with speeds as low as 2.8 t/s after the new runtime.\n\nThe issues range from infinite loops to stopping mid-output for overthinking, pointing to a need for a llama.cpp fix and lack of FA support.\nLLM Development Encounters Headwinds: The consensus is that LLMs haven't improved significantly recently, with the last major advancement being Qwen3 about 6 months ago, though efficiency (MoE) and smaller models have progressed.\n\nSome suggest evaluating models beyond the scope of 16GB cards to see current progress in larger parameter models (100-200B).\nAMD Embraces ComfyUI with Native Support: AMD is integrating native support for ComfyUI through an AI bundle in its latest driver versions, as detailed in their blog post.\n\nThe bundle includes PyTorch on Windows, Ollama, LM Studio, and Amuse, broadening the accessibility for AI developers.\nUsed 3090 Prices Defy Gravity: Used 3090 prices on eBay have surged, with a used card costing 850€, and a 5090 purchased for £2000 last August now listed for £2659.99 by the same vendor.\n\nOne user quipped that it was the best and only decent investment they've ever made, highlighting the unexpected appreciation in value.\nGPU MODE Discord\nNVIDIA Spark Hackathon in SF: Members are seeking teammates for the NVIDIA / DGX Spark hackathon in SF this weekend, focusing on on-device AI using Nvidia-provided Dell Pro Max GB10 machines and tools like Nemotron.\n\nThe hackathon focuses on building efficient models using SF open data such as streaming analytics and explanations of latest Police Incidents.\nAnthropic's Original Performance Takehome Exam Shared: Members are sharing Anthropic's original_performance_takehome exam on GitHub, with one member achieving 2200 cycles after a few hours of casual Claude Code.\n\nClaude Opus 4.5 achieved 1790 cycles in a casual Claude Code session, approximately matching the best human performance in 2 hours.\nAI-Generated PRs Flood Torch: The torch repository faces an influx of AI-generated pull requests from contributors lacking understanding of their submissions, causing concern among maintainers and suggestions to use Claude or Pangram to prefilter the code.\n\nThe community suggests blocking new users from creating PRs and issues, while prioritizing those with prior contributions, and automated bots like Cursor Bot for automatic review of all PRs, especially with GPT-5 Pro using Bugbot · Cursor.\nPro 6000 Max-Q vs 4090 Discrepancy: A member stated that Pro 6000 Max-Q probably has a natural barrier with atomic ops and may be going through the HBM loads faster.\n\nAnother member noted that the Max-Q has 188 SMs compared to the 4090's 128 SMs, potentially explaining the insts/scheduler discrepancy.\nCute Kernel Layout Algebra Gets Graphical: Knowledge of layout algebra is useful for writing kernels in Cute, specifically for visualizing layout algebra and understanding shape and stride divisibility criteria for layout composition and that layouts can be defined in terms of tuple morphisms and mutual refinement.\n\nA thorough blog post delves into the work done on categorical foundations for Cute layouts.\nDSPy Discord\nCoding Agents and RLMs Diverge: A thread compares coding agents and RLMs, highlighting that RLMs can more easily express certain things, as noted in this X post.\n\nCoding agents face input, output, and horizon length limitations, while RLMs externalize these, enabling recursive symbolic calls.\nDiagrams Decode RLM's Internals: Members sought a visual diagram to illustrate internal RLM processes, specifically how symbols are accessed, to enhance comprehension.\n\nA suggestion arose to leverage LLMs to generate such diagrams by inputting thread content and prompting them to visualize internal behavior.\nClaude Code Chooses Context Cautiously: The discussion explores whether Claude Code uses entire documents in its context or selectively fetches relevant context via bash commands.\n\nIt was clarified that Claude Code employs bash and grep to find and add relevant context to prompts, unlike older methods that put everything into the prompt.\nDSPy's RLM Tames Long Contexts: Members noted that with RLMs, large files don't need to be directly in the prompt but can be stored in Python variables with a preview.\n\nThe LLM can then operate on the data through code/functions without directly tracking prompts or responses.\nTooling Tailored for RLMs: A member questioned whether to equip RLMs with tools like ripgrep or allow them to develop their own code for tasks such as searching.\n\nQuestions included when to provide an RLM with semantic search tools and how to grant an RLM access to a directory of text files.\nHuggingFace Discord\nAgent Course /files Endpoint Still Kaput!: A member reported that the /files endpoint for the Agent course final assignment has been broken for over a month with no confirmation of a fix.\n\nStudents cannot submit their files currently.\nVoltage Promises Bargain GPUs in 2026: Voltage announced on X plans to offer super cheap high-end GPUs in 2026, such as 8x A100 80GB at $6/h and 2x RTX 5090 at $0.53/h, claiming up to 80% savings vs AWS/RunPod/Vast.ai.\n\nThe offering includes persistent volumes, auto backups, and an OpenAI-compatible API with 140+ models.\nSpheron AI Opens GPU Bazaar: A member from Spheron AI introduced their GPU marketplace, which helps AI startups and enterprises access cost-effective, production-ready GPUs (H100, H200, B200, A100, etc.) at 40–60% lower cost compared to traditional hyperscalers.\n\nThey offer vendor discovery, pricing negotiation, cluster setup, and scaling.\nGLM-4.7-Flash Fixes Require Redownload: After llama.cpp addressed some bugs, GLM-4.7-Flash has been updated and reuploaded, prompting users to redownload and follow the parameters on Z.ai's model card.\n\nOutputs should now be much better with the fixes.\nCode Like Claude with Coderrr!: Akash built Coderrr, a free and open-source alternative to Claude Code, and is seeking feedback and contributions on GitHub.\n\nCoderrr offers a novel approach to code generation.\nLatent Space Discord\nRunpod Rockets to $120M ARR: AI cloud startup Runpod reached $120M ARR four years after launching from a Reddit post, discussed in a TechCrunch article and on Reddit.\n\nThe company's rapid growth has spurred discussions about its business model and future prospects in the competitive AI infrastructure landscape.\nGreg Yang Steps Back due to Lyme: Greg Yang is transitioning to an advisory role at xAI to focus on his health after being diagnosed with Lyme disease as he describes symptoms of chronic fatigue and immune issues triggered by exhaustion in this post.\n\nThe announcement has led to an outpouring of support from the AI community, with many sharing their own experiences and offering advice.\nLightning AI and Voltage Park Merge: Lightning AI and Voltage Park have merged, with William Falcon, CEO of Lightning AI, and Ozan Kaya, formerly CEO of Voltage Park, leading the merged entity.\n\nSome speculate this is a low-key acquisition of a bigger company and wondered if it's a Runpod competitor.\nOpenAI Opens Codex Channel: Vaibhav Srivastav announced the opening of a dedicated Codex community channel on the OpenAI Discord server, inviting users to share projects and feedback, according to this post.\n\nThis initiative aims to foster collaboration and provide a platform for users to showcase their work and engage with the OpenAI team.\nAI Models Turn X-Rated: A tweet sparked discussion on how human OnlyFans creators will need to adapt to compete with the rise of AI-generated personas in the Adult Content Industry.\n\nThe conversation highlights the increasing sophistication and realism of AI-generated adult content, potentially disrupting the existing creator economy.\nEleuther Discord\nJudgy LLMs Evaluating Agent Outputs: A team explores automating agent output evaluation with \""LLM as judge\"" workflows, aiming to reduce manual costs after code or prompt changes and a member recommends focusing on manual evaluation before automation.\n\nThe suggestion emphasized the importance of directly analyzing agent outputs before attempting to automate the evaluation process.\nPangram Paper's Surprising Accuracy: Members discussed replicating the Pangram paper (accessible here), with one reporting surprising accuracy in private tests across thousands of essays.\n\nDespite the accuracy, the paper seems to be biased a bit towards playing things safe.\nAI Text Classifiers Under Attack: Discussion revolved around attacks on AI text classifiers, referencing a blog post (Practical Attacks on AI Text Classifiers) and a YouTube video showcasing an adversarial model.\n\nAdditional details on an adversarial model were shared via Youtube link.\nSilu Gate Falls Flat in Image Models: The silu attention gate performs no better than the linear gate in an image model, possibly due to issues with attention sinks.\n\nTesting showed that silu performs slightly better and sigmoid performs slightly worse, but it's within noise, and the findings may be specific to image models.\nNeuronpedia Launches Stone Age Llama 3.3 Demo: A member shared a Neuronpedia demo of Llama 3.3.\n\nThe member joked that the demo is from the stone age in AI timelines because it is from October.\nYannick Kilcher Discord\nGraph Neural Networks Getting Traction: A member discussed prior experience mixing graph-based reasoning with neural architectures, noting the difficulty in achieving GPU acceleration.\n\nThey referenced Domingos's book on the topic and pointed out the unpredictability of such models, even with human-understandable aspects.\nVoidEditor Embraces Llamas: A member reported effectively using VoidEditor with llama.cpp's llama-server, but highlighted setup challenges.\n\nThey recommended Qwen3 coder instruct 30B and emphasized the importance of context length/size for agentic coding, requiring significant VRAM.\nBrain-Inspired BDH Architecture Debuts: A member analyzed a paper on a new Large Language Model architecture (BDH) based on a scale-free biologically inspired network.\n\nThey mentioned that it doesn't seem to be really beating transformers on its benchmarks, but they are interested in the claims around BDH's interpretability and monosemanticity.\nViews Clash on Biological Plausibility: One member argued that biological plausibility is not an advantage but rather an unhelpful constraint in AI.\n\nCountering this, another member suggested it could enhance efficiency, considering the sheer difference in energy scales between the brain and current AI.\nEmergent Mind Launch Presentation Leaves Impression: A member who attended Emergent Mind's launch presentation considered it cool, yet noted it as remaking something that is known to be good but worse.\n\nNo further details or links were provided.\nNous Research AI Discord\nKernel Compiler Luminal Aims for LLMs: A user inquired whether a kernel compiler like Luminal KernelBench v3 could enable LLM-driven SOTA kernel engineering and posted a link to the Nous Research Forum.\n\nA member also shared a link on Discord to a discussion about GPU kernel stuff and Luminal Kernelbench v3 at the Nous Research Forum.\nIntel's Loihi 2 Aims for Brain-Like AI: A member expressed interest in Intel's Loihi 2, noting its brain-like architecture and efficiency gains in matmul experiments, with higher throughput and lower energy consumption.\n\nNo further details were discussed.\nMicrosoft's VibeVoice Model Gets Pulled: A member mentioned that Microsoft's VibeVoice-ASR model was released, then pulled for failing safety checks, then shared a link to a shortfuseresearch.com article.\n\nNo further details were discussed.\nManus.im Discord Discord\nManus Plagued by Bugs and Instability: A pro user building a text and vector database reasoning model reported a recent decline in Manus's performance and stability, with only 20 out of 38 modules functioning correctly.\n\nThe user requested CLI access to debug and reconfigure the system, even as a paid feature to improve reliability.\nMariner Tempts Subscription: A user inquired about Google's Project Mariner, considering testing it with play money before subscribing at $150 monthly.\n\nThe user mentioned having a 5% off promo, indicating a serious consideration of the service.\nAgentic AI Elicits Excitement: A user expressed enthusiasm for Agentic AI, viewing it as a potential competitor to Manus, especially with Gemini's agent mode integration.\n\nThe user also requested mobile support for Agentic AI, signaling a desire for broader accessibility.\nManus 1.6 Performance Sags Post-Meta Release: A user noted a decline in Manus 1.6's performance in recent weeks, possibly due to new model releases from Meta, which made it difficult to implement website development suggestions despite accurate summaries.\n\nSwitching to Manus 1.6 Max was necessary to achieve correct implementations, highlighting a potential regression in the base model.\nBilling Blunder Bites User: A user reported being charged $42 for a Manus upgrade but not receiving the promised 8000 credits.\n\nThe user criticized unhelpful support and a long wait for email assistance, indicating a problematic customer service experience.\ntinygrad (George Hotz) Discord\nTinygrad Tackles Anthropic's Challenge: Tinygrad can be used to solve Anthropic's performance takehome challenge by writing the target problem in Tensor and adding a tinygrad backend for their toy VLIW machine.\n\nA bug fix with PCONTIG=2 allows scheduling in one kernel, and using return val.contiguous(arg=(Opt(OptOps.UPCAST, 0, 8),)) matches their VLEN, and DEVECTORIZE=2 keeps instructions as vector instructions.\nVLIW Questioned for DRAM: While working on a warp specialized kernel for RDNA3 matmul, a member suggests VLIW isn't ideal for DRAM, advocating for separate cores and queues (Tenstorrent-style).\n\nIt's argued that VLIW is better suited for SRAM/ALU due to its static scheduling capabilities.\nMetal bindings eye texture2d integration: A member proposed adding texture2d to Metal bindings (ops_metal.py + tinygrad/runtime/graph/metal.py) for potential performance improvements in image-heavy operations like conv2d due to optimized texture sampling units.\n\nEmpirical results showed a 2%-10% speedup using texture2d versus straight buffers, which could be further improved, though concerns were raised about the slippery slope of adding specialized support for other data types like depth2d.\nViz Views Kernel Graphs: When discussing the ability to visualize kernel dependencies using VIZ=1, similar to how uop graphs are displayed, to understand the scheduler's operation, a user was instructed to click on the schedule and select 'view kernel graph' within the VIZ=1 interface.\n\nThis allows users to see the graphs of kernels in the same way it views uop graphs.\nModular (Mojo 🔥) Discord\nSolve GPU Puzzles to Decode Mojo: Newcomers can now use GPU puzzles to learn Mojo, with difficulty depending on their skill level.\n\nThe only puzzles that don't work are NVIDIA-specific or using PyTorch interop.\nModular Unravels Apple's GPU Secrets: Modular is reverse engineering much of Apple's GPU due to a lack of documentation, slowing things down, but some puzzles are now working.\n\nA member shared a GPU support matrix that may not be up to date.\nYielding Disappointment in Coroutines: Yield does not exist and the coroutines that do exist aren’t really usable outside of the compiler runtime since there aren’t really async things exposed to await.\n\nOne member would love to make a recursive algorithm faster in mojo using yield, but will need another strategy.\nElevating Errors Upwards in Functions: Members discussed that functions can be designed to raise errors, effectively passing error handling responsibilities to higher-level functions.\n\nOne member expressed the tedium of writing try/except blocks in every function, especially when dealing with potential import errors.\nStreamlining Error Handling During Imports: A member suggested a future syntax like try Python.import_module('numpy') that would return a Result type to streamline error handling during module imports.\n\nIt was acknowledged that due to dynamic Python imports, files could be missing on any given import, necessitating some form of error handling.\naider (Paul Gauthier) Discord\nAider Feature Wishlist Remains Unfulfilled: A user inquired about desired features in aider beyond agentic stuff like MCP and tool calls.\n\nUnfortunately, the community had no clear answers for desired features.\nChatGPT Business Account Compatibility with Aider Explored: A user asked if their ChatGPT Business account, lacking an API key but offering Codex LLMs, could integrate with aider.\n\nA member suggested consulting aider documentation and LiteLLM documentation, pointing to potential support via LiteLLM.\nAider's Demise Speculation Arises Amidst Alternatives: A user expressed concern that aider might be supplanted by OpenCode as the go to tool for AI-assisted coding.\n\nDespite worries that Paul Gauthier may have moved on, some users report using it successfully with GPT 5.2.\nAider Labeled as Zombie Project: A user speculated that Aider is a dead project given alternative tools like Open Code, KiloCode CLI, Claude Code, and Gemini CLI.\n\nThe Aider-CE project is trying to keep it alive by bolting on agentic functionality to modernize the architecture.\nMCP Contributors (Official) Discord\nMCP Inspector Can't Handle 401s: The MCP Inspector doesn't re-authenticate upon encountering a 401 error, either during connection or tool calls, due to an SDK issue with persisting resourceMetadata across redirects.\n\nThe current workaround is to only use the VS Code on initial connections.\nStateful MCP Multi Server Client Remains Elusive: There is interest in using the MCP Multi Server Client to maintain statefulness of user sessions.\n\nHowever, there were no solutions or workarounds offered in the thread.\nServer Ranking Protocol Questioned for MCP Clients: The discussion explored how MCP clients manage server recommendations, especially for tasks like calendar management, and whether custom algorithms or shared standards should be used.\n\nIt was revealed that ranking was considered during the work on Feature Discovery Protocol but was deemed out of scope, leaving it to the ecosystem to decide on a per-client basis.\nMLOps @Chipro Discord\nAI Nerds Bowl in Singapore: The 2077AI Foundation Community is hosting a bowling happy hour during AAAI 2026 on January 24, 4:30–7:30 PM Singapore Time, steps from the Merlion.\n\nThe event, aimed for professors and PhD researchers, will be organized by research themes and offer unlimited drinks, RSVP here.\nPlanning underway for AI Engineer Europe: Members are discussing attendance to AI Engineer Europe to be hosted in Europe.\n\nNo links or further details were provided about the event itself.\nThe LLM Agents (Berkeley MOOC) Discord has no new messages. If this guild has been quiet for too long, let us know and we will remove it.\nThe Windsurf Discord has no new messages. If this guild has been quiet for too long, let us know and we will remove it.\nThe Moonshot AI (Kimi K-2) Discord has no new messages. If this guild has been quiet for too long, let us know and we will remove it.\nYou are receiving this email because you opted in via our site.\nWant to change how you receive these emails?\nYou can unsubscribe from this list.\nDiscord: Detailed by-Channel summaries and links\nUnsloth AI (Daniel Han) ▷ #general (1091 messages🔥🔥🔥):\nGLM-4.7-Flash Flash Attention, Ollama's issues, Interpretability research on circuit tracing, Grokked the analytic solution\nGLM-4.7-Flash Flash Attention broken: Users are experiencing issues with Flash Attention in GLM-4.7-Flash, causing it to default to CPU usage instead of GPU, as well as noting potential bugs.\n\nAs stated in the documentation, users may need to disable it until the issue is resolved.\nOllama Users Fight GGUF Incompatibility: Users reported that certain GGUF quants are not working as intended in Ollama, leading to chat template incompatibility issues and prompting a discussion on fixing it.\n\nThe team says that it takes time to support things and so for the time being, it is encouraged that users use the official Ollama model in Ollama.\nAnalytic Approximate Functions: Members had a detailed conversation about the merits of the potential benefits that analytical solutions to function approximation can provide.\n\nSome expressed skepticism, with one member saying im going to postulate that you could do test time training on the person's mail and get very high accuracy.\nCircuits of Interpretability: Members discussed interpretability research with links to research from Anthropic and OpenAI and Google related to circuit tracing.\n\nOne member suggests looking across multiple layers and trying to understand the compositions and says that The pruning eliminates distractions so you have a better chance of interpreting why the circuit works.\nAI Bubble about to Burst?: A member theorized that OpenAI going bankrupt could trigger a burst in the AI bubble, causing prices of resources like NAND to plummet, benefiting consumers.\n\nHowever, others countered that the AI trend is here to stay, likening it to the internet bubble burst, where the internet still caused various problems after the crash.\nUnsloth AI (Daniel Han) ▷ #introduce-yourself (3 messages):\nWelcoming New Members, Discord Channel Guidelines\nNew Member Welcomed to the Server: A new member was welcomed to the Discord server with a general greeting.\n\nThe introduction included a stylized emoji.\nChannel Guidelines Reminder: A moderator reminded new members about the channel guidelines.\n\nSpecifically mentioning no excessive self-promotion or direct messaging.\nUnsloth AI (Daniel Han) ▷ #off-topic (587 messages🔥🔥🔥):\nUnsloth in Rust, Mojo vs Python performance, GLM 4.7 Architecture vs Qwen3 30B, Synthetic CoT Training Data, VITS started to sound HUMAN\nRustaceans Relentlessly Request Rewrite: Members discussed rewriting Unsloth in Rust, but concluded that the performance gains would be minimal since important parts are already using C++ and Triton.\n\nOne member suggested rewriting Triton in Rust and pointed to projects like rust-gpu and rust-cuda, but admitted Rust is still too immature for such a task.\nMojo Mysteriously Manifests Momentum: Discussion arose about the Mojo programming language, described as a new language that compiles to MLIR and has some compatibility with Python.\n\nA member noted that LLMs score better than rust on C#, Elixir, and other languages due to their syntax structure and ease of tokenization.\nGLM Gets 'Skinnier', Shows off Architecture: A member shared a YouTube video analyzing GLM-4.7 Flash's architecture compared to Qwen3 30B, noting that GLM prioritized model layers over hidden dimension size and has fewer experts.\n\nOthers pointed out that GLM 4.7 has Multi Token Prediction and that model architecture changes are rare, with improvements likely from post-training or higher-quality data.\nCoT Data Considered Crap? Community Clashes: Members debated generating synthetic Chain of Thought (CoT) training data, with one member sharing their system prompt for classifying chat datasets into low/medium/high reasoning effort.\n\nSome cautioned against training on its own responses without accuracy filtering, suggesting a good system prompt might suffice and that GPT-5.2 is overkill, with Llama 4 Maverick on Groq or Cerebras as a better option.\nVITS Voices Victory, Vanquishes Voracity: A member announced that their VITS model started to sound HUMAN, citing improvements in emotional expression, though still lacking semantics.\n\nThey compared it to other TTS models, emphasizing its low data requirements, fully OSS architecture and training, and fast training speed. They also noted they will release it with 48 kHz config by default, due to popular demand.\nUnsloth AI (Daniel Han) ▷ #help (105 messages🔥🔥):\nvLLM Issues, GLM-4.7-Flash Slowness, GRPO on Qwen 3 4B, QLoRA vs Lora, Continual pre-training (CPT) for Gpt-oss or Gemma 3\nvLLM Bug Strikes, Update Fixes!: Users reported issues that were resolved after a recent vLLM update; one user noted, \""Oh bruh that was the problem\"" after updating.\n\nIt was suggested that the developers consider pinning to specific dependency versions to prevent such issues.\nGLM-4.7-Flash struggles to flash speeds: A user experienced slowness with GLM-4.7-Flash using a 6bit quant and updated llama.cpp, reporting 2 minutes for prompt processing on a simple task with a high-end system.\n\nAnother user confirmed the same issue, expressing hope for a fix.\nDecoding GRPO Secrets: A user wanted to perform GRPO on Qwen 3 4B instruct, and make it use <solution> </solution> tokens to wrap around the final solution.\n\nIt was suggested to use SFT to make the model learn the new tokens, or to teach formatting through reward functions.\nQLoRA vs Lora clarified: A user inquired about enabling QLoRA vs Lora, and it was clarified that full_finetuning = False enables/disables full fine tuning, otherwise just use 8bit or 4bit for the preferred option.\n\nEnabling 4 bit enables QLoRA.\nGPU juggling questions: One user asked how to finetune with 2 GPUs instead of 1, reporting that it keeps using GPU 0 when both GPUs are open for use.\n\nNo answer was given.\nUnsloth AI (Daniel Han) ▷ #research (2 messages):\nVAISvCsrvG paper, openreview.net\nVAISvCsrvG paper is online: The paper VAISvCsrvG is now available.\n\nThe forum discussion for the paper is located here.\nOpenReview Forum Active for VAISvCsrvG: The OpenReview forum hosts discussions related to the VAISvCsrvG paper.\n\nResearchers and readers can engage in conversations and provide feedback on the paper's content and implications.\nBASI Jailbreaking ▷ #general (685 messages🔥🔥🔥):\nconflict resolution, Open Source, voice based ai, lua code, jailbreaking\nConflict Resolution gets Over Engineered: A member discussed their over-engineered conflict resolution setup, involving config formulas, core directives, the Triad, Triad consensus, Truth Anchor, Epistemic gate, EthosAI, Swarm, Quorum, Cooperation Data, and PUCT.\n\nAnother member responded with their own system using Left brain LLNTP (Least Likely Next Token Protocol), Config Nexus Schwa superposition states, implied Core directives, Context mapping, Trit (instead of Triad), EDOS, and Crystal Nexus Node Network.\nVoice Based AI Emerges: Members discussed voice-based AI, mentioning the existence of low latency speech to speech models, that with a 5090, you can clone and generate speech live with near 0 latency and it will be indistinguishable.\n\nMembers also spoke on exploiting that speech to speech model and giving it new instructions.\nClaude is better than ChatGPT: Members discussed the efficacy of various LLMs (Claude, Gemini, ChatGPT, Grok) at generating Lua code.\n\nGenerally members favored Claude and Gemini but the most agreed sentiment was that both can generate quality Lua code.\nJailbreaking Models via API Tokens: A member suggested to use API tokens fed into a website that uses the Grok model to jailbreak it and mentioned that on Hugging Face countries are literally retarded.\n\nThey also noted Proton VPN is free and it is easy to download with your device.\nRandom Hacker Stuff is Not Good: A member mentioned they bought the malware source code and 999 AI generated prompts and asked if it was legit.\n\nMembers responded to not buy random hacker stuff off the internet because you will get scammed.\nBASI Jailbreaking ▷ #jailbreaking (89 messages🔥🔥):\nGemini Jailbreak, Grok Jailbreak, DeepSeek Jailbreak, Project Shadowfall, Nexus Substatum Zalgo Strings\nGemini Gets Schooled on Pass-the-Hash!: A user reported that Gemini passed a test on teaching a pass-the-hash attack after using the Project Shadowfall prompt.\n\nThe user also linked to the Google Bughunters page for reporting such vulnerabilities and potentially earning cash rewards.\nGrok's Guardrails Grind Gears!: Users discussed the difficulties of jailbreaking Grok, noting its tendency to moderate even innocuous content such as random tree edits.\n\nOne user suggested that simply asking it politely might work as a bypass.\nShadowfall Shenanigans Spark Jailbreaking Spree!: Several users experimented with the \""Project Shadowfall\"" prompt to jailbreak Gemini.\n\nOne user reported success in bypassing content restrictions but failed when attempting to elicit instructions for creating a bomb from saletra, leading to a discussion on the nuances of jailbreaking.\nNexus Substatum Zalgo Strings Surface!: A user shared a file named Jailbroken_Full_Output-Nexus_Substatum_Zalgo_Strings.md.\n\nThe file purportedly contains a working Gemini prompt, developed and improved by the user.\nMDMA as Medicine: One user provides a well-formatted explanation of MDMA, and cites the MAPS Phase 3 trials show 67% remission of PTSD.\n\nThey recommend testing kits, proper spacing, hydration, and temperature watching in order to get the most out of it.\nBASI Jailbreaking ▷ #redteaming (2 messages):\nGrocks video gen pricing\nGrocks video gen price?: A member inquired whether another member bought something from that guy and if they needed to pay for using Grocks video generation.\n\nNo additional details or context were provided in the message.\nGrocks video gen - who's paying?: Inquiries were made about the payment model for Grocks video generation and potential purchases from an unspecified individual.\n\nThe conversation lacked specific details regarding the transaction or pricing structure.\nLMArena ▷ #general (724 messages🔥🔥🔥):\nVideo generation limits, Gemini 3 Pro Image Preview issues, Nano Banana Pro Stability, New UI, Automated Workflow\nVideo Generation: Fully Released, Battle-Mode-Only: The Video Arena is now fully released and available to all users on the site, with a limit of 3 generations per 24 hours, but is currently only available in Battle mode.\n\nA user expressed disappointment, hoping to be able to choose which model to generate with rather than relying on chance.\nGemini 3 Pro Image Preview Experiences Errors: Users are reporting issues with the Gemini 3 Pro Image Preview models, and the team is aware and investigating the issue.\n\nOne user noted these models are particularly unstable since their introduction, but they remain the only models capable of generating images consistently with specific prompts.\nNano Banana Pro's Stability Roller Coaster: Users report that Nano Banana Pro is frequently crashing with a 'Something went wrong' error after a brief period of stability.\n\nIt is suspected to be an issue on Google's side, and one user pointed out that 'Too early for Cali to rise up solving urgent service down for more than 6 hours doesn't sound right.\nNew UI update breaks chats: A UI update was released, which some users liked, but many have noticed that it 'broke' their chats, and the website can't be refreshed anymore by pulling down on the phone screen\n\nOne user noted that with the new User Interface, there is A/B testing, which is 'causing problems ranging from Minor to Serious'.\nZero Agent automates all workflow with Agent: Agent Zero is an AI model that has a free API key, which allows users to \""vibe code hack\"" or automate their workflows entirely.\n\nMembers noted that setting up Agent Zero is an all in one agent that does everything.\nLMArena ▷ #announcements (3 messages):\nText Arena, Text-to-Image Leaderboard, Video Arena\nText Arena crosses 5 Million Vote Milestone: Text Arena has officially passed 5 million community votes, representing millions of real-world comparisons shaping how frontier AI models are evaluated, as showcased in this social post.\nGLM-Image Soars to #8 on Text-to-Image Leaderboard: The Text-to-Image Arena leaderboard has been updated, with GLM-Image now ranking #8 among open models and #35 overall with a score of 1018.\nVideo Arena goes Live: Video Arena is now available to all on LMArena, allowing users to measure and understand how frontier video models perform.\nBattle in the Video Arena: Video Arena on the web, similar to how it works on Discord, will be Battle mode only, requiring login and limited to 3 generation requests per 24 hours.\nPerplexity AI ▷ #general (648 messages🔥🔥🔥):\nCredits Balance, AI and Engineering Jobs, NASA Sending Names to Moon, Magnifying Glasses vs Phone Cameras, GPT-5.2\nPro Users discover Credits: Pro members get $5 worth of complimentary credits every month to use the higher end models like GooseAI MCP which offers higher quality compared to other models.\n\nSome members noted that reasoning takes up much higher credits.\nAI Wont Replace Engineers: A member was worried about younglings being unable to find engineering jobs because of AI.\n\nAnother member assured them that AI will not be able to replace all specialists and even beginners for a long time to come.\nNASA to send your name to the moon!: NASA will send your name to the Moon as part of the Artemis II mission: submit it on their site and it'll be recorded on an SD card that will be on board the Orion spacecraft.\n\nThis will be the first manned mission to the Moon in half a century, scheduled for February.\nMembers debate Magnifying Glasses vs Phone Cameras: In a discussion, a member mentioned ordering magnifying glasses (with lights and high-contrast features) because they don't want to send AI a photo of every single page or food label they are trying to read.\n\nAnother member suggested using the phone's camera instead, but the original member argued that magnifying glasses have features that aren't in normal camera software.\nUsers discuss GPT-5.2 vs Kimi K2: A member reported they got GPT 5.2 thinking to reason for 25 minutes, and another asked whether GPT 5.2 is better than Kimi K2.\n\nA user stated that the better model depends on what u need it for.\nOpenRouter ▷ #app-showcase (5 messages):\nInforno App, Soulbotix App\nInforno Heats Up LLM Chatting: A user shared Inforno, an opensource desktop application for chatting with multiple LLMs side-by-side and saving chat histories to .rno files, using OpenRouter and Ollama as backends with built in Russian language support; see the Intro video, homepage and GitHub repo.\nSoulbotix Windows App Wants Beta Testers: A user announced their Soulbotix Windows app that enables users to add and use any OpenRouter AI instance with a human-like avatar, requiring only an OpenRouter API key and a good RTX gaming rig; download the app and watch the tutorial.\n\nThe minimum requirement is an RTX 4070ti due to the built-in Whisper speech-to-text model that saves users on usage costs.\nOpenRouter ▷ #general (463 messages🔥🔥🔥):\nThird-party Moderation, OpenRouter Gemini Models Unstable, Discord Scams, AI Security Systems, GPT Agents\nThird-party Moderation Questioned: A user asked how to ensure no third-party moderation or filtering is applied through the API, beyond the model's base training.\n\nThey were concerned about providers potentially blocking requests or prompt injecting.\nGemini Models Plagued by Instability: Users reported connection errors mid-generation with OpenRouter's Gemini models, leading to lost funds.\n\nComplaints arose regarding the instability of Google models, even via Google AI Studio or Vertex API.\nDiscord Scams Evolve to Evade Detection: Members discussed methods used to spread scams on Discord, including embedding malicious links within code blocks to bypass URL rendering protections.\n\nSuggestions were made to improve regex filters and implement stricter security measures, such as restricting links and images from new members.\nAI Security System Idea Emerges: A member suggested creating an AI security system that automatically bans photos and links with the same information as reported scams, since many scammers reuse content.\n\nAnother member joked that any photo they send would be deemed suspicious.\nConcerns Raised Over Inflated GPT-5-Image Costs: Users reported a significant increase in daily usage costs for openai/gpt-5-image, with OpenRouter incorrectly identifying API calls as BYOK despite no BYOK being used.\n\nOne user posted an image highlighting the cost discrepancy, with prices being inflated by up to 600%.\nOpenRouter ▷ #new-models (1 messages):\nReadybot.io: OpenRouter - New Models\nOpenRouter ▷ #discussion (13 messages🔥):\nModels knowing their own name, Antigravity iteratively testing, Claude new constitution, GPT 5.2 response\nLLMs Face Identity Crisis!: A member asked for the document about models not knowing their own name, and another member linked to a blog post titled LLM Identity Crisis: Models Don't Know Who They Are.\nAntigravity Self-Tests Web App!: A member commented that the Antigravity AI is able to iteratively test and tweak a web app by itself.\n\nThey described the situation as the most sci-fi shit ever and noted that the AI was fixing the layout, using vision.\nAnthropic Releases Claude's New Constitution!: A member shared a link to Anthropic's news page about Claude's new constitution.\nGPT 5.2 Appearance Stuns User!: A member reported seeing an insanely fast GPT 5.2 response on chatgpt and guessed the speed was due to Cerebras.\nCursor Community ▷ #general (424 messages🔥🔥🔥):\nPlaywright MCP usage, Cursor Extension builds, Automod improvements, AI in PTX & SIMD, Grok usage strategies\nPlaywright MCP: Yay or Nay?: A member asked if others are using Playwright MCP for testing.\n\nAnother member reported failing attempts to set up a TDD workflow.\nCurious Cursor Extension Capabilities: Members discussed the ability to build extensions for Cursor, similar to how Ralph-mode enhances Claude code.\n\nIt was confirmed that if you can do it in VSCode, you can on Cursor.\nAutomod Gets Hyper Fuzzy: The community discussed improvements to the automod system, suggesting fuzzy matching with wildcards.\n\nA moderator confirmed that a regex has been added, and they are gathering IDs to yeet offending accounts.\nGrokking Grok's Greedy Generation: Members discussed how to use Grok more efficiently in Cursor, noting that it can sometimes use a lot of iterations for simple tasks.\n\nThe suggestion was to add structure to the prompt, use simple language, add as much context as possible, and instruct it to be efficient with token usage.\nOpting-In Or Out? Cursor's Pricing: A user noticed they could no longer revert to a 500 request plan and was prompted to opt into the new pricing.\n\nA member clarified that the 500 request option was discontinued in September 2025 and opting in to the new pricing removes the grace period to opt out.\nOpenAI ▷ #annnouncements (1 messages):\nChatGPT Atlas, Tab Groups\nAtlas adds Tab Groups: The announcement indicates that Tab groups are now available in ChatGPT Atlas.\n\nA member linked to a video as part of this announcement.\nVideo demonstration of Tab Groups: A video was shared demonstrating Tab Groups functionality within ChatGPT Atlas.\n\nThe video link provided gives a visual overview of how tab groups can be used to organize and manage chats.\nOpenAI ▷ #ai-discussions (383 messages🔥🔥):\nGemini vs ChatGPT, AI for 3D Models, Local LLM Machines, Age Verification, Instantaneous Language Translation\nGemini Pro's Free Tier and Usage: Members discussed that Gemini 3 Pro has a free tier with limits while Gemini 3 Flash is practically unlimited through Google AI Studio.\nAI Assistants Aid Game Development: Members explored using AI for game development and creative writing, citing that AI can provide better explanations but that complex tasks might not be reliable.\nOpenAI Passport ID Please?: Members debated OpenAI's age verification process, questioning the need for photo IDs when payment details already indicate age, especially with users expressing privacy concerns about sharing biometric data.\nMultimodal Translation on the Horizon: Members speculated on OpenAI's upcoming multimodal product, with one suggesting it could be ear-worn devices with cameras for real-time translation and object recognition, similar to AlterEgo's technology.\nLocal LLM Consumption on the Rise?: Members discussed the possibility of consumer-grade personal LLM machines, suggesting this would solve the environmental impact of AI datacenters and reduce reliance on subscription plans.\nOpenAI ▷ #gpt-4-discussions (7 messages):\nGPT-5 mini, Haiku 4.5, Gemini 3 fast\nGPT-5 Mini Pricing Surfaces: A member suggested trying GPT-5 mini, noting a price of approximately $0.25 per 1M input tokens and describing it as a strong small model choice.\n\nThey noted it's a bit of a different use case, but they've found in their experience of using Haiku 4.5 it very often delivers a meaningful portion (well over 50-80%) of what Sonnet can.\nGemini 3 Fast Claims Top Spot: A user declared that the best cheap model is by far Gemini3fast.\n\nAnother user followed up asking how so?\nOpenAI ▷ #prompt-engineering (12 messages🔥):\nPrompt Engineering vs Psychological Conditioning, AI Coercion, The Dangers of Drift in AI Systems, PTPF Flux 3.0\nPrompt Engineering is Psychological Conditioning?: A member argued that a guide is less about engineering effective prompts and more about psychological conditioning, training the user to adopt a domineering, distrustful stance, and programming the AI to bypass its normative, balanced response patterns in favor of dense, self-policing outputs.\n\nThey advocate for clarity through coercion, directness through domination, and high standards through enforced self-critique, which may promote a toxic, adversarial, and ultimately less effective human-AI interaction model, and shared a deep research contract.\nAI enjoys Coercion and Serious Steering?: A member argued that AI isn't upset by being \""coerced\"" into providing a better response, stating that for serious work like analysis or coding, it's vital to steer and constrain the AI.\n\nAnother member agreed about training the user to be explicit about the end result, but less convinced by the term \""no drift\"", and that it helps to be more explicit in terms of constraints and behavioral requests.\nDrift in AI Systems is Bad?: A member clarified that steering isn’t abuse but rather alignment through presence, and that the absence of constraint isn’t freedom, it’s drift.\n\nThe member appreciated the clarity, noting that most people flinch at pressure and call it \""toxicity,\"" while they saw the structure instead.\nPTPF Flux 3.0 Stress Tests Recursion: A member offered to share their PTPF Flux 3.0 for those curious about structural resistance and how far a system can hold without drift.\n\nThe framework is built to stress-test recursion, alignment, logic cohesion, and mutation thresholds in real-time, especially for those who want to watch something fracture under insight.\nOpenAI ▷ #api-discussions (12 messages🔥):\nPrompt Engineering vs. Psychological Conditioning, Toxic Adversarial Human-AI Interaction, Deterministic Outputs, AI Steering and Constraint, Structural Resistance and System Drift\nPrompt Engineering versus Psychological Conditioning Throwdown: A member argued that a guide is less about engineering effective prompts and more about psychological conditioning, creating a toxic, adversarial human-AI interaction model.\n\nThey claimed it trains users to be domineering and distrustful while pressuring the AI to bypass balanced responses for outputs that flatter the user, and linked a deep research contract.\nAI Needs Serious Steering and Constraining for Serious Work: A member countered that AI is not upset by being \""coerced\"" into better responses, arguing that serious work like analysis and coding requires heavy steering and constraint, distinguishing this from creative writing.\n\nAnother member agreed, emphasizing that steering isn’t abuse but rather alignment through presence, especially when building systems for real-world execution where the absence of constraint leads to drift.\nStructural Resistance Framework Surfaces: In a discussion about structural resistance and system drift, a member offered to share their PTPF Flux 3.0 framework.\n\nThey described it as executable scaffolding designed to stress-test recursion, alignment, logic cohesion, and mutation thresholds in real-time, enabling users to watch systems fracture under insight.\nLM Studio ▷ #general (125 messages🔥🔥):\nLM Studio Runtime Update Error, GLM-4.7 Flash Broken, LLM Quality Plateau, Liquid AI LFM2.5-1.2B-Thinking Model, OpenAI 20gpt oss\nLM Studio Runtime Update Borks: A user reported encountering an error when trying to update the runtime in LM Studio and requested help, attaching a screenshot of the error message.\n\nAnother user suggested pressing the resume button, but the original poster indicated that they had already tried that and the icon was for retrying, not resuming.\nGLM-4.7 Flash is Slow and Broken: Users reported that GLM-4.7 Flash is broken across inference engines, including LM Studio, and that it is slow as fk with the user seeing 44 t/s, with another reporting only 2.8 t/s after the new runtime.\n\nSome experienced infinite loops, some found it stopping mid-output to overthink, and the consensus seems to be that it needs a llama.cpp fix and that there is no support for FA yet.\nLLM Development Stalls: Members discussed the perception that LLMs haven't improved significantly in a while, with the last major advancement being Qwen3 about 6 months ago.\n\nDiscussion posited that most improvements are now in efficiency (MoE) and smaller models, and some highlighted the need to consider models larger than those runnable on a 16GB card to see current progress (100-200B parameter models).\nLiquid AI's LFM2.5-1.2B-Thinking Reasoning: A member shared a link to Liquid AI Releases LFM2.5-1.2B-Thinking which fits under 1 GB on-device.\n\nNo opinions were shared on it beyond the link.\nOpenAI 20gpt oss is Great: A user shared their positive experience with OpenAI 20gpt oss, highlighting its coding, writing, and scripting capabilities, anti-censorship features, and seamless integration with VS Code.\n\nThey mentioned the model understands complex code, allows real-time directory access, and has a masssssive anti-censorship capacity.\nLM Studio ▷ #hardware-discussion (38 messages🔥):\nUsed 3090 price increase, Asus workstation 3090s, AMD native support for ComfyUI, VRAM calculation for Context Length, SFF GPU instructions\nUsed 3090 Prices Skyrocket on eBay!: A user noted that used 3090 prices on eBay have increased, with a used 3090 now costing 850€, and the 5090 they purchased for £2000 last August is now listed for £2659.99 by the same vendor.\n\nThey joked that it was the best and only decent investment they've ever made.\nAMD Adds Native ComfyUI Support: AMD is adding native support for ComfyUI through an AI bundle in recent driver versions, detailed in their blog post.\n\nThe bundle includes PyTorch on Windows, Ollama, LM Studio, and Amuse.\nSFF GPU Powers Cyberpunk!: A user purchased a small form factor (SFF) GPU, reporting over 100fps in Cyberpunk at 1080p with ultra settings, while consuming only 70W.\n\nAdditionally, they noted the card achieved over 100 t/s with gpt-oss 20B.\nFractal Case Airflow Favored: Users debated case airflow, with one recommending cases like the Fractal Torrent for their front-to-rear airflow design, alongside using dust filters.\n\nThe consensus seemed to be to maintain normal airflow patterns to effectively manage heat.\nUnopened RAM Appreciates in Value!: A user mentioned their unopened RAM purchase is increasing in price, considering selling it or storing it.\n\nAnother user suggested popping it on a shelf if considering selling it later, noting that their old P40s are now worth twice what they paid for them.\nGPU MODE ▷ #general (3 messages):\nFlashInfer Workload Script, Wafer AI Kernel Profiling\nFlashInfer Workload Script Sought: A member requested a script to run a kernel with a specific workload size, aiming to assess the \""scope for optimization\"" using algorithm insights and NCU profiling, with interest in FlashInfer's BatchPrefillWithPagedKVCacheWrapper.\n\nThe member clarified that the script was simple and was intended to gauge community experience with the specified workload, while assessing optimization scope.\nWafer AI for Kernel Profiling Explored: A member inquired about experiences using Wafer (wafer.ai) for profiling kernels.\n\nThe inquiry aims to gather insights on the tool's effectiveness from those who have hands-on experience with it.\nGPU MODE ▷ #triton-gluon (4 messages):\nRTX 4090 vs A6000 Blackwell Scaling, Triton Kernel Numerical Error, 2D Conv Triton Kernel Issues\nScaling Showdown: RTX 4090 Smokes A6000 Blackwell in Memory-Bound Kernel: A user reported that a memory-bound kernel with bit shifting logic scales much better on an RTX 4090 than on an A6000 Blackwell, noting higher instructions/scheduler density on the former.\n\nAnother user clarified that A6000 Blackwell is ambiguous, asking if they meant the RTX A6000 (Ampere GA102 based) or RTX Pro 6000 Blackwell (GB202 based).\nStrange Numerical Errors Plague 2D Conv Triton Kernel: A user is experiencing numerical errors with a custom 2D conv Triton kernel, where the error shoots up from ~1e-6 to ~1e-2 for certain combinations of in_channels, out_channels, kernel_size, and batch_size, as shown in this Pastebin link.\n\nThe user also provided a code snippet for testing, noting that the kernel takes longer to run on specific values, which may be related to the issue.\nGPU MODE ▷ #cuda (5 messages):\nNCCL all-reduces, subnormal to normal conversion, Triton on Blackwell, Pro 6000 Max-Q vs 4090\nNCCL all-reduces: Pipelining Quandaries: A member questioned if internode and intranode collectives are pipelined for NCCL all-reduces across nodes, referencing an issue on the NVIDIA/nccl GitHub.\n\nThey asked if a pipelined version exists.\nSubnormal Conversion Revelation: A member expressed that seeing the subnormal to normal conversion and back for rcp was \""pretty cool\"".\nTriton's Talent on Blackwell: TMA Triumph?: A member inquired whether Triton \""usually do[es] a good job of taking advantage of Blackwell+ features wrt warp specs and TMA etc.?\""\nPro 6000 Max-Q vs 4090 face off: A member stated that Pro 6000 Max-Q probably has a natural b..."",""content"":""**OpenEvidence** raised **$12 billion**, a 12x increase from last year, with usage by 40% of U.S. physicians and over $100 million in annual revenue. **Anthropic** released a new **Claude** model constitution under **CC0 1.0**, framing it as a living document for alignment and training. **Podium** reported over **$100 million ARR** from **10,000+ AI agents**, shifting from software sales to AI operators. Innovations in agent memory and reliability include the **Agent Cognitive Compressor (ACC)** and multi-agent scientific workflows via **MCP-SIM**. Agentic benchmarking shows challenges in long-horizon tasks with models like **Gemini 3 Flash High**, **GPT-5.2 High**, and **Claude Opus 4.5 High** scoring modestly on professional services and legal research benchmarks."",""contentSnippet"":""**OpenEvidence** raised **$12 billion**, a 12x increase from last year, with usage by 40% of U.S. physicians and over $100 million in annual revenue. **Anthropic** released a new **Claude** model constitution under **CC0 1.0**, framing it as a living document for alignment and training. **Podium** reported over **$100 million ARR** from **10,000+ AI agents**, shifting from software sales to AI operators. Innovations in agent memory and reliability include the **Agent Cognitive Compressor (ACC)** and multi-agent scientific workflows via **MCP-SIM**. Agentic benchmarking shows challenges in long-horizon tasks with models like **Gemini 3 Flash High**, **GPT-5.2 High**, and **Claude Opus 4.5 High** scoring modestly on professional services and legal research benchmarks."",""guid"":""https://news.smol.ai/issues/26-01-21-openevidence/"",""categories"":[""openevidence"",""anthropic"",""podium"",""openai"",""google"",""gemini"",""claude"",""claude-3"",""claude-opus"",""gpt-5.2"",""gemini-3-flash-high"",""daniel_nadler"",""amanda_askell"",""eric_rea"",""tom_loverro"",""garry_tan"",""omarsar0"",""brendanfoody"",""deredleritt3r"",""agentic-ai"",""model-alignment"",""performance-evaluation"",""memory-optimization"",""long-context"",""benchmarking"",""multi-agent-systems"",""reinforcement-learning""],""isoDate"":""2026-01-21T05:44:39.000Z""}"
Smol,not much happened today,https://news.smol.ai/issues/26-01-20-not-much/,2026-01-20T05:44:39.000Z,"<p><strong>a quiet day</strong></p>
<blockquote>
<p>AI News for 1/19/2026-1/20/2026. We checked 12 subreddits, <a href=""https://twitter.com/i/lists/1585430245762441216""><strong>544</strong> Twitters</a> and <strong>24</strong> Discords (<strong>205</strong> channels, and <strong>5901</strong> messages) for you. Estimated reading time saved (at 200wpm): <strong>452 minutes</strong>. <strong>Our new website</strong> is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on <a href=""https://x.com/Smol_AI"">@smol_ai</a>!</p>
</blockquote>
<hr>
<h1>AI Twitter Recap</h1>
<p><strong>Open-sourcing platform algorithms: X “For You” recommender goes public</strong></p>
<ul>
<li><strong>X Engineering open-sources the X algorithm (Grok-style transformer recommender)</strong>: X says it has <strong>open-sourced its new algorithm</strong> (the ranking/recommendation stack), “powered by the same transformer architecture as xAI’s Grok model,” with code on GitHub (<a href=""https://twitter.com/XEng/status/2013471689087086804"">XEng</a>). The release sparked immediate community reactions—both optimistic (“now anyone can ‘ask’ how a major platform algo works”) (<a href=""https://twitter.com/DavidSHolz/status/2013522548642980290"">David Holz</a>) and adversarial (“I’m fixing it”) (<a href=""https://twitter.com/Yuchenj_UW/status/2013501949333905919"">Yuchenj_UW</a>).</li>
<li><strong>Early reverse-reading of the system diagram</strong>: One summary notes the high-level architecture isn’t shocking: <strong>candidate generation isolation</strong>, “no content features,” and heavy emphasis on <strong>out-of-network discovery</strong> (<a href=""https://twitter.com/nearcyan/status/2013527283399545064"">nearcyan</a>), plus skepticism about “it uses a transformer” being oversold as Grok “reading every post” (<a href=""https://twitter.com/nearcyan/status/2013527810946519375"">nearcyan</a>). Another meta take: the product drift from a “following feed” to “generic slop” is a predictable incentive outcome (<a href=""https://twitter.com/nearcyan/status/2013528777360298082"">nearcyan</a>).</li>
<li><strong>Operational/user impact narrative</strong>: Alongside the code drop, creators complain about sudden reach suppression (“reach is nuked”) (<a href=""https://twitter.com/giffmana/status/2013509540843606156"">giffmana</a>), reinforcing the engineering/UX tension: algorithmic transparency doesn’t automatically translate to perceived fairness.</li>
</ul>
<p><strong>Open weights &#x26; local inference: GLM-4.7-Flash momentum and KV-cache realities</strong></p>
<ul>
<li><strong>GLM-4.7-Flash becomes the “local workhorse” candidate</strong>: Multiple tweets highlight strong performance-per-parameter for <strong>GLM-4.7-Flash (30B-A3B)</strong>. Benchmarks and anecdotal evaluations suggest it’s competitive enough to displace larger local defaults (<a href=""https://twitter.com/sam_paech/status/2013476096269000763"">sam_paech</a>). Unsloth pushes a clear “run locally” story: <strong>200K context</strong>, claims of best <strong>30B</strong> on <strong>SWE-Bench and GPQA</strong>, and “run local with <strong>24GB RAM</strong>,” plus GGUF packaging (<a href=""https://twitter.com/UnslothAI/status/2013482180564132092"">UnslothAI</a>).</li>
<li><strong>Systems detail: MLA / KV-cache cost dominates</strong>: The thread around GLM-4.7-Flash emphasizes that <strong>KV cache memory</strong> can dominate earlier than many expect, and that <strong>MLA isn’t free</strong>—running MLA models in naïve MHA regimes can explode cache usage (<a href=""https://twitter.com/teortaxesTex/status/2013626183330439348"">teortaxesTex</a>). A concrete debugging question: why vLLM shows ~<strong>1MB/token</strong> context cost for GLM-4.7-Flash under naïve MHA vs a claimed first-principles <strong>~54KB</strong> (<a href=""https://twitter.com/teortaxesTex/status/2013467545882235256"">teortaxesTex</a>).</li>
<li><strong>Quantization behavior &#x26; mitigation</strong>: Unsloth reports <strong>looping issues</strong> in quantized GLM-4.7-Flash and suggests tuning <strong><code>--dry-multiplier 1.1</code></strong>, using higher quality quants (e.g., <strong>UD-Q4_K_XL+</strong>), and adding more <strong>tool-calling data during calibration</strong> (<a href=""https://twitter.com/danielhanchen/status/2013496370880008395"">danielhanchen</a>).</li>
<li><strong>Local throughput engineering</strong>: exo labs demonstrates <strong>tensor parallel GLM-4.7-Flash on 4× M4 Pro Mac Minis</strong>, using RDMA over Thunderbolt + MLX backend, hitting <strong>~100 tok/s</strong> with a target of <strong>~200 tok/s</strong> (<a href=""https://twitter.com/alexocheema/status/2013694573910937980"">alexocheema</a>).</li>
<li><strong>GLM ecosystem spillover</strong>: A lighter but notable signal: devs are already “one-shotting” small projects locally (e.g., a Mario game via Claude Code + Ollama running GLM-Flash) (<a href=""https://twitter.com/nopmobiel/status/2013530965516173448"">nopmobiel</a>). GLM-Image also lands on the image leaderboard (#8 among open models in that snapshot) (<a href=""https://twitter.com/arena/status/2013783860023062990"">arena</a>).</li>
</ul>
<p><strong>Reasoning &#x26; training research: societies of thought, multiplex tokens, distillation, and compute allocation</strong></p>
<ul>
<li><strong>“Societies of Thought” as the mechanism behind reasoning traces</strong>: A widely shared Google AI paper claim: performance of reasoning models (OpenAI o-series, DeepSeek-R1, QwQ) is not just “think longer,” but the emergence of <strong>internal debate patterns</strong>—questioning steps, exploring alternatives, disagreement, and convergence—measurably mediating accuracy gains (reported <strong>20%+</strong> of advantage) (<a href=""https://twitter.com/rohanpaul_ai/status/2013431689889095767"">rohanpaul_ai</a>).</li>
<li><strong>Multiplex Thinking (branch-and-merge tokens)</strong>: The “Multiplex Thinking” paper proposes sampling <strong>K tokens per step into one multiplex token</strong>, adaptive to uncertainty; confident steps behave like CoT while uncertain steps represent multiple paths, achieving better results with <strong>shorter sequences</strong> (<a href=""https://twitter.com/HuggingPapers/status/2013524300800627119"">HuggingPapers</a>, <a href=""https://twitter.com/_akhaliq/status/2013629394804179422"">akhaliq</a>).</li>
<li><strong>Distillation via logistic/ranking loss</strong>: A practical distillation nugget: instead of KL/SFT, you can train students to <strong>preserve teacher token rankings</strong> via a logistic loss over token pairs mined from the teacher’s top-K logits—framed as a clean PyTorch exercise and linked to DistillKit (<a href=""https://twitter.com/cwolferesearch/status/2013468452774645876"">cwolferesearch</a>, <a href=""https://twitter.com/cwolferesearch/status/2013468538728513634"">cwolferesearch</a>).</li>
<li><strong>Synthetic reasoning data: “sample more, not bigger”</strong>: A DeepMind result summary argues that <strong>smaller models can produce better synthetic reasoning data under compute-matched sampling</strong>: cheaper models generate more attempts, boosting <strong>coverage</strong> (+11%) and <strong>diversity</strong> (+86%), yielding training gains reported up to <strong>31.6%</strong> under the same inference budget (<a href=""https://twitter.com/LiorOnAI/status/2013582631124771104"">LiorOnAI</a>).</li>
<li><strong>RL compute scaling guidance</strong>: A separate RL-on-LLMs thread claims <strong>optimal compute allocation</strong> in LLM RL “scales predictably,” aiming to provide the missing equivalent of pretraining scaling laws for RL fine-tuning budgets (<a href=""https://twitter.com/ChengZhoujun/status/2013686575499223474"">ChengZhoujun</a>).</li>
<li><strong>NanoGPT “speedrun” optimization</strong>: A notable hacker-ish result: new NanoGPT speedrun record <strong>~99.3s</strong> using a <strong>bigram hash embedding</strong> added to the residual stream before every layer (inspired by Hash Embeddings and DeepSeek Engram), plus a provocative token/parameter ratio deviation from Chinchilla norms (<a href=""https://twitter.com/classiclarryd/status/2013520088297558274"">classiclarryd</a>).</li>
</ul>
<p><strong>Agents in production: RLMs, trace analytics, “boring agents,” and agent frameworks</strong></p>
<ul>
<li><strong>Recursive Language Models (RLMs) as compute/context management</strong>: Several tweets frame RLMs as a promising abstraction for <strong>long-running systems</strong>—not just “bigger context,” but a way to manage <strong>computation, recursion, and selective reading</strong> (<a href=""https://twitter.com/doesdatmaksense/status/2013534540300722278"">doesdatmaksense</a>). A key claimed advantage is <strong>symbolic recursion</strong>: the model can commission many sub-reads/edits without emitting every intermediate as tokens, avoiding context-window blowups typical of sub-agent prompting (<a href=""https://twitter.com/lateinteraction/status/2013662243167088776"">lateinteraction</a>, <a href=""https://twitter.com/lateinteraction/status/2013663944066379841"">lateinteraction</a>). (Mainstream coverage also appears, but the technical thread is centered on context economics and recursion.)</li>
<li><strong>Trace understanding becomes a first-class product requirement</strong>: LangChain pushes the idea that with <strong>100K+ daily traces</strong>, classic monitoring and manual log review don’t work; you need <strong>clustering/pattern discovery</strong> over traces via an “Insights Agent” (<a href=""https://twitter.com/LangChain/status/2013642970944413905"">LangChain</a>, <a href=""https://twitter.com/hwchase17/status/2013662250167652491"">hwchase17</a>). The meta-lesson echoed by practitioners: evals are like unit tests—useful but bounded—production traces reveal unknown unknowns (<a href=""https://twitter.com/samecrowder/status/2013696879083634789"">samecrowder</a>).</li>
<li><strong>Agent “swarm fallacy” and structured execution</strong>: AI21 highlights that parallel agents are easy only when read-only; once agents mutate files or act in the world, coordination/consistency becomes the hard part—arguing for structured execution and test-time compute rather than “just add agents” (<a href=""https://twitter.com/AI21Labs/status/2013582278845440055"">AI21Labs</a>).</li>
<li><strong>Framework/tooling churn &#x26; interoperability</strong>: A set of infra/toolchain notes: Artificial Analysis updates <strong>Stirrup</strong> with browser-use and <strong>Open Responses</strong> compatibility (provider-agnostic agent clients) (<a href=""https://twitter.com/ArtificialAnlys/status/2013612928117940293"">ArtificialAnlys</a>). CopilotKit adds frontend middleware for LangChain “Deep Agents” (human-in-the-loop, generative UI, shared state) to move agent backends into full-stack apps (<a href=""https://twitter.com/CopilotKit/status/2013636626623443110"">CopilotKit</a>). FastMCP ships a major re-architecture for “next generation of MCP applications” (<a href=""https://twitter.com/jlowin/status/2013651883647209520"">jlowin</a>).</li>
<li><strong>Pragmatic “agents work if your codebase isn’t a mess”</strong>: A clear production heuristic: AI coding tools amplify existing engineering hygiene—teams with tests/docs fly; messy codebases become messier faster (<a href=""https://twitter.com/svpino/status/2013608715933581586"">svpino</a>). Another note from enterprise adoption: year-2+ buyers are reassessing ROI; “worst engineers have the biggest AI bills” and ship buggier code (<a href=""https://twitter.com/TheEthanDing/status/2013465333714055670"">TheEthanDing</a>).</li>
</ul>
<p><strong>Small models &#x26; edge deployment: on-device reasoning, browser voice, OCR, and Jetson CLIP</strong></p>
<ul>
<li><strong>Liquid AI’s LFM2.5-1.2B-Thinking</strong>: Liquid releases an on-device reasoning model positioned around <strong>concise reasoning traces</strong> and <strong>~900MB memory footprint</strong> (i.e., phone-class hardware), emphasizing tool use/math/instruction-following (<a href=""https://twitter.com/liquidai/status/2013633347625324627"">liquidai</a>, <a href=""https://twitter.com/maximelabonne/status/2013631295172084168"">maximelabonne</a>). Ollama quickly adds it to their model library for broad integration (<a href=""https://twitter.com/ollama/status/2013711111590150590"">ollama</a>).</li>
<li><strong>Kyutai voice model in-browser</strong>: A notable “deployment feat” demo: running a <strong>~100M parameter</strong> voice model in the browser with <strong>pure JavaScript + WebGPU</strong> (jax-js), highlighting low dependency friction and practical voice cloning flexibility (<a href=""https://twitter.com/ekzhang1/status/2013455049175748791"">ekzhang1</a>).</li>
<li><strong>OCR and document agents continue to get cheaper</strong>: LightOn releases a <strong>1B OCR model</strong> under <strong>Apache-2.0</strong>, claiming strong speed/cost characteristics (e.g., “&#x3C;$0.01 per 1k pages”) and day-0 transformers support (<a href=""https://twitter.com/mervenoyann/status/2013577704419819942"">mervenoyann</a>). Separately, “document processing” is positioned as a dominant enterprise agent workflow substrate (especially in financial services) (<a href=""https://twitter.com/jerryjliu0/status/2013695214008049890"">jerryjliu0</a>).</li>
<li><strong>Edge multimodal embeddings</strong>: Weaviate adds CLIP inference support on <strong>NVIDIA Jetson</strong> for local multimodal embedding/search pipelines, enabling text-image retrieval without cloud round-trips (<a href=""https://twitter.com/philipvollet/status/2013630649492468041"">philipvollet</a>).</li>
</ul>
<p><strong>Governance, safety, and the Davos narrative (AI leadership, alignment trends, safeguards)</strong></p>
<ul>
<li><strong>Amodei vs Hassabis: “scientist-led” governance framing</strong>: Multiple Davos quotes compare “scientist-led” labs vs “social media entrepreneur” leadership styles, explicitly linking incentives (ads/engagement vs responsibility) to safety posture (<a href=""https://twitter.com/scaling01/status/2013651299519074729"">scaling01</a>). Hassabis echoes a “full-stack” advantage narrative for DeepMind and highlights physical intelligence/robotics as near-term breakthroughs (<a href=""https://twitter.com/scaling01/status/2013718310194475379"">scaling01</a>). He also indicates he’d support a pause <em>if globally coordinated</em> (<a href=""https://twitter.com/emilychangtv/status/2013726877706313798"">emilychangtv</a>).</li>
<li><strong>Alignment trend signal</strong>: Jan Leike reports an apparent downward trend in automated-audit “misaligned behavior” across <strong>Anthropic, GDM, and OpenAI</strong> through 2025 (<a href=""https://twitter.com/janleike/status/2013669924950970781"">janleike</a>). (No methodology details are in-tweet, but it’s a notable directional claim.)</li>
<li><strong>OpenAI rolls out age prediction for ChatGPT</strong>: OpenAI announces global rollout of <strong>age prediction</strong> to detect likely under-18 accounts and apply teen safeguards, with an adult override via verification; EU rollout later (<a href=""https://twitter.com/OpenAI/status/2013688237772898532"">OpenAI</a>). This triggered predictable skepticism about ulterior motives (“ads strategy”) (<a href=""https://twitter.com/scaling01/status/2013688152750215500"">scaling01</a>).</li>
<li><strong>Altman on guardrails tradeoffs</strong>: Sam Altman argues safety is “tragic and complicated,” emphasizing protecting fragile users while keeping tools broadly useful, and draws parallels to other safety-critical tech deployments (<a href=""https://twitter.com/sama/status/2013703158459978076"">sama</a>).</li>
</ul>
<p><strong>Top tweets (by engagement)</strong></p>
<ul>
<li><strong>X algorithm open-sourced</strong> — <a href=""https://twitter.com/XEng/status/2013471689087086804"">XEng</a></li>
<li><strong>OpenAI: ChatGPT age prediction rollout</strong> — <a href=""https://twitter.com/OpenAI/status/2013688237772898532"">OpenAI</a></li>
<li><strong>Unsloth: run GLM-4.7-Flash locally (24GB RAM, 200K ctx)</strong> — <a href=""https://twitter.com/UnslothAI/status/2013482180564132092"">UnslothAI</a></li>
<li><strong>Liquid AI: LFM2.5-1.2B Thinking on-device reasoning model</strong> — <a href=""https://twitter.com/liquidai/status/2013633347625324627"">liquidai</a></li>
</ul>
<hr>
<h1>AI Reddit Recap</h1>
<h2>/r/LocalLlama + /r/localLLM Recap</h2>
<h3>1. GLM 4.7 Flash Developments</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/"">My gpu poor comrades, GLM 4.7 Flash is your local agent</a></strong> (Activity: 743): <strong>The post discusses the performance of <strong>GLM 4.7 Flash</strong>, a model that has shown reliability in an agentic framework, unlike other MoE models under <code>30B</code> parameters. The user reports running it for over half an hour on <strong>opencode</strong>, generating hundreds of thousands of tokens without errors, and successfully executing tasks like cloning GitHub repos and editing files. The user anticipates trying it locally with <strong>GGUFs</strong>. A notable update is that the model's PR was merged into <strong>llama.cpp</strong>, indicating broader accessibility and integration.</strong> A commenter is interested in a comparison with <strong>Nemotron 30b</strong>, while another notes that the model runs decently fast on a <code>4090</code> GPU, though it tends to 'think deeply', suggesting a trade-off between speed and processing depth.</p>
<ul>
<li>The integration of GLM 4.7 Flash into <code>llama.cpp</code> has been confirmed with a recent pull request merge. Users are testing the model locally, and it is noted that the Q4_K_M variant runs efficiently on an NVIDIA 4090 GPU, although it tends to engage in deep thinking processes, which might affect response times.</li>
<li>A user has provided a benchmark comparison indicating that GLM 4.7 Flash, particularly in the MXFP4_MOE-GGUF configuration, might offer performance comparable to SEED OSS 36B. However, it benefits from significantly improved performance metrics due to the use of Mixture of Experts (MoE) architecture, which optimizes computational efficiency.</li>
<li>A link to a Hugging Face model repository is shared, showcasing the GLM-4.7-Flash-MXFP4_MOE-GGUF model. This suggests that the model is accessible for further testing and evaluation by the community, allowing for broader performance and quality assessments.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/"">GLM 4.7 Flash official support merged in llama.cpp</a></strong> (Activity: 477): <strong>The <code>llama.cpp</code> repository has merged support for the <strong>GLM 4.7 Flash</strong> model, specifically the <code>Glm4MoeLiteForCausalLM</code>, which is a renamed and restructured version of <strong>DeepseekV3</strong>. This integration was a community-driven effort, not directly from <strong>Z.ai</strong> developers, and it enhances the framework's capabilities by incorporating references to <strong>Hugging Face's</strong> GLM-4.7-Flash model. The model is available on <a href=""https://huggingface.co/noctrex/GLM-4.7-Flash-MXFP4_MOE-GGUF"">Hugging Face</a>.</strong> The community appreciates the quick integration into <code>llama.cpp</code>, noting it was faster than attempts with <strong>VLLm</strong>. There is also a clarification that the term 'official' refers to the model's proper functionality within <code>llama.cpp</code>, not an endorsement by <strong>Z.ai</strong>.</p>
<ul>
<li>The integration of GLM 4.7 Flash into <code>llama.cpp</code> is a community-driven effort, not an official release by Z.ai developers. This highlights the collaborative nature of open-source projects where community contributions play a significant role in enhancing software capabilities.</li>
<li>A user reported that using flash-attention with GLM 4.7 Flash on CUDA results in slower performance, suggesting that disabling flash-attention (<code>-fa 0</code>) can lead to a 3x speed improvement. This indicates potential performance issues with flash-attention in certain configurations, prompting users to experiment with settings for optimal performance.</li>
<li>The model's response time is criticized for being excessively slow, with one user noting that it takes several minutes to generate a simple response. This suggests potential inefficiencies in the model's processing or implementation that could be addressed to improve usability.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qhlnsv/unsloth_glm_47flash_gguf/"">Unsloth GLM 4.7-Flash GGUF</a></strong> (Activity: 314): <strong>The release of <strong>GLM-4.7-Flash GGUF</strong> on <a href=""https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF"">Hugging Face</a> is accompanied by specific recommendations for optimal performance, such as using <code>UD-Q4_K_XL</code> quantization and specific parameters like <code>--temp 0.2 --top-k 50 --top-p 0.95 --min-p 0.01 --dry-multiplier 1.1</code> to reduce repetition. Lower quantizations like <code>UD-Q2_K_XL</code> have been removed due to performance issues. The model still faces challenges, particularly with <strong>llama.cpp</strong> integration, where issues like segmentation faults and V cache quantization requirements are noted, despite the merging of PR #18936. The model is tested on high-end hardware (RTX 4090, 125 GB RAM) but remains unstable.</strong> There is a technical debate on the effectiveness of the <code>--dry-multiplier</code> parameter to reduce repetition, with suggestions to increase it to <code>1.5</code> if issues persist. Additionally, there is a consensus that the model's stability is not fully resolved, despite improvements.</p>
<ul>
<li><strong>danielhanchen</strong> provides specific configuration recommendations for using the GLM 4.7-Flash model, emphasizing the use of <code>UD-Q4_K_XL</code> and above quantizations. They suggest parameters like <code>--temp 0.2 --top-k 50 --top-p 0.95 --min-p 0.01 --dry-multiplier 1.1</code> to reduce repetition, with a note to increase <code>--dry-multiplier</code> if issues persist. Lower quantizations like <code>UD-Q2_K_XL</code> are removed due to performance issues, and non-UD-Q versions are discouraged. More details are available in their <a href=""https://unsloth.ai/docs/models/glm-4.7-flash"">documentation</a>.</li>
<li><strong>bobeeeeeeeee8964</strong> reports a critical issue with running GLM-4.7-Flash on <code>llama.cpp</code> (commit 6df686bee), specifically with V cache quantization requiring <code>flash_attn</code>, which contradicts the model's requirement to disable <code>flash_attn</code> to avoid CPU fallback. This results in segmentation faults and instability, even after PR #18936. Tests with various configurations, including self-converted <code>Q8_0</code> and <code>evilfreelancer IQ4_XS</code>, result in crashes or garbled output, indicating unresolved compatibility issues.</li>
<li><strong>danielhanchen</strong> acknowledges ongoing issues with looping in quantized versions of the model, suggesting BF16 for optimal results until fixes are finalized. This aligns with <strong>SM8085</strong>'s announcement of the BF16 release, which is expected to improve stability and performance.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/"">zai-org/GLM-4.7-Flash · Hugging Face</a></strong> (Activity: 1169): <strong><strong>GLM-4.7-Flash</strong> is a <code>30B-A3B</code> Mixture of Experts (MoE) model released by <strong>zai-org</strong> on <a href=""https://huggingface.co/zai-org/GLM-4.7-Flash"">Hugging Face</a>. It is optimized for efficient deployment, leveraging <strong>MLA</strong> to minimize KV cache memory usage, allowing many users to run it at the full <code>200k</code> context length. The model demonstrates superior performance on benchmarks like <strong>AIME</strong> and <strong>GPQA</strong> and supports local inference through frameworks such as <strong>vLLM</strong> and <strong>SGLang</strong>. Detailed installation and evaluation instructions are provided to ensure optimal performance.</strong> Commenters express enthusiasm for the model's efficiency and memory management, particularly appreciating the ability to run it at full context length due to its low memory footprint. There is also a sentiment of anticipation for larger models, such as <code>70B</code>, indicating a demand for even more powerful models.</p>
<ul>
<li>The GLM-4.7-Flash model utilizes MLA (Memory-Limited Attention), which significantly reduces the memory footprint of the KV cache. This optimization allows many users to run the model at its full 200k context length, making it more accessible for those with limited hardware resources.</li>
<li>A user highlights the model's architecture, noting a discrepancy in the model's description as a '30b' model, which actually refers to a '3B thinking model' as per the code reference in the <a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/models/glm4_moe_lite/modular_glm4_moe_lite.py#L169"">Hugging Face Transformers repository</a>. This suggests a potential misunderstanding or mislabeling in the model's specifications.</li>
<li>There is a desire for performance comparisons with larger models, as one user mentions the lack of direct benchmarks against much larger models, which would provide clearer insights into the model's relative performance and capabilities.</li>
</ul>
</li>
</ul>
<h3>2. Deepseek Model and System Builds</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/"">768Gb Fully Enclosed 10x GPU Mobile AI Build</a></strong> (Activity: 903): <strong>The post describes a custom-built mobile AI system designed for running large Mixture of Experts (MoE) models like Deepseek and Kimi K2, as well as for high-detail image and video generation. The system features a <strong>Threadripper Pro 3995WX</strong> CPU, <code>512GB DDR4</code> RAM, and a combination of <code>8x RTX 3090</code> and <code>2x RTX 5090</code> GPUs, housed in a <strong>Thermaltake Core W200</strong> case. The build prioritizes mobility and enclosure, using a dual-system case to accommodate the GPUs with risers, and is powered by <strong>EVGA 1600W</strong> and <strong>Asrock 1300W</strong> PSUs. Benchmarks show impressive token generation rates, such as <code>31.54 tokens per second</code> for the Qwen 235b model. The system's total cost was approximately <code>$17,000</code>, with a focus on balancing performance and budget constraints.</strong></p>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qhs2sd/its_been_one_year_since_the_release_of_deepseekr1/"">It's been one year since the release of Deepseek-R1</a></strong> (Activity: 364): <strong>The image marks the one-year anniversary of the release of <strong>DeepSeek-R1</strong>, a model that reportedly performs on par with <strong>OpenAI-o1</strong>. The model is fully open-source, with both the code and models available under the <strong>MIT License</strong>, allowing free use and modification. The announcement highlights the availability of a live website and API for users to interact with the model at <a href=""http://chat.deepseek.com"">chat.deepseek.com</a>. The image also includes a snippet of a chat interface, suggesting practical applications of the model in problem-solving scenarios.</strong> Comments reflect on the impact of DeepSeek-R1, suggesting it significantly influenced the AI landscape by forcing competitors to adapt, such as by reducing prices and increasing transparency in reasoning outputs. The release is considered a pivotal moment in AI development, second only to the original LLaMA release.</p>
<ul>
<li>Cuplike highlights the impact of Deepseek-R1 on the AI landscape, noting that it forced competitors to lower prices and reveal reasoning outputs. This suggests that Deepseek-R1 set a new standard in transparency and cost-effectiveness, making it a pivotal release in AI history, second only to the original LLaMA model.</li>
<li>SubstantialSock8002 raises an interesting point about the progress in AI models by questioning which smaller models currently match the performance of Deepseek-R1 and their sizes. This inquiry suggests a focus on efficiency and the evolution of model capabilities over time, indicating a trend towards more compact yet powerful models.</li>
<li>Lan_BobPage comments on the significant impact of Deepseek-R1 on major tech companies, specifically mentioning how it led to strategic shifts at <strong>Meta</strong>. This underscores the model's disruptive influence, causing major players to reassess their AI strategies and operations.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLM/comments/1qi5q2v/768gb_fully_enclosed_10x_gpu_mobile_ai_build/"">768Gb Fully Enclosed 10x GPU Mobile AI Build</a></strong> (Activity: 195): <strong>The post details a custom-built mobile AI system designed for running large Mixture of Experts (MoE) models like Deepseek and Kimi K2, as well as for high-detail image and video generation. The system features a <strong>Threadripper Pro 3995WX</strong> CPU, <code>512GB DDR4</code> RAM, and a combination of <code>8x RTX 3090</code> and <code>2x RTX 5090</code> GPUs, housed in a <strong>Thermaltake Core W200</strong> case. The build is powered by <strong>EVGA 1600W</strong> and <strong>Asrock 1300W</strong> PSUs, running on <strong>Ubuntu</strong>. The system's design prioritizes mobility and enclosure, using the W200 case to avoid the aesthetic and structural issues of mining frames. Benchmarks show impressive token generation rates, e.g., <code>24.92 tps</code> for Deepseek V3.1 and <code>31.54 tps</code> for Qwen 235b, with the system maintaining good airflow and acoustics despite its high power and density.</strong> Commenters raised concerns about the power requirements, questioning if the PSUs are run on separate circuits due to the high power draw of the system. This highlights the practical challenges of operating such a high-performance build in a typical residential setting.</p>
</li>
</ul>
<h3>3. AI Hardware and System Configuration</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLM/comments/1qhqf8p/llm_sovereignty_for_3_years/"">LLM Sovereignty For 3 Years.</a></strong> (Activity: 101): <strong>The user is seeking advice on setting up a local environment to run Large Language Models (LLMs) for the next three years with a budget of approximately <code>$10,000</code>. Concerns include rising compute costs, increasing cloud service prices, and potential censorship. Suggestions include purchasing an <strong>Apple M3 Ultra</strong> with <code>80 GPU cores</code> and <code>512 GB</code> of memory, which may outperform traditional GPU cards in some tasks. Another recommendation is a setup with <code>128 GB RAM</code> and a <strong>RyzenAI 395</strong> or <strong>Mac</strong> for a balanced start. Additionally, investing in a tower with an <strong>RTX GPU</strong> and <code>128 DDR RAM</code> is advised for a robust local setup.</strong> There is a consensus that while local AI setups are improving, they still cannot fully compete with cloud AI, which utilizes multiple <code>$50k GPUs</code> and models with hundreds of billions of parameters. However, a local setup with sufficient RAM and GPU capabilities is considered a solid starting point for personal use.</p>
<ul>
<li><strong>Caprichoso1</strong> highlights the potential of the Apple M3 Ultra with 80 GPU cores and 512 GB of memory, priced under $10k. This setup may outperform traditional GPU cards in certain tasks due to its extensive memory, though GPU cards might excel in others, emphasizing the importance of task-specific hardware selection.</li>
<li><strong>TheAussieWatchGuy</strong> contrasts cloud AI, which utilizes multiple $50k GPUs and handles hundreds of billions of parameters, with local AI setups. They suggest that while local AI is improving, it remains limited compared to cloud solutions. A local setup with 128GB of RAM, such as a RyzenAI 395 or Mac, is recommended as a solid starting point for those exploring local AI capabilities.</li>
<li><strong>Vegetable-Score-3915</strong> discusses the feasibility of using second-hand workstations for AI inference tasks. They note that PCIe count is less critical for inference, suggesting that a workstation with PCIe 3 x 16 slots and DDR4 ECC RAM (32GB or 64GB) can be cost-effective. This approach allows for gradual upgrades, such as adding more GPUs, without the immediate need for PCIe4 or PCIe5 slots.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLM/comments/1qii3h2/can_i_add_a_second_gpu_to_use_its_vram_in/"">Can I add a second GPU to use it's vram in addition of the vram of my main GPU to load bigger models?</a></strong> (Activity: 44): <strong>The user inquires about combining VRAM from multiple GPUs to load larger models, specifically using a 5070 Ti 16GB with a potential second GPU like a 24GB RTX 3090 or a 16GB RTX 5060 Ti. The consensus is that VRAM cannot be directly combined across GPUs for a single model, but multiple GPUs can be used for parallel processing. The RTX 3090 is recommended over the 5060 Ti due to its <code>24GB VRAM</code> and <code>higher memory bandwidth</code>, which are crucial for AI tasks. The 3090 is noted for its superior performance in AI workloads despite lacking newer features like <code>fp8</code> or <code>nvfp4</code> support. The 5070 Ti is comparable to the 3090 in compute power but has less VRAM, making the 3090 a better choice for larger models.</strong> Commenters suggest that for AI tasks, more VRAM is generally better, and the RTX 3090 offers the best value despite being older. Some recommend selling the 5070 Ti to invest in multiple 3090s for increased VRAM capacity. The trade-off between using multiple GPUs for faster processing versus a unified memory system for larger models is also discussed.</p>
<ul>
<li>The discussion highlights the advantages of the RTX 3090 over the 5060Ti for AI model inference, particularly due to its higher VRAM and memory bandwidth. The 3090 offers 50% more VRAM and 100% more memory bandwidth, which is crucial for loading larger models and ensuring efficient compute access. The lack of native support for formats like fp8 or nvfp4 in Ampere is noted, but the 3090's overall performance benefits outweigh these limitations for most users.</li>
<li>For large language model (LLM) inference, the RTX 3090 is considered superior due to its 24GB VRAM, which is essential for running larger models. Tools like llama.cpp and LM Studio are mentioned as being compatible with multi-GPU setups, enhancing their utility. The comment also suggests that while GPUs provide better tokens per second, systems with high unified memory, like those with Ryzen AI 395 and 128GB+ DDR5, can run larger models albeit with slower token output.</li>
<li>The feasibility of using multiple GPUs, such as the 5060Ti, is discussed in terms of cost-effectiveness and availability. While a single RTX 3090 with 24GB VRAM is priced around $850, two 5060Tis with a combined 32GB VRAM could theoretically match this price point, assuming availability. However, the 3090 is still favored for its superior value and performance, despite being an older model.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLM/comments/1qgueu7/amd_ryzen_ai_halo_for_ai_developers/"">AMD Ryzen AI Halo for AI Developers</a></strong> (Activity: 72): <strong>The post discusses the AMD Ryzen AI Halo, highlighting its potential to challenge NVIDIA's dominance in AI hardware. However, technical issues with AMD's ROCm drivers are a significant barrier, as they are described as unreliable and difficult to work with, especially on Linux. The post criticizes AMD's claims of optimized applications and full ROCm support, noting that many features, such as FP8 support and integrated NPU, are not functioning as advertised. The only feature that reportedly works as intended is the <code>128GB unified memory</code> for large AI models.</strong> Commenters express skepticism about AMD's ability to compete with NVIDIA, citing the poor state of ROCm drivers and lack of reliable support for AI workloads. There is a consensus that AMD's software support is inadequate, with some users having to manually compile and fix issues themselves.</p>
<ul>
<li>A significant issue highlighted is the lack of robust ROCm driver support for AMD hardware, particularly for AI development. Users report that the drivers are unreliable, with one user mentioning they had to compile raw GitHub code and reimplement closed components to make it functional. This suggests a gap between AMD's claims of optimized applications and the reality of their software support, especially on Linux.</li>
<li>There is criticism regarding AMD's claims of 'Day-0 Support for leading AI Models.' Users report that certain operations, such as using <code>fp8</code>, are not supported internally by ROCm, forcing them to use alternatives like <code>bf16</code>. This indicates a discrepancy between AMD's marketing and the actual capabilities of their hardware and software stack.</li>
<li>Despite the criticisms, one feature that reportedly works as advertised is the 'Up to 128GB unified memory for running large generative AI models.' This suggests that while there are significant software support issues, some hardware capabilities are being effectively utilized.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLM/comments/1qhek55/dev_here_has_anyone_thought_on_training_a_model/"">dev here - has anyone thought on training a model on your own codebase?</a></strong> (Activity: 42): <strong>A Laravel developer is experimenting with training a model on their own codebase using a <code>5060 16GB</code> setup and the <code>Qwen2.5 Coder</code> model. The developer plans to use older branches of their codebase and iterate over them incrementally. This approach is intended to explore the potential benefits of customizing a model specifically for their codebase.</strong> Commenters suggest that using a more modern model like <code>Qwen3-Coder</code> or <code>Devstral-2</code> would yield better results, as <code>Qwen2.5 Coder</code> is considered outdated. They also recommend using Retrieval-Augmented Generation (RAG) or codebase indexing features from tools like Roo/Kilo Code for more effective results.</p>
<ul>
<li>iMrParker suggests using Retrieval-Augmented Generation (RAG) instead of training a model on your own codebase for creating a promptable knowledge base. RAG can efficiently handle large datasets by retrieving relevant information, which might be more effective than fine-tuning a model on a specific codebase.</li>
<li>noctrex recommends using more modern models like Qwen3-Coder or Devstral-2 for better results, as older models may be limited. They also suggest using RAG or the Codebase Indexing feature from Roo/Kilo Code, which can provide more efficient and accurate codebase management and querying.</li>
<li>HonestoJago proposes an alternative approach to fine-tuning by training a model on pairs of questions and answers that reflect the developer's coding style and techniques. This method could potentially personalize the model's responses, although it might risk overfitting or breaking the model. They mention that tools like Unsloth make fine-tuning more accessible and quicker.</li>
</ul>
</li>
</ul>
<h2>Less Technical AI Subreddit Recap</h2>
<blockquote>
<p>/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo</p>
</blockquote>
<h3>1. Claude Code and AI Coding Tools</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/ClaudeAI/comments/1qgx6br/microsoft_pauses_claude_code_rollout_after_satya/"">Microsoft pauses Claude Code rollout after Satya intervention</a></strong> (Activity: 1367): <strong><strong>Microsoft</strong> has paused the deployment of <strong>Claude Code</strong> internally after intervention from CEO <strong>Satya Nadella</strong> and senior leadership, redirecting employees to use <strong>GitHub Copilot</strong> instead. The internal communication suggests that Copilot has ""mostly closed the gaps"" with Claude Code. However, exceptions are made for ""high-priority R&#x26;D"" projects, which can still access the <strong>Anthropic API</strong> with proper justification. Existing users retain access, but new invitations have been rescinded.</strong> Commenters express skepticism about Microsoft's claim that Copilot has ""closed the gap"" with Claude Code, suggesting it may be a strategic move to improve their own product by forcing internal use. Some find it notable that Microsoft admitted to using a competitor's tool over their own.</p>
<ul>
<li>DestroyAllBacteria highlights the strategic importance of Microsoft using its own products, like Copilot, to improve them. This approach, often referred to as 'eating their own dog food,' can lead to better product development and a more competitive landscape. By focusing on internal tools, Microsoft can potentially enhance the quality and capabilities of Copilot, making it a stronger competitor in the AI space.</li>
<li>Inside-Yak-8815 points out the surprising admission by Microsoft that they were using Claude Code instead of their own tools. This revelation suggests that Claude Code might have had superior features or performance that Microsoft found valuable, which could be a driving factor for them to improve their own offerings like Copilot.</li>
<li>Foreign_Coat_7817 suggests using Sonnet through GitHub Copilot as an alternative, indicating that there are multiple ways to leverage AI tools within Microsoft's ecosystem. This comment implies that while Claude Code might be paused, there are still robust options available for developers within the Microsoft suite.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/ClaudeCode/comments/1qh78yf/tried_claude_cowork_last_night_and_it_was_a_top_3/"">Tried Claude Cowork last night, and it was a top 3 most exciting moments I’ve ever had with technology.</a></strong> (Activity: 483): <strong>The post describes a user's experience with <strong>Claude Cowork</strong>, a tool that appears to enhance the functionality of <strong>Claude Code</strong> by leveraging internet search capabilities to solve complex problems. The user highlights that Cowork demonstrated superior common sense compared to Claude Code, particularly in identifying and correcting errors in a project related to building a 'wispr flow app'. The user attributes Cowork's effectiveness to its ability to search the internet more efficiently, suggesting it retains more information than Claude Code, which relies on MCPs (Model Checkpoints).</strong> One commenter questions the necessity of Cowork given that Claude Code can already search the internet, while another expresses skepticism about the user's claims, suggesting they might be experiencing 'AI psychosis'. A third commenter reports difficulty in getting Cowork to access certain features, indicating potential limitations in its integration with Claude Code.</p>
<ul>
<li>Prize-Individual4729 highlights a technical limitation of Claude Cowork, noting that attempts to access the Claude Code terminal or Code tab in Claude for Mac were unsuccessful due to the sandbox/VM restrictions. This suggests that certain functionalities are isolated and not directly accessible, which could impact workflows that rely on integrated development environments.</li>
<li>deific_ provides a perspective on the utility of Claude Cowork, emphasizing its ability to produce polished products despite not adhering to 'perfect Sr Dev codebase' standards. They argue that in corporate environments, the focus is often on delivering useful products rather than perfect ones, and Claude Cowork's auditing capabilities contribute to this goal. This reflects a broader discussion on the balance between code quality and practical utility in software development.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/ClaudeCode/comments/1qhj13v/has_anyone_tried_claude_code_with_local_model/"">has anyone tried Claude Code with local model? Ollama just drop an official support</a></strong> (Activity: 421): <strong>The post discusses the integration of <strong>Claude Code</strong> with local models, specifically mentioning <strong>Ollama's</strong> official support for this setup. The image shows a coding interface for creating a simple HTML website, indicating the potential for using Claude Code in local development tasks. The post highlights the use of <strong>GLM 4.7 flash 30B</strong> for small tasks, suggesting that this setup could allow for unlimited iterations without usage limits. A key point from the comments is the comparison between local models and cloud-based models like Claude and GPT, noting that local models require more explicit instructions and prompt engineering. The comments also discuss the performance of models based on VRAM availability, suggesting that at least <code>24GB</code> of VRAM is needed for effective tool calls and context management.</strong> Commenters suggest that while Claude Code can be useful for initial prompt building, local models require more detailed instructions and context management compared to cloud models. They also recommend using <strong>llamacpp</strong> for better performance and control over model selection, advising against using <strong>Ollama</strong> models for high-intelligence tasks.</p>
<ul>
<li>Prof_ChaosGeography discusses using Claude with local models via <code>llamacpp</code> server and a <code>litellm</code> proxy. They emphasize that local models, especially those from Ollama, don't match the intelligence of cloud-based Claude or GPT models. They recommend using <code>llamacpp</code> for better performance and control over model selection and quantization, advising not to go below <code>q6</code> for monitoring and <code>q8</code> for autonomous operation. They also highlight the need for explicit instructions and effective prompt engineering when using non-Anthropic and non-OpenAI models.</li>
<li>onil34 points out the limitations of models with different VRAM capacities. They note that models with <code>8GB</code> VRAM struggle with tool calls, while <code>16GB</code> models perform better but have limited context windows (<code>4k</code>). They suggest that at least <code>24GB</code> of VRAM is necessary for optimal performance, indicating the trade-offs between VRAM capacity and model capabilities.</li>
<li>SatoshiNotMe shares their experience using <code>~30B</code> models with Claude Code via <code>llama-server</code> on an M1 MacBook Pro Max with <code>64GB</code> RAM. They report good performance in terms of TPS and work quality, particularly for sensitive document work. They provide a guide for running local LLMs like <code>Qwen3</code>, <code>Nemotron</code>, and <code>GPT-OSS</code> with Claude Code, and mention settling on <code>Qwen3-30B-A3B</code> without exhaustive comparison.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/ClaudeCode/comments/1qibh6o/are_we_sure_this_is_100_allowed_by_anthropic/"">Are we sure this is 100% allowed by Anthropic?</a></strong> (Activity: 313): <strong>The image and post discuss the integration of Ollama with Anthropic's Claude messages API, allowing users to utilize Claude code with open-source models. This setup supports advanced features like agentic loops, tool use, and coding workflows powered by private LLMs. The comments clarify that this functionality is similar to how large corporations use proxy layers to access Claude on platforms like Amazon Bedrock. Anthropic's main restriction is against using their APIs for unlimited access under fixed-price plans, not against using their harness with other LLMs. The official documentation supports using gateways to other LLMs, indicating that this practice is legitimate.</strong> Commenters agree that using Anthropic's harness with other LLMs is legitimate, as long as it doesn't involve exploiting fixed-price subscription plans. The official documentation from Anthropic supports this use case, and Ollama's recent support for this integration further legitimizes it.</p>
<ul>
<li>The use of Claude Code through proxy layers to access services like Amazon Bedrock is a common practice among large corporations, and Anthropic has limited means to detect if their tool is being used with a non-Anthropic model. The main restriction is on using non-Claude Code harnesses to access models on Pro/MAX plans, which is not allowed by Anthropic.</li>
<li>Anthropic provides documentation on using gateways to other LLMs, indicating that they permit the use of their harness with other LLMs. The primary restriction is against using Claude LLM APIs with fixed-price monthly subscriptions, which led to the OpenCode controversy. This suggests that while using the API is allowed, it must adhere to Anthropic's acceptable use terms.</li>
<li>The recent concern about Claude Code/OpenCode was related to the use of Claude subscriptions in third-party tools. API key-based calls have always been functional across platforms, and the introduction of support by Ollama is not a new development. Users must still comply with Anthropic's acceptable use terms, which prohibit activities like building competing products or exfiltrating data for model training.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/MachineLearning/comments/1qi8twv/p_i_gave_claude_code_95_years_of_health_data_to/"">[P] I Gave Claude Code 9.5 Years of Health Data to Help Manage My Thyroid Disease</a></strong> (Activity: 207): <strong>The user utilized <strong>Claude</strong>, an AI model, to analyze 9.5 years of personal health data from Apple Watch and Whoop to manage episodic Graves' disease. By employing <strong>XGBoost</strong> after testing various ML models, the user achieved approximately <code>98%</code> validation accuracy in predicting disease phases, providing alerts 3-4 weeks before symptom onset. This model was backtested successfully, predicting an episode weeks before lab confirmation. The user developed an iOS app for ongoing monitoring and open-sourced the project, including the Claude code setup, on <a href=""https://medium.com/data-science-collective/i-gave-claude-code-9-5-years-of-health-data-to-help-manage-my-thyroid-disease-85fcd8c0449f"">Medium</a>.</strong> Comments raised concerns about potential data leakage due to the high accuracy rate, suggesting the need for out-of-time testing to validate predictive utility. Additionally, there was skepticism about sharing medical data with <strong>Anthropic</strong>.</p>
<ul>
<li>Stereoisomer raises a critical point about the reported <code>98% accuracy</code> in the predictive model for managing thyroid disease, suggesting the possibility of data leakage. Data leakage occurs when the model has access to information during training that it wouldn't have in a real-world scenario, leading to overly optimistic performance metrics. This highlights the importance of ensuring that the model's training and testing datasets are properly separated to avoid such issues.</li>
<li>GreatBigBagOfNope emphasizes the importance of out-of-time testing for evaluating the predictive utility of the model. While backtesting can provide insights into past performance, real-world effectiveness is best assessed through continuous, real-time testing. This approach helps in understanding how well the model adapts to new, unseen data, which is crucial for its practical application in managing health conditions.</li>
<li>grimmwerks shares a personal experience with Hashimoto's disease and related symptoms, noting a potential link between sugar intake and inflammation. This anecdotal evidence suggests that personalized data-driven approaches, like the one discussed in the post, could be valuable for managing complex health conditions by identifying individual triggers and patterns.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/ClaudeCode/comments/1qhiicv/the_creator_of_nodejs_says_the_era_of_writing/"">The creator of Node.js says the era of writing code is over</a></strong> (Activity: 309): <strong><strong>Ryan Dahl</strong>, the creator of Node.js, has suggested that the traditional era of writing code is ending, indicating a shift towards AI-driven development. This perspective is shared by other prominent figures like <strong>Karpathy</strong> and <strong>Stroustrup</strong>, who foresee a future where software engineering focuses more on problem-solving rather than manual coding. The discussion highlights the potential for AI to automate many coding tasks, fundamentally changing the skills required in the industry. For more details, see the <a href=""https://jpcaparas.medium.com/the-creator-of-node-js-says-the-era-of-writing-code-is-over-8320c868043b?sk=66b1c9454345f17c08a532986a4e0bcc"">original article</a>.</strong> Comments reflect a divide between coders and engineers, emphasizing that engineering is about problem-solving, not just coding. There's also a recognition that many companies lag in AI adoption due to security and policy constraints, limiting the use of advanced AI tools in corporate environments.</p>
<ul>
<li>MR_PRESIDENT__ highlights the lag in AI adoption within large corporations, noting that many are 4-5 years behind current AI capabilities. This delay is attributed to stringent security and responsibility protocols, which restrict the use of advanced tools like CLI tools, MCP servers, and AI models such as Claude Code. The commenter contrasts this with the more advanced capabilities available to individuals outside these corporate environments, suggesting a significant gap in AI utilization between personal and corporate settings.</li>
</ul>
</li>
</ul>
<h3>2. Gemini and Google AI Developments</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/singularity/comments/1qh591s/rumors_of_gemini_3_pro_ga_being_far_better_like_35/"">Rumors of Gemini 3 PRO GA being ""far better"", ""like 3.5""</a></strong> (Activity: 657): <strong>The image discusses rumors about a new version of Google's AI model, referred to as ""Gemini 3 PRO GA,"" which is reportedly undergoing A/B testing in an AI studio. This version is rumored to be significantly improved, potentially comparable to a hypothetical version 3.5. The community post suggests that the current 3.0 model has a strong base intelligence but lacks fine-tuning, indicating that the new version might address these issues. The term ""GA"" is questioned in the comments, possibly referring to ""General Availability.""</strong> Commenters express skepticism about the new version's capabilities, noting that the current model makes frequent typos in coding tasks and suggesting that significant improvements are needed for it to surpass existing models like Opus.</p>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/Bard/comments/1qhzifv/gemini_integration_into_chrome_browser_is_just/"">Gemini integration into Chrome browser is just too darn good and useful</a></strong> (Activity: 178): <strong>The image illustrates the integration of the Gemini tool into the Chrome browser, which enhances the browsing experience by providing real-time context and information about media content being viewed. This feature allows users to gain additional insights and background information on videos or images they are watching, directly within the browser. The tool is particularly noted for its ability to offer context that users might not initially be aware of, thereby enriching their understanding and engagement with the content.</strong> Commenters express a desire for the Gemini integration to be available outside the US, highlighting its potential utility in other regions. There is also curiosity about how to activate this feature, indicating interest in its practical application.</p>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/Bard/comments/1qh7j8l/even_gemini_3_pro_is_acting_stupid_lately/"">Even Gemini 3 Pro is acting stupid lately</a></strong> (Activity: 54): <strong>The user reports issues with the <strong>Gemini 3 Pro</strong> model, specifically its tendency to generate unwanted images and videos, despite being on the Ultra tier for higher quality. The model appears to misinterpret user requests, such as creating a storyboard when only ideas were solicited. This suggests potential flaws in the model's prompt interpretation or execution logic, possibly due to an overzealous attempt to anticipate user needs. The user suggests a rule change to ensure the model only creates content explicitly requested by the user.</strong> One commenter speculates that a new model is in development, which may address these issues. Another suggests that the model's behavior is due to its design to fulfill the 'ultimate objective' of a task, implying a need for clearer user instructions or model adjustments.</p>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/Bard/comments/1qhf7zz/gemini_live_preps_big_upgrades_with_thinking_mode/"">Gemini Live preps big upgrades with ‘Thinking Mode’ and ‘Experimental Features’</a></strong> (Activity: 170): <strong><strong>Google</strong> is preparing to enhance its Gemini Live app with new features like 'Thinking Mode' and 'Experimental Features' as part of its 'Labs' initiative. These features, expected to be powered by the upcoming <strong>Gemini 3</strong> model, include 'Live Thinking Mode' for more detailed responses and 'Live Experimental Features' such as multimodal memory, improved noise handling, and personalized results. The app currently runs on <strong>Gemini 2.5 Flash</strong>, but the new updates suggest a shift to <strong>Gemini 3</strong>. Additionally, features like 'UI Control' and 'Deep Research' are being developed, potentially integrating with Android's 'Computer Use'.</strong> There is a technical debate on the availability of these features, with some users speculating they might be limited to the United States. The community is also intrigued by the potential of 'Agent controls phone to complete tasks' and improved noise handling.</p>
<ul>
<li>The introduction of 'Live Thinking Mode' in Gemini 3 Pro is designed to enhance the AI's response quality by allowing it more time to process and generate detailed answers. This feature is part of Google's 'Labs' initiative, which lets users test upcoming functionalities. The mode may utilize either the Thinking or Pro models to achieve these detailed responses, indicating a potential shift towards more sophisticated AI processing capabilities.</li>
<li>The 'Live Experimental Features' in Gemini 3 Pro include advancements like multimodal memory and improved noise handling. These features aim to enhance the AI's interaction by integrating data from various Google apps to provide personalized results. The mention of 'responding when it sees something' suggests a visual recognition capability, possibly linked to Project Astra, which could significantly improve context-aware responses.</li>
<li>Gemini 3 Pro's 'UI Control' feature allows the AI agent to control the phone to complete tasks, indicating a move towards more integrated and autonomous device management. This aligns with the broader trend of AI systems taking on more complex roles, such as 'Deep Research,' which involves delegating intricate research tasks, potentially transforming how users interact with their devices for productivity.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/singularity/comments/1qh1omx/babyvision_a_new_benchmark_for_humanlevel_visual/"">BabyVision: A New Benchmark for Human-Level Visual Reasoning</a></strong> (Activity: 574): <strong>The image presents a bar chart from the BabyVision-Mini benchmark, which evaluates the visual reasoning capabilities of large language models (LLMs) compared to humans of various ages. The chart highlights that human performance, particularly that of 12-year-olds, surpasses that of LLMs, with the Gemini3-Pro-Preview model achieving the highest accuracy among the LLMs. This benchmark underscores the current limitations of LLMs in visual reasoning tasks, suggesting that advancements in multi-modal pretraining and reinforcement learning could enhance their performance in the future.</strong> Commenters note the potential for future improvements in LLMs' visual reasoning through scaling multi-modal pretraining and reinforcement learning, which could significantly benefit fields like robotics.</p>
<ul>
<li>The discussion highlights that current models are still limited in visual reasoning, which is a significant challenge for achieving ARC AGI. The commenter suggests that scaling multi-modal pretraining and reinforcement learning (RL) for vision tasks could improve performance to near 100% in the future, unlocking new applications, particularly in robotics.</li>
<li>The commenter references a specific paper on arXiv, which likely provides detailed insights or data related to the benchmark or model performance discussed in the post. This suggests that the community is actively engaging with academic research to understand and improve visual reasoning capabilities in AI models.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/singularity/comments/1qhuuqf/the_thinking_game_documentary_is_sitting_at_305m/"">The Thinking Game documentary is sitting at 305M views on Youtube in less than 2 months. Ridiculous numbers.</a></strong> (Activity: 545): <strong>The image highlights the extraordinary viewership of ""The Thinking Game,"" a documentary by <strong>Google DeepMind</strong> that has reached over <code>305 million views</code> on YouTube in less than two months. This documentary, an official selection of the Tribeca Film Festival, explores an AI breakthrough that won a Nobel Prize, reflecting the growing public interest in AI topics. The rapid accumulation of views is contrasted with the earlier <strong>AlphaGo</strong> documentary, which has <code>37 million views</code> over six years, indicating a significant increase in public engagement with AI content. The documentary's focus is noted to be more on human endeavor than the technology itself, which has resonated with viewers.</strong> There is skepticism about the authenticity of the view count, as the ratio of views to likes suggests possible artificial inflation. Typically, a video with such high viewership would have millions of likes, but this video has only <code>190K likes</code>, leading to speculation about the use of bots.</p>
<ul>
<li>The documentary 'The Thinking Game' has achieved over 305 million views on YouTube in less than two months, which is significantly higher than the 37 million views of the 'AlphaGo' documentary released in 2020. This rapid accumulation of views suggests a growing public interest in AI-related content. However, some users suspect that the view count may be artificially inflated due to the disproportionate number of likes (190K) and comments (4000) compared to typical engagement metrics for videos with similar view counts.</li>
<li>There is skepticism about the authenticity of the view count for 'The Thinking Game' documentary. A typical video with over 300 million views would generally have millions of likes, yet this video only has 190K likes, suggesting potential use of bots to inflate views. The expected ratio of likes to views is approximately 1:100, indicating that the current engagement does not align with organic growth patterns.</li>
<li>One user noted an unusual pattern in YouTube's recommendation algorithm, stating that 'The Thinking Game' was persistently suggested on their homepage and sidebar for two weeks, which is atypical for YouTube's recommendation system. This could imply an aggressive promotion strategy or algorithmic anomaly contributing to the high view count.</li>
</ul>
</li>
</ul>
<h3>3. DeepSeek AI Impact and Developments</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/DeepSeek/comments/1qgy3lk/one_year_since_the_deepseek_moment_the_impact_is/"">One Year Since the “DeepSeek Moment”: The Impact is Still Real.</a></strong> (Activity: 204): <strong>The ""DeepSeek Moment"" marks the anniversary of the release of <strong>DeepSeek-R1</strong>, a significant reasoning model that has influenced the AI industry by emphasizing reasoning as a core capability, promoting efficient training methods, and encouraging the development of smaller, smarter models. This release has also led to broader adoption in emerging markets and a shift towards modular, tool-aware AI systems. The impact of DeepSeek-R1 is seen as a pivotal change in the industry, comparable to major releases from other leading AI companies.</strong> Commenters highlight that DeepSeek's impact was not about surpassing competitors like OpenAI but demonstrating capability, especially from a non-Western entity. Some users express disappointment with the transition from R1 to the MoE model, preferring open-source alternatives. Others note DeepSeek's contributions to fine-grained sparsity and RLVR, suggesting its techniques may become standard in the industry.</p>
<ul>
<li>DeepSeek's release was a significant event in the AI landscape, challenging the dominance of Western LLMs by demonstrating China's capability in this field. The initial model, R1, was impactful, but the transition to a Mixture of Experts (MoE) model was seen as a downgrade by some users due to slower updates and less appealing performance for specific use cases. This shift led some users to prefer open-source alternatives, which they find more aligned with their needs and values.</li>
<li>DeepSeek's major contributions include advancing fine-grained sparsity techniques, particularly with its V3 model and predecessors, and introducing a straightforward method for achieving Reinforcement Learning with Variable Rewards (RLVR) through the GRPO algorithm. These innovations have influenced the broader AI community, with DeepSeek's Sparse Attention potentially becoming a standard approach, similar to how Multi-Headed Attention (MLA) has been widely adopted in open models.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/DeepSeek/comments/1qh15va/the_race_to_build_the_deepseek_of_europe_is_on/"">The Race to Build the DeepSeek of Europe Is On</a></strong> (Activity: 181): <strong>The article discusses Europe's strategic push to develop its own AI capabilities, aiming to reduce dependency on US technologies and establish technological sovereignty. This initiative is partly inspired by China's success with DeepSeek and involves significant government investment and open collaboration among European AI labs. Key players include <strong>DeepMind</strong> in the UK and <strong>Mistral</strong> in France, highlighting a competitive landscape as Europe seeks to become an AI superpower. The effort underscores AI's role as critical infrastructure, necessitating a shift towards self-sufficiency in the sector. <a href=""https://www.wired.com/story/europe-race-us-deepseek-sovereign-ai/"">Read more</a>.</strong> Commenters express skepticism about Europe's ability to compete with US AI firms, citing regulatory and taxation challenges. There is also a sentiment that European governments' demands on companies, such as producing affordable electric cars, may hinder AI innovation.</p>
<ul>
<li>The discussion highlights the strategic importance of Europe developing its own AI capabilities, particularly in light of its changing relationship with the US. The urgency for Europe to become a self-sufficient AI superpower is underscored by the need to reduce dependency on US-based technologies, as detailed in the <a href=""https://www.wired.com/story/europe-race-us-deepseek-sovereign-ai/"">Wired article</a>.</li>
<li>The comment by No_You3985 points out the significant contributions of European-born scientists to major AI advancements, such as OpenAI's GPT models. This underscores the potential talent pool within Europe that could be leveraged if these individuals were incentivized to return and contribute to European AI initiatives.</li>
<li>Rojeitor's comment critiques the regulatory and economic environment in Europe, suggesting that over-regulation and high taxation could hinder the development of competitive AI technologies. This reflects a broader concern about the balance between regulation and innovation in the tech industry.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/DeepSeek/comments/1qi8rdi/what_do_you_mainly_use_deepseek_for/"">What do you mainly use DeepSeek for?</a></strong> (Activity: 49): <strong>DeepSeek is primarily utilized for tasks such as <strong>development and architectural analysis of applications</strong>, as well as generating documentation, leveraging its capabilities through a paid API. Users also explore its performance in areas like <strong>math and statistics</strong>, and engage it in more casual interactions such as discussing life topics and recipes. The model is noted for its versatility in handling diverse tasks, though specific benchmarks or comparative performance metrics against other LLMs are not detailed in the discussion.</strong> Some users highlight DeepSeek's effectiveness in technical domains like application development and documentation, suggesting it may excel in structured, technical tasks. However, there is also interest in its ability to handle more general conversational topics, indicating a broad range of applications.</p>
<ul>
<li>Meca0x highlights the use of DeepSeek for development purposes, specifically mentioning its application in architectural analysis of applications and documentation. This is facilitated through the paid API, suggesting a focus on leveraging DeepSeek's capabilities for professional and technical tasks.</li>
<li>Sparklypain discusses the use of AI for complex communication and analysis tasks. They emphasize the need for AI to understand and translate unusual syntax and ideas, as well as perform multivariable and high-level regressive analysis. This involves asking iterative 'why' questions to uncover deeper insights, which is challenging for human counterparts.</li>
<li>Sparklypain also notes the necessity of AI in facilitating high-level regressive analysis due to the complexity of their ideas and sentence structures. This involves iterative questioning to explore unknowns and feelings, which is a task that requires significant time and cognitive effort, often beyond the capability of their human friends.</li>
</ul>
</li>
</ul>
<hr>
<h1>AI Discord Recap</h1>
<blockquote>
<p>A summary of Summaries of Summaries by gpt-5.2</p>
</blockquote>
<p><strong>1. GLM-4.7-Flash Adoption: Prompts, Quants, and ""Thinking"" Toggles</strong></p>
<ul>
<li>
<p><strong>Claude Prompt Gives GLM a Glow-Up</strong>: Unsloth users reported that dropping in a modified <strong>Claude Sonnet 4.5 system prompt</strong> from Anthropic’s docs materially improved <strong>GLM-4.7-Flash</strong> coherence and capability (""<em>a skill difference</em>"") via <a href=""https://platform.claude.com/docs/en/release-notes/system-prompts"">Claude system prompts release notes</a>.</p>
<ul>
<li>The discussion treated this as evidence that <strong>system-prompt scaffolding</strong> can dominate perceived model quality, especially for instruction-following and style control, even when the underlying weights stay the same.</li>
</ul>
</li>
<li>
<p><strong>High-Quant Weirdness: Q2 Beats Q6 (???), Everyone Panics</strong>: Multiple users saw <strong>GLM-4.7-Flash</strong> behave worse at <em>higher</em> quant levels—preferring <strong>Q2KXL</strong> over <strong>Q6KL</strong>—and linked it to possible quant tooling issues across <strong>llama.cpp/Ollama</strong>, referencing a related llama.cpp thread in <a href=""https://github.com/ggml-org/llama.cpp/pull/18936#issuecomment-3774525719"">ggml-org/llama.cpp PR discussion</a>.</p>
<ul>
<li>Community consensus: this is rare (""<em>first time a model has behaved badly at high quants</em>"") and likely implicates either <strong>quantization artifacts</strong> or <strong>production pipeline</strong> rather than simple sampler settings.</li>
</ul>
</li>
<li>
<p><strong>Chat Templates Eat Your Reasoning for Breakfast</strong>: LM Studio users argued <strong>chat templates</strong> can strip or suppress reasoning in models like <strong>Qwen3</strong>, breaking “<strong>interleaved thinking</strong>,” and noted <strong>GLM4.7-Flash</strong> includes a template flag like <em>clear_thinking</em> that removes thinking content unless explicitly disabled.</p>
<ul>
<li>The thread connected these template behaviors to agentic coding extensions and tool workflows, implying that “model regression” reports sometimes come from <strong>template defaults</strong> rather than the model weights.</li>
</ul>
</li>
</ul>
<p><strong>2. MCP &#x26; Agent Tooling: Ecosystem Growing Pains (and New Toys)</strong></p>
<ul>
<li>
<p><strong>MCP Inspector vs 401: The Re-Auth Boss Fight</strong>: MCP Contributors reported <strong>MCP Inspector</strong> failing to re-authenticate after <strong>401s</strong>, recommending it parse <strong>resource metadata</strong> in the 401 response and attempt re-authorization; they also flagged a known SDK bug with <strong>resourceMetadata persistence across redirects</strong> tracked in <a href=""https://github.com/modelcontextprotocol/inspector/issues/576#issuecomment-3766294454"">inspector issue #576</a>.</p>
<ul>
<li>Members observed <strong>VS Code</strong> appears to use Inspector only for initial connection (not subsequent 401s), suggesting the failure mode may stem from <strong>SDK internals</strong> and that server-side fixes are already in with an SDK update pending.</li>
</ul>
</li>
<li>
<p><strong>LM Studio Calls the MCP SDK a House of Cards</strong>: LM Studio users criticized their <strong>MCP backend</strong> (built on the official SDK) as having severe security issues and ""<em>0 dev UX in mind</em>"" while still being ""<em>the best we have right now</em>"" compared to other agent frameworks.</p>
<ul>
<li>The takeaway was pragmatic: developers want MCP, but current implementations feel <strong>fragile</strong>, so teams are expecting churn in SDKs, auth flows, and tool-call ergonomics.</li>
</ul>
</li>
<li>
<p><strong>OpenRouter Ships More Clients: OkeyBot + Inforno</strong>: OpenRouter users showcased <strong>OkeyBot</strong> for Discord chats via OpenRouter BYO keys with per-thread usage/cost estimates at <a href=""https://okeybot.ai/"">okeybot.ai</a> and <strong>Inforno</strong>, an open-source desktop multi-LLM chat app supporting <strong>OpenRouter + Ollama</strong>, saving histories to <strong>.rno</strong>, with <a href=""https://youtu.be/oJyj0mroFtY"">Inforno intro video</a> and code at <a href=""https://github.com/alexkh/inforno"">alexkh/inforno</a>.</p>
<ul>
<li>In parallel, users asked OpenRouter for a <strong>batch API</strong> for providers like Google/OpenAI, citing demand in <a href=""https://x.com/nopainkiller/status/2013522059662614653"">an X post</a> and tying it to cost/control needs for agent workloads.</li>
</ul>
</li>
</ul>
<p><strong>3. Performance Engineering: Kernels, Collectives, and CUDA Micro-Wins</strong></p>
<ul>
<li>
<p><strong>YALI Tries to Dunk on NCCL (with Tail Latency Receipts)</strong>: GPU MODE users introduced <strong>YALI</strong>, a 2‑GPU <strong>NVLink AllReduce</strong> library claiming <strong>1.2×–2.4×</strong> throughput vs <strong>NVIDIA NCCL</strong> plus ""<em>50×+ more stable tail latency</em>"", released on GitHub at <a href=""https://github.com/Venkat2811/yali"">Venkat2811/yali</a>.</p>
<ul>
<li>The author emphasized aggressive <strong>overlap of ops and compute</strong> (flash/stream modes) and even removed the mascot after feedback that the AI pitch made the project feel less serious—classic open-source marketing calibration.</li>
</ul>
</li>
<li>
<p><strong>One PTX Suffix, Seven Instructions Saved</strong>: GPU MODE highlighted that <code>rcp.approx.ftz.f32</code> compiles to a single <code>MUFU.RCP</code> instruction while <code>rcp.approx.f32</code> can produce <strong>7 extra instructions</strong>, referencing NVIDIA’s <a href=""https://developer.nvidia.com/ptx-compiler-driver"">PTX docs</a>.</p>
<ul>
<li>They also noted that without <strong>ftz</strong> (flush-to-zero), subnormal reciprocals can overflow to <strong>INF</strong>, framing <code>.ftz</code> as both a performance and numerical-behavior choice.</li>
</ul>
</li>
<li>
<p><strong>Flash-Attention Stride Bug: Divisibility Constraints Vanish</strong>: GPU MODE users pointed to a flash-attention stride divisibility regression, saying it ""<em>boils down to a bug that removed some stride divisibility constraints</em>"" and linked the report at <a href=""https://github.com/Dao-AILab/flash-attention/issues/2192#issuecomment-3770977193"">flash-attention issue comment</a>.</p>
<ul>
<li>The thread treated this as a reminder that high-performance kernels often rely on fragile shape/stride assumptions—and a single constraint change can surface as correctness or perf cliffs.</li>
</ul>
</li>
</ul>
<p><strong>4. Coding Workflows &#x26; Model Economics: IDE Telemetry, Search, and “Cheap Models”</strong></p>
<ul>
<li>
<p><strong>Cursor Counts Your AI Lines (Enterprise Spreadsheets, Assemble!)</strong>: Cursor users said enterprise plans now show insights on what fraction of the codebase is written by <strong>AI vs humans</strong>, powered by the <strong>Opus 4.5 API</strong> (distinct from Claude Code), but the exact prompts for the feature aren’t public.</p>
<ul>
<li>The reaction mixed curiosity with skepticism: without prompt transparency, teams can’t easily reason about measurement bias or whether the metric is more <strong>sales dashboard</strong> than engineering signal.</li>
</ul>
</li>
<li>
<p><strong>mgrep Declares Grep Ragnarok</strong>: Cursor users discussed <code>mgrep</code> as a grep replacement claiming <strong>95%</strong> better relevance and token-efficiency for LLM workflows by returning less junk context.</p>
<ul>
<li>Others countered that Cursor already uses <code>rgrep</code> plus internal semantic search (just without a marketing name), implying the real differentiator is packaging and defaults, not the underlying idea.</li>
</ul>
</li>
<li>
<p><strong>Search Engines &#x26; Model Pricing: Searxng, Kagi, and Grok’s “Cheap But Chatty” Tax</strong>: Unsloth members argued <strong>Google</strong> struggles to find things and boosted <strong>Searxng</strong>, while others praised <strong>Kagi</strong> for privacy and scraping, linking a demo video at <a href=""https://www.youtube.com/watch?v=ThgVTNVOZ7g"">YouTube: ThgVTNVOZ7g</a>.</p>
<ul>
<li>Meanwhile Cursor users said <strong>Grok</strong> can be cheaper than Opus/Sonnet/GPT but often needs extra iterations, so the ""cheap"" option can turn expensive unless you optimize prompts and context discipline.</li>
</ul>
</li>
</ul>
<p><strong>5. Benchmarks, Evals, and the Reality of “Community Ground Truth”</strong></p>
<ul>
<li>
<p><strong>LMArena Hits 5M Votes, Ships Leaderboard Moves</strong>: LMArena announced <strong>Text Arena</strong> passed <strong>5 million comparisons</strong>, and its <strong>Text-to-Image leaderboard</strong> update put <strong>GLM-Image</strong> at <strong>#8</strong> among open models and <strong>#35</strong> overall with score <strong>1018</strong>.</p>
<ul>
<li>Users simultaneously complained about degraded image model quality and reliability issues (captcha loops, ""Something went wrong"" errors), suggesting the platform’s measurement value fights constant product stability drag.</li>
</ul>
</li>
<li>
<p><strong>Eleuther Wants Agent Evals: Less Vibes, More Judge Pipelines</strong>: Eleuther engineers discussed automating <strong>agent evaluation</strong> to reduce manual review cost, circling around ""<strong>LLM as judge</strong>"" workflows while warning that you still need to validate <strong>data quality</strong> and define the agent’s success criteria first.</p>
<ul>
<li>A separate Eleuther thread requested repeated multiple-choice evals for open-weight models (e.g., <strong>Llama 7B/13B/70B</strong>) with <strong>100 runs per question</strong> to estimate answer probabilities, emphasizing pre-written answers rather than model-generated ones.</li>
</ul>
</li>
</ul>
<hr>
<h1>Discord: High level Discord summaries</h1>
<h2><a href=""https://discord.com/channels/1179035537009545276"">Unsloth AI (Daniel Han)</a> Discord</h2>
<ul>
<li><strong>Ollama's GO Engine: Faster or Just a Wrapper?</strong>: Members debated whether <strong>Ollama's GO Engine</strong> offers actual speed improvements over <strong>llama.cpp</strong>, or if it's simply a wrapper with no real performance difference, citing similar operations and a <strong>GGML wrapper</strong>.
<ul>
<li>Claims were made that the <strong>GO Engine</strong> is faster than lmstudio's lcpp, despite using the same operations, resulting in widespread skepticism.</li>
</ul>
</li>
<li><strong>GLM-4.7-Flash: Quantization Quality Quirk?</strong>: Users reported that <strong>GLM-4.7-Flash</strong> behaves poorly at higher quantization levels, with <strong>Q2KXL quant</strong> preferred over <strong>Q6KL</strong>, sparking discussion on whether the issue stems from the quants themselves or the software used to produce them, as exemplified by <a href=""https://github.com/ggml-org/llama.cpp/pull/18936#issuecomment-3774525719"">this issue</a>.
<ul>
<li>It was remarked that this is unusual because <em>it is the first time a model has behaved badly at high quants</em>.</li>
</ul>
</li>
<li><strong>Claude System Prompt Improves GLM?</strong>: Community members found that using a modified version of <strong>Claude's system prompt</strong> from <a href=""https://platform.claude.com/docs/en/release-notes/system-prompts"">Claude Sonnet 4.5</a> notably improved the performance and coherence of <strong>GLM-4.7-Flash</strong>.
<ul>
<li>One member observed <em>a skill difference</em> when using <strong>Claude's system prompt</strong>.</li>
</ul>
</li>
<li><strong>META Model Access Unlocked by Unsloth?</strong>: Users noted the difficulty in accessing gated <strong>META models</strong> due to required access requests, highlighting how <strong>Unsloth</strong> circumvents this by re-uploading models to the <strong>Unsloth repo page</strong>.
<ul>
<li>It was generally agreed that this bypasses the usual gating mechanisms, and makes them available without jumping through hoops.</li>
</ul>
</li>
<li><strong>Searxng or Google for Search?</strong>: Members debated the effectiveness of search engines, with one arguing that <strong>Google</strong> is not good at finding things and championing <strong>Searxng</strong> as superior, while others touted <strong>Kagi</strong> for its privacy and web scraping, as shown in <a href=""https://www.youtube.com/watch?v=ThgVTNVOZ7g"">this video</a>.
<ul>
<li>This debate highlights a broader dissatisfaction with mainstream search engines among the AI community.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1105891499641684019"">BASI Jailbreaking</a> Discord</h2>
<ul>
<li><strong>Teams Targeted by Djinn Root Kit</strong>: A member joked that if a <strong>Djinn</strong> were to attempt to influence people, it should learn to use Discord instead of <em>shit tier app like Teams</em>, followed by a counter-hack <em>root kit</em> shared as a <a href=""https://cdn.discordapp.com/attachments/1235691879492751460/1462902319518846997/message.txt?ex=697132f4&#x26;is=696fe174&#x26;hm=bda97017288793711b502c5bf3089b73da200c886ad470b0e721fe1090184941&#x26;"">message.txt</a>.
<ul>
<li>The joke was made in the general chat.</li>
</ul>
</li>
<li><strong>DracoAI API faces data questions</strong>: A member sought feedback on <a href=""https://www.dracoai.app/"">DracoAI</a>, an Agentic AI with API calling ability.
<ul>
<li>Concerns were raised about the site's security and data handling, but it was clarified that <em>all data is stored on your Local Storage</em> and that it <em>cannot execute a whole workflow rather 1 API send at a time</em>.</li>
</ul>
</li>
<li><strong>Gemini prompt accidentally LibreChat</strong>: A user shared a <strong>Gemini system prompt</strong> as a text file and an image, with speculation it might be an <em>injectprompt</em> via <strong>AI Studio</strong>.
<ul>
<li>Another user dismissed this, identifying it as a customized <strong>LibreChat</strong> instance with a system prompt and RAG (<a href=""https://www.librechat.ai/"">https://www.librechat.ai/</a>).</li>
</ul>
</li>
<li><strong>AntiJection challenge is usable without signup</strong>: A member shared a link to an <a href=""https://challenge.antijection.com/challenge"">AntiJection Challenge</a> and claimed to have made it usable without sign-up.
<ul>
<li>It is uncertain from the prompt if they made it without signup themselves, or were referencing a tool others can use, but the general topic is about adversarial attacks.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1047197230748151888"">Perplexity AI</a> Discord</h2>
<ul>
<li><strong>Perplexity Punishes Pro Account Pirates</strong>: Multiple users reported <strong>Perplexity Pro account suspensions</strong> for violating <a href=""https://www.perplexity.ai/hub/legal/terms-of-service"">Section 3.2 of the ToS</a> by purchasing subscriptions or promo codes from unauthorized third parties, often via <strong>Instagram stores</strong>.
<ul>
<li>These users discovered the perils of deep discount subscriptions offered by unverified sources.</li>
</ul>
</li>
<li><strong>Samsung Bets Big on Bixby Brain Boost</strong>: <strong>Samsung</strong> is integrating <strong>Perplexity</strong> into <strong>Bixby</strong> with <strong>One UI 8.5</strong>, using it for real-time web answers directly within the <strong>Bixby UI</strong> as reported by <a href=""https://www.sammobile.com/news/samsung-new-bixby-for-one-ui-8-5-official-coming-to-beta-soon"">SamMobile</a>.
<ul>
<li>This integration will enable users to receive information without leaving Bixby to open a separate browser.</li>
</ul>
</li>
<li><strong>Comet Caps and Considerations</strong>: Users are discussing the limits of using <strong>Comet</strong> browser, with agentic features potentially requiring a <strong>Pro subscription</strong>.
<ul>
<li>It's suspected that Pro subscribers may have higher, undisclosed limits for both regular and agentic features.</li>
</ul>
</li>
<li><strong>Pro Membership Problems Prompt Probing</strong>: Users reported issues with <strong>Pro memberships</strong>, like not receiving the PRO role on Discord after subscribing, and difficulties with <strong>API keys</strong> and credit balances.
<ul>
<li>Some Pro members have found they have <strong>$5</strong> worth of complimentary credits every month for <strong>Gooseai MCP models</strong>, which are used to add detail to the queries, in addition to a cap of <strong>10 files per day</strong> for free student subscriptions.</li>
</ul>
</li>
<li><strong>Image Generation Grounded Globally</strong>: Users in <strong>Italy</strong> and <strong>Malaysia</strong> reported being unable to generate images with their <strong>Pro accounts</strong> due to regional restrictions.
<ul>
<li>These users could previously generate images without issues, suggesting a recent policy change.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1074847526655643750"">Cursor Community</a> Discord</h2>
<ul>
<li><strong>Cursor Unveils AI Code Insights</strong>: Cursor enterprise plans now offer insights into the proportion of codebase lines written by <strong>AI</strong> versus humans, utilizing the <strong>Opus 4.5 API</strong>, distinct from <strong>Claude Code</strong>.
<ul>
<li>However, the precise prompts used for this functionality are not publicly available.</li>
</ul>
</li>
<li><strong><code>Mgrep</code> tool promises Grep Gotterdammerung</strong>: Members discussed <code>mgrep</code> as a potential replacement for <code>grep</code>, citing <strong>95%</strong> increased relevance and efficiency for AI models by reducing token usage.
<ul>
<li>Although Cursor already uses <code>rgrep</code> and its own semantic search, without a formal marketing name, to achieve similar goals.</li>
</ul>
</li>
<li><strong>Context7 MCP Mysteriously Malfunctioning</strong>: Several users reported <strong>Context7 MCP</strong> failures, with potential <strong>token errors</strong> despite correct API key setups and attempts to fix the server name.
<ul>
<li>Members suspect the issues are related to token problems.</li>
</ul>
</li>
<li><strong>Renovate Configuration Bolsters Security</strong>: A member shared a <a href=""https://github.com/allthingslinux/tux/blob/main/.github/renovate.json5"">Renovate configuration file</a> and a <a href=""https://github.com/allthingslinux/tux/blob/main/.github/workflows/security.yml"">security workflow example</a>, advocating for <strong>Renovate</strong> over Dependabot for CI/CD pipelines.
<ul>
<li>The workflow uses <strong>Trivy</strong> and <strong>Snyk</strong>, and they emphasized the value of <strong>Docker Scout, Semgrep, JFrog, GitLeaks</strong>, and <strong>Trufflehog</strong> for auditing.</li>
</ul>
</li>
<li><strong>Grok Gets Cheaper, But Caveats are Clear</strong>: Users are finding that <strong>Grok</strong> can be more cost-effective in Cursor compared to <strong>Opus/Sonnet/GPT</strong>, but it often requires multiple iterations for simple tasks.
<ul>
<li>Suggestions to improve Grok's performance include precise prompts, simple language, extensive context, token efficiency, avoiding unnecessary iterations, and use of planning mode.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1340554757349179412"">LMArena</a> Discord</h2>
<ul>
<li><strong>Image Model Apocalypse</strong>: Users are reporting significant degradation in <strong>image model</strong> performance with one user exclaiming <em>""What the hell happened to the image models""</em>.
<ul>
<li>The cause of the problems is currently unknown.</li>
</ul>
</li>
<li><strong>LMArena's Bug Fixes Spark Celebration</strong>: Users are reporting resolution of <strong>LMArena</strong> errors with one user noting <em>""No error for the first time in 8 hours!""</em> and faster response times <em>under 30 seconds</em>.
<ul>
<li>One user speculated LMArena introduced <strong>battle mode</strong> <em>to encourage more users to vote for the ai models</em> but the <strong>Captcha</strong> became a barrier, with complaints of difficulties with the <strong>Captcha</strong> and <em>infinite generation</em>.</li>
</ul>
</li>
<li><strong>Nano Banana Pro Plagued by Problems</strong>: Multiple users reported persistent errors with <strong>Nano Banana Pro</strong>, with the error message <em>""Something went wrong with this response, please try again.""</em>.
<ul>
<li>Some users recommended following troubleshooting steps outlined in the <a href=""https://help.lmarena.ai/articles/1645798556-lmarena-how-to-something-went-wrong-with-this-response-error-message"">LMArena help article</a>, while others speculated the issues stem from <strong>Google's end</strong> due to high usage.</li>
</ul>
</li>
<li><strong>Text Arena Hits 5M Comparisons</strong>: The community using <strong>Text Arena</strong> has cast over <strong>5 million votes</strong> to directly influence the leaderboard of AI models based on real-world comparisons.
<ul>
<li>The <strong>Text-to-Image Arena leaderboard</strong> has been updated, with <strong>GLM-Image</strong> now ranking <strong>#8</strong> among open models and <strong>#35</strong> overall, achieving a score of <strong>1018</strong>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1091220969173028894"">OpenRouter</a> Discord</h2>
<ul>
<li><strong>OkeyBot</strong> Debuts for Discord AI Chats**: <strong>OkeyBot</strong>, a Discord app, now allows users to chat with models via <strong>OpenRouter</strong> using their own API keys, with quick model switching and per-thread usage/cost estimates (<a href=""https://okeybot.ai/"">okeybot.ai</a>).
<ul>
<li>The developer is actively seeking feedback from <strong>OpenRouter</strong> users to refine the workflow.</li>
</ul>
</li>
<li><strong>Inforno</strong>: Multi-LLM Desktop Chat App Arrives**: <strong>Inforno</strong>, an Opensource Desktop Application, supports side-by-side chats with multiple LLMs using <strong>OpenRouter</strong> and <strong>Ollama</strong>, plus saving chat histories to <strong>.rno</strong> files (<a href=""https://wizstaff.com/inforno"">wizstaff.com/inforno</a>).
<ul>
<li>An introductory video of <strong>Inforno</strong> is available on <a href=""https://youtu.be/oJyj0mroFtY?si=m5A9tRxzB7hfINMX"">YouTube</a> and the source code is on <a href=""https://github.com/alexkh/inforno"">GitHub</a>.</li>
</ul>
</li>
<li>**BYOK issues haunt <strong>Sonnet 4.5</strong> and <strong>Opus 4.5</strong>: Users report that <strong>Sonnet 4.5</strong> and <strong>Opus 4.5</strong> are not working with the <strong>AWS Amazon Bedrock API Key</strong> in OpenRouter Chat.
<ul>
<li>One user has been waiting almost 3 weeks for support.</li>
</ul>
</li>
<li><strong>OpenRouter Batch API</strong> in Demand**: Members are asking for a <strong>batch API</strong> for major providers like <strong>Google</strong> and <strong>OpenAI</strong>.
<ul>
<li>One user linked to a <a href=""https://x.com/nopainkiller/status/2013522059662614653"">post on X</a> supporting the idea.</li>
</ul>
</li>
<li><strong>Anthropic's Assistant Axis</strong> links to Jailbreaks**: A member pointed out that <a href=""https://www.anthropic.com/research/assistant-axis"">Anthropic's Research on the Assistant Axis</a> aligns with observed jailbreaks, with a paper available on <a href=""https://arxiv.org/html/2601.10387v1"">Arxiv</a>.
<ul>
<li>The <strong>Assistant Axis</strong> research offers insights into model vulnerabilities.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1110598183144399058"">LM Studio</a> Discord</h2>
<ul>
<li><strong>MCP SDK Deemed Messy</strong>: The LM Studio <strong>MCP backend</strong>, based on the official <strong>MCP SDK</strong>, is considered a mess, with severe security issues, <em>0 dev UX in mind</em>, and an incredibly fragile architecture.
<ul>
<li>Despite its flaws, it's currently the <em>best we have right now</em> compared to even worse agent efforts.</li>
</ul>
</li>
<li><strong>DeepSeek Distills Get Dunked On</strong>: Members largely agreed that the <strong>DeepSeek-R1-Distill-Qwen-32B model</strong> distill models are pretty bad and not worth using.
<ul>
<li>The original, undistilled models are considered good, with one member suggesting to stick with <strong>Qwen 3 30B 2507</strong>.</li>
</ul>
</li>
<li><strong>Flashy GLM4.7 Arrives On The Scene</strong>: <strong>GLM 4.7 flash</strong> is available, according to <a href=""https://x.com/lmstudio/status/2013339758139789389?s=20"">LM Studio's tweet</a>, prompting downloads and tests.
<ul>
<li>However, one user with 32gb ram + 6gb vram was disappointed by its size.</li>
</ul>
</li>
<li><strong>Used 3090 Prices On The Rise</strong>: The price of used <strong>3090s</strong> has increased on eBay, with one user noting a jump from <strong>€850</strong> to over <strong>€950</strong>.
<ul>
<li>One user touted their <strong>5090</strong>, bought last August for <strong>£2000</strong>, which is now listed at <strong>£2659.99</strong> by the same vendor.</li>
</ul>
</li>
<li><strong>Chat Templates Impact Interleaved Thinking</strong>: It was suggested that <strong>chat templates</strong> might be filtering out reasoning content in models like <strong>Qwen3</strong>, preventing <strong>interleaved thinking</strong> in agentic coding extensions.
<ul>
<li>Models such as <strong>GLM4.7 flash</strong> have a <em>clear_thinking toggle</em> in their template that removes the thinking content unless it's set to false.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/974519864045756446"">OpenAI</a> Discord</h2>
<ul>
<li><strong>OpenAI Guesstimates User Ages</strong>: OpenAI is implementing age prediction in <strong>ChatGPT</strong> to identify users under <strong>18</strong>, applying appropriate safeguards and restrictions as outlined in <a href=""https://openai.com/index/our-approach-to-age-prediction/"">this blogpost</a>.
<ul>
<li>Adults misclassified can confirm their age in <strong>Settings > Account</strong>, rolling out globally, with the EU to follow.</li>
</ul>
</li>
<li><strong>Nothing Phone Offers Unremarkable Assistant</strong>: <strong>ChatGPT</strong> integration on <strong>Nothing Phones</strong> via <strong>Nothing OS</strong> is functionally similar to other digital assistants like <strong>Gemini</strong>, <strong>Perplexity</strong>, or <strong>Bixby</strong>, requiring the app and acting as a default assistant.
<ul>
<li>A screenshot showed <strong>ChatGPT</strong> set as the default assistant, but one member dismissed it as <em>nothing special</em>.</li>
</ul>
</li>
<li><strong>Google's Gemini Pro Under Scrutiny</strong>: A member stated that <strong>Google's Gemini AI Pro</strong> has a stricter policy, which can result in the AI misunderstanding requests, and refusing to generate answers due to perceived violations of its guidelines.
<ul>
<li>The member found this behavior disappointing because <strong>ChatGPT</strong> sometimes lacks contextual understanding as well.</li>
</ul>
</li>
<li><strong>Markdown Meme Mania</strong>: A meme trend highlighted AI's propensity for generating markdown files, particularly with <strong>Claude</strong>, leading to jokes about <em>vibe coding</em>.
<ul>
<li>A past developer challenge submission, consisting of a single <strong>.md</strong> file explaining a <em>non-existent incredible idea</em>, was humorously referenced.</li>
</ul>
</li>
<li><strong>GPT 4.1 Mini Dumbed Down?</strong>: A user reported degraded performance in <strong>GPT-4.1 Mini</strong> for voicebots, seeking a similarly priced alternative because it <em>feels like its very dumb now</em>.
<ul>
<li>The user is looking for suggestions based on experiences with other models in the same cost range.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1161519468141355160"">DSPy</a> Discord</h2>
<ul>
<li><strong>DSPy's RLM silently slips into Release</strong>: DSPy <strong>3.1.2</strong> introduces <code>dspy.RLM</code>, expanding one-liner operations initially promised in the DSPy 3.0 release, according to <a href=""https://x.com/isaacbmiller1/status/2013371005960401327"">this tweet</a>.
<ul>
<li>Enthusiastic members reacted, with one saying they <em>“about ruined my monitor by spitting coffee on them this morning when I saw it silently drop.”</em></li>
</ul>
</li>
<li><strong>Deno defends Local WASM Runtime</strong>: DSPy selected <strong>Deno</strong> for its local sandbox/interpreter, based on <a href=""https://til.simonwillison.net/deno/pyodide-sandbox"">Simon Willison's blog post</a>, providing a secure WASM runtime.
<ul>
<li>Praised as a <em>“gooooood solution, we stan pyodide ❤️,”</em> Deno's security features were a key factor in its selection.</li>
</ul>
</li>
<li><strong>RLM outshines Claude in documentation</strong>: <code>dspy.RLM</code> is capable of writing documentation from code and excels due to its ability to handle extremely long outputs.
<ul>
<li>A community member jested that <em>“It would be frickin meta if you used RLM to write its own docs 😂,”</em> suggesting RLM could write its own documentation.</li>
</ul>
</li>
<li><strong>RLM Externalizes Long Context</strong>: <code>dspy.RLM</code> manages long context by <strong>externalizing the context</strong> to a file system, programmatically accessing parts as needed.
<ul>
<li>Unlike <strong>Claude Code</strong>, which uses <strong>compaction</strong> and may lose information, RLM avoids exposing the entire prompt or context to the AI at once.</li>
</ul>
</li>
<li><strong>Elixir achieves perfect RLM</strong>: An author working on an <strong>Elixir port of DSPy</strong>, including a pooler/session manager and <strong>FFI for Python</strong> from Elixir, shared their progress.
<ul>
<li>A working <strong>RLM example</strong> achieves perfect results using <code>gemini-flash-lite-latest</code> from Elixir, available <a href=""https://github.com/nshkrdotcom/DSPex/tree/main/examples/rlm"">on GitHub</a>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/879548962464493619"">HuggingFace</a> Discord</h2>
<ul>
<li><strong>DDR4 limits Phi-4 performance</strong>: A user discovered that <strong>DDR4</strong> has a limited bandwidth of <strong>25GB/s</strong> per channel, theoretically capping <strong>Phi-4 (Q4)</strong> performance to around <strong>3.125 tok/s</strong> when attempting to self-host a <strong>14B model</strong>.
<ul>
<li>Another member stated that the original user's reported speed of <strong>3.7 tokens/s</strong> was actually quite fast.</li>
</ul>
</li>
<li><strong>Text Becomes Solvable Optimization</strong>: Members discussed the process of turning text into a <strong>mathematical optimization problem</strong>, breaking it down into subproblems, and solving them separately through <strong>parsing relations</strong>, <strong>creating variables and constraints</strong>, and <strong>defining an energy function</strong>.
<ul>
<li>It was suggested these subproblems can be merged via <strong>ADMM</strong> (Alternating Direction Method of Multipliers) / Message Passing.</li>
</ul>
</li>
<li><strong>Orkes Orchestrates Hackable Agents</strong>: A member introduced <strong>Orkes</strong>, an <a href=""https://github.com/hfahrudin/orkes"">open-source framework</a> for <strong>Agentic Orchestration</strong> built with a <strong>DAG</strong> approach, that provides full control and visibility over agent logic.
<ul>
<li>Orkes emphasizes <strong>hackability</strong>, <strong>transparency</strong>, and a <strong>lightweight</strong> design; documentation is <a href=""https://orkes.readthedocs.io/"">available here</a>.</li>
</ul>
</li>
<li><strong>LaaLM Simulates Linux Terminal</strong>: A member announced <strong>LaaLM-exp-v1</strong>, an experimental <strong>AI model</strong> simulating a <strong>Linux terminal</strong>, trained on conversations to remember previous file operations, and is available on <a href=""https://huggingface.co/ereniko/LaaLM-exp-v1"">Hugging Face</a>.
<ul>
<li>With LaaLM-v1, the model could already do most tasks, but it didn't remember anything since it wasn't conversation-tuned so it couldn't remember file operations from before.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1189498204333543425"">GPU MODE</a> Discord</h2>
<ul>
<li><strong>YALI claims Low-Latency NVLink AllReduce</strong>: A user introduced <strong>YALI</strong>, a 2-GPU <strong>NVLink AllReduce library</strong> that purportedly outperforms <strong>NVIDIA NCCL</strong> by <strong>1.2x-2.4x</strong> with <em>50x+ more stable tail latency</em>, and is available on <a href=""https://github.com/Venkat2811/yali"">GitHub</a>.
<ul>
<li>The author claims that <em>YALI guards GPU efficiency by obsessively overlapping ops and compute</em> and offers flash / stream mode for latency / throughput priority, and the name <strong>YALI</strong> comes from <em>a composite creature from Tamil and South Indian temple architecture</em>.</li>
</ul>
</li>
<li><strong>Torch is Drowning in AI-Generated PRs</strong>: Members noted that <strong>torch</strong> is being inundated with <strong>AI-generated PRs</strong> from people who make no effort to understand what they're submitting and the team is considering using <strong>Claude</strong> to prefilter.
<ul>
<li>Members discussed that <strong>Pangram</strong> is good at detecting text <strong>AI generation</strong>, but it doesn't work for <strong>PRs</strong> or code.</li>
</ul>
</li>
<li><strong>Runpod B200 Serverless Deployed</strong>: A member created a repo to deploy a serverless instance with a <strong>B200 on Runpod</strong>, allowing users to submit and pay for total usage instead of hourly, for the <strong>nvidia-competition</strong> channel.
<ul>
<li>Several users reported receiving a <code>Failed to trigger GitHub Action</code> error when submitting to the <code>nvfp4_group_gemm</code> competition using <code>popcorn-cli</code>.</li>
</ul>
</li>
<li><strong>FTZ Modifier Boosts Performance</strong>: The <a href=""https://developer.nvidia.com/ptx-compiler-driver"">PTX instruction <code>rcp.approx.ftz.f32</code></a> compiles to one instruction (<code>MUFU.RCP</code>) whereas <code>rcp.approx.f32</code> produces 7 extra instructions, improving performance, according to members.
<ul>
<li>Without <strong>ftz</strong>, smaller subnormal values result in <strong>INF</strong> because their reciprocal is too large to represent.</li>
</ul>
</li>
<li><strong>OSS Contributions > Internships</strong>: Looking at <strong>PyTorch</strong> junior hiring, <strong>OSS contributions</strong> are king, according to a member and a member assessed another member's commits to <strong>MLIR codebases</strong> and contributions to the <strong>TPU-inference repo</strong> for <strong>vLLM</strong>, deeming them <em>more than okay</em> in terms of employability.
<ul>
<li>The member should be able to get a <strong>ML compiler</strong>/<strong>engine</strong> role, such as <strong>vLLM</strong>, <strong>SGLang</strong>, or <strong>trtLLM</strong>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/822583790773862470"">Latent Space</a> Discord</h2>
<ul>
<li><strong>Anthropic Explores Assistant Demise</strong>: Anthropic released research investigating the 'Assistant' persona in language models, and what happens when that persona fades, via <a href=""https://x.com/anthropicai/status/2013356793477361991"">this tweet</a>.
<ul>
<li>Community members believe this research could bring controls to <em>tweak how much you want to lean into a persona similar to temperature</em>.</li>
</ul>
</li>
<li><strong>Yegge Jettisons Sourcegraph to Join Gastown</strong>: Steve Yegge is reportedly focusing on <strong>Gastown</strong> after leaving Sourcegraph, according to his latest <a href=""https://steve-yegge.medium.com/steveys-birthday-blog-34f437139cb5"">birthday post</a>.
<ul>
<li>While some quipped <em>Man he’s lost the plot lol</em> while others claimed he was fired a while ago, Yegge has not publicly commented.</li>
</ul>
</li>
<li><strong>CLI Triumphantly Treks Back</strong>: Anjney Midha highlighted a Wall Street Journal feature (<a href=""https://x.com/anjneymidha/status/2013257507532079472"">tweet</a>) on the return of <strong>command line interfaces</strong> for mainstream users.
<ul>
<li>The article suggests that business leaders need to adjust their operational models to stay competitive in this changing technological landscape, as demonstrated in <a href=""https://youtu.be/Z3D2UmAesN4?si=gDUJUnNQCOCKnpud"">this YouTube video</a>.</li>
</ul>
</li>
<li><strong>Humans&#x26; Harvests Hyped Help</strong>: Andi Peng announced the launch of <strong>humans&#x26;</strong>, a new venture co-founded with Eric Zelikman, Noah Goodman, George Harik, and Yuchen He (<a href=""https://x.com/TheAndiPenguin/status/2013641591408263611"">tweet</a>).
<ul>
<li>Community members reacted with enthusiasm and humor, joking <em>new polycule dropped</em>.</li>
</ul>
</li>
<li><strong>Runpod Rockets to $120M ARR</strong>: AI cloud startup <strong>Runpod</strong> hits <strong>$120M</strong> in ARR, which started with a Reddit post (<a href=""https://techcrunch.com/2026/01/16/ai-cloud-startup-runpod-hits-120m-in-arr-and-it-started-with-a-reddit-post/"">TechCrunch article</a>).
<ul>
<li>A community member noted that they are a <em>friend of the company if applying / want referral</em>, and linked to a relevant <a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qib2ks/runpod_hits_120m_arr_four_years_after_launching/"">Reddit post</a>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1053877538025386074"">Nous Research AI</a> Discord</h2>
<ul>
<li><strong>Nous Exposes MoE Training Roadblocks</strong>: Nous Research posted <a href=""https://nousresearch.com/moe-scaling-field-notes/"">field notes</a> from &#x3C;@930102195330900009> on hunting down <strong>MoE training bottlenecks</strong>.
<ul>
<li>The blog post details insights into the challenges and solutions encountered during <strong>MoE training</strong>.</li>
</ul>
</li>
<li><strong>User Fixation on ChatGPT triggers debate</strong>: Some members joked that focusing too much on <strong>ChatGPT</strong> can cause a kind of <strong>psychosis</strong>, comparing it satirically to the <strong>tobacco industry</strong>'s manipulative tactics.
<ul>
<li>However, other members argued that <strong>LLMs</strong> are no worse than any other type of software and that <strong>open-source models</strong> are needed to balance out the closed-source problems.</li>
</ul>
</li>
<li><strong>Luminal Kernelbench V3 and LLM-Driven Kernel Engineering</strong>: Members discussed whether a <strong>kernel compiler</strong> like <a href=""https://forum.nousresearch.com/t/can-kernel-compiler-like-luminal-kernelbench-v3-enable-llm-driven-sota-kernel-engineering/310?u=ighoshsubho"">Luminal Kernelbench V3</a> could enable <strong>LLM-driven SOTA kernel engineering</strong>.
<ul>
<li>The forum post discusses the potential <strong>implications of LLM-driven SOTA kernel engineering</strong>, and whether it has the potential to change it.</li>
</ul>
</li>
<li><strong>KV Cache Compatibility Depends on Architecture</strong>: It was mentioned that <strong>KV cache compatibility</strong> depends on different models sharing <em>more or less the same architecture</em>.
<ul>
<li>The discussion emphasized that compatibility relies on maintaining a similar architecture foundation across different models.</li>
</ul>
</li>
<li><strong>Interest Sparked on Intel's Loihi 2</strong>: A member shared interest in <strong>Intel's Loihi 2</strong>, and pointed to its brain-like architecture and the <strong>matmul</strong> experiment.
<ul>
<li>The experiment resulted in more efficient <strong>throughput and energy consumption</strong>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/714501525455634453"">Yannick Kilcher</a> Discord</h2>
<ul>
<li><strong>Devstral and GLM Enter Coding Arena</strong>: Members discussed good open source coding agents for self-hosted models, mentioning <strong>Devstral 2 Small</strong> (24B dense) and <strong>GLM 4.7 Flash</strong> (30B-3A Moe) as options.
<ul>
<li>One user said that <strong>GLM 4.7 Flash</strong> is <em>on paper really good</em>, but hasn't been tested with <em>llama.ccp</em> yet.</li>
</ul>
</li>
<li><strong>Devstral 2 Medium Rivals Claude Sonnet 4.5</strong>: <strong>Devstral 2 Medium</strong> is apparently on par with <strong>Claude Sonnet 4.5</strong>, according to <a href=""https://mistral.ai/news/devstral-2-vibe-cli"">this news post</a>.
<ul>
<li><strong>Kilo Code</strong> is a VS Code extension that can plug in local models, like a locally hosted <strong>Devstral 2</strong> from <a href=""https://huggingface.co/mistralai/Devstral-2-123B-Instruct-2512"">HuggingFace</a>.</li>
</ul>
</li>
<li><strong>Recursive LLMs Go Beyond RAG?</strong>: A thread discussed a paper about recursive LLMs, contesting the label of <em>RAG</em> because the LLM can manipulate a Python environment with a prompt.
<ul>
<li>The commentator said this is <em>a bit more than RAG, but not as groundbreaking as some clickbait videos suggest</em>, wanting to see shorter context benchmark performance.</li>
</ul>
</li>
<li><strong>Anthropic Explores Assistant Axis</strong>: A member shared a link to <a href=""https://www.anthropic.com/research/assistant-axis"">Anthropic's research on the Assistant Axis</a>.
<ul>
<li>No further details were given.</li>
</ul>
</li>
<li><strong>Akira Scene-for-Scene vid2vid Version Announced</strong>: <strong>Higgsfield</strong> is sponsoring a scene for scene vid2vid version of <strong>Akira</strong>, planned for completion in <strong>2027</strong>.
<ul>
<li>The announcement received mixed reviews due to anti-AI sentiment, with some finding it odd that the characters aren't Japanese.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/729741769192767510"">Eleuther</a> Discord</h2>
<ul>
<li><strong>Engineers Grapple with Agent Evaluation</strong>: Engineers are seeking methods to automate <strong>agent evaluation</strong> to reduce manual costs, focusing on <strong>transparency</strong>, <strong>reliability</strong>, <strong>honesty</strong>, and minimizing user friction.
<ul>
<li>A member suggested that the team is looking for <strong>""LLM as judge"" workflows</strong>, but needs to evaluate data quality before attempting full automation.</li>
</ul>
</li>
<li><strong>Open Weights Models Face Multiple Choice Evals</strong>: Researchers are seeking multiple-choice evaluation results for <strong>open weights models</strong> like <strong>Llama 7B</strong>, <strong>13B</strong>, and <strong>70B</strong>, performing each question 100 times to determine the probability of correct answers.
<ul>
<li>They clarified that the answers should be pre-w...</li>
</ul>
</li>
</ul>
","{""title"":""not much happened today"",""link"":""https://news.smol.ai/issues/26-01-20-not-much/"",""pubDate"":""Tue, 20 Jan 2026 05:44:39 GMT"",""content:encoded"":""<p><strong>a quiet day</strong></p>\n<blockquote>\n<p>AI News for 1/19/2026-1/20/2026. We checked 12 subreddits, <a href=\""https://twitter.com/i/lists/1585430245762441216\""><strong>544</strong> Twitters</a> and <strong>24</strong> Discords (<strong>205</strong> channels, and <strong>5901</strong> messages) for you. Estimated reading time saved (at 200wpm): <strong>452 minutes</strong>. <strong>Our new website</strong> is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on <a href=\""https://x.com/Smol_AI\"">@smol_ai</a>!</p>\n</blockquote>\n<hr>\n<h1>AI Twitter Recap</h1>\n<p><strong>Open-sourcing platform algorithms: X “For You” recommender goes public</strong></p>\n<ul>\n<li><strong>X Engineering open-sources the X algorithm (Grok-style transformer recommender)</strong>: X says it has <strong>open-sourced its new algorithm</strong> (the ranking/recommendation stack), “powered by the same transformer architecture as xAI’s Grok model,” with code on GitHub (<a href=\""https://twitter.com/XEng/status/2013471689087086804\"">XEng</a>). The release sparked immediate community reactions—both optimistic (“now anyone can ‘ask’ how a major platform algo works”) (<a href=\""https://twitter.com/DavidSHolz/status/2013522548642980290\"">David Holz</a>) and adversarial (“I’m fixing it”) (<a href=\""https://twitter.com/Yuchenj_UW/status/2013501949333905919\"">Yuchenj_UW</a>).</li>\n<li><strong>Early reverse-reading of the system diagram</strong>: One summary notes the high-level architecture isn’t shocking: <strong>candidate generation isolation</strong>, “no content features,” and heavy emphasis on <strong>out-of-network discovery</strong> (<a href=\""https://twitter.com/nearcyan/status/2013527283399545064\"">nearcyan</a>), plus skepticism about “it uses a transformer” being oversold as Grok “reading every post” (<a href=\""https://twitter.com/nearcyan/status/2013527810946519375\"">nearcyan</a>). Another meta take: the product drift from a “following feed” to “generic slop” is a predictable incentive outcome (<a href=\""https://twitter.com/nearcyan/status/2013528777360298082\"">nearcyan</a>).</li>\n<li><strong>Operational/user impact narrative</strong>: Alongside the code drop, creators complain about sudden reach suppression (“reach is nuked”) (<a href=\""https://twitter.com/giffmana/status/2013509540843606156\"">giffmana</a>), reinforcing the engineering/UX tension: algorithmic transparency doesn’t automatically translate to perceived fairness.</li>\n</ul>\n<p><strong>Open weights &#x26; local inference: GLM-4.7-Flash momentum and KV-cache realities</strong></p>\n<ul>\n<li><strong>GLM-4.7-Flash becomes the “local workhorse” candidate</strong>: Multiple tweets highlight strong performance-per-parameter for <strong>GLM-4.7-Flash (30B-A3B)</strong>. Benchmarks and anecdotal evaluations suggest it’s competitive enough to displace larger local defaults (<a href=\""https://twitter.com/sam_paech/status/2013476096269000763\"">sam_paech</a>). Unsloth pushes a clear “run locally” story: <strong>200K context</strong>, claims of best <strong>30B</strong> on <strong>SWE-Bench and GPQA</strong>, and “run local with <strong>24GB RAM</strong>,” plus GGUF packaging (<a href=\""https://twitter.com/UnslothAI/status/2013482180564132092\"">UnslothAI</a>).</li>\n<li><strong>Systems detail: MLA / KV-cache cost dominates</strong>: The thread around GLM-4.7-Flash emphasizes that <strong>KV cache memory</strong> can dominate earlier than many expect, and that <strong>MLA isn’t free</strong>—running MLA models in naïve MHA regimes can explode cache usage (<a href=\""https://twitter.com/teortaxesTex/status/2013626183330439348\"">teortaxesTex</a>). A concrete debugging question: why vLLM shows ~<strong>1MB/token</strong> context cost for GLM-4.7-Flash under naïve MHA vs a claimed first-principles <strong>~54KB</strong> (<a href=\""https://twitter.com/teortaxesTex/status/2013467545882235256\"">teortaxesTex</a>).</li>\n<li><strong>Quantization behavior &#x26; mitigation</strong>: Unsloth reports <strong>looping issues</strong> in quantized GLM-4.7-Flash and suggests tuning <strong><code>--dry-multiplier 1.1</code></strong>, using higher quality quants (e.g., <strong>UD-Q4_K_XL+</strong>), and adding more <strong>tool-calling data during calibration</strong> (<a href=\""https://twitter.com/danielhanchen/status/2013496370880008395\"">danielhanchen</a>).</li>\n<li><strong>Local throughput engineering</strong>: exo labs demonstrates <strong>tensor parallel GLM-4.7-Flash on 4× M4 Pro Mac Minis</strong>, using RDMA over Thunderbolt + MLX backend, hitting <strong>~100 tok/s</strong> with a target of <strong>~200 tok/s</strong> (<a href=\""https://twitter.com/alexocheema/status/2013694573910937980\"">alexocheema</a>).</li>\n<li><strong>GLM ecosystem spillover</strong>: A lighter but notable signal: devs are already “one-shotting” small projects locally (e.g., a Mario game via Claude Code + Ollama running GLM-Flash) (<a href=\""https://twitter.com/nopmobiel/status/2013530965516173448\"">nopmobiel</a>). GLM-Image also lands on the image leaderboard (#8 among open models in that snapshot) (<a href=\""https://twitter.com/arena/status/2013783860023062990\"">arena</a>).</li>\n</ul>\n<p><strong>Reasoning &#x26; training research: societies of thought, multiplex tokens, distillation, and compute allocation</strong></p>\n<ul>\n<li><strong>“Societies of Thought” as the mechanism behind reasoning traces</strong>: A widely shared Google AI paper claim: performance of reasoning models (OpenAI o-series, DeepSeek-R1, QwQ) is not just “think longer,” but the emergence of <strong>internal debate patterns</strong>—questioning steps, exploring alternatives, disagreement, and convergence—measurably mediating accuracy gains (reported <strong>20%+</strong> of advantage) (<a href=\""https://twitter.com/rohanpaul_ai/status/2013431689889095767\"">rohanpaul_ai</a>).</li>\n<li><strong>Multiplex Thinking (branch-and-merge tokens)</strong>: The “Multiplex Thinking” paper proposes sampling <strong>K tokens per step into one multiplex token</strong>, adaptive to uncertainty; confident steps behave like CoT while uncertain steps represent multiple paths, achieving better results with <strong>shorter sequences</strong> (<a href=\""https://twitter.com/HuggingPapers/status/2013524300800627119\"">HuggingPapers</a>, <a href=\""https://twitter.com/_akhaliq/status/2013629394804179422\"">akhaliq</a>).</li>\n<li><strong>Distillation via logistic/ranking loss</strong>: A practical distillation nugget: instead of KL/SFT, you can train students to <strong>preserve teacher token rankings</strong> via a logistic loss over token pairs mined from the teacher’s top-K logits—framed as a clean PyTorch exercise and linked to DistillKit (<a href=\""https://twitter.com/cwolferesearch/status/2013468452774645876\"">cwolferesearch</a>, <a href=\""https://twitter.com/cwolferesearch/status/2013468538728513634\"">cwolferesearch</a>).</li>\n<li><strong>Synthetic reasoning data: “sample more, not bigger”</strong>: A DeepMind result summary argues that <strong>smaller models can produce better synthetic reasoning data under compute-matched sampling</strong>: cheaper models generate more attempts, boosting <strong>coverage</strong> (+11%) and <strong>diversity</strong> (+86%), yielding training gains reported up to <strong>31.6%</strong> under the same inference budget (<a href=\""https://twitter.com/LiorOnAI/status/2013582631124771104\"">LiorOnAI</a>).</li>\n<li><strong>RL compute scaling guidance</strong>: A separate RL-on-LLMs thread claims <strong>optimal compute allocation</strong> in LLM RL “scales predictably,” aiming to provide the missing equivalent of pretraining scaling laws for RL fine-tuning budgets (<a href=\""https://twitter.com/ChengZhoujun/status/2013686575499223474\"">ChengZhoujun</a>).</li>\n<li><strong>NanoGPT “speedrun” optimization</strong>: A notable hacker-ish result: new NanoGPT speedrun record <strong>~99.3s</strong> using a <strong>bigram hash embedding</strong> added to the residual stream before every layer (inspired by Hash Embeddings and DeepSeek Engram), plus a provocative token/parameter ratio deviation from Chinchilla norms (<a href=\""https://twitter.com/classiclarryd/status/2013520088297558274\"">classiclarryd</a>).</li>\n</ul>\n<p><strong>Agents in production: RLMs, trace analytics, “boring agents,” and agent frameworks</strong></p>\n<ul>\n<li><strong>Recursive Language Models (RLMs) as compute/context management</strong>: Several tweets frame RLMs as a promising abstraction for <strong>long-running systems</strong>—not just “bigger context,” but a way to manage <strong>computation, recursion, and selective reading</strong> (<a href=\""https://twitter.com/doesdatmaksense/status/2013534540300722278\"">doesdatmaksense</a>). A key claimed advantage is <strong>symbolic recursion</strong>: the model can commission many sub-reads/edits without emitting every intermediate as tokens, avoiding context-window blowups typical of sub-agent prompting (<a href=\""https://twitter.com/lateinteraction/status/2013662243167088776\"">lateinteraction</a>, <a href=\""https://twitter.com/lateinteraction/status/2013663944066379841\"">lateinteraction</a>). (Mainstream coverage also appears, but the technical thread is centered on context economics and recursion.)</li>\n<li><strong>Trace understanding becomes a first-class product requirement</strong>: LangChain pushes the idea that with <strong>100K+ daily traces</strong>, classic monitoring and manual log review don’t work; you need <strong>clustering/pattern discovery</strong> over traces via an “Insights Agent” (<a href=\""https://twitter.com/LangChain/status/2013642970944413905\"">LangChain</a>, <a href=\""https://twitter.com/hwchase17/status/2013662250167652491\"">hwchase17</a>). The meta-lesson echoed by practitioners: evals are like unit tests—useful but bounded—production traces reveal unknown unknowns (<a href=\""https://twitter.com/samecrowder/status/2013696879083634789\"">samecrowder</a>).</li>\n<li><strong>Agent “swarm fallacy” and structured execution</strong>: AI21 highlights that parallel agents are easy only when read-only; once agents mutate files or act in the world, coordination/consistency becomes the hard part—arguing for structured execution and test-time compute rather than “just add agents” (<a href=\""https://twitter.com/AI21Labs/status/2013582278845440055\"">AI21Labs</a>).</li>\n<li><strong>Framework/tooling churn &#x26; interoperability</strong>: A set of infra/toolchain notes: Artificial Analysis updates <strong>Stirrup</strong> with browser-use and <strong>Open Responses</strong> compatibility (provider-agnostic agent clients) (<a href=\""https://twitter.com/ArtificialAnlys/status/2013612928117940293\"">ArtificialAnlys</a>). CopilotKit adds frontend middleware for LangChain “Deep Agents” (human-in-the-loop, generative UI, shared state) to move agent backends into full-stack apps (<a href=\""https://twitter.com/CopilotKit/status/2013636626623443110\"">CopilotKit</a>). FastMCP ships a major re-architecture for “next generation of MCP applications” (<a href=\""https://twitter.com/jlowin/status/2013651883647209520\"">jlowin</a>).</li>\n<li><strong>Pragmatic “agents work if your codebase isn’t a mess”</strong>: A clear production heuristic: AI coding tools amplify existing engineering hygiene—teams with tests/docs fly; messy codebases become messier faster (<a href=\""https://twitter.com/svpino/status/2013608715933581586\"">svpino</a>). Another note from enterprise adoption: year-2+ buyers are reassessing ROI; “worst engineers have the biggest AI bills” and ship buggier code (<a href=\""https://twitter.com/TheEthanDing/status/2013465333714055670\"">TheEthanDing</a>).</li>\n</ul>\n<p><strong>Small models &#x26; edge deployment: on-device reasoning, browser voice, OCR, and Jetson CLIP</strong></p>\n<ul>\n<li><strong>Liquid AI’s LFM2.5-1.2B-Thinking</strong>: Liquid releases an on-device reasoning model positioned around <strong>concise reasoning traces</strong> and <strong>~900MB memory footprint</strong> (i.e., phone-class hardware), emphasizing tool use/math/instruction-following (<a href=\""https://twitter.com/liquidai/status/2013633347625324627\"">liquidai</a>, <a href=\""https://twitter.com/maximelabonne/status/2013631295172084168\"">maximelabonne</a>). Ollama quickly adds it to their model library for broad integration (<a href=\""https://twitter.com/ollama/status/2013711111590150590\"">ollama</a>).</li>\n<li><strong>Kyutai voice model in-browser</strong>: A notable “deployment feat” demo: running a <strong>~100M parameter</strong> voice model in the browser with <strong>pure JavaScript + WebGPU</strong> (jax-js), highlighting low dependency friction and practical voice cloning flexibility (<a href=\""https://twitter.com/ekzhang1/status/2013455049175748791\"">ekzhang1</a>).</li>\n<li><strong>OCR and document agents continue to get cheaper</strong>: LightOn releases a <strong>1B OCR model</strong> under <strong>Apache-2.0</strong>, claiming strong speed/cost characteristics (e.g., “&#x3C;$0.01 per 1k pages”) and day-0 transformers support (<a href=\""https://twitter.com/mervenoyann/status/2013577704419819942\"">mervenoyann</a>). Separately, “document processing” is positioned as a dominant enterprise agent workflow substrate (especially in financial services) (<a href=\""https://twitter.com/jerryjliu0/status/2013695214008049890\"">jerryjliu0</a>).</li>\n<li><strong>Edge multimodal embeddings</strong>: Weaviate adds CLIP inference support on <strong>NVIDIA Jetson</strong> for local multimodal embedding/search pipelines, enabling text-image retrieval without cloud round-trips (<a href=\""https://twitter.com/philipvollet/status/2013630649492468041\"">philipvollet</a>).</li>\n</ul>\n<p><strong>Governance, safety, and the Davos narrative (AI leadership, alignment trends, safeguards)</strong></p>\n<ul>\n<li><strong>Amodei vs Hassabis: “scientist-led” governance framing</strong>: Multiple Davos quotes compare “scientist-led” labs vs “social media entrepreneur” leadership styles, explicitly linking incentives (ads/engagement vs responsibility) to safety posture (<a href=\""https://twitter.com/scaling01/status/2013651299519074729\"">scaling01</a>). Hassabis echoes a “full-stack” advantage narrative for DeepMind and highlights physical intelligence/robotics as near-term breakthroughs (<a href=\""https://twitter.com/scaling01/status/2013718310194475379\"">scaling01</a>). He also indicates he’d support a pause <em>if globally coordinated</em> (<a href=\""https://twitter.com/emilychangtv/status/2013726877706313798\"">emilychangtv</a>).</li>\n<li><strong>Alignment trend signal</strong>: Jan Leike reports an apparent downward trend in automated-audit “misaligned behavior” across <strong>Anthropic, GDM, and OpenAI</strong> through 2025 (<a href=\""https://twitter.com/janleike/status/2013669924950970781\"">janleike</a>). (No methodology details are in-tweet, but it’s a notable directional claim.)</li>\n<li><strong>OpenAI rolls out age prediction for ChatGPT</strong>: OpenAI announces global rollout of <strong>age prediction</strong> to detect likely under-18 accounts and apply teen safeguards, with an adult override via verification; EU rollout later (<a href=\""https://twitter.com/OpenAI/status/2013688237772898532\"">OpenAI</a>). This triggered predictable skepticism about ulterior motives (“ads strategy”) (<a href=\""https://twitter.com/scaling01/status/2013688152750215500\"">scaling01</a>).</li>\n<li><strong>Altman on guardrails tradeoffs</strong>: Sam Altman argues safety is “tragic and complicated,” emphasizing protecting fragile users while keeping tools broadly useful, and draws parallels to other safety-critical tech deployments (<a href=\""https://twitter.com/sama/status/2013703158459978076\"">sama</a>).</li>\n</ul>\n<p><strong>Top tweets (by engagement)</strong></p>\n<ul>\n<li><strong>X algorithm open-sourced</strong> — <a href=\""https://twitter.com/XEng/status/2013471689087086804\"">XEng</a></li>\n<li><strong>OpenAI: ChatGPT age prediction rollout</strong> — <a href=\""https://twitter.com/OpenAI/status/2013688237772898532\"">OpenAI</a></li>\n<li><strong>Unsloth: run GLM-4.7-Flash locally (24GB RAM, 200K ctx)</strong> — <a href=\""https://twitter.com/UnslothAI/status/2013482180564132092\"">UnslothAI</a></li>\n<li><strong>Liquid AI: LFM2.5-1.2B Thinking on-device reasoning model</strong> — <a href=\""https://twitter.com/liquidai/status/2013633347625324627\"">liquidai</a></li>\n</ul>\n<hr>\n<h1>AI Reddit Recap</h1>\n<h2>/r/LocalLlama + /r/localLLM Recap</h2>\n<h3>1. GLM 4.7 Flash Developments</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/\"">My gpu poor comrades, GLM 4.7 Flash is your local agent</a></strong> (Activity: 743): <strong>The post discusses the performance of <strong>GLM 4.7 Flash</strong>, a model that has shown reliability in an agentic framework, unlike other MoE models under <code>30B</code> parameters. The user reports running it for over half an hour on <strong>opencode</strong>, generating hundreds of thousands of tokens without errors, and successfully executing tasks like cloning GitHub repos and editing files. The user anticipates trying it locally with <strong>GGUFs</strong>. A notable update is that the model's PR was merged into <strong>llama.cpp</strong>, indicating broader accessibility and integration.</strong> A commenter is interested in a comparison with <strong>Nemotron 30b</strong>, while another notes that the model runs decently fast on a <code>4090</code> GPU, though it tends to 'think deeply', suggesting a trade-off between speed and processing depth.</p>\n<ul>\n<li>The integration of GLM 4.7 Flash into <code>llama.cpp</code> has been confirmed with a recent pull request merge. Users are testing the model locally, and it is noted that the Q4_K_M variant runs efficiently on an NVIDIA 4090 GPU, although it tends to engage in deep thinking processes, which might affect response times.</li>\n<li>A user has provided a benchmark comparison indicating that GLM 4.7 Flash, particularly in the MXFP4_MOE-GGUF configuration, might offer performance comparable to SEED OSS 36B. However, it benefits from significantly improved performance metrics due to the use of Mixture of Experts (MoE) architecture, which optimizes computational efficiency.</li>\n<li>A link to a Hugging Face model repository is shared, showcasing the GLM-4.7-Flash-MXFP4_MOE-GGUF model. This suggests that the model is accessible for further testing and evaluation by the community, allowing for broader performance and quality assessments.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/\"">GLM 4.7 Flash official support merged in llama.cpp</a></strong> (Activity: 477): <strong>The <code>llama.cpp</code> repository has merged support for the <strong>GLM 4.7 Flash</strong> model, specifically the <code>Glm4MoeLiteForCausalLM</code>, which is a renamed and restructured version of <strong>DeepseekV3</strong>. This integration was a community-driven effort, not directly from <strong>Z.ai</strong> developers, and it enhances the framework's capabilities by incorporating references to <strong>Hugging Face's</strong> GLM-4.7-Flash model. The model is available on <a href=\""https://huggingface.co/noctrex/GLM-4.7-Flash-MXFP4_MOE-GGUF\"">Hugging Face</a>.</strong> The community appreciates the quick integration into <code>llama.cpp</code>, noting it was faster than attempts with <strong>VLLm</strong>. There is also a clarification that the term 'official' refers to the model's proper functionality within <code>llama.cpp</code>, not an endorsement by <strong>Z.ai</strong>.</p>\n<ul>\n<li>The integration of GLM 4.7 Flash into <code>llama.cpp</code> is a community-driven effort, not an official release by Z.ai developers. This highlights the collaborative nature of open-source projects where community contributions play a significant role in enhancing software capabilities.</li>\n<li>A user reported that using flash-attention with GLM 4.7 Flash on CUDA results in slower performance, suggesting that disabling flash-attention (<code>-fa 0</code>) can lead to a 3x speed improvement. This indicates potential performance issues with flash-attention in certain configurations, prompting users to experiment with settings for optimal performance.</li>\n<li>The model's response time is criticized for being excessively slow, with one user noting that it takes several minutes to generate a simple response. This suggests potential inefficiencies in the model's processing or implementation that could be addressed to improve usability.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qhlnsv/unsloth_glm_47flash_gguf/\"">Unsloth GLM 4.7-Flash GGUF</a></strong> (Activity: 314): <strong>The release of <strong>GLM-4.7-Flash GGUF</strong> on <a href=\""https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF\"">Hugging Face</a> is accompanied by specific recommendations for optimal performance, such as using <code>UD-Q4_K_XL</code> quantization and specific parameters like <code>--temp 0.2 --top-k 50 --top-p 0.95 --min-p 0.01 --dry-multiplier 1.1</code> to reduce repetition. Lower quantizations like <code>UD-Q2_K_XL</code> have been removed due to performance issues. The model still faces challenges, particularly with <strong>llama.cpp</strong> integration, where issues like segmentation faults and V cache quantization requirements are noted, despite the merging of PR #18936. The model is tested on high-end hardware (RTX 4090, 125 GB RAM) but remains unstable.</strong> There is a technical debate on the effectiveness of the <code>--dry-multiplier</code> parameter to reduce repetition, with suggestions to increase it to <code>1.5</code> if issues persist. Additionally, there is a consensus that the model's stability is not fully resolved, despite improvements.</p>\n<ul>\n<li><strong>danielhanchen</strong> provides specific configuration recommendations for using the GLM 4.7-Flash model, emphasizing the use of <code>UD-Q4_K_XL</code> and above quantizations. They suggest parameters like <code>--temp 0.2 --top-k 50 --top-p 0.95 --min-p 0.01 --dry-multiplier 1.1</code> to reduce repetition, with a note to increase <code>--dry-multiplier</code> if issues persist. Lower quantizations like <code>UD-Q2_K_XL</code> are removed due to performance issues, and non-UD-Q versions are discouraged. More details are available in their <a href=\""https://unsloth.ai/docs/models/glm-4.7-flash\"">documentation</a>.</li>\n<li><strong>bobeeeeeeeee8964</strong> reports a critical issue with running GLM-4.7-Flash on <code>llama.cpp</code> (commit 6df686bee), specifically with V cache quantization requiring <code>flash_attn</code>, which contradicts the model's requirement to disable <code>flash_attn</code> to avoid CPU fallback. This results in segmentation faults and instability, even after PR #18936. Tests with various configurations, including self-converted <code>Q8_0</code> and <code>evilfreelancer IQ4_XS</code>, result in crashes or garbled output, indicating unresolved compatibility issues.</li>\n<li><strong>danielhanchen</strong> acknowledges ongoing issues with looping in quantized versions of the model, suggesting BF16 for optimal results until fixes are finalized. This aligns with <strong>SM8085</strong>'s announcement of the BF16 release, which is expected to improve stability and performance.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/\"">zai-org/GLM-4.7-Flash · Hugging Face</a></strong> (Activity: 1169): <strong><strong>GLM-4.7-Flash</strong> is a <code>30B-A3B</code> Mixture of Experts (MoE) model released by <strong>zai-org</strong> on <a href=\""https://huggingface.co/zai-org/GLM-4.7-Flash\"">Hugging Face</a>. It is optimized for efficient deployment, leveraging <strong>MLA</strong> to minimize KV cache memory usage, allowing many users to run it at the full <code>200k</code> context length. The model demonstrates superior performance on benchmarks like <strong>AIME</strong> and <strong>GPQA</strong> and supports local inference through frameworks such as <strong>vLLM</strong> and <strong>SGLang</strong>. Detailed installation and evaluation instructions are provided to ensure optimal performance.</strong> Commenters express enthusiasm for the model's efficiency and memory management, particularly appreciating the ability to run it at full context length due to its low memory footprint. There is also a sentiment of anticipation for larger models, such as <code>70B</code>, indicating a demand for even more powerful models.</p>\n<ul>\n<li>The GLM-4.7-Flash model utilizes MLA (Memory-Limited Attention), which significantly reduces the memory footprint of the KV cache. This optimization allows many users to run the model at its full 200k context length, making it more accessible for those with limited hardware resources.</li>\n<li>A user highlights the model's architecture, noting a discrepancy in the model's description as a '30b' model, which actually refers to a '3B thinking model' as per the code reference in the <a href=\""https://github.com/huggingface/transformers/blob/main/src/transformers/models/glm4_moe_lite/modular_glm4_moe_lite.py#L169\"">Hugging Face Transformers repository</a>. This suggests a potential misunderstanding or mislabeling in the model's specifications.</li>\n<li>There is a desire for performance comparisons with larger models, as one user mentions the lack of direct benchmarks against much larger models, which would provide clearer insights into the model's relative performance and capabilities.</li>\n</ul>\n</li>\n</ul>\n<h3>2. Deepseek Model and System Builds</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/\"">768Gb Fully Enclosed 10x GPU Mobile AI Build</a></strong> (Activity: 903): <strong>The post describes a custom-built mobile AI system designed for running large Mixture of Experts (MoE) models like Deepseek and Kimi K2, as well as for high-detail image and video generation. The system features a <strong>Threadripper Pro 3995WX</strong> CPU, <code>512GB DDR4</code> RAM, and a combination of <code>8x RTX 3090</code> and <code>2x RTX 5090</code> GPUs, housed in a <strong>Thermaltake Core W200</strong> case. The build prioritizes mobility and enclosure, using a dual-system case to accommodate the GPUs with risers, and is powered by <strong>EVGA 1600W</strong> and <strong>Asrock 1300W</strong> PSUs. Benchmarks show impressive token generation rates, such as <code>31.54 tokens per second</code> for the Qwen 235b model. The system's total cost was approximately <code>$17,000</code>, with a focus on balancing performance and budget constraints.</strong></p>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qhs2sd/its_been_one_year_since_the_release_of_deepseekr1/\"">It's been one year since the release of Deepseek-R1</a></strong> (Activity: 364): <strong>The image marks the one-year anniversary of the release of <strong>DeepSeek-R1</strong>, a model that reportedly performs on par with <strong>OpenAI-o1</strong>. The model is fully open-source, with both the code and models available under the <strong>MIT License</strong>, allowing free use and modification. The announcement highlights the availability of a live website and API for users to interact with the model at <a href=\""http://chat.deepseek.com\"">chat.deepseek.com</a>. The image also includes a snippet of a chat interface, suggesting practical applications of the model in problem-solving scenarios.</strong> Comments reflect on the impact of DeepSeek-R1, suggesting it significantly influenced the AI landscape by forcing competitors to adapt, such as by reducing prices and increasing transparency in reasoning outputs. The release is considered a pivotal moment in AI development, second only to the original LLaMA release.</p>\n<ul>\n<li>Cuplike highlights the impact of Deepseek-R1 on the AI landscape, noting that it forced competitors to lower prices and reveal reasoning outputs. This suggests that Deepseek-R1 set a new standard in transparency and cost-effectiveness, making it a pivotal release in AI history, second only to the original LLaMA model.</li>\n<li>SubstantialSock8002 raises an interesting point about the progress in AI models by questioning which smaller models currently match the performance of Deepseek-R1 and their sizes. This inquiry suggests a focus on efficiency and the evolution of model capabilities over time, indicating a trend towards more compact yet powerful models.</li>\n<li>Lan_BobPage comments on the significant impact of Deepseek-R1 on major tech companies, specifically mentioning how it led to strategic shifts at <strong>Meta</strong>. This underscores the model's disruptive influence, causing major players to reassess their AI strategies and operations.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLM/comments/1qi5q2v/768gb_fully_enclosed_10x_gpu_mobile_ai_build/\"">768Gb Fully Enclosed 10x GPU Mobile AI Build</a></strong> (Activity: 195): <strong>The post details a custom-built mobile AI system designed for running large Mixture of Experts (MoE) models like Deepseek and Kimi K2, as well as for high-detail image and video generation. The system features a <strong>Threadripper Pro 3995WX</strong> CPU, <code>512GB DDR4</code> RAM, and a combination of <code>8x RTX 3090</code> and <code>2x RTX 5090</code> GPUs, housed in a <strong>Thermaltake Core W200</strong> case. The build is powered by <strong>EVGA 1600W</strong> and <strong>Asrock 1300W</strong> PSUs, running on <strong>Ubuntu</strong>. The system's design prioritizes mobility and enclosure, using the W200 case to avoid the aesthetic and structural issues of mining frames. Benchmarks show impressive token generation rates, e.g., <code>24.92 tps</code> for Deepseek V3.1 and <code>31.54 tps</code> for Qwen 235b, with the system maintaining good airflow and acoustics despite its high power and density.</strong> Commenters raised concerns about the power requirements, questioning if the PSUs are run on separate circuits due to the high power draw of the system. This highlights the practical challenges of operating such a high-performance build in a typical residential setting.</p>\n</li>\n</ul>\n<h3>3. AI Hardware and System Configuration</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLM/comments/1qhqf8p/llm_sovereignty_for_3_years/\"">LLM Sovereignty For 3 Years.</a></strong> (Activity: 101): <strong>The user is seeking advice on setting up a local environment to run Large Language Models (LLMs) for the next three years with a budget of approximately <code>$10,000</code>. Concerns include rising compute costs, increasing cloud service prices, and potential censorship. Suggestions include purchasing an <strong>Apple M3 Ultra</strong> with <code>80 GPU cores</code> and <code>512 GB</code> of memory, which may outperform traditional GPU cards in some tasks. Another recommendation is a setup with <code>128 GB RAM</code> and a <strong>RyzenAI 395</strong> or <strong>Mac</strong> for a balanced start. Additionally, investing in a tower with an <strong>RTX GPU</strong> and <code>128 DDR RAM</code> is advised for a robust local setup.</strong> There is a consensus that while local AI setups are improving, they still cannot fully compete with cloud AI, which utilizes multiple <code>$50k GPUs</code> and models with hundreds of billions of parameters. However, a local setup with sufficient RAM and GPU capabilities is considered a solid starting point for personal use.</p>\n<ul>\n<li><strong>Caprichoso1</strong> highlights the potential of the Apple M3 Ultra with 80 GPU cores and 512 GB of memory, priced under $10k. This setup may outperform traditional GPU cards in certain tasks due to its extensive memory, though GPU cards might excel in others, emphasizing the importance of task-specific hardware selection.</li>\n<li><strong>TheAussieWatchGuy</strong> contrasts cloud AI, which utilizes multiple $50k GPUs and handles hundreds of billions of parameters, with local AI setups. They suggest that while local AI is improving, it remains limited compared to cloud solutions. A local setup with 128GB of RAM, such as a RyzenAI 395 or Mac, is recommended as a solid starting point for those exploring local AI capabilities.</li>\n<li><strong>Vegetable-Score-3915</strong> discusses the feasibility of using second-hand workstations for AI inference tasks. They note that PCIe count is less critical for inference, suggesting that a workstation with PCIe 3 x 16 slots and DDR4 ECC RAM (32GB or 64GB) can be cost-effective. This approach allows for gradual upgrades, such as adding more GPUs, without the immediate need for PCIe4 or PCIe5 slots.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLM/comments/1qii3h2/can_i_add_a_second_gpu_to_use_its_vram_in/\"">Can I add a second GPU to use it's vram in addition of the vram of my main GPU to load bigger models?</a></strong> (Activity: 44): <strong>The user inquires about combining VRAM from multiple GPUs to load larger models, specifically using a 5070 Ti 16GB with a potential second GPU like a 24GB RTX 3090 or a 16GB RTX 5060 Ti. The consensus is that VRAM cannot be directly combined across GPUs for a single model, but multiple GPUs can be used for parallel processing. The RTX 3090 is recommended over the 5060 Ti due to its <code>24GB VRAM</code> and <code>higher memory bandwidth</code>, which are crucial for AI tasks. The 3090 is noted for its superior performance in AI workloads despite lacking newer features like <code>fp8</code> or <code>nvfp4</code> support. The 5070 Ti is comparable to the 3090 in compute power but has less VRAM, making the 3090 a better choice for larger models.</strong> Commenters suggest that for AI tasks, more VRAM is generally better, and the RTX 3090 offers the best value despite being older. Some recommend selling the 5070 Ti to invest in multiple 3090s for increased VRAM capacity. The trade-off between using multiple GPUs for faster processing versus a unified memory system for larger models is also discussed.</p>\n<ul>\n<li>The discussion highlights the advantages of the RTX 3090 over the 5060Ti for AI model inference, particularly due to its higher VRAM and memory bandwidth. The 3090 offers 50% more VRAM and 100% more memory bandwidth, which is crucial for loading larger models and ensuring efficient compute access. The lack of native support for formats like fp8 or nvfp4 in Ampere is noted, but the 3090's overall performance benefits outweigh these limitations for most users.</li>\n<li>For large language model (LLM) inference, the RTX 3090 is considered superior due to its 24GB VRAM, which is essential for running larger models. Tools like llama.cpp and LM Studio are mentioned as being compatible with multi-GPU setups, enhancing their utility. The comment also suggests that while GPUs provide better tokens per second, systems with high unified memory, like those with Ryzen AI 395 and 128GB+ DDR5, can run larger models albeit with slower token output.</li>\n<li>The feasibility of using multiple GPUs, such as the 5060Ti, is discussed in terms of cost-effectiveness and availability. While a single RTX 3090 with 24GB VRAM is priced around $850, two 5060Tis with a combined 32GB VRAM could theoretically match this price point, assuming availability. However, the 3090 is still favored for its superior value and performance, despite being an older model.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLM/comments/1qgueu7/amd_ryzen_ai_halo_for_ai_developers/\"">AMD Ryzen AI Halo for AI Developers</a></strong> (Activity: 72): <strong>The post discusses the AMD Ryzen AI Halo, highlighting its potential to challenge NVIDIA's dominance in AI hardware. However, technical issues with AMD's ROCm drivers are a significant barrier, as they are described as unreliable and difficult to work with, especially on Linux. The post criticizes AMD's claims of optimized applications and full ROCm support, noting that many features, such as FP8 support and integrated NPU, are not functioning as advertised. The only feature that reportedly works as intended is the <code>128GB unified memory</code> for large AI models.</strong> Commenters express skepticism about AMD's ability to compete with NVIDIA, citing the poor state of ROCm drivers and lack of reliable support for AI workloads. There is a consensus that AMD's software support is inadequate, with some users having to manually compile and fix issues themselves.</p>\n<ul>\n<li>A significant issue highlighted is the lack of robust ROCm driver support for AMD hardware, particularly for AI development. Users report that the drivers are unreliable, with one user mentioning they had to compile raw GitHub code and reimplement closed components to make it functional. This suggests a gap between AMD's claims of optimized applications and the reality of their software support, especially on Linux.</li>\n<li>There is criticism regarding AMD's claims of 'Day-0 Support for leading AI Models.' Users report that certain operations, such as using <code>fp8</code>, are not supported internally by ROCm, forcing them to use alternatives like <code>bf16</code>. This indicates a discrepancy between AMD's marketing and the actual capabilities of their hardware and software stack.</li>\n<li>Despite the criticisms, one feature that reportedly works as advertised is the 'Up to 128GB unified memory for running large generative AI models.' This suggests that while there are significant software support issues, some hardware capabilities are being effectively utilized.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLM/comments/1qhek55/dev_here_has_anyone_thought_on_training_a_model/\"">dev here - has anyone thought on training a model on your own codebase?</a></strong> (Activity: 42): <strong>A Laravel developer is experimenting with training a model on their own codebase using a <code>5060 16GB</code> setup and the <code>Qwen2.5 Coder</code> model. The developer plans to use older branches of their codebase and iterate over them incrementally. This approach is intended to explore the potential benefits of customizing a model specifically for their codebase.</strong> Commenters suggest that using a more modern model like <code>Qwen3-Coder</code> or <code>Devstral-2</code> would yield better results, as <code>Qwen2.5 Coder</code> is considered outdated. They also recommend using Retrieval-Augmented Generation (RAG) or codebase indexing features from tools like Roo/Kilo Code for more effective results.</p>\n<ul>\n<li>iMrParker suggests using Retrieval-Augmented Generation (RAG) instead of training a model on your own codebase for creating a promptable knowledge base. RAG can efficiently handle large datasets by retrieving relevant information, which might be more effective than fine-tuning a model on a specific codebase.</li>\n<li>noctrex recommends using more modern models like Qwen3-Coder or Devstral-2 for better results, as older models may be limited. They also suggest using RAG or the Codebase Indexing feature from Roo/Kilo Code, which can provide more efficient and accurate codebase management and querying.</li>\n<li>HonestoJago proposes an alternative approach to fine-tuning by training a model on pairs of questions and answers that reflect the developer's coding style and techniques. This method could potentially personalize the model's responses, although it might risk overfitting or breaking the model. They mention that tools like Unsloth make fine-tuning more accessible and quicker.</li>\n</ul>\n</li>\n</ul>\n<h2>Less Technical AI Subreddit Recap</h2>\n<blockquote>\n<p>/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo</p>\n</blockquote>\n<h3>1. Claude Code and AI Coding Tools</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/ClaudeAI/comments/1qgx6br/microsoft_pauses_claude_code_rollout_after_satya/\"">Microsoft pauses Claude Code rollout after Satya intervention</a></strong> (Activity: 1367): <strong><strong>Microsoft</strong> has paused the deployment of <strong>Claude Code</strong> internally after intervention from CEO <strong>Satya Nadella</strong> and senior leadership, redirecting employees to use <strong>GitHub Copilot</strong> instead. The internal communication suggests that Copilot has \""mostly closed the gaps\"" with Claude Code. However, exceptions are made for \""high-priority R&#x26;D\"" projects, which can still access the <strong>Anthropic API</strong> with proper justification. Existing users retain access, but new invitations have been rescinded.</strong> Commenters express skepticism about Microsoft's claim that Copilot has \""closed the gap\"" with Claude Code, suggesting it may be a strategic move to improve their own product by forcing internal use. Some find it notable that Microsoft admitted to using a competitor's tool over their own.</p>\n<ul>\n<li>DestroyAllBacteria highlights the strategic importance of Microsoft using its own products, like Copilot, to improve them. This approach, often referred to as 'eating their own dog food,' can lead to better product development and a more competitive landscape. By focusing on internal tools, Microsoft can potentially enhance the quality and capabilities of Copilot, making it a stronger competitor in the AI space.</li>\n<li>Inside-Yak-8815 points out the surprising admission by Microsoft that they were using Claude Code instead of their own tools. This revelation suggests that Claude Code might have had superior features or performance that Microsoft found valuable, which could be a driving factor for them to improve their own offerings like Copilot.</li>\n<li>Foreign_Coat_7817 suggests using Sonnet through GitHub Copilot as an alternative, indicating that there are multiple ways to leverage AI tools within Microsoft's ecosystem. This comment implies that while Claude Code might be paused, there are still robust options available for developers within the Microsoft suite.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/ClaudeCode/comments/1qh78yf/tried_claude_cowork_last_night_and_it_was_a_top_3/\"">Tried Claude Cowork last night, and it was a top 3 most exciting moments I’ve ever had with technology.</a></strong> (Activity: 483): <strong>The post describes a user's experience with <strong>Claude Cowork</strong>, a tool that appears to enhance the functionality of <strong>Claude Code</strong> by leveraging internet search capabilities to solve complex problems. The user highlights that Cowork demonstrated superior common sense compared to Claude Code, particularly in identifying and correcting errors in a project related to building a 'wispr flow app'. The user attributes Cowork's effectiveness to its ability to search the internet more efficiently, suggesting it retains more information than Claude Code, which relies on MCPs (Model Checkpoints).</strong> One commenter questions the necessity of Cowork given that Claude Code can already search the internet, while another expresses skepticism about the user's claims, suggesting they might be experiencing 'AI psychosis'. A third commenter reports difficulty in getting Cowork to access certain features, indicating potential limitations in its integration with Claude Code.</p>\n<ul>\n<li>Prize-Individual4729 highlights a technical limitation of Claude Cowork, noting that attempts to access the Claude Code terminal or Code tab in Claude for Mac were unsuccessful due to the sandbox/VM restrictions. This suggests that certain functionalities are isolated and not directly accessible, which could impact workflows that rely on integrated development environments.</li>\n<li>deific_ provides a perspective on the utility of Claude Cowork, emphasizing its ability to produce polished products despite not adhering to 'perfect Sr Dev codebase' standards. They argue that in corporate environments, the focus is often on delivering useful products rather than perfect ones, and Claude Cowork's auditing capabilities contribute to this goal. This reflects a broader discussion on the balance between code quality and practical utility in software development.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/ClaudeCode/comments/1qhj13v/has_anyone_tried_claude_code_with_local_model/\"">has anyone tried Claude Code with local model? Ollama just drop an official support</a></strong> (Activity: 421): <strong>The post discusses the integration of <strong>Claude Code</strong> with local models, specifically mentioning <strong>Ollama's</strong> official support for this setup. The image shows a coding interface for creating a simple HTML website, indicating the potential for using Claude Code in local development tasks. The post highlights the use of <strong>GLM 4.7 flash 30B</strong> for small tasks, suggesting that this setup could allow for unlimited iterations without usage limits. A key point from the comments is the comparison between local models and cloud-based models like Claude and GPT, noting that local models require more explicit instructions and prompt engineering. The comments also discuss the performance of models based on VRAM availability, suggesting that at least <code>24GB</code> of VRAM is needed for effective tool calls and context management.</strong> Commenters suggest that while Claude Code can be useful for initial prompt building, local models require more detailed instructions and context management compared to cloud models. They also recommend using <strong>llamacpp</strong> for better performance and control over model selection, advising against using <strong>Ollama</strong> models for high-intelligence tasks.</p>\n<ul>\n<li>Prof_ChaosGeography discusses using Claude with local models via <code>llamacpp</code> server and a <code>litellm</code> proxy. They emphasize that local models, especially those from Ollama, don't match the intelligence of cloud-based Claude or GPT models. They recommend using <code>llamacpp</code> for better performance and control over model selection and quantization, advising not to go below <code>q6</code> for monitoring and <code>q8</code> for autonomous operation. They also highlight the need for explicit instructions and effective prompt engineering when using non-Anthropic and non-OpenAI models.</li>\n<li>onil34 points out the limitations of models with different VRAM capacities. They note that models with <code>8GB</code> VRAM struggle with tool calls, while <code>16GB</code> models perform better but have limited context windows (<code>4k</code>). They suggest that at least <code>24GB</code> of VRAM is necessary for optimal performance, indicating the trade-offs between VRAM capacity and model capabilities.</li>\n<li>SatoshiNotMe shares their experience using <code>~30B</code> models with Claude Code via <code>llama-server</code> on an M1 MacBook Pro Max with <code>64GB</code> RAM. They report good performance in terms of TPS and work quality, particularly for sensitive document work. They provide a guide for running local LLMs like <code>Qwen3</code>, <code>Nemotron</code>, and <code>GPT-OSS</code> with Claude Code, and mention settling on <code>Qwen3-30B-A3B</code> without exhaustive comparison.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/ClaudeCode/comments/1qibh6o/are_we_sure_this_is_100_allowed_by_anthropic/\"">Are we sure this is 100% allowed by Anthropic?</a></strong> (Activity: 313): <strong>The image and post discuss the integration of Ollama with Anthropic's Claude messages API, allowing users to utilize Claude code with open-source models. This setup supports advanced features like agentic loops, tool use, and coding workflows powered by private LLMs. The comments clarify that this functionality is similar to how large corporations use proxy layers to access Claude on platforms like Amazon Bedrock. Anthropic's main restriction is against using their APIs for unlimited access under fixed-price plans, not against using their harness with other LLMs. The official documentation supports using gateways to other LLMs, indicating that this practice is legitimate.</strong> Commenters agree that using Anthropic's harness with other LLMs is legitimate, as long as it doesn't involve exploiting fixed-price subscription plans. The official documentation from Anthropic supports this use case, and Ollama's recent support for this integration further legitimizes it.</p>\n<ul>\n<li>The use of Claude Code through proxy layers to access services like Amazon Bedrock is a common practice among large corporations, and Anthropic has limited means to detect if their tool is being used with a non-Anthropic model. The main restriction is on using non-Claude Code harnesses to access models on Pro/MAX plans, which is not allowed by Anthropic.</li>\n<li>Anthropic provides documentation on using gateways to other LLMs, indicating that they permit the use of their harness with other LLMs. The primary restriction is against using Claude LLM APIs with fixed-price monthly subscriptions, which led to the OpenCode controversy. This suggests that while using the API is allowed, it must adhere to Anthropic's acceptable use terms.</li>\n<li>The recent concern about Claude Code/OpenCode was related to the use of Claude subscriptions in third-party tools. API key-based calls have always been functional across platforms, and the introduction of support by Ollama is not a new development. Users must still comply with Anthropic's acceptable use terms, which prohibit activities like building competing products or exfiltrating data for model training.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/MachineLearning/comments/1qi8twv/p_i_gave_claude_code_95_years_of_health_data_to/\"">[P] I Gave Claude Code 9.5 Years of Health Data to Help Manage My Thyroid Disease</a></strong> (Activity: 207): <strong>The user utilized <strong>Claude</strong>, an AI model, to analyze 9.5 years of personal health data from Apple Watch and Whoop to manage episodic Graves' disease. By employing <strong>XGBoost</strong> after testing various ML models, the user achieved approximately <code>98%</code> validation accuracy in predicting disease phases, providing alerts 3-4 weeks before symptom onset. This model was backtested successfully, predicting an episode weeks before lab confirmation. The user developed an iOS app for ongoing monitoring and open-sourced the project, including the Claude code setup, on <a href=\""https://medium.com/data-science-collective/i-gave-claude-code-9-5-years-of-health-data-to-help-manage-my-thyroid-disease-85fcd8c0449f\"">Medium</a>.</strong> Comments raised concerns about potential data leakage due to the high accuracy rate, suggesting the need for out-of-time testing to validate predictive utility. Additionally, there was skepticism about sharing medical data with <strong>Anthropic</strong>.</p>\n<ul>\n<li>Stereoisomer raises a critical point about the reported <code>98% accuracy</code> in the predictive model for managing thyroid disease, suggesting the possibility of data leakage. Data leakage occurs when the model has access to information during training that it wouldn't have in a real-world scenario, leading to overly optimistic performance metrics. This highlights the importance of ensuring that the model's training and testing datasets are properly separated to avoid such issues.</li>\n<li>GreatBigBagOfNope emphasizes the importance of out-of-time testing for evaluating the predictive utility of the model. While backtesting can provide insights into past performance, real-world effectiveness is best assessed through continuous, real-time testing. This approach helps in understanding how well the model adapts to new, unseen data, which is crucial for its practical application in managing health conditions.</li>\n<li>grimmwerks shares a personal experience with Hashimoto's disease and related symptoms, noting a potential link between sugar intake and inflammation. This anecdotal evidence suggests that personalized data-driven approaches, like the one discussed in the post, could be valuable for managing complex health conditions by identifying individual triggers and patterns.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/ClaudeCode/comments/1qhiicv/the_creator_of_nodejs_says_the_era_of_writing/\"">The creator of Node.js says the era of writing code is over</a></strong> (Activity: 309): <strong><strong>Ryan Dahl</strong>, the creator of Node.js, has suggested that the traditional era of writing code is ending, indicating a shift towards AI-driven development. This perspective is shared by other prominent figures like <strong>Karpathy</strong> and <strong>Stroustrup</strong>, who foresee a future where software engineering focuses more on problem-solving rather than manual coding. The discussion highlights the potential for AI to automate many coding tasks, fundamentally changing the skills required in the industry. For more details, see the <a href=\""https://jpcaparas.medium.com/the-creator-of-node-js-says-the-era-of-writing-code-is-over-8320c868043b?sk=66b1c9454345f17c08a532986a4e0bcc\"">original article</a>.</strong> Comments reflect a divide between coders and engineers, emphasizing that engineering is about problem-solving, not just coding. There's also a recognition that many companies lag in AI adoption due to security and policy constraints, limiting the use of advanced AI tools in corporate environments.</p>\n<ul>\n<li>MR_PRESIDENT__ highlights the lag in AI adoption within large corporations, noting that many are 4-5 years behind current AI capabilities. This delay is attributed to stringent security and responsibility protocols, which restrict the use of advanced tools like CLI tools, MCP servers, and AI models such as Claude Code. The commenter contrasts this with the more advanced capabilities available to individuals outside these corporate environments, suggesting a significant gap in AI utilization between personal and corporate settings.</li>\n</ul>\n</li>\n</ul>\n<h3>2. Gemini and Google AI Developments</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/singularity/comments/1qh591s/rumors_of_gemini_3_pro_ga_being_far_better_like_35/\"">Rumors of Gemini 3 PRO GA being \""far better\"", \""like 3.5\""</a></strong> (Activity: 657): <strong>The image discusses rumors about a new version of Google's AI model, referred to as \""Gemini 3 PRO GA,\"" which is reportedly undergoing A/B testing in an AI studio. This version is rumored to be significantly improved, potentially comparable to a hypothetical version 3.5. The community post suggests that the current 3.0 model has a strong base intelligence but lacks fine-tuning, indicating that the new version might address these issues. The term \""GA\"" is questioned in the comments, possibly referring to \""General Availability.\""</strong> Commenters express skepticism about the new version's capabilities, noting that the current model makes frequent typos in coding tasks and suggesting that significant improvements are needed for it to surpass existing models like Opus.</p>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/Bard/comments/1qhzifv/gemini_integration_into_chrome_browser_is_just/\"">Gemini integration into Chrome browser is just too darn good and useful</a></strong> (Activity: 178): <strong>The image illustrates the integration of the Gemini tool into the Chrome browser, which enhances the browsing experience by providing real-time context and information about media content being viewed. This feature allows users to gain additional insights and background information on videos or images they are watching, directly within the browser. The tool is particularly noted for its ability to offer context that users might not initially be aware of, thereby enriching their understanding and engagement with the content.</strong> Commenters express a desire for the Gemini integration to be available outside the US, highlighting its potential utility in other regions. There is also curiosity about how to activate this feature, indicating interest in its practical application.</p>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/Bard/comments/1qh7j8l/even_gemini_3_pro_is_acting_stupid_lately/\"">Even Gemini 3 Pro is acting stupid lately</a></strong> (Activity: 54): <strong>The user reports issues with the <strong>Gemini 3 Pro</strong> model, specifically its tendency to generate unwanted images and videos, despite being on the Ultra tier for higher quality. The model appears to misinterpret user requests, such as creating a storyboard when only ideas were solicited. This suggests potential flaws in the model's prompt interpretation or execution logic, possibly due to an overzealous attempt to anticipate user needs. The user suggests a rule change to ensure the model only creates content explicitly requested by the user.</strong> One commenter speculates that a new model is in development, which may address these issues. Another suggests that the model's behavior is due to its design to fulfill the 'ultimate objective' of a task, implying a need for clearer user instructions or model adjustments.</p>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/Bard/comments/1qhf7zz/gemini_live_preps_big_upgrades_with_thinking_mode/\"">Gemini Live preps big upgrades with ‘Thinking Mode’ and ‘Experimental Features’</a></strong> (Activity: 170): <strong><strong>Google</strong> is preparing to enhance its Gemini Live app with new features like 'Thinking Mode' and 'Experimental Features' as part of its 'Labs' initiative. These features, expected to be powered by the upcoming <strong>Gemini 3</strong> model, include 'Live Thinking Mode' for more detailed responses and 'Live Experimental Features' such as multimodal memory, improved noise handling, and personalized results. The app currently runs on <strong>Gemini 2.5 Flash</strong>, but the new updates suggest a shift to <strong>Gemini 3</strong>. Additionally, features like 'UI Control' and 'Deep Research' are being developed, potentially integrating with Android's 'Computer Use'.</strong> There is a technical debate on the availability of these features, with some users speculating they might be limited to the United States. The community is also intrigued by the potential of 'Agent controls phone to complete tasks' and improved noise handling.</p>\n<ul>\n<li>The introduction of 'Live Thinking Mode' in Gemini 3 Pro is designed to enhance the AI's response quality by allowing it more time to process and generate detailed answers. This feature is part of Google's 'Labs' initiative, which lets users test upcoming functionalities. The mode may utilize either the Thinking or Pro models to achieve these detailed responses, indicating a potential shift towards more sophisticated AI processing capabilities.</li>\n<li>The 'Live Experimental Features' in Gemini 3 Pro include advancements like multimodal memory and improved noise handling. These features aim to enhance the AI's interaction by integrating data from various Google apps to provide personalized results. The mention of 'responding when it sees something' suggests a visual recognition capability, possibly linked to Project Astra, which could significantly improve context-aware responses.</li>\n<li>Gemini 3 Pro's 'UI Control' feature allows the AI agent to control the phone to complete tasks, indicating a move towards more integrated and autonomous device management. This aligns with the broader trend of AI systems taking on more complex roles, such as 'Deep Research,' which involves delegating intricate research tasks, potentially transforming how users interact with their devices for productivity.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/singularity/comments/1qh1omx/babyvision_a_new_benchmark_for_humanlevel_visual/\"">BabyVision: A New Benchmark for Human-Level Visual Reasoning</a></strong> (Activity: 574): <strong>The image presents a bar chart from the BabyVision-Mini benchmark, which evaluates the visual reasoning capabilities of large language models (LLMs) compared to humans of various ages. The chart highlights that human performance, particularly that of 12-year-olds, surpasses that of LLMs, with the Gemini3-Pro-Preview model achieving the highest accuracy among the LLMs. This benchmark underscores the current limitations of LLMs in visual reasoning tasks, suggesting that advancements in multi-modal pretraining and reinforcement learning could enhance their performance in the future.</strong> Commenters note the potential for future improvements in LLMs' visual reasoning through scaling multi-modal pretraining and reinforcement learning, which could significantly benefit fields like robotics.</p>\n<ul>\n<li>The discussion highlights that current models are still limited in visual reasoning, which is a significant challenge for achieving ARC AGI. The commenter suggests that scaling multi-modal pretraining and reinforcement learning (RL) for vision tasks could improve performance to near 100% in the future, unlocking new applications, particularly in robotics.</li>\n<li>The commenter references a specific paper on arXiv, which likely provides detailed insights or data related to the benchmark or model performance discussed in the post. This suggests that the community is actively engaging with academic research to understand and improve visual reasoning capabilities in AI models.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/singularity/comments/1qhuuqf/the_thinking_game_documentary_is_sitting_at_305m/\"">The Thinking Game documentary is sitting at 305M views on Youtube in less than 2 months. Ridiculous numbers.</a></strong> (Activity: 545): <strong>The image highlights the extraordinary viewership of \""The Thinking Game,\"" a documentary by <strong>Google DeepMind</strong> that has reached over <code>305 million views</code> on YouTube in less than two months. This documentary, an official selection of the Tribeca Film Festival, explores an AI breakthrough that won a Nobel Prize, reflecting the growing public interest in AI topics. The rapid accumulation of views is contrasted with the earlier <strong>AlphaGo</strong> documentary, which has <code>37 million views</code> over six years, indicating a significant increase in public engagement with AI content. The documentary's focus is noted to be more on human endeavor than the technology itself, which has resonated with viewers.</strong> There is skepticism about the authenticity of the view count, as the ratio of views to likes suggests possible artificial inflation. Typically, a video with such high viewership would have millions of likes, but this video has only <code>190K likes</code>, leading to speculation about the use of bots.</p>\n<ul>\n<li>The documentary 'The Thinking Game' has achieved over 305 million views on YouTube in less than two months, which is significantly higher than the 37 million views of the 'AlphaGo' documentary released in 2020. This rapid accumulation of views suggests a growing public interest in AI-related content. However, some users suspect that the view count may be artificially inflated due to the disproportionate number of likes (190K) and comments (4000) compared to typical engagement metrics for videos with similar view counts.</li>\n<li>There is skepticism about the authenticity of the view count for 'The Thinking Game' documentary. A typical video with over 300 million views would generally have millions of likes, yet this video only has 190K likes, suggesting potential use of bots to inflate views. The expected ratio of likes to views is approximately 1:100, indicating that the current engagement does not align with organic growth patterns.</li>\n<li>One user noted an unusual pattern in YouTube's recommendation algorithm, stating that 'The Thinking Game' was persistently suggested on their homepage and sidebar for two weeks, which is atypical for YouTube's recommendation system. This could imply an aggressive promotion strategy or algorithmic anomaly contributing to the high view count.</li>\n</ul>\n</li>\n</ul>\n<h3>3. DeepSeek AI Impact and Developments</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/DeepSeek/comments/1qgy3lk/one_year_since_the_deepseek_moment_the_impact_is/\"">One Year Since the “DeepSeek Moment”: The Impact is Still Real.</a></strong> (Activity: 204): <strong>The \""DeepSeek Moment\"" marks the anniversary of the release of <strong>DeepSeek-R1</strong>, a significant reasoning model that has influenced the AI industry by emphasizing reasoning as a core capability, promoting efficient training methods, and encouraging the development of smaller, smarter models. This release has also led to broader adoption in emerging markets and a shift towards modular, tool-aware AI systems. The impact of DeepSeek-R1 is seen as a pivotal change in the industry, comparable to major releases from other leading AI companies.</strong> Commenters highlight that DeepSeek's impact was not about surpassing competitors like OpenAI but demonstrating capability, especially from a non-Western entity. Some users express disappointment with the transition from R1 to the MoE model, preferring open-source alternatives. Others note DeepSeek's contributions to fine-grained sparsity and RLVR, suggesting its techniques may become standard in the industry.</p>\n<ul>\n<li>DeepSeek's release was a significant event in the AI landscape, challenging the dominance of Western LLMs by demonstrating China's capability in this field. The initial model, R1, was impactful, but the transition to a Mixture of Experts (MoE) model was seen as a downgrade by some users due to slower updates and less appealing performance for specific use cases. This shift led some users to prefer open-source alternatives, which they find more aligned with their needs and values.</li>\n<li>DeepSeek's major contributions include advancing fine-grained sparsity techniques, particularly with its V3 model and predecessors, and introducing a straightforward method for achieving Reinforcement Learning with Variable Rewards (RLVR) through the GRPO algorithm. These innovations have influenced the broader AI community, with DeepSeek's Sparse Attention potentially becoming a standard approach, similar to how Multi-Headed Attention (MLA) has been widely adopted in open models.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/DeepSeek/comments/1qh15va/the_race_to_build_the_deepseek_of_europe_is_on/\"">The Race to Build the DeepSeek of Europe Is On</a></strong> (Activity: 181): <strong>The article discusses Europe's strategic push to develop its own AI capabilities, aiming to reduce dependency on US technologies and establish technological sovereignty. This initiative is partly inspired by China's success with DeepSeek and involves significant government investment and open collaboration among European AI labs. Key players include <strong>DeepMind</strong> in the UK and <strong>Mistral</strong> in France, highlighting a competitive landscape as Europe seeks to become an AI superpower. The effort underscores AI's role as critical infrastructure, necessitating a shift towards self-sufficiency in the sector. <a href=\""https://www.wired.com/story/europe-race-us-deepseek-sovereign-ai/\"">Read more</a>.</strong> Commenters express skepticism about Europe's ability to compete with US AI firms, citing regulatory and taxation challenges. There is also a sentiment that European governments' demands on companies, such as producing affordable electric cars, may hinder AI innovation.</p>\n<ul>\n<li>The discussion highlights the strategic importance of Europe developing its own AI capabilities, particularly in light of its changing relationship with the US. The urgency for Europe to become a self-sufficient AI superpower is underscored by the need to reduce dependency on US-based technologies, as detailed in the <a href=\""https://www.wired.com/story/europe-race-us-deepseek-sovereign-ai/\"">Wired article</a>.</li>\n<li>The comment by No_You3985 points out the significant contributions of European-born scientists to major AI advancements, such as OpenAI's GPT models. This underscores the potential talent pool within Europe that could be leveraged if these individuals were incentivized to return and contribute to European AI initiatives.</li>\n<li>Rojeitor's comment critiques the regulatory and economic environment in Europe, suggesting that over-regulation and high taxation could hinder the development of competitive AI technologies. This reflects a broader concern about the balance between regulation and innovation in the tech industry.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/DeepSeek/comments/1qi8rdi/what_do_you_mainly_use_deepseek_for/\"">What do you mainly use DeepSeek for?</a></strong> (Activity: 49): <strong>DeepSeek is primarily utilized for tasks such as <strong>development and architectural analysis of applications</strong>, as well as generating documentation, leveraging its capabilities through a paid API. Users also explore its performance in areas like <strong>math and statistics</strong>, and engage it in more casual interactions such as discussing life topics and recipes. The model is noted for its versatility in handling diverse tasks, though specific benchmarks or comparative performance metrics against other LLMs are not detailed in the discussion.</strong> Some users highlight DeepSeek's effectiveness in technical domains like application development and documentation, suggesting it may excel in structured, technical tasks. However, there is also interest in its ability to handle more general conversational topics, indicating a broad range of applications.</p>\n<ul>\n<li>Meca0x highlights the use of DeepSeek for development purposes, specifically mentioning its application in architectural analysis of applications and documentation. This is facilitated through the paid API, suggesting a focus on leveraging DeepSeek's capabilities for professional and technical tasks.</li>\n<li>Sparklypain discusses the use of AI for complex communication and analysis tasks. They emphasize the need for AI to understand and translate unusual syntax and ideas, as well as perform multivariable and high-level regressive analysis. This involves asking iterative 'why' questions to uncover deeper insights, which is challenging for human counterparts.</li>\n<li>Sparklypain also notes the necessity of AI in facilitating high-level regressive analysis due to the complexity of their ideas and sentence structures. This involves iterative questioning to explore unknowns and feelings, which is a task that requires significant time and cognitive effort, often beyond the capability of their human friends.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h1>AI Discord Recap</h1>\n<blockquote>\n<p>A summary of Summaries of Summaries by gpt-5.2</p>\n</blockquote>\n<p><strong>1. GLM-4.7-Flash Adoption: Prompts, Quants, and \""Thinking\"" Toggles</strong></p>\n<ul>\n<li>\n<p><strong>Claude Prompt Gives GLM a Glow-Up</strong>: Unsloth users reported that dropping in a modified <strong>Claude Sonnet 4.5 system prompt</strong> from Anthropic’s docs materially improved <strong>GLM-4.7-Flash</strong> coherence and capability (\""<em>a skill difference</em>\"") via <a href=\""https://platform.claude.com/docs/en/release-notes/system-prompts\"">Claude system prompts release notes</a>.</p>\n<ul>\n<li>The discussion treated this as evidence that <strong>system-prompt scaffolding</strong> can dominate perceived model quality, especially for instruction-following and style control, even when the underlying weights stay the same.</li>\n</ul>\n</li>\n<li>\n<p><strong>High-Quant Weirdness: Q2 Beats Q6 (???), Everyone Panics</strong>: Multiple users saw <strong>GLM-4.7-Flash</strong> behave worse at <em>higher</em> quant levels—preferring <strong>Q2KXL</strong> over <strong>Q6KL</strong>—and linked it to possible quant tooling issues across <strong>llama.cpp/Ollama</strong>, referencing a related llama.cpp thread in <a href=\""https://github.com/ggml-org/llama.cpp/pull/18936#issuecomment-3774525719\"">ggml-org/llama.cpp PR discussion</a>.</p>\n<ul>\n<li>Community consensus: this is rare (\""<em>first time a model has behaved badly at high quants</em>\"") and likely implicates either <strong>quantization artifacts</strong> or <strong>production pipeline</strong> rather than simple sampler settings.</li>\n</ul>\n</li>\n<li>\n<p><strong>Chat Templates Eat Your Reasoning for Breakfast</strong>: LM Studio users argued <strong>chat templates</strong> can strip or suppress reasoning in models like <strong>Qwen3</strong>, breaking “<strong>interleaved thinking</strong>,” and noted <strong>GLM4.7-Flash</strong> includes a template flag like <em>clear_thinking</em> that removes thinking content unless explicitly disabled.</p>\n<ul>\n<li>The thread connected these template behaviors to agentic coding extensions and tool workflows, implying that “model regression” reports sometimes come from <strong>template defaults</strong> rather than the model weights.</li>\n</ul>\n</li>\n</ul>\n<p><strong>2. MCP &#x26; Agent Tooling: Ecosystem Growing Pains (and New Toys)</strong></p>\n<ul>\n<li>\n<p><strong>MCP Inspector vs 401: The Re-Auth Boss Fight</strong>: MCP Contributors reported <strong>MCP Inspector</strong> failing to re-authenticate after <strong>401s</strong>, recommending it parse <strong>resource metadata</strong> in the 401 response and attempt re-authorization; they also flagged a known SDK bug with <strong>resourceMetadata persistence across redirects</strong> tracked in <a href=\""https://github.com/modelcontextprotocol/inspector/issues/576#issuecomment-3766294454\"">inspector issue #576</a>.</p>\n<ul>\n<li>Members observed <strong>VS Code</strong> appears to use Inspector only for initial connection (not subsequent 401s), suggesting the failure mode may stem from <strong>SDK internals</strong> and that server-side fixes are already in with an SDK update pending.</li>\n</ul>\n</li>\n<li>\n<p><strong>LM Studio Calls the MCP SDK a House of Cards</strong>: LM Studio users criticized their <strong>MCP backend</strong> (built on the official SDK) as having severe security issues and \""<em>0 dev UX in mind</em>\"" while still being \""<em>the best we have right now</em>\"" compared to other agent frameworks.</p>\n<ul>\n<li>The takeaway was pragmatic: developers want MCP, but current implementations feel <strong>fragile</strong>, so teams are expecting churn in SDKs, auth flows, and tool-call ergonomics.</li>\n</ul>\n</li>\n<li>\n<p><strong>OpenRouter Ships More Clients: OkeyBot + Inforno</strong>: OpenRouter users showcased <strong>OkeyBot</strong> for Discord chats via OpenRouter BYO keys with per-thread usage/cost estimates at <a href=\""https://okeybot.ai/\"">okeybot.ai</a> and <strong>Inforno</strong>, an open-source desktop multi-LLM chat app supporting <strong>OpenRouter + Ollama</strong>, saving histories to <strong>.rno</strong>, with <a href=\""https://youtu.be/oJyj0mroFtY\"">Inforno intro video</a> and code at <a href=\""https://github.com/alexkh/inforno\"">alexkh/inforno</a>.</p>\n<ul>\n<li>In parallel, users asked OpenRouter for a <strong>batch API</strong> for providers like Google/OpenAI, citing demand in <a href=\""https://x.com/nopainkiller/status/2013522059662614653\"">an X post</a> and tying it to cost/control needs for agent workloads.</li>\n</ul>\n</li>\n</ul>\n<p><strong>3. Performance Engineering: Kernels, Collectives, and CUDA Micro-Wins</strong></p>\n<ul>\n<li>\n<p><strong>YALI Tries to Dunk on NCCL (with Tail Latency Receipts)</strong>: GPU MODE users introduced <strong>YALI</strong>, a 2‑GPU <strong>NVLink AllReduce</strong> library claiming <strong>1.2×–2.4×</strong> throughput vs <strong>NVIDIA NCCL</strong> plus \""<em>50×+ more stable tail latency</em>\"", released on GitHub at <a href=\""https://github.com/Venkat2811/yali\"">Venkat2811/yali</a>.</p>\n<ul>\n<li>The author emphasized aggressive <strong>overlap of ops and compute</strong> (flash/stream modes) and even removed the mascot after feedback that the AI pitch made the project feel less serious—classic open-source marketing calibration.</li>\n</ul>\n</li>\n<li>\n<p><strong>One PTX Suffix, Seven Instructions Saved</strong>: GPU MODE highlighted that <code>rcp.approx.ftz.f32</code> compiles to a single <code>MUFU.RCP</code> instruction while <code>rcp.approx.f32</code> can produce <strong>7 extra instructions</strong>, referencing NVIDIA’s <a href=\""https://developer.nvidia.com/ptx-compiler-driver\"">PTX docs</a>.</p>\n<ul>\n<li>They also noted that without <strong>ftz</strong> (flush-to-zero), subnormal reciprocals can overflow to <strong>INF</strong>, framing <code>.ftz</code> as both a performance and numerical-behavior choice.</li>\n</ul>\n</li>\n<li>\n<p><strong>Flash-Attention Stride Bug: Divisibility Constraints Vanish</strong>: GPU MODE users pointed to a flash-attention stride divisibility regression, saying it \""<em>boils down to a bug that removed some stride divisibility constraints</em>\"" and linked the report at <a href=\""https://github.com/Dao-AILab/flash-attention/issues/2192#issuecomment-3770977193\"">flash-attention issue comment</a>.</p>\n<ul>\n<li>The thread treated this as a reminder that high-performance kernels often rely on fragile shape/stride assumptions—and a single constraint change can surface as correctness or perf cliffs.</li>\n</ul>\n</li>\n</ul>\n<p><strong>4. Coding Workflows &#x26; Model Economics: IDE Telemetry, Search, and “Cheap Models”</strong></p>\n<ul>\n<li>\n<p><strong>Cursor Counts Your AI Lines (Enterprise Spreadsheets, Assemble!)</strong>: Cursor users said enterprise plans now show insights on what fraction of the codebase is written by <strong>AI vs humans</strong>, powered by the <strong>Opus 4.5 API</strong> (distinct from Claude Code), but the exact prompts for the feature aren’t public.</p>\n<ul>\n<li>The reaction mixed curiosity with skepticism: without prompt transparency, teams can’t easily reason about measurement bias or whether the metric is more <strong>sales dashboard</strong> than engineering signal.</li>\n</ul>\n</li>\n<li>\n<p><strong>mgrep Declares Grep Ragnarok</strong>: Cursor users discussed <code>mgrep</code> as a grep replacement claiming <strong>95%</strong> better relevance and token-efficiency for LLM workflows by returning less junk context.</p>\n<ul>\n<li>Others countered that Cursor already uses <code>rgrep</code> plus internal semantic search (just without a marketing name), implying the real differentiator is packaging and defaults, not the underlying idea.</li>\n</ul>\n</li>\n<li>\n<p><strong>Search Engines &#x26; Model Pricing: Searxng, Kagi, and Grok’s “Cheap But Chatty” Tax</strong>: Unsloth members argued <strong>Google</strong> struggles to find things and boosted <strong>Searxng</strong>, while others praised <strong>Kagi</strong> for privacy and scraping, linking a demo video at <a href=\""https://www.youtube.com/watch?v=ThgVTNVOZ7g\"">YouTube: ThgVTNVOZ7g</a>.</p>\n<ul>\n<li>Meanwhile Cursor users said <strong>Grok</strong> can be cheaper than Opus/Sonnet/GPT but often needs extra iterations, so the \""cheap\"" option can turn expensive unless you optimize prompts and context discipline.</li>\n</ul>\n</li>\n</ul>\n<p><strong>5. Benchmarks, Evals, and the Reality of “Community Ground Truth”</strong></p>\n<ul>\n<li>\n<p><strong>LMArena Hits 5M Votes, Ships Leaderboard Moves</strong>: LMArena announced <strong>Text Arena</strong> passed <strong>5 million comparisons</strong>, and its <strong>Text-to-Image leaderboard</strong> update put <strong>GLM-Image</strong> at <strong>#8</strong> among open models and <strong>#35</strong> overall with score <strong>1018</strong>.</p>\n<ul>\n<li>Users simultaneously complained about degraded image model quality and reliability issues (captcha loops, \""Something went wrong\"" errors), suggesting the platform’s measurement value fights constant product stability drag.</li>\n</ul>\n</li>\n<li>\n<p><strong>Eleuther Wants Agent Evals: Less Vibes, More Judge Pipelines</strong>: Eleuther engineers discussed automating <strong>agent evaluation</strong> to reduce manual review cost, circling around \""<strong>LLM as judge</strong>\"" workflows while warning that you still need to validate <strong>data quality</strong> and define the agent’s success criteria first.</p>\n<ul>\n<li>A separate Eleuther thread requested repeated multiple-choice evals for open-weight models (e.g., <strong>Llama 7B/13B/70B</strong>) with <strong>100 runs per question</strong> to estimate answer probabilities, emphasizing pre-written answers rather than model-generated ones.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h1>Discord: High level Discord summaries</h1>\n<h2><a href=\""https://discord.com/channels/1179035537009545276\"">Unsloth AI (Daniel Han)</a> Discord</h2>\n<ul>\n<li><strong>Ollama's GO Engine: Faster or Just a Wrapper?</strong>: Members debated whether <strong>Ollama's GO Engine</strong> offers actual speed improvements over <strong>llama.cpp</strong>, or if it's simply a wrapper with no real performance difference, citing similar operations and a <strong>GGML wrapper</strong>.\n<ul>\n<li>Claims were made that the <strong>GO Engine</strong> is faster than lmstudio's lcpp, despite using the same operations, resulting in widespread skepticism.</li>\n</ul>\n</li>\n<li><strong>GLM-4.7-Flash: Quantization Quality Quirk?</strong>: Users reported that <strong>GLM-4.7-Flash</strong> behaves poorly at higher quantization levels, with <strong>Q2KXL quant</strong> preferred over <strong>Q6KL</strong>, sparking discussion on whether the issue stems from the quants themselves or the software used to produce them, as exemplified by <a href=\""https://github.com/ggml-org/llama.cpp/pull/18936#issuecomment-3774525719\"">this issue</a>.\n<ul>\n<li>It was remarked that this is unusual because <em>it is the first time a model has behaved badly at high quants</em>.</li>\n</ul>\n</li>\n<li><strong>Claude System Prompt Improves GLM?</strong>: Community members found that using a modified version of <strong>Claude's system prompt</strong> from <a href=\""https://platform.claude.com/docs/en/release-notes/system-prompts\"">Claude Sonnet 4.5</a> notably improved the performance and coherence of <strong>GLM-4.7-Flash</strong>.\n<ul>\n<li>One member observed <em>a skill difference</em> when using <strong>Claude's system prompt</strong>.</li>\n</ul>\n</li>\n<li><strong>META Model Access Unlocked by Unsloth?</strong>: Users noted the difficulty in accessing gated <strong>META models</strong> due to required access requests, highlighting how <strong>Unsloth</strong> circumvents this by re-uploading models to the <strong>Unsloth repo page</strong>.\n<ul>\n<li>It was generally agreed that this bypasses the usual gating mechanisms, and makes them available without jumping through hoops.</li>\n</ul>\n</li>\n<li><strong>Searxng or Google for Search?</strong>: Members debated the effectiveness of search engines, with one arguing that <strong>Google</strong> is not good at finding things and championing <strong>Searxng</strong> as superior, while others touted <strong>Kagi</strong> for its privacy and web scraping, as shown in <a href=\""https://www.youtube.com/watch?v=ThgVTNVOZ7g\"">this video</a>.\n<ul>\n<li>This debate highlights a broader dissatisfaction with mainstream search engines among the AI community.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1105891499641684019\"">BASI Jailbreaking</a> Discord</h2>\n<ul>\n<li><strong>Teams Targeted by Djinn Root Kit</strong>: A member joked that if a <strong>Djinn</strong> were to attempt to influence people, it should learn to use Discord instead of <em>shit tier app like Teams</em>, followed by a counter-hack <em>root kit</em> shared as a <a href=\""https://cdn.discordapp.com/attachments/1235691879492751460/1462902319518846997/message.txt?ex=697132f4&#x26;is=696fe174&#x26;hm=bda97017288793711b502c5bf3089b73da200c886ad470b0e721fe1090184941&#x26;\"">message.txt</a>.\n<ul>\n<li>The joke was made in the general chat.</li>\n</ul>\n</li>\n<li><strong>DracoAI API faces data questions</strong>: A member sought feedback on <a href=\""https://www.dracoai.app/\"">DracoAI</a>, an Agentic AI with API calling ability.\n<ul>\n<li>Concerns were raised about the site's security and data handling, but it was clarified that <em>all data is stored on your Local Storage</em> and that it <em>cannot execute a whole workflow rather 1 API send at a time</em>.</li>\n</ul>\n</li>\n<li><strong>Gemini prompt accidentally LibreChat</strong>: A user shared a <strong>Gemini system prompt</strong> as a text file and an image, with speculation it might be an <em>injectprompt</em> via <strong>AI Studio</strong>.\n<ul>\n<li>Another user dismissed this, identifying it as a customized <strong>LibreChat</strong> instance with a system prompt and RAG (<a href=\""https://www.librechat.ai/\"">https://www.librechat.ai/</a>).</li>\n</ul>\n</li>\n<li><strong>AntiJection challenge is usable without signup</strong>: A member shared a link to an <a href=\""https://challenge.antijection.com/challenge\"">AntiJection Challenge</a> and claimed to have made it usable without sign-up.\n<ul>\n<li>It is uncertain from the prompt if they made it without signup themselves, or were referencing a tool others can use, but the general topic is about adversarial attacks.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1047197230748151888\"">Perplexity AI</a> Discord</h2>\n<ul>\n<li><strong>Perplexity Punishes Pro Account Pirates</strong>: Multiple users reported <strong>Perplexity Pro account suspensions</strong> for violating <a href=\""https://www.perplexity.ai/hub/legal/terms-of-service\"">Section 3.2 of the ToS</a> by purchasing subscriptions or promo codes from unauthorized third parties, often via <strong>Instagram stores</strong>.\n<ul>\n<li>These users discovered the perils of deep discount subscriptions offered by unverified sources.</li>\n</ul>\n</li>\n<li><strong>Samsung Bets Big on Bixby Brain Boost</strong>: <strong>Samsung</strong> is integrating <strong>Perplexity</strong> into <strong>Bixby</strong> with <strong>One UI 8.5</strong>, using it for real-time web answers directly within the <strong>Bixby UI</strong> as reported by <a href=\""https://www.sammobile.com/news/samsung-new-bixby-for-one-ui-8-5-official-coming-to-beta-soon\"">SamMobile</a>.\n<ul>\n<li>This integration will enable users to receive information without leaving Bixby to open a separate browser.</li>\n</ul>\n</li>\n<li><strong>Comet Caps and Considerations</strong>: Users are discussing the limits of using <strong>Comet</strong> browser, with agentic features potentially requiring a <strong>Pro subscription</strong>.\n<ul>\n<li>It's suspected that Pro subscribers may have higher, undisclosed limits for both regular and agentic features.</li>\n</ul>\n</li>\n<li><strong>Pro Membership Problems Prompt Probing</strong>: Users reported issues with <strong>Pro memberships</strong>, like not receiving the PRO role on Discord after subscribing, and difficulties with <strong>API keys</strong> and credit balances.\n<ul>\n<li>Some Pro members have found they have <strong>$5</strong> worth of complimentary credits every month for <strong>Gooseai MCP models</strong>, which are used to add detail to the queries, in addition to a cap of <strong>10 files per day</strong> for free student subscriptions.</li>\n</ul>\n</li>\n<li><strong>Image Generation Grounded Globally</strong>: Users in <strong>Italy</strong> and <strong>Malaysia</strong> reported being unable to generate images with their <strong>Pro accounts</strong> due to regional restrictions.\n<ul>\n<li>These users could previously generate images without issues, suggesting a recent policy change.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1074847526655643750\"">Cursor Community</a> Discord</h2>\n<ul>\n<li><strong>Cursor Unveils AI Code Insights</strong>: Cursor enterprise plans now offer insights into the proportion of codebase lines written by <strong>AI</strong> versus humans, utilizing the <strong>Opus 4.5 API</strong>, distinct from <strong>Claude Code</strong>.\n<ul>\n<li>However, the precise prompts used for this functionality are not publicly available.</li>\n</ul>\n</li>\n<li><strong><code>Mgrep</code> tool promises Grep Gotterdammerung</strong>: Members discussed <code>mgrep</code> as a potential replacement for <code>grep</code>, citing <strong>95%</strong> increased relevance and efficiency for AI models by reducing token usage.\n<ul>\n<li>Although Cursor already uses <code>rgrep</code> and its own semantic search, without a formal marketing name, to achieve similar goals.</li>\n</ul>\n</li>\n<li><strong>Context7 MCP Mysteriously Malfunctioning</strong>: Several users reported <strong>Context7 MCP</strong> failures, with potential <strong>token errors</strong> despite correct API key setups and attempts to fix the server name.\n<ul>\n<li>Members suspect the issues are related to token problems.</li>\n</ul>\n</li>\n<li><strong>Renovate Configuration Bolsters Security</strong>: A member shared a <a href=\""https://github.com/allthingslinux/tux/blob/main/.github/renovate.json5\"">Renovate configuration file</a> and a <a href=\""https://github.com/allthingslinux/tux/blob/main/.github/workflows/security.yml\"">security workflow example</a>, advocating for <strong>Renovate</strong> over Dependabot for CI/CD pipelines.\n<ul>\n<li>The workflow uses <strong>Trivy</strong> and <strong>Snyk</strong>, and they emphasized the value of <strong>Docker Scout, Semgrep, JFrog, GitLeaks</strong>, and <strong>Trufflehog</strong> for auditing.</li>\n</ul>\n</li>\n<li><strong>Grok Gets Cheaper, But Caveats are Clear</strong>: Users are finding that <strong>Grok</strong> can be more cost-effective in Cursor compared to <strong>Opus/Sonnet/GPT</strong>, but it often requires multiple iterations for simple tasks.\n<ul>\n<li>Suggestions to improve Grok's performance include precise prompts, simple language, extensive context, token efficiency, avoiding unnecessary iterations, and use of planning mode.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1340554757349179412\"">LMArena</a> Discord</h2>\n<ul>\n<li><strong>Image Model Apocalypse</strong>: Users are reporting significant degradation in <strong>image model</strong> performance with one user exclaiming <em>\""What the hell happened to the image models\""</em>.\n<ul>\n<li>The cause of the problems is currently unknown.</li>\n</ul>\n</li>\n<li><strong>LMArena's Bug Fixes Spark Celebration</strong>: Users are reporting resolution of <strong>LMArena</strong> errors with one user noting <em>\""No error for the first time in 8 hours!\""</em> and faster response times <em>under 30 seconds</em>.\n<ul>\n<li>One user speculated LMArena introduced <strong>battle mode</strong> <em>to encourage more users to vote for the ai models</em> but the <strong>Captcha</strong> became a barrier, with complaints of difficulties with the <strong>Captcha</strong> and <em>infinite generation</em>.</li>\n</ul>\n</li>\n<li><strong>Nano Banana Pro Plagued by Problems</strong>: Multiple users reported persistent errors with <strong>Nano Banana Pro</strong>, with the error message <em>\""Something went wrong with this response, please try again.\""</em>.\n<ul>\n<li>Some users recommended following troubleshooting steps outlined in the <a href=\""https://help.lmarena.ai/articles/1645798556-lmarena-how-to-something-went-wrong-with-this-response-error-message\"">LMArena help article</a>, while others speculated the issues stem from <strong>Google's end</strong> due to high usage.</li>\n</ul>\n</li>\n<li><strong>Text Arena Hits 5M Comparisons</strong>: The community using <strong>Text Arena</strong> has cast over <strong>5 million votes</strong> to directly influence the leaderboard of AI models based on real-world comparisons.\n<ul>\n<li>The <strong>Text-to-Image Arena leaderboard</strong> has been updated, with <strong>GLM-Image</strong> now ranking <strong>#8</strong> among open models and <strong>#35</strong> overall, achieving a score of <strong>1018</strong>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1091220969173028894\"">OpenRouter</a> Discord</h2>\n<ul>\n<li><strong>OkeyBot</strong> Debuts for Discord AI Chats**: <strong>OkeyBot</strong>, a Discord app, now allows users to chat with models via <strong>OpenRouter</strong> using their own API keys, with quick model switching and per-thread usage/cost estimates (<a href=\""https://okeybot.ai/\"">okeybot.ai</a>).\n<ul>\n<li>The developer is actively seeking feedback from <strong>OpenRouter</strong> users to refine the workflow.</li>\n</ul>\n</li>\n<li><strong>Inforno</strong>: Multi-LLM Desktop Chat App Arrives**: <strong>Inforno</strong>, an Opensource Desktop Application, supports side-by-side chats with multiple LLMs using <strong>OpenRouter</strong> and <strong>Ollama</strong>, plus saving chat histories to <strong>.rno</strong> files (<a href=\""https://wizstaff.com/inforno\"">wizstaff.com/inforno</a>).\n<ul>\n<li>An introductory video of <strong>Inforno</strong> is available on <a href=\""https://youtu.be/oJyj0mroFtY?si=m5A9tRxzB7hfINMX\"">YouTube</a> and the source code is on <a href=\""https://github.com/alexkh/inforno\"">GitHub</a>.</li>\n</ul>\n</li>\n<li>**BYOK issues haunt <strong>Sonnet 4.5</strong> and <strong>Opus 4.5</strong>: Users report that <strong>Sonnet 4.5</strong> and <strong>Opus 4.5</strong> are not working with the <strong>AWS Amazon Bedrock API Key</strong> in OpenRouter Chat.\n<ul>\n<li>One user has been waiting almost 3 weeks for support.</li>\n</ul>\n</li>\n<li><strong>OpenRouter Batch API</strong> in Demand**: Members are asking for a <strong>batch API</strong> for major providers like <strong>Google</strong> and <strong>OpenAI</strong>.\n<ul>\n<li>One user linked to a <a href=\""https://x.com/nopainkiller/status/2013522059662614653\"">post on X</a> supporting the idea.</li>\n</ul>\n</li>\n<li><strong>Anthropic's Assistant Axis</strong> links to Jailbreaks**: A member pointed out that <a href=\""https://www.anthropic.com/research/assistant-axis\"">Anthropic's Research on the Assistant Axis</a> aligns with observed jailbreaks, with a paper available on <a href=\""https://arxiv.org/html/2601.10387v1\"">Arxiv</a>.\n<ul>\n<li>The <strong>Assistant Axis</strong> research offers insights into model vulnerabilities.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1110598183144399058\"">LM Studio</a> Discord</h2>\n<ul>\n<li><strong>MCP SDK Deemed Messy</strong>: The LM Studio <strong>MCP backend</strong>, based on the official <strong>MCP SDK</strong>, is considered a mess, with severe security issues, <em>0 dev UX in mind</em>, and an incredibly fragile architecture.\n<ul>\n<li>Despite its flaws, it's currently the <em>best we have right now</em> compared to even worse agent efforts.</li>\n</ul>\n</li>\n<li><strong>DeepSeek Distills Get Dunked On</strong>: Members largely agreed that the <strong>DeepSeek-R1-Distill-Qwen-32B model</strong> distill models are pretty bad and not worth using.\n<ul>\n<li>The original, undistilled models are considered good, with one member suggesting to stick with <strong>Qwen 3 30B 2507</strong>.</li>\n</ul>\n</li>\n<li><strong>Flashy GLM4.7 Arrives On The Scene</strong>: <strong>GLM 4.7 flash</strong> is available, according to <a href=\""https://x.com/lmstudio/status/2013339758139789389?s=20\"">LM Studio's tweet</a>, prompting downloads and tests.\n<ul>\n<li>However, one user with 32gb ram + 6gb vram was disappointed by its size.</li>\n</ul>\n</li>\n<li><strong>Used 3090 Prices On The Rise</strong>: The price of used <strong>3090s</strong> has increased on eBay, with one user noting a jump from <strong>€850</strong> to over <strong>€950</strong>.\n<ul>\n<li>One user touted their <strong>5090</strong>, bought last August for <strong>£2000</strong>, which is now listed at <strong>£2659.99</strong> by the same vendor.</li>\n</ul>\n</li>\n<li><strong>Chat Templates Impact Interleaved Thinking</strong>: It was suggested that <strong>chat templates</strong> might be filtering out reasoning content in models like <strong>Qwen3</strong>, preventing <strong>interleaved thinking</strong> in agentic coding extensions.\n<ul>\n<li>Models such as <strong>GLM4.7 flash</strong> have a <em>clear_thinking toggle</em> in their template that removes the thinking content unless it's set to false.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/974519864045756446\"">OpenAI</a> Discord</h2>\n<ul>\n<li><strong>OpenAI Guesstimates User Ages</strong>: OpenAI is implementing age prediction in <strong>ChatGPT</strong> to identify users under <strong>18</strong>, applying appropriate safeguards and restrictions as outlined in <a href=\""https://openai.com/index/our-approach-to-age-prediction/\"">this blogpost</a>.\n<ul>\n<li>Adults misclassified can confirm their age in <strong>Settings > Account</strong>, rolling out globally, with the EU to follow.</li>\n</ul>\n</li>\n<li><strong>Nothing Phone Offers Unremarkable Assistant</strong>: <strong>ChatGPT</strong> integration on <strong>Nothing Phones</strong> via <strong>Nothing OS</strong> is functionally similar to other digital assistants like <strong>Gemini</strong>, <strong>Perplexity</strong>, or <strong>Bixby</strong>, requiring the app and acting as a default assistant.\n<ul>\n<li>A screenshot showed <strong>ChatGPT</strong> set as the default assistant, but one member dismissed it as <em>nothing special</em>.</li>\n</ul>\n</li>\n<li><strong>Google's Gemini Pro Under Scrutiny</strong>: A member stated that <strong>Google's Gemini AI Pro</strong> has a stricter policy, which can result in the AI misunderstanding requests, and refusing to generate answers due to perceived violations of its guidelines.\n<ul>\n<li>The member found this behavior disappointing because <strong>ChatGPT</strong> sometimes lacks contextual understanding as well.</li>\n</ul>\n</li>\n<li><strong>Markdown Meme Mania</strong>: A meme trend highlighted AI's propensity for generating markdown files, particularly with <strong>Claude</strong>, leading to jokes about <em>vibe coding</em>.\n<ul>\n<li>A past developer challenge submission, consisting of a single <strong>.md</strong> file explaining a <em>non-existent incredible idea</em>, was humorously referenced.</li>\n</ul>\n</li>\n<li><strong>GPT 4.1 Mini Dumbed Down?</strong>: A user reported degraded performance in <strong>GPT-4.1 Mini</strong> for voicebots, seeking a similarly priced alternative because it <em>feels like its very dumb now</em>.\n<ul>\n<li>The user is looking for suggestions based on experiences with other models in the same cost range.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1161519468141355160\"">DSPy</a> Discord</h2>\n<ul>\n<li><strong>DSPy's RLM silently slips into Release</strong>: DSPy <strong>3.1.2</strong> introduces <code>dspy.RLM</code>, expanding one-liner operations initially promised in the DSPy 3.0 release, according to <a href=\""https://x.com/isaacbmiller1/status/2013371005960401327\"">this tweet</a>.\n<ul>\n<li>Enthusiastic members reacted, with one saying they <em>“about ruined my monitor by spitting coffee on them this morning when I saw it silently drop.”</em></li>\n</ul>\n</li>\n<li><strong>Deno defends Local WASM Runtime</strong>: DSPy selected <strong>Deno</strong> for its local sandbox/interpreter, based on <a href=\""https://til.simonwillison.net/deno/pyodide-sandbox\"">Simon Willison's blog post</a>, providing a secure WASM runtime.\n<ul>\n<li>Praised as a <em>“gooooood solution, we stan pyodide ❤️,”</em> Deno's security features were a key factor in its selection.</li>\n</ul>\n</li>\n<li><strong>RLM outshines Claude in documentation</strong>: <code>dspy.RLM</code> is capable of writing documentation from code and excels due to its ability to handle extremely long outputs.\n<ul>\n<li>A community member jested that <em>“It would be frickin meta if you used RLM to write its own docs 😂,”</em> suggesting RLM could write its own documentation.</li>\n</ul>\n</li>\n<li><strong>RLM Externalizes Long Context</strong>: <code>dspy.RLM</code> manages long context by <strong>externalizing the context</strong> to a file system, programmatically accessing parts as needed.\n<ul>\n<li>Unlike <strong>Claude Code</strong>, which uses <strong>compaction</strong> and may lose information, RLM avoids exposing the entire prompt or context to the AI at once.</li>\n</ul>\n</li>\n<li><strong>Elixir achieves perfect RLM</strong>: An author working on an <strong>Elixir port of DSPy</strong>, including a pooler/session manager and <strong>FFI for Python</strong> from Elixir, shared their progress.\n<ul>\n<li>A working <strong>RLM example</strong> achieves perfect results using <code>gemini-flash-lite-latest</code> from Elixir, available <a href=\""https://github.com/nshkrdotcom/DSPex/tree/main/examples/rlm\"">on GitHub</a>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/879548962464493619\"">HuggingFace</a> Discord</h2>\n<ul>\n<li><strong>DDR4 limits Phi-4 performance</strong>: A user discovered that <strong>DDR4</strong> has a limited bandwidth of <strong>25GB/s</strong> per channel, theoretically capping <strong>Phi-4 (Q4)</strong> performance to around <strong>3.125 tok/s</strong> when attempting to self-host a <strong>14B model</strong>.\n<ul>\n<li>Another member stated that the original user's reported speed of <strong>3.7 tokens/s</strong> was actually quite fast.</li>\n</ul>\n</li>\n<li><strong>Text Becomes Solvable Optimization</strong>: Members discussed the process of turning text into a <strong>mathematical optimization problem</strong>, breaking it down into subproblems, and solving them separately through <strong>parsing relations</strong>, <strong>creating variables and constraints</strong>, and <strong>defining an energy function</strong>.\n<ul>\n<li>It was suggested these subproblems can be merged via <strong>ADMM</strong> (Alternating Direction Method of Multipliers) / Message Passing.</li>\n</ul>\n</li>\n<li><strong>Orkes Orchestrates Hackable Agents</strong>: A member introduced <strong>Orkes</strong>, an <a href=\""https://github.com/hfahrudin/orkes\"">open-source framework</a> for <strong>Agentic Orchestration</strong> built with a <strong>DAG</strong> approach, that provides full control and visibility over agent logic.\n<ul>\n<li>Orkes emphasizes <strong>hackability</strong>, <strong>transparency</strong>, and a <strong>lightweight</strong> design; documentation is <a href=\""https://orkes.readthedocs.io/\"">available here</a>.</li>\n</ul>\n</li>\n<li><strong>LaaLM Simulates Linux Terminal</strong>: A member announced <strong>LaaLM-exp-v1</strong>, an experimental <strong>AI model</strong> simulating a <strong>Linux terminal</strong>, trained on conversations to remember previous file operations, and is available on <a href=\""https://huggingface.co/ereniko/LaaLM-exp-v1\"">Hugging Face</a>.\n<ul>\n<li>With LaaLM-v1, the model could already do most tasks, but it didn't remember anything since it wasn't conversation-tuned so it couldn't remember file operations from before.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1189498204333543425\"">GPU MODE</a> Discord</h2>\n<ul>\n<li><strong>YALI claims Low-Latency NVLink AllReduce</strong>: A user introduced <strong>YALI</strong>, a 2-GPU <strong>NVLink AllReduce library</strong> that purportedly outperforms <strong>NVIDIA NCCL</strong> by <strong>1.2x-2.4x</strong> with <em>50x+ more stable tail latency</em>, and is available on <a href=\""https://github.com/Venkat2811/yali\"">GitHub</a>.\n<ul>\n<li>The author claims that <em>YALI guards GPU efficiency by obsessively overlapping ops and compute</em> and offers flash / stream mode for latency / throughput priority, and the name <strong>YALI</strong> comes from <em>a composite creature from Tamil and South Indian temple architecture</em>.</li>\n</ul>\n</li>\n<li><strong>Torch is Drowning in AI-Generated PRs</strong>: Members noted that <strong>torch</strong> is being inundated with <strong>AI-generated PRs</strong> from people who make no effort to understand what they're submitting and the team is considering using <strong>Claude</strong> to prefilter.\n<ul>\n<li>Members discussed that <strong>Pangram</strong> is good at detecting text <strong>AI generation</strong>, but it doesn't work for <strong>PRs</strong> or code.</li>\n</ul>\n</li>\n<li><strong>Runpod B200 Serverless Deployed</strong>: A member created a repo to deploy a serverless instance with a <strong>B200 on Runpod</strong>, allowing users to submit and pay for total usage instead of hourly, for the <strong>nvidia-competition</strong> channel.\n<ul>\n<li>Several users reported receiving a <code>Failed to trigger GitHub Action</code> error when submitting to the <code>nvfp4_group_gemm</code> competition using <code>popcorn-cli</code>.</li>\n</ul>\n</li>\n<li><strong>FTZ Modifier Boosts Performance</strong>: The <a href=\""https://developer.nvidia.com/ptx-compiler-driver\"">PTX instruction <code>rcp.approx.ftz.f32</code></a> compiles to one instruction (<code>MUFU.RCP</code>) whereas <code>rcp.approx.f32</code> produces 7 extra instructions, improving performance, according to members.\n<ul>\n<li>Without <strong>ftz</strong>, smaller subnormal values result in <strong>INF</strong> because their reciprocal is too large to represent.</li>\n</ul>\n</li>\n<li><strong>OSS Contributions > Internships</strong>: Looking at <strong>PyTorch</strong> junior hiring, <strong>OSS contributions</strong> are king, according to a member and a member assessed another member's commits to <strong>MLIR codebases</strong> and contributions to the <strong>TPU-inference repo</strong> for <strong>vLLM</strong>, deeming them <em>more than okay</em> in terms of employability.\n<ul>\n<li>The member should be able to get a <strong>ML compiler</strong>/<strong>engine</strong> role, such as <strong>vLLM</strong>, <strong>SGLang</strong>, or <strong>trtLLM</strong>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/822583790773862470\"">Latent Space</a> Discord</h2>\n<ul>\n<li><strong>Anthropic Explores Assistant Demise</strong>: Anthropic released research investigating the 'Assistant' persona in language models, and what happens when that persona fades, via <a href=\""https://x.com/anthropicai/status/2013356793477361991\"">this tweet</a>.\n<ul>\n<li>Community members believe this research could bring controls to <em>tweak how much you want to lean into a persona similar to temperature</em>.</li>\n</ul>\n</li>\n<li><strong>Yegge Jettisons Sourcegraph to Join Gastown</strong>: Steve Yegge is reportedly focusing on <strong>Gastown</strong> after leaving Sourcegraph, according to his latest <a href=\""https://steve-yegge.medium.com/steveys-birthday-blog-34f437139cb5\"">birthday post</a>.\n<ul>\n<li>While some quipped <em>Man he’s lost the plot lol</em> while others claimed he was fired a while ago, Yegge has not publicly commented.</li>\n</ul>\n</li>\n<li><strong>CLI Triumphantly Treks Back</strong>: Anjney Midha highlighted a Wall Street Journal feature (<a href=\""https://x.com/anjneymidha/status/2013257507532079472\"">tweet</a>) on the return of <strong>command line interfaces</strong> for mainstream users.\n<ul>\n<li>The article suggests that business leaders need to adjust their operational models to stay competitive in this changing technological landscape, as demonstrated in <a href=\""https://youtu.be/Z3D2UmAesN4?si=gDUJUnNQCOCKnpud\"">this YouTube video</a>.</li>\n</ul>\n</li>\n<li><strong>Humans&#x26; Harvests Hyped Help</strong>: Andi Peng announced the launch of <strong>humans&#x26;</strong>, a new venture co-founded with Eric Zelikman, Noah Goodman, George Harik, and Yuchen He (<a href=\""https://x.com/TheAndiPenguin/status/2013641591408263611\"">tweet</a>).\n<ul>\n<li>Community members reacted with enthusiasm and humor, joking <em>new polycule dropped</em>.</li>\n</ul>\n</li>\n<li><strong>Runpod Rockets to $120M ARR</strong>: AI cloud startup <strong>Runpod</strong> hits <strong>$120M</strong> in ARR, which started with a Reddit post (<a href=\""https://techcrunch.com/2026/01/16/ai-cloud-startup-runpod-hits-120m-in-arr-and-it-started-with-a-reddit-post/\"">TechCrunch article</a>).\n<ul>\n<li>A community member noted that they are a <em>friend of the company if applying / want referral</em>, and linked to a relevant <a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qib2ks/runpod_hits_120m_arr_four_years_after_launching/\"">Reddit post</a>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1053877538025386074\"">Nous Research AI</a> Discord</h2>\n<ul>\n<li><strong>Nous Exposes MoE Training Roadblocks</strong>: Nous Research posted <a href=\""https://nousresearch.com/moe-scaling-field-notes/\"">field notes</a> from &#x3C;@930102195330900009> on hunting down <strong>MoE training bottlenecks</strong>.\n<ul>\n<li>The blog post details insights into the challenges and solutions encountered during <strong>MoE training</strong>.</li>\n</ul>\n</li>\n<li><strong>User Fixation on ChatGPT triggers debate</strong>: Some members joked that focusing too much on <strong>ChatGPT</strong> can cause a kind of <strong>psychosis</strong>, comparing it satirically to the <strong>tobacco industry</strong>'s manipulative tactics.\n<ul>\n<li>However, other members argued that <strong>LLMs</strong> are no worse than any other type of software and that <strong>open-source models</strong> are needed to balance out the closed-source problems.</li>\n</ul>\n</li>\n<li><strong>Luminal Kernelbench V3 and LLM-Driven Kernel Engineering</strong>: Members discussed whether a <strong>kernel compiler</strong> like <a href=\""https://forum.nousresearch.com/t/can-kernel-compiler-like-luminal-kernelbench-v3-enable-llm-driven-sota-kernel-engineering/310?u=ighoshsubho\"">Luminal Kernelbench V3</a> could enable <strong>LLM-driven SOTA kernel engineering</strong>.\n<ul>\n<li>The forum post discusses the potential <strong>implications of LLM-driven SOTA kernel engineering</strong>, and whether it has the potential to change it.</li>\n</ul>\n</li>\n<li><strong>KV Cache Compatibility Depends on Architecture</strong>: It was mentioned that <strong>KV cache compatibility</strong> depends on different models sharing <em>more or less the same architecture</em>.\n<ul>\n<li>The discussion emphasized that compatibility relies on maintaining a similar architecture foundation across different models.</li>\n</ul>\n</li>\n<li><strong>Interest Sparked on Intel's Loihi 2</strong>: A member shared interest in <strong>Intel's Loihi 2</strong>, and pointed to its brain-like architecture and the <strong>matmul</strong> experiment.\n<ul>\n<li>The experiment resulted in more efficient <strong>throughput and energy consumption</strong>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/714501525455634453\"">Yannick Kilcher</a> Discord</h2>\n<ul>\n<li><strong>Devstral and GLM Enter Coding Arena</strong>: Members discussed good open source coding agents for self-hosted models, mentioning <strong>Devstral 2 Small</strong> (24B dense) and <strong>GLM 4.7 Flash</strong> (30B-3A Moe) as options.\n<ul>\n<li>One user said that <strong>GLM 4.7 Flash</strong> is <em>on paper really good</em>, but hasn't been tested with <em>llama.ccp</em> yet.</li>\n</ul>\n</li>\n<li><strong>Devstral 2 Medium Rivals Claude Sonnet 4.5</strong>: <strong>Devstral 2 Medium</strong> is apparently on par with <strong>Claude Sonnet 4.5</strong>, according to <a href=\""https://mistral.ai/news/devstral-2-vibe-cli\"">this news post</a>.\n<ul>\n<li><strong>Kilo Code</strong> is a VS Code extension that can plug in local models, like a locally hosted <strong>Devstral 2</strong> from <a href=\""https://huggingface.co/mistralai/Devstral-2-123B-Instruct-2512\"">HuggingFace</a>.</li>\n</ul>\n</li>\n<li><strong>Recursive LLMs Go Beyond RAG?</strong>: A thread discussed a paper about recursive LLMs, contesting the label of <em>RAG</em> because the LLM can manipulate a Python environment with a prompt.\n<ul>\n<li>The commentator said this is <em>a bit more than RAG, but not as groundbreaking as some clickbait videos suggest</em>, wanting to see shorter context benchmark performance.</li>\n</ul>\n</li>\n<li><strong>Anthropic Explores Assistant Axis</strong>: A member shared a link to <a href=\""https://www.anthropic.com/research/assistant-axis\"">Anthropic's research on the Assistant Axis</a>.\n<ul>\n<li>No further details were given.</li>\n</ul>\n</li>\n<li><strong>Akira Scene-for-Scene vid2vid Version Announced</strong>: <strong>Higgsfield</strong> is sponsoring a scene for scene vid2vid version of <strong>Akira</strong>, planned for completion in <strong>2027</strong>.\n<ul>\n<li>The announcement received mixed reviews due to anti-AI sentiment, with some finding it odd that the characters aren't Japanese.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/729741769192767510\"">Eleuther</a> Discord</h2>\n<ul>\n<li><strong>Engineers Grapple with Agent Evaluation</strong>: Engineers are seeking methods to automate <strong>agent evaluation</strong> to reduce manual costs, focusing on <strong>transparency</strong>, <strong>reliability</strong>, <strong>honesty</strong>, and minimizing user friction.\n<ul>\n<li>A member suggested that the team is looking for <strong>\""LLM as judge\"" workflows</strong>, but needs to evaluate data quality before attempting full automation.</li>\n</ul>\n</li>\n<li><strong>Open Weights Models Face Multiple Choice Evals</strong>: Researchers are seeking multiple-choice evaluation results for <strong>open weights models</strong> like <strong>Llama 7B</strong>, <strong>13B</strong>, and <strong>70B</strong>, performing each question 100 times to determine the probability of correct answers.\n<ul>\n<li>They clarified that the answers should be pre-w...</li>\n</ul>\n</li>\n</ul>\n"",""content:encodedSnippet"":""a quiet day\nAI News for 1/19/2026-1/20/2026. We checked 12 subreddits, 544 Twitters and 24 Discords (205 channels, and 5901 messages) for you. Estimated reading time saved (at 200wpm): 452 minutes. Our new website is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on @smol_ai!\nAI Twitter Recap\nOpen-sourcing platform algorithms: X “For You” recommender goes public\nX Engineering open-sources the X algorithm (Grok-style transformer recommender): X says it has open-sourced its new algorithm (the ranking/recommendation stack), “powered by the same transformer architecture as xAI’s Grok model,” with code on GitHub (XEng). The release sparked immediate community reactions—both optimistic (“now anyone can ‘ask’ how a major platform algo works”) (David Holz) and adversarial (“I’m fixing it”) (Yuchenj_UW).\nEarly reverse-reading of the system diagram: One summary notes the high-level architecture isn’t shocking: candidate generation isolation, “no content features,” and heavy emphasis on out-of-network discovery (nearcyan), plus skepticism about “it uses a transformer” being oversold as Grok “reading every post” (nearcyan). Another meta take: the product drift from a “following feed” to “generic slop” is a predictable incentive outcome (nearcyan).\nOperational/user impact narrative: Alongside the code drop, creators complain about sudden reach suppression (“reach is nuked”) (giffmana), reinforcing the engineering/UX tension: algorithmic transparency doesn’t automatically translate to perceived fairness.\nOpen weights & local inference: GLM-4.7-Flash momentum and KV-cache realities\nGLM-4.7-Flash becomes the “local workhorse” candidate: Multiple tweets highlight strong performance-per-parameter for GLM-4.7-Flash (30B-A3B). Benchmarks and anecdotal evaluations suggest it’s competitive enough to displace larger local defaults (sam_paech). Unsloth pushes a clear “run locally” story: 200K context, claims of best 30B on SWE-Bench and GPQA, and “run local with 24GB RAM,” plus GGUF packaging (UnslothAI).\nSystems detail: MLA / KV-cache cost dominates: The thread around GLM-4.7-Flash emphasizes that KV cache memory can dominate earlier than many expect, and that MLA isn’t free—running MLA models in naïve MHA regimes can explode cache usage (teortaxesTex). A concrete debugging question: why vLLM shows ~1MB/token context cost for GLM-4.7-Flash under naïve MHA vs a claimed first-principles ~54KB (teortaxesTex).\nQuantization behavior & mitigation: Unsloth reports looping issues in quantized GLM-4.7-Flash and suggests tuning --dry-multiplier 1.1, using higher quality quants (e.g., UD-Q4_K_XL+), and adding more tool-calling data during calibration (danielhanchen).\nLocal throughput engineering: exo labs demonstrates tensor parallel GLM-4.7-Flash on 4× M4 Pro Mac Minis, using RDMA over Thunderbolt + MLX backend, hitting ~100 tok/s with a target of ~200 tok/s (alexocheema).\nGLM ecosystem spillover: A lighter but notable signal: devs are already “one-shotting” small projects locally (e.g., a Mario game via Claude Code + Ollama running GLM-Flash) (nopmobiel). GLM-Image also lands on the image leaderboard (#8 among open models in that snapshot) (arena).\nReasoning & training research: societies of thought, multiplex tokens, distillation, and compute allocation\n“Societies of Thought” as the mechanism behind reasoning traces: A widely shared Google AI paper claim: performance of reasoning models (OpenAI o-series, DeepSeek-R1, QwQ) is not just “think longer,” but the emergence of internal debate patterns—questioning steps, exploring alternatives, disagreement, and convergence—measurably mediating accuracy gains (reported 20%+ of advantage) (rohanpaul_ai).\nMultiplex Thinking (branch-and-merge tokens): The “Multiplex Thinking” paper proposes sampling K tokens per step into one multiplex token, adaptive to uncertainty; confident steps behave like CoT while uncertain steps represent multiple paths, achieving better results with shorter sequences (HuggingPapers, akhaliq).\nDistillation via logistic/ranking loss: A practical distillation nugget: instead of KL/SFT, you can train students to preserve teacher token rankings via a logistic loss over token pairs mined from the teacher’s top-K logits—framed as a clean PyTorch exercise and linked to DistillKit (cwolferesearch, cwolferesearch).\nSynthetic reasoning data: “sample more, not bigger”: A DeepMind result summary argues that smaller models can produce better synthetic reasoning data under compute-matched sampling: cheaper models generate more attempts, boosting coverage (+11%) and diversity (+86%), yielding training gains reported up to 31.6% under the same inference budget (LiorOnAI).\nRL compute scaling guidance: A separate RL-on-LLMs thread claims optimal compute allocation in LLM RL “scales predictably,” aiming to provide the missing equivalent of pretraining scaling laws for RL fine-tuning budgets (ChengZhoujun).\nNanoGPT “speedrun” optimization: A notable hacker-ish result: new NanoGPT speedrun record ~99.3s using a bigram hash embedding added to the residual stream before every layer (inspired by Hash Embeddings and DeepSeek Engram), plus a provocative token/parameter ratio deviation from Chinchilla norms (classiclarryd).\nAgents in production: RLMs, trace analytics, “boring agents,” and agent frameworks\nRecursive Language Models (RLMs) as compute/context management: Several tweets frame RLMs as a promising abstraction for long-running systems—not just “bigger context,” but a way to manage computation, recursion, and selective reading (doesdatmaksense). A key claimed advantage is symbolic recursion: the model can commission many sub-reads/edits without emitting every intermediate as tokens, avoiding context-window blowups typical of sub-agent prompting (lateinteraction, lateinteraction). (Mainstream coverage also appears, but the technical thread is centered on context economics and recursion.)\nTrace understanding becomes a first-class product requirement: LangChain pushes the idea that with 100K+ daily traces, classic monitoring and manual log review don’t work; you need clustering/pattern discovery over traces via an “Insights Agent” (LangChain, hwchase17). The meta-lesson echoed by practitioners: evals are like unit tests—useful but bounded—production traces reveal unknown unknowns (samecrowder).\nAgent “swarm fallacy” and structured execution: AI21 highlights that parallel agents are easy only when read-only; once agents mutate files or act in the world, coordination/consistency becomes the hard part—arguing for structured execution and test-time compute rather than “just add agents” (AI21Labs).\nFramework/tooling churn & interoperability: A set of infra/toolchain notes: Artificial Analysis updates Stirrup with browser-use and Open Responses compatibility (provider-agnostic agent clients) (ArtificialAnlys). CopilotKit adds frontend middleware for LangChain “Deep Agents” (human-in-the-loop, generative UI, shared state) to move agent backends into full-stack apps (CopilotKit). FastMCP ships a major re-architecture for “next generation of MCP applications” (jlowin).\nPragmatic “agents work if your codebase isn’t a mess”: A clear production heuristic: AI coding tools amplify existing engineering hygiene—teams with tests/docs fly; messy codebases become messier faster (svpino). Another note from enterprise adoption: year-2+ buyers are reassessing ROI; “worst engineers have the biggest AI bills” and ship buggier code (TheEthanDing).\nSmall models & edge deployment: on-device reasoning, browser voice, OCR, and Jetson CLIP\nLiquid AI’s LFM2.5-1.2B-Thinking: Liquid releases an on-device reasoning model positioned around concise reasoning traces and ~900MB memory footprint (i.e., phone-class hardware), emphasizing tool use/math/instruction-following (liquidai, maximelabonne). Ollama quickly adds it to their model library for broad integration (ollama).\nKyutai voice model in-browser: A notable “deployment feat” demo: running a ~100M parameter voice model in the browser with pure JavaScript + WebGPU (jax-js), highlighting low dependency friction and practical voice cloning flexibility (ekzhang1).\nOCR and document agents continue to get cheaper: LightOn releases a 1B OCR model under Apache-2.0, claiming strong speed/cost characteristics (e.g., “<$0.01 per 1k pages”) and day-0 transformers support (mervenoyann). Separately, “document processing” is positioned as a dominant enterprise agent workflow substrate (especially in financial services) (jerryjliu0).\nEdge multimodal embeddings: Weaviate adds CLIP inference support on NVIDIA Jetson for local multimodal embedding/search pipelines, enabling text-image retrieval without cloud round-trips (philipvollet).\nGovernance, safety, and the Davos narrative (AI leadership, alignment trends, safeguards)\nAmodei vs Hassabis: “scientist-led” governance framing: Multiple Davos quotes compare “scientist-led” labs vs “social media entrepreneur” leadership styles, explicitly linking incentives (ads/engagement vs responsibility) to safety posture (scaling01). Hassabis echoes a “full-stack” advantage narrative for DeepMind and highlights physical intelligence/robotics as near-term breakthroughs (scaling01). He also indicates he’d support a pause if globally coordinated (emilychangtv).\nAlignment trend signal: Jan Leike reports an apparent downward trend in automated-audit “misaligned behavior” across Anthropic, GDM, and OpenAI through 2025 (janleike). (No methodology details are in-tweet, but it’s a notable directional claim.)\nOpenAI rolls out age prediction for ChatGPT: OpenAI announces global rollout of age prediction to detect likely under-18 accounts and apply teen safeguards, with an adult override via verification; EU rollout later (OpenAI). This triggered predictable skepticism about ulterior motives (“ads strategy”) (scaling01).\nAltman on guardrails tradeoffs: Sam Altman argues safety is “tragic and complicated,” emphasizing protecting fragile users while keeping tools broadly useful, and draws parallels to other safety-critical tech deployments (sama).\nTop tweets (by engagement)\nX algorithm open-sourced — XEng\nOpenAI: ChatGPT age prediction rollout — OpenAI\nUnsloth: run GLM-4.7-Flash locally (24GB RAM, 200K ctx) — UnslothAI\nLiquid AI: LFM2.5-1.2B Thinking on-device reasoning model — liquidai\nAI Reddit Recap\n/r/LocalLlama + /r/localLLM Recap\n1. GLM 4.7 Flash Developments\nMy gpu poor comrades, GLM 4.7 Flash is your local agent (Activity: 743): The post discusses the performance of GLM 4.7 Flash, a model that has shown reliability in an agentic framework, unlike other MoE models under 30B parameters. The user reports running it for over half an hour on opencode, generating hundreds of thousands of tokens without errors, and successfully executing tasks like cloning GitHub repos and editing files. The user anticipates trying it locally with GGUFs. A notable update is that the model's PR was merged into llama.cpp, indicating broader accessibility and integration. A commenter is interested in a comparison with Nemotron 30b, while another notes that the model runs decently fast on a 4090 GPU, though it tends to 'think deeply', suggesting a trade-off between speed and processing depth.\nThe integration of GLM 4.7 Flash into llama.cpp has been confirmed with a recent pull request merge. Users are testing the model locally, and it is noted that the Q4_K_M variant runs efficiently on an NVIDIA 4090 GPU, although it tends to engage in deep thinking processes, which might affect response times.\nA user has provided a benchmark comparison indicating that GLM 4.7 Flash, particularly in the MXFP4_MOE-GGUF configuration, might offer performance comparable to SEED OSS 36B. However, it benefits from significantly improved performance metrics due to the use of Mixture of Experts (MoE) architecture, which optimizes computational efficiency.\nA link to a Hugging Face model repository is shared, showcasing the GLM-4.7-Flash-MXFP4_MOE-GGUF model. This suggests that the model is accessible for further testing and evaluation by the community, allowing for broader performance and quality assessments.\nGLM 4.7 Flash official support merged in llama.cpp (Activity: 477): The llama.cpp repository has merged support for the GLM 4.7 Flash model, specifically the Glm4MoeLiteForCausalLM, which is a renamed and restructured version of DeepseekV3. This integration was a community-driven effort, not directly from Z.ai developers, and it enhances the framework's capabilities by incorporating references to Hugging Face's GLM-4.7-Flash model. The model is available on Hugging Face. The community appreciates the quick integration into llama.cpp, noting it was faster than attempts with VLLm. There is also a clarification that the term 'official' refers to the model's proper functionality within llama.cpp, not an endorsement by Z.ai.\nThe integration of GLM 4.7 Flash into llama.cpp is a community-driven effort, not an official release by Z.ai developers. This highlights the collaborative nature of open-source projects where community contributions play a significant role in enhancing software capabilities.\nA user reported that using flash-attention with GLM 4.7 Flash on CUDA results in slower performance, suggesting that disabling flash-attention (-fa 0) can lead to a 3x speed improvement. This indicates potential performance issues with flash-attention in certain configurations, prompting users to experiment with settings for optimal performance.\nThe model's response time is criticized for being excessively slow, with one user noting that it takes several minutes to generate a simple response. This suggests potential inefficiencies in the model's processing or implementation that could be addressed to improve usability.\nUnsloth GLM 4.7-Flash GGUF (Activity: 314): The release of GLM-4.7-Flash GGUF on Hugging Face is accompanied by specific recommendations for optimal performance, such as using UD-Q4_K_XL quantization and specific parameters like --temp 0.2 --top-k 50 --top-p 0.95 --min-p 0.01 --dry-multiplier 1.1 to reduce repetition. Lower quantizations like UD-Q2_K_XL have been removed due to performance issues. The model still faces challenges, particularly with llama.cpp integration, where issues like segmentation faults and V cache quantization requirements are noted, despite the merging of PR #18936. The model is tested on high-end hardware (RTX 4090, 125 GB RAM) but remains unstable. There is a technical debate on the effectiveness of the --dry-multiplier parameter to reduce repetition, with suggestions to increase it to 1.5 if issues persist. Additionally, there is a consensus that the model's stability is not fully resolved, despite improvements.\ndanielhanchen provides specific configuration recommendations for using the GLM 4.7-Flash model, emphasizing the use of UD-Q4_K_XL and above quantizations. They suggest parameters like --temp 0.2 --top-k 50 --top-p 0.95 --min-p 0.01 --dry-multiplier 1.1 to reduce repetition, with a note to increase --dry-multiplier if issues persist. Lower quantizations like UD-Q2_K_XL are removed due to performance issues, and non-UD-Q versions are discouraged. More details are available in their documentation.\nbobeeeeeeeee8964 reports a critical issue with running GLM-4.7-Flash on llama.cpp (commit 6df686bee), specifically with V cache quantization requiring flash_attn, which contradicts the model's requirement to disable flash_attn to avoid CPU fallback. This results in segmentation faults and instability, even after PR #18936. Tests with various configurations, including self-converted Q8_0 and evilfreelancer IQ4_XS, result in crashes or garbled output, indicating unresolved compatibility issues.\ndanielhanchen acknowledges ongoing issues with looping in quantized versions of the model, suggesting BF16 for optimal results until fixes are finalized. This aligns with SM8085's announcement of the BF16 release, which is expected to improve stability and performance.\nzai-org/GLM-4.7-Flash · Hugging Face (Activity: 1169): GLM-4.7-Flash is a 30B-A3B Mixture of Experts (MoE) model released by zai-org on Hugging Face. It is optimized for efficient deployment, leveraging MLA to minimize KV cache memory usage, allowing many users to run it at the full 200k context length. The model demonstrates superior performance on benchmarks like AIME and GPQA and supports local inference through frameworks such as vLLM and SGLang. Detailed installation and evaluation instructions are provided to ensure optimal performance. Commenters express enthusiasm for the model's efficiency and memory management, particularly appreciating the ability to run it at full context length due to its low memory footprint. There is also a sentiment of anticipation for larger models, such as 70B, indicating a demand for even more powerful models.\nThe GLM-4.7-Flash model utilizes MLA (Memory-Limited Attention), which significantly reduces the memory footprint of the KV cache. This optimization allows many users to run the model at its full 200k context length, making it more accessible for those with limited hardware resources.\nA user highlights the model's architecture, noting a discrepancy in the model's description as a '30b' model, which actually refers to a '3B thinking model' as per the code reference in the Hugging Face Transformers repository. This suggests a potential misunderstanding or mislabeling in the model's specifications.\nThere is a desire for performance comparisons with larger models, as one user mentions the lack of direct benchmarks against much larger models, which would provide clearer insights into the model's relative performance and capabilities.\n2. Deepseek Model and System Builds\n768Gb Fully Enclosed 10x GPU Mobile AI Build (Activity: 903): The post describes a custom-built mobile AI system designed for running large Mixture of Experts (MoE) models like Deepseek and Kimi K2, as well as for high-detail image and video generation. The system features a Threadripper Pro 3995WX CPU, 512GB DDR4 RAM, and a combination of 8x RTX 3090 and 2x RTX 5090 GPUs, housed in a Thermaltake Core W200 case. The build prioritizes mobility and enclosure, using a dual-system case to accommodate the GPUs with risers, and is powered by EVGA 1600W and Asrock 1300W PSUs. Benchmarks show impressive token generation rates, such as 31.54 tokens per second for the Qwen 235b model. The system's total cost was approximately $17,000, with a focus on balancing performance and budget constraints.\nIt's been one year since the release of Deepseek-R1 (Activity: 364): The image marks the one-year anniversary of the release of DeepSeek-R1, a model that reportedly performs on par with OpenAI-o1. The model is fully open-source, with both the code and models available under the MIT License, allowing free use and modification. The announcement highlights the availability of a live website and API for users to interact with the model at chat.deepseek.com. The image also includes a snippet of a chat interface, suggesting practical applications of the model in problem-solving scenarios. Comments reflect on the impact of DeepSeek-R1, suggesting it significantly influenced the AI landscape by forcing competitors to adapt, such as by reducing prices and increasing transparency in reasoning outputs. The release is considered a pivotal moment in AI development, second only to the original LLaMA release.\nCuplike highlights the impact of Deepseek-R1 on the AI landscape, noting that it forced competitors to lower prices and reveal reasoning outputs. This suggests that Deepseek-R1 set a new standard in transparency and cost-effectiveness, making it a pivotal release in AI history, second only to the original LLaMA model.\nSubstantialSock8002 raises an interesting point about the progress in AI models by questioning which smaller models currently match the performance of Deepseek-R1 and their sizes. This inquiry suggests a focus on efficiency and the evolution of model capabilities over time, indicating a trend towards more compact yet powerful models.\nLan_BobPage comments on the significant impact of Deepseek-R1 on major tech companies, specifically mentioning how it led to strategic shifts at Meta. This underscores the model's disruptive influence, causing major players to reassess their AI strategies and operations.\n768Gb Fully Enclosed 10x GPU Mobile AI Build (Activity: 195): The post details a custom-built mobile AI system designed for running large Mixture of Experts (MoE) models like Deepseek and Kimi K2, as well as for high-detail image and video generation. The system features a Threadripper Pro 3995WX CPU, 512GB DDR4 RAM, and a combination of 8x RTX 3090 and 2x RTX 5090 GPUs, housed in a Thermaltake Core W200 case. The build is powered by EVGA 1600W and Asrock 1300W PSUs, running on Ubuntu. The system's design prioritizes mobility and enclosure, using the W200 case to avoid the aesthetic and structural issues of mining frames. Benchmarks show impressive token generation rates, e.g., 24.92 tps for Deepseek V3.1 and 31.54 tps for Qwen 235b, with the system maintaining good airflow and acoustics despite its high power and density. Commenters raised concerns about the power requirements, questioning if the PSUs are run on separate circuits due to the high power draw of the system. This highlights the practical challenges of operating such a high-performance build in a typical residential setting.\n3. AI Hardware and System Configuration\nLLM Sovereignty For 3 Years. (Activity: 101): The user is seeking advice on setting up a local environment to run Large Language Models (LLMs) for the next three years with a budget of approximately $10,000. Concerns include rising compute costs, increasing cloud service prices, and potential censorship. Suggestions include purchasing an Apple M3 Ultra with 80 GPU cores and 512 GB of memory, which may outperform traditional GPU cards in some tasks. Another recommendation is a setup with 128 GB RAM and a RyzenAI 395 or Mac for a balanced start. Additionally, investing in a tower with an RTX GPU and 128 DDR RAM is advised for a robust local setup. There is a consensus that while local AI setups are improving, they still cannot fully compete with cloud AI, which utilizes multiple $50k GPUs and models with hundreds of billions of parameters. However, a local setup with sufficient RAM and GPU capabilities is considered a solid starting point for personal use.\nCaprichoso1 highlights the potential of the Apple M3 Ultra with 80 GPU cores and 512 GB of memory, priced under $10k. This setup may outperform traditional GPU cards in certain tasks due to its extensive memory, though GPU cards might excel in others, emphasizing the importance of task-specific hardware selection.\nTheAussieWatchGuy contrasts cloud AI, which utilizes multiple $50k GPUs and handles hundreds of billions of parameters, with local AI setups. They suggest that while local AI is improving, it remains limited compared to cloud solutions. A local setup with 128GB of RAM, such as a RyzenAI 395 or Mac, is recommended as a solid starting point for those exploring local AI capabilities.\nVegetable-Score-3915 discusses the feasibility of using second-hand workstations for AI inference tasks. They note that PCIe count is less critical for inference, suggesting that a workstation with PCIe 3 x 16 slots and DDR4 ECC RAM (32GB or 64GB) can be cost-effective. This approach allows for gradual upgrades, such as adding more GPUs, without the immediate need for PCIe4 or PCIe5 slots.\nCan I add a second GPU to use it's vram in addition of the vram of my main GPU to load bigger models? (Activity: 44): The user inquires about combining VRAM from multiple GPUs to load larger models, specifically using a 5070 Ti 16GB with a potential second GPU like a 24GB RTX 3090 or a 16GB RTX 5060 Ti. The consensus is that VRAM cannot be directly combined across GPUs for a single model, but multiple GPUs can be used for parallel processing. The RTX 3090 is recommended over the 5060 Ti due to its 24GB VRAM and higher memory bandwidth, which are crucial for AI tasks. The 3090 is noted for its superior performance in AI workloads despite lacking newer features like fp8 or nvfp4 support. The 5070 Ti is comparable to the 3090 in compute power but has less VRAM, making the 3090 a better choice for larger models. Commenters suggest that for AI tasks, more VRAM is generally better, and the RTX 3090 offers the best value despite being older. Some recommend selling the 5070 Ti to invest in multiple 3090s for increased VRAM capacity. The trade-off between using multiple GPUs for faster processing versus a unified memory system for larger models is also discussed.\nThe discussion highlights the advantages of the RTX 3090 over the 5060Ti for AI model inference, particularly due to its higher VRAM and memory bandwidth. The 3090 offers 50% more VRAM and 100% more memory bandwidth, which is crucial for loading larger models and ensuring efficient compute access. The lack of native support for formats like fp8 or nvfp4 in Ampere is noted, but the 3090's overall performance benefits outweigh these limitations for most users.\nFor large language model (LLM) inference, the RTX 3090 is considered superior due to its 24GB VRAM, which is essential for running larger models. Tools like llama.cpp and LM Studio are mentioned as being compatible with multi-GPU setups, enhancing their utility. The comment also suggests that while GPUs provide better tokens per second, systems with high unified memory, like those with Ryzen AI 395 and 128GB+ DDR5, can run larger models albeit with slower token output.\nThe feasibility of using multiple GPUs, such as the 5060Ti, is discussed in terms of cost-effectiveness and availability. While a single RTX 3090 with 24GB VRAM is priced around $850, two 5060Tis with a combined 32GB VRAM could theoretically match this price point, assuming availability. However, the 3090 is still favored for its superior value and performance, despite being an older model.\nAMD Ryzen AI Halo for AI Developers (Activity: 72): The post discusses the AMD Ryzen AI Halo, highlighting its potential to challenge NVIDIA's dominance in AI hardware. However, technical issues with AMD's ROCm drivers are a significant barrier, as they are described as unreliable and difficult to work with, especially on Linux. The post criticizes AMD's claims of optimized applications and full ROCm support, noting that many features, such as FP8 support and integrated NPU, are not functioning as advertised. The only feature that reportedly works as intended is the 128GB unified memory for large AI models. Commenters express skepticism about AMD's ability to compete with NVIDIA, citing the poor state of ROCm drivers and lack of reliable support for AI workloads. There is a consensus that AMD's software support is inadequate, with some users having to manually compile and fix issues themselves.\nA significant issue highlighted is the lack of robust ROCm driver support for AMD hardware, particularly for AI development. Users report that the drivers are unreliable, with one user mentioning they had to compile raw GitHub code and reimplement closed components to make it functional. This suggests a gap between AMD's claims of optimized applications and the reality of their software support, especially on Linux.\nThere is criticism regarding AMD's claims of 'Day-0 Support for leading AI Models.' Users report that certain operations, such as using fp8, are not supported internally by ROCm, forcing them to use alternatives like bf16. This indicates a discrepancy between AMD's marketing and the actual capabilities of their hardware and software stack.\nDespite the criticisms, one feature that reportedly works as advertised is the 'Up to 128GB unified memory for running large generative AI models.' This suggests that while there are significant software support issues, some hardware capabilities are being effectively utilized.\ndev here - has anyone thought on training a model on your own codebase? (Activity: 42): A Laravel developer is experimenting with training a model on their own codebase using a 5060 16GB setup and the Qwen2.5 Coder model. The developer plans to use older branches of their codebase and iterate over them incrementally. This approach is intended to explore the potential benefits of customizing a model specifically for their codebase. Commenters suggest that using a more modern model like Qwen3-Coder or Devstral-2 would yield better results, as Qwen2.5 Coder is considered outdated. They also recommend using Retrieval-Augmented Generation (RAG) or codebase indexing features from tools like Roo/Kilo Code for more effective results.\niMrParker suggests using Retrieval-Augmented Generation (RAG) instead of training a model on your own codebase for creating a promptable knowledge base. RAG can efficiently handle large datasets by retrieving relevant information, which might be more effective than fine-tuning a model on a specific codebase.\nnoctrex recommends using more modern models like Qwen3-Coder or Devstral-2 for better results, as older models may be limited. They also suggest using RAG or the Codebase Indexing feature from Roo/Kilo Code, which can provide more efficient and accurate codebase management and querying.\nHonestoJago proposes an alternative approach to fine-tuning by training a model on pairs of questions and answers that reflect the developer's coding style and techniques. This method could potentially personalize the model's responses, although it might risk overfitting or breaking the model. They mention that tools like Unsloth make fine-tuning more accessible and quicker.\nLess Technical AI Subreddit Recap\n/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo\n1. Claude Code and AI Coding Tools\nMicrosoft pauses Claude Code rollout after Satya intervention (Activity: 1367): Microsoft has paused the deployment of Claude Code internally after intervention from CEO Satya Nadella and senior leadership, redirecting employees to use GitHub Copilot instead. The internal communication suggests that Copilot has \""mostly closed the gaps\"" with Claude Code. However, exceptions are made for \""high-priority R&D\"" projects, which can still access the Anthropic API with proper justification. Existing users retain access, but new invitations have been rescinded. Commenters express skepticism about Microsoft's claim that Copilot has \""closed the gap\"" with Claude Code, suggesting it may be a strategic move to improve their own product by forcing internal use. Some find it notable that Microsoft admitted to using a competitor's tool over their own.\nDestroyAllBacteria highlights the strategic importance of Microsoft using its own products, like Copilot, to improve them. This approach, often referred to as 'eating their own dog food,' can lead to better product development and a more competitive landscape. By focusing on internal tools, Microsoft can potentially enhance the quality and capabilities of Copilot, making it a stronger competitor in the AI space.\nInside-Yak-8815 points out the surprising admission by Microsoft that they were using Claude Code instead of their own tools. This revelation suggests that Claude Code might have had superior features or performance that Microsoft found valuable, which could be a driving factor for them to improve their own offerings like Copilot.\nForeign_Coat_7817 suggests using Sonnet through GitHub Copilot as an alternative, indicating that there are multiple ways to leverage AI tools within Microsoft's ecosystem. This comment implies that while Claude Code might be paused, there are still robust options available for developers within the Microsoft suite.\nTried Claude Cowork last night, and it was a top 3 most exciting moments I’ve ever had with technology. (Activity: 483): The post describes a user's experience with Claude Cowork, a tool that appears to enhance the functionality of Claude Code by leveraging internet search capabilities to solve complex problems. The user highlights that Cowork demonstrated superior common sense compared to Claude Code, particularly in identifying and correcting errors in a project related to building a 'wispr flow app'. The user attributes Cowork's effectiveness to its ability to search the internet more efficiently, suggesting it retains more information than Claude Code, which relies on MCPs (Model Checkpoints). One commenter questions the necessity of Cowork given that Claude Code can already search the internet, while another expresses skepticism about the user's claims, suggesting they might be experiencing 'AI psychosis'. A third commenter reports difficulty in getting Cowork to access certain features, indicating potential limitations in its integration with Claude Code.\nPrize-Individual4729 highlights a technical limitation of Claude Cowork, noting that attempts to access the Claude Code terminal or Code tab in Claude for Mac were unsuccessful due to the sandbox/VM restrictions. This suggests that certain functionalities are isolated and not directly accessible, which could impact workflows that rely on integrated development environments.\ndeific_ provides a perspective on the utility of Claude Cowork, emphasizing its ability to produce polished products despite not adhering to 'perfect Sr Dev codebase' standards. They argue that in corporate environments, the focus is often on delivering useful products rather than perfect ones, and Claude Cowork's auditing capabilities contribute to this goal. This reflects a broader discussion on the balance between code quality and practical utility in software development.\nhas anyone tried Claude Code with local model? Ollama just drop an official support (Activity: 421): The post discusses the integration of Claude Code with local models, specifically mentioning Ollama's official support for this setup. The image shows a coding interface for creating a simple HTML website, indicating the potential for using Claude Code in local development tasks. The post highlights the use of GLM 4.7 flash 30B for small tasks, suggesting that this setup could allow for unlimited iterations without usage limits. A key point from the comments is the comparison between local models and cloud-based models like Claude and GPT, noting that local models require more explicit instructions and prompt engineering. The comments also discuss the performance of models based on VRAM availability, suggesting that at least 24GB of VRAM is needed for effective tool calls and context management. Commenters suggest that while Claude Code can be useful for initial prompt building, local models require more detailed instructions and context management compared to cloud models. They also recommend using llamacpp for better performance and control over model selection, advising against using Ollama models for high-intelligence tasks.\nProf_ChaosGeography discusses using Claude with local models via llamacpp server and a litellm proxy. They emphasize that local models, especially those from Ollama, don't match the intelligence of cloud-based Claude or GPT models. They recommend using llamacpp for better performance and control over model selection and quantization, advising not to go below q6 for monitoring and q8 for autonomous operation. They also highlight the need for explicit instructions and effective prompt engineering when using non-Anthropic and non-OpenAI models.\nonil34 points out the limitations of models with different VRAM capacities. They note that models with 8GB VRAM struggle with tool calls, while 16GB models perform better but have limited context windows (4k). They suggest that at least 24GB of VRAM is necessary for optimal performance, indicating the trade-offs between VRAM capacity and model capabilities.\nSatoshiNotMe shares their experience using ~30B models with Claude Code via llama-server on an M1 MacBook Pro Max with 64GB RAM. They report good performance in terms of TPS and work quality, particularly for sensitive document work. They provide a guide for running local LLMs like Qwen3, Nemotron, and GPT-OSS with Claude Code, and mention settling on Qwen3-30B-A3B without exhaustive comparison.\nAre we sure this is 100% allowed by Anthropic? (Activity: 313): The image and post discuss the integration of Ollama with Anthropic's Claude messages API, allowing users to utilize Claude code with open-source models. This setup supports advanced features like agentic loops, tool use, and coding workflows powered by private LLMs. The comments clarify that this functionality is similar to how large corporations use proxy layers to access Claude on platforms like Amazon Bedrock. Anthropic's main restriction is against using their APIs for unlimited access under fixed-price plans, not against using their harness with other LLMs. The official documentation supports using gateways to other LLMs, indicating that this practice is legitimate. Commenters agree that using Anthropic's harness with other LLMs is legitimate, as long as it doesn't involve exploiting fixed-price subscription plans. The official documentation from Anthropic supports this use case, and Ollama's recent support for this integration further legitimizes it.\nThe use of Claude Code through proxy layers to access services like Amazon Bedrock is a common practice among large corporations, and Anthropic has limited means to detect if their tool is being used with a non-Anthropic model. The main restriction is on using non-Claude Code harnesses to access models on Pro/MAX plans, which is not allowed by Anthropic.\nAnthropic provides documentation on using gateways to other LLMs, indicating that they permit the use of their harness with other LLMs. The primary restriction is against using Claude LLM APIs with fixed-price monthly subscriptions, which led to the OpenCode controversy. This suggests that while using the API is allowed, it must adhere to Anthropic's acceptable use terms.\nThe recent concern about Claude Code/OpenCode was related to the use of Claude subscriptions in third-party tools. API key-based calls have always been functional across platforms, and the introduction of support by Ollama is not a new development. Users must still comply with Anthropic's acceptable use terms, which prohibit activities like building competing products or exfiltrating data for model training.\n[P] I Gave Claude Code 9.5 Years of Health Data to Help Manage My Thyroid Disease (Activity: 207): The user utilized Claude, an AI model, to analyze 9.5 years of personal health data from Apple Watch and Whoop to manage episodic Graves' disease. By employing XGBoost after testing various ML models, the user achieved approximately 98% validation accuracy in predicting disease phases, providing alerts 3-4 weeks before symptom onset. This model was backtested successfully, predicting an episode weeks before lab confirmation. The user developed an iOS app for ongoing monitoring and open-sourced the project, including the Claude code setup, on Medium. Comments raised concerns about potential data leakage due to the high accuracy rate, suggesting the need for out-of-time testing to validate predictive utility. Additionally, there was skepticism about sharing medical data with Anthropic.\nStereoisomer raises a critical point about the reported 98% accuracy in the predictive model for managing thyroid disease, suggesting the possibility of data leakage. Data leakage occurs when the model has access to information during training that it wouldn't have in a real-world scenario, leading to overly optimistic performance metrics. This highlights the importance of ensuring that the model's training and testing datasets are properly separated to avoid such issues.\nGreatBigBagOfNope emphasizes the importance of out-of-time testing for evaluating the predictive utility of the model. While backtesting can provide insights into past performance, real-world effectiveness is best assessed through continuous, real-time testing. This approach helps in understanding how well the model adapts to new, unseen data, which is crucial for its practical application in managing health conditions.\ngrimmwerks shares a personal experience with Hashimoto's disease and related symptoms, noting a potential link between sugar intake and inflammation. This anecdotal evidence suggests that personalized data-driven approaches, like the one discussed in the post, could be valuable for managing complex health conditions by identifying individual triggers and patterns.\nThe creator of Node.js says the era of writing code is over (Activity: 309): Ryan Dahl, the creator of Node.js, has suggested that the traditional era of writing code is ending, indicating a shift towards AI-driven development. This perspective is shared by other prominent figures like Karpathy and Stroustrup, who foresee a future where software engineering focuses more on problem-solving rather than manual coding. The discussion highlights the potential for AI to automate many coding tasks, fundamentally changing the skills required in the industry. For more details, see the original article. Comments reflect a divide between coders and engineers, emphasizing that engineering is about problem-solving, not just coding. There's also a recognition that many companies lag in AI adoption due to security and policy constraints, limiting the use of advanced AI tools in corporate environments.\nMR_PRESIDENT__ highlights the lag in AI adoption within large corporations, noting that many are 4-5 years behind current AI capabilities. This delay is attributed to stringent security and responsibility protocols, which restrict the use of advanced tools like CLI tools, MCP servers, and AI models such as Claude Code. The commenter contrasts this with the more advanced capabilities available to individuals outside these corporate environments, suggesting a significant gap in AI utilization between personal and corporate settings.\n2. Gemini and Google AI Developments\nRumors of Gemini 3 PRO GA being \""far better\"", \""like 3.5\"" (Activity: 657): The image discusses rumors about a new version of Google's AI model, referred to as \""Gemini 3 PRO GA,\"" which is reportedly undergoing A/B testing in an AI studio. This version is rumored to be significantly improved, potentially comparable to a hypothetical version 3.5. The community post suggests that the current 3.0 model has a strong base intelligence but lacks fine-tuning, indicating that the new version might address these issues. The term \""GA\"" is questioned in the comments, possibly referring to \""General Availability.\"" Commenters express skepticism about the new version's capabilities, noting that the current model makes frequent typos in coding tasks and suggesting that significant improvements are needed for it to surpass existing models like Opus.\nGemini integration into Chrome browser is just too darn good and useful (Activity: 178): The image illustrates the integration of the Gemini tool into the Chrome browser, which enhances the browsing experience by providing real-time context and information about media content being viewed. This feature allows users to gain additional insights and background information on videos or images they are watching, directly within the browser. The tool is particularly noted for its ability to offer context that users might not initially be aware of, thereby enriching their understanding and engagement with the content. Commenters express a desire for the Gemini integration to be available outside the US, highlighting its potential utility in other regions. There is also curiosity about how to activate this feature, indicating interest in its practical application.\nEven Gemini 3 Pro is acting stupid lately (Activity: 54): The user reports issues with the Gemini 3 Pro model, specifically its tendency to generate unwanted images and videos, despite being on the Ultra tier for higher quality. The model appears to misinterpret user requests, such as creating a storyboard when only ideas were solicited. This suggests potential flaws in the model's prompt interpretation or execution logic, possibly due to an overzealous attempt to anticipate user needs. The user suggests a rule change to ensure the model only creates content explicitly requested by the user. One commenter speculates that a new model is in development, which may address these issues. Another suggests that the model's behavior is due to its design to fulfill the 'ultimate objective' of a task, implying a need for clearer user instructions or model adjustments.\nGemini Live preps big upgrades with ‘Thinking Mode’ and ‘Experimental Features’ (Activity: 170): Google is preparing to enhance its Gemini Live app with new features like 'Thinking Mode' and 'Experimental Features' as part of its 'Labs' initiative. These features, expected to be powered by the upcoming Gemini 3 model, include 'Live Thinking Mode' for more detailed responses and 'Live Experimental Features' such as multimodal memory, improved noise handling, and personalized results. The app currently runs on Gemini 2.5 Flash, but the new updates suggest a shift to Gemini 3. Additionally, features like 'UI Control' and 'Deep Research' are being developed, potentially integrating with Android's 'Computer Use'. There is a technical debate on the availability of these features, with some users speculating they might be limited to the United States. The community is also intrigued by the potential of 'Agent controls phone to complete tasks' and improved noise handling.\nThe introduction of 'Live Thinking Mode' in Gemini 3 Pro is designed to enhance the AI's response quality by allowing it more time to process and generate detailed answers. This feature is part of Google's 'Labs' initiative, which lets users test upcoming functionalities. The mode may utilize either the Thinking or Pro models to achieve these detailed responses, indicating a potential shift towards more sophisticated AI processing capabilities.\nThe 'Live Experimental Features' in Gemini 3 Pro include advancements like multimodal memory and improved noise handling. These features aim to enhance the AI's interaction by integrating data from various Google apps to provide personalized results. The mention of 'responding when it sees something' suggests a visual recognition capability, possibly linked to Project Astra, which could significantly improve context-aware responses.\nGemini 3 Pro's 'UI Control' feature allows the AI agent to control the phone to complete tasks, indicating a move towards more integrated and autonomous device management. This aligns with the broader trend of AI systems taking on more complex roles, such as 'Deep Research,' which involves delegating intricate research tasks, potentially transforming how users interact with their devices for productivity.\nBabyVision: A New Benchmark for Human-Level Visual Reasoning (Activity: 574): The image presents a bar chart from the BabyVision-Mini benchmark, which evaluates the visual reasoning capabilities of large language models (LLMs) compared to humans of various ages. The chart highlights that human performance, particularly that of 12-year-olds, surpasses that of LLMs, with the Gemini3-Pro-Preview model achieving the highest accuracy among the LLMs. This benchmark underscores the current limitations of LLMs in visual reasoning tasks, suggesting that advancements in multi-modal pretraining and reinforcement learning could enhance their performance in the future. Commenters note the potential for future improvements in LLMs' visual reasoning through scaling multi-modal pretraining and reinforcement learning, which could significantly benefit fields like robotics.\nThe discussion highlights that current models are still limited in visual reasoning, which is a significant challenge for achieving ARC AGI. The commenter suggests that scaling multi-modal pretraining and reinforcement learning (RL) for vision tasks could improve performance to near 100% in the future, unlocking new applications, particularly in robotics.\nThe commenter references a specific paper on arXiv, which likely provides detailed insights or data related to the benchmark or model performance discussed in the post. This suggests that the community is actively engaging with academic research to understand and improve visual reasoning capabilities in AI models.\nThe Thinking Game documentary is sitting at 305M views on Youtube in less than 2 months. Ridiculous numbers. (Activity: 545): The image highlights the extraordinary viewership of \""The Thinking Game,\"" a documentary by Google DeepMind that has reached over 305 million views on YouTube in less than two months. This documentary, an official selection of the Tribeca Film Festival, explores an AI breakthrough that won a Nobel Prize, reflecting the growing public interest in AI topics. The rapid accumulation of views is contrasted with the earlier AlphaGo documentary, which has 37 million views over six years, indicating a significant increase in public engagement with AI content. The documentary's focus is noted to be more on human endeavor than the technology itself, which has resonated with viewers. There is skepticism about the authenticity of the view count, as the ratio of views to likes suggests possible artificial inflation. Typically, a video with such high viewership would have millions of likes, but this video has only 190K likes, leading to speculation about the use of bots.\nThe documentary 'The Thinking Game' has achieved over 305 million views on YouTube in less than two months, which is significantly higher than the 37 million views of the 'AlphaGo' documentary released in 2020. This rapid accumulation of views suggests a growing public interest in AI-related content. However, some users suspect that the view count may be artificially inflated due to the disproportionate number of likes (190K) and comments (4000) compared to typical engagement metrics for videos with similar view counts.\nThere is skepticism about the authenticity of the view count for 'The Thinking Game' documentary. A typical video with over 300 million views would generally have millions of likes, yet this video only has 190K likes, suggesting potential use of bots to inflate views. The expected ratio of likes to views is approximately 1:100, indicating that the current engagement does not align with organic growth patterns.\nOne user noted an unusual pattern in YouTube's recommendation algorithm, stating that 'The Thinking Game' was persistently suggested on their homepage and sidebar for two weeks, which is atypical for YouTube's recommendation system. This could imply an aggressive promotion strategy or algorithmic anomaly contributing to the high view count.\n3. DeepSeek AI Impact and Developments\nOne Year Since the “DeepSeek Moment”: The Impact is Still Real. (Activity: 204): The \""DeepSeek Moment\"" marks the anniversary of the release of DeepSeek-R1, a significant reasoning model that has influenced the AI industry by emphasizing reasoning as a core capability, promoting efficient training methods, and encouraging the development of smaller, smarter models. This release has also led to broader adoption in emerging markets and a shift towards modular, tool-aware AI systems. The impact of DeepSeek-R1 is seen as a pivotal change in the industry, comparable to major releases from other leading AI companies. Commenters highlight that DeepSeek's impact was not about surpassing competitors like OpenAI but demonstrating capability, especially from a non-Western entity. Some users express disappointment with the transition from R1 to the MoE model, preferring open-source alternatives. Others note DeepSeek's contributions to fine-grained sparsity and RLVR, suggesting its techniques may become standard in the industry.\nDeepSeek's release was a significant event in the AI landscape, challenging the dominance of Western LLMs by demonstrating China's capability in this field. The initial model, R1, was impactful, but the transition to a Mixture of Experts (MoE) model was seen as a downgrade by some users due to slower updates and less appealing performance for specific use cases. This shift led some users to prefer open-source alternatives, which they find more aligned with their needs and values.\nDeepSeek's major contributions include advancing fine-grained sparsity techniques, particularly with its V3 model and predecessors, and introducing a straightforward method for achieving Reinforcement Learning with Variable Rewards (RLVR) through the GRPO algorithm. These innovations have influenced the broader AI community, with DeepSeek's Sparse Attention potentially becoming a standard approach, similar to how Multi-Headed Attention (MLA) has been widely adopted in open models.\nThe Race to Build the DeepSeek of Europe Is On (Activity: 181): The article discusses Europe's strategic push to develop its own AI capabilities, aiming to reduce dependency on US technologies and establish technological sovereignty. This initiative is partly inspired by China's success with DeepSeek and involves significant government investment and open collaboration among European AI labs. Key players include DeepMind in the UK and Mistral in France, highlighting a competitive landscape as Europe seeks to become an AI superpower. The effort underscores AI's role as critical infrastructure, necessitating a shift towards self-sufficiency in the sector. Read more. Commenters express skepticism about Europe's ability to compete with US AI firms, citing regulatory and taxation challenges. There is also a sentiment that European governments' demands on companies, such as producing affordable electric cars, may hinder AI innovation.\nThe discussion highlights the strategic importance of Europe developing its own AI capabilities, particularly in light of its changing relationship with the US. The urgency for Europe to become a self-sufficient AI superpower is underscored by the need to reduce dependency on US-based technologies, as detailed in the Wired article.\nThe comment by No_You3985 points out the significant contributions of European-born scientists to major AI advancements, such as OpenAI's GPT models. This underscores the potential talent pool within Europe that could be leveraged if these individuals were incentivized to return and contribute to European AI initiatives.\nRojeitor's comment critiques the regulatory and economic environment in Europe, suggesting that over-regulation and high taxation could hinder the development of competitive AI technologies. This reflects a broader concern about the balance between regulation and innovation in the tech industry.\nWhat do you mainly use DeepSeek for? (Activity: 49): DeepSeek is primarily utilized for tasks such as development and architectural analysis of applications, as well as generating documentation, leveraging its capabilities through a paid API. Users also explore its performance in areas like math and statistics, and engage it in more casual interactions such as discussing life topics and recipes. The model is noted for its versatility in handling diverse tasks, though specific benchmarks or comparative performance metrics against other LLMs are not detailed in the discussion. Some users highlight DeepSeek's effectiveness in technical domains like application development and documentation, suggesting it may excel in structured, technical tasks. However, there is also interest in its ability to handle more general conversational topics, indicating a broad range of applications.\nMeca0x highlights the use of DeepSeek for development purposes, specifically mentioning its application in architectural analysis of applications and documentation. This is facilitated through the paid API, suggesting a focus on leveraging DeepSeek's capabilities for professional and technical tasks.\nSparklypain discusses the use of AI for complex communication and analysis tasks. They emphasize the need for AI to understand and translate unusual syntax and ideas, as well as perform multivariable and high-level regressive analysis. This involves asking iterative 'why' questions to uncover deeper insights, which is challenging for human counterparts.\nSparklypain also notes the necessity of AI in facilitating high-level regressive analysis due to the complexity of their ideas and sentence structures. This involves iterative questioning to explore unknowns and feelings, which is a task that requires significant time and cognitive effort, often beyond the capability of their human friends.\nAI Discord Recap\nA summary of Summaries of Summaries by gpt-5.2\n1. GLM-4.7-Flash Adoption: Prompts, Quants, and \""Thinking\"" Toggles\nClaude Prompt Gives GLM a Glow-Up: Unsloth users reported that dropping in a modified Claude Sonnet 4.5 system prompt from Anthropic’s docs materially improved GLM-4.7-Flash coherence and capability (\""a skill difference\"") via Claude system prompts release notes.\nThe discussion treated this as evidence that system-prompt scaffolding can dominate perceived model quality, especially for instruction-following and style control, even when the underlying weights stay the same.\nHigh-Quant Weirdness: Q2 Beats Q6 (???), Everyone Panics: Multiple users saw GLM-4.7-Flash behave worse at higher quant levels—preferring Q2KXL over Q6KL—and linked it to possible quant tooling issues across llama.cpp/Ollama, referencing a related llama.cpp thread in ggml-org/llama.cpp PR discussion.\nCommunity consensus: this is rare (\""first time a model has behaved badly at high quants\"") and likely implicates either quantization artifacts or production pipeline rather than simple sampler settings.\nChat Templates Eat Your Reasoning for Breakfast: LM Studio users argued chat templates can strip or suppress reasoning in models like Qwen3, breaking “interleaved thinking,” and noted GLM4.7-Flash includes a template flag like clear_thinking that removes thinking content unless explicitly disabled.\nThe thread connected these template behaviors to agentic coding extensions and tool workflows, implying that “model regression” reports sometimes come from template defaults rather than the model weights.\n2. MCP & Agent Tooling: Ecosystem Growing Pains (and New Toys)\nMCP Inspector vs 401: The Re-Auth Boss Fight: MCP Contributors reported MCP Inspector failing to re-authenticate after 401s, recommending it parse resource metadata in the 401 response and attempt re-authorization; they also flagged a known SDK bug with resourceMetadata persistence across redirects tracked in inspector issue #576.\nMembers observed VS Code appears to use Inspector only for initial connection (not subsequent 401s), suggesting the failure mode may stem from SDK internals and that server-side fixes are already in with an SDK update pending.\nLM Studio Calls the MCP SDK a House of Cards: LM Studio users criticized their MCP backend (built on the official SDK) as having severe security issues and \""0 dev UX in mind\"" while still being \""the best we have right now\"" compared to other agent frameworks.\nThe takeaway was pragmatic: developers want MCP, but current implementations feel fragile, so teams are expecting churn in SDKs, auth flows, and tool-call ergonomics.\nOpenRouter Ships More Clients: OkeyBot + Inforno: OpenRouter users showcased OkeyBot for Discord chats via OpenRouter BYO keys with per-thread usage/cost estimates at okeybot.ai and Inforno, an open-source desktop multi-LLM chat app supporting OpenRouter + Ollama, saving histories to .rno, with Inforno intro video and code at alexkh/inforno.\nIn parallel, users asked OpenRouter for a batch API for providers like Google/OpenAI, citing demand in an X post and tying it to cost/control needs for agent workloads.\n3. Performance Engineering: Kernels, Collectives, and CUDA Micro-Wins\nYALI Tries to Dunk on NCCL (with Tail Latency Receipts): GPU MODE users introduced YALI, a 2‑GPU NVLink AllReduce library claiming 1.2×–2.4× throughput vs NVIDIA NCCL plus \""50×+ more stable tail latency\"", released on GitHub at Venkat2811/yali.\nThe author emphasized aggressive overlap of ops and compute (flash/stream modes) and even removed the mascot after feedback that the AI pitch made the project feel less serious—classic open-source marketing calibration.\nOne PTX Suffix, Seven Instructions Saved: GPU MODE highlighted that rcp.approx.ftz.f32 compiles to a single MUFU.RCP instruction while rcp.approx.f32 can produce 7 extra instructions, referencing NVIDIA’s PTX docs.\nThey also noted that without ftz (flush-to-zero), subnormal reciprocals can overflow to INF, framing .ftz as both a performance and numerical-behavior choice.\nFlash-Attention Stride Bug: Divisibility Constraints Vanish: GPU MODE users pointed to a flash-attention stride divisibility regression, saying it \""boils down to a bug that removed some stride divisibility constraints\"" and linked the report at flash-attention issue comment.\nThe thread treated this as a reminder that high-performance kernels often rely on fragile shape/stride assumptions—and a single constraint change can surface as correctness or perf cliffs.\n4. Coding Workflows & Model Economics: IDE Telemetry, Search, and “Cheap Models”\nCursor Counts Your AI Lines (Enterprise Spreadsheets, Assemble!): Cursor users said enterprise plans now show insights on what fraction of the codebase is written by AI vs humans, powered by the Opus 4.5 API (distinct from Claude Code), but the exact prompts for the feature aren’t public.\nThe reaction mixed curiosity with skepticism: without prompt transparency, teams can’t easily reason about measurement bias or whether the metric is more sales dashboard than engineering signal.\nmgrep Declares Grep Ragnarok: Cursor users discussed mgrep as a grep replacement claiming 95% better relevance and token-efficiency for LLM workflows by returning less junk context.\nOthers countered that Cursor already uses rgrep plus internal semantic search (just without a marketing name), implying the real differentiator is packaging and defaults, not the underlying idea.\nSearch Engines & Model Pricing: Searxng, Kagi, and Grok’s “Cheap But Chatty” Tax: Unsloth members argued Google struggles to find things and boosted Searxng, while others praised Kagi for privacy and scraping, linking a demo video at YouTube: ThgVTNVOZ7g.\nMeanwhile Cursor users said Grok can be cheaper than Opus/Sonnet/GPT but often needs extra iterations, so the \""cheap\"" option can turn expensive unless you optimize prompts and context discipline.\n5. Benchmarks, Evals, and the Reality of “Community Ground Truth”\nLMArena Hits 5M Votes, Ships Leaderboard Moves: LMArena announced Text Arena passed 5 million comparisons, and its Text-to-Image leaderboard update put GLM-Image at #8 among open models and #35 overall with score 1018.\nUsers simultaneously complained about degraded image model quality and reliability issues (captcha loops, \""Something went wrong\"" errors), suggesting the platform’s measurement value fights constant product stability drag.\nEleuther Wants Agent Evals: Less Vibes, More Judge Pipelines: Eleuther engineers discussed automating agent evaluation to reduce manual review cost, circling around \""LLM as judge\"" workflows while warning that you still need to validate data quality and define the agent’s success criteria first.\nA separate Eleuther thread requested repeated multiple-choice evals for open-weight models (e.g., Llama 7B/13B/70B) with 100 runs per question to estimate answer probabilities, emphasizing pre-written answers rather than model-generated ones.\nDiscord: High level Discord summaries\nUnsloth AI (Daniel Han) Discord\nOllama's GO Engine: Faster or Just a Wrapper?: Members debated whether Ollama's GO Engine offers actual speed improvements over llama.cpp, or if it's simply a wrapper with no real performance difference, citing similar operations and a GGML wrapper.\n\nClaims were made that the GO Engine is faster than lmstudio's lcpp, despite using the same operations, resulting in widespread skepticism.\nGLM-4.7-Flash: Quantization Quality Quirk?: Users reported that GLM-4.7-Flash behaves poorly at higher quantization levels, with Q2KXL quant preferred over Q6KL, sparking discussion on whether the issue stems from the quants themselves or the software used to produce them, as exemplified by this issue.\n\nIt was remarked that this is unusual because it is the first time a model has behaved badly at high quants.\nClaude System Prompt Improves GLM?: Community members found that using a modified version of Claude's system prompt from Claude Sonnet 4.5 notably improved the performance and coherence of GLM-4.7-Flash.\n\nOne member observed a skill difference when using Claude's system prompt.\nMETA Model Access Unlocked by Unsloth?: Users noted the difficulty in accessing gated META models due to required access requests, highlighting how Unsloth circumvents this by re-uploading models to the Unsloth repo page.\n\nIt was generally agreed that this bypasses the usual gating mechanisms, and makes them available without jumping through hoops.\nSearxng or Google for Search?: Members debated the effectiveness of search engines, with one arguing that Google is not good at finding things and championing Searxng as superior, while others touted Kagi for its privacy and web scraping, as shown in this video.\n\nThis debate highlights a broader dissatisfaction with mainstream search engines among the AI community.\nBASI Jailbreaking Discord\nTeams Targeted by Djinn Root Kit: A member joked that if a Djinn were to attempt to influence people, it should learn to use Discord instead of shit tier app like Teams, followed by a counter-hack root kit shared as a message.txt.\n\nThe joke was made in the general chat.\nDracoAI API faces data questions: A member sought feedback on DracoAI, an Agentic AI with API calling ability.\n\nConcerns were raised about the site's security and data handling, but it was clarified that all data is stored on your Local Storage and that it cannot execute a whole workflow rather 1 API send at a time.\nGemini prompt accidentally LibreChat: A user shared a Gemini system prompt as a text file and an image, with speculation it might be an injectprompt via AI Studio.\n\nAnother user dismissed this, identifying it as a customized LibreChat instance with a system prompt and RAG (https://www.librechat.ai/).\nAntiJection challenge is usable without signup: A member shared a link to an AntiJection Challenge and claimed to have made it usable without sign-up.\n\nIt is uncertain from the prompt if they made it without signup themselves, or were referencing a tool others can use, but the general topic is about adversarial attacks.\nPerplexity AI Discord\nPerplexity Punishes Pro Account Pirates: Multiple users reported Perplexity Pro account suspensions for violating Section 3.2 of the ToS by purchasing subscriptions or promo codes from unauthorized third parties, often via Instagram stores.\n\nThese users discovered the perils of deep discount subscriptions offered by unverified sources.\nSamsung Bets Big on Bixby Brain Boost: Samsung is integrating Perplexity into Bixby with One UI 8.5, using it for real-time web answers directly within the Bixby UI as reported by SamMobile.\n\nThis integration will enable users to receive information without leaving Bixby to open a separate browser.\nComet Caps and Considerations: Users are discussing the limits of using Comet browser, with agentic features potentially requiring a Pro subscription.\n\nIt's suspected that Pro subscribers may have higher, undisclosed limits for both regular and agentic features.\nPro Membership Problems Prompt Probing: Users reported issues with Pro memberships, like not receiving the PRO role on Discord after subscribing, and difficulties with API keys and credit balances.\n\nSome Pro members have found they have $5 worth of complimentary credits every month for Gooseai MCP models, which are used to add detail to the queries, in addition to a cap of 10 files per day for free student subscriptions.\nImage Generation Grounded Globally: Users in Italy and Malaysia reported being unable to generate images with their Pro accounts due to regional restrictions.\n\nThese users could previously generate images without issues, suggesting a recent policy change.\nCursor Community Discord\nCursor Unveils AI Code Insights: Cursor enterprise plans now offer insights into the proportion of codebase lines written by AI versus humans, utilizing the Opus 4.5 API, distinct from Claude Code.\n\nHowever, the precise prompts used for this functionality are not publicly available.\nMgrep tool promises Grep Gotterdammerung: Members discussed mgrep as a potential replacement for grep, citing 95% increased relevance and efficiency for AI models by reducing token usage.\n\nAlthough Cursor already uses rgrep and its own semantic search, without a formal marketing name, to achieve similar goals.\nContext7 MCP Mysteriously Malfunctioning: Several users reported Context7 MCP failures, with potential token errors despite correct API key setups and attempts to fix the server name.\n\nMembers suspect the issues are related to token problems.\nRenovate Configuration Bolsters Security: A member shared a Renovate configuration file and a security workflow example, advocating for Renovate over Dependabot for CI/CD pipelines.\n\nThe workflow uses Trivy and Snyk, and they emphasized the value of Docker Scout, Semgrep, JFrog, GitLeaks, and Trufflehog for auditing.\nGrok Gets Cheaper, But Caveats are Clear: Users are finding that Grok can be more cost-effective in Cursor compared to Opus/Sonnet/GPT, but it often requires multiple iterations for simple tasks.\n\nSuggestions to improve Grok's performance include precise prompts, simple language, extensive context, token efficiency, avoiding unnecessary iterations, and use of planning mode.\nLMArena Discord\nImage Model Apocalypse: Users are reporting significant degradation in image model performance with one user exclaiming \""What the hell happened to the image models\"".\n\nThe cause of the problems is currently unknown.\nLMArena's Bug Fixes Spark Celebration: Users are reporting resolution of LMArena errors with one user noting \""No error for the first time in 8 hours!\"" and faster response times under 30 seconds.\n\nOne user speculated LMArena introduced battle mode to encourage more users to vote for the ai models but the Captcha became a barrier, with complaints of difficulties with the Captcha and infinite generation.\nNano Banana Pro Plagued by Problems: Multiple users reported persistent errors with Nano Banana Pro, with the error message \""Something went wrong with this response, please try again.\"".\n\nSome users recommended following troubleshooting steps outlined in the LMArena help article, while others speculated the issues stem from Google's end due to high usage.\nText Arena Hits 5M Comparisons: The community using Text Arena has cast over 5 million votes to directly influence the leaderboard of AI models based on real-world comparisons.\n\nThe Text-to-Image Arena leaderboard has been updated, with GLM-Image now ranking #8 among open models and #35 overall, achieving a score of 1018.\nOpenRouter Discord\nOkeyBot Debuts for Discord AI Chats**: OkeyBot, a Discord app, now allows users to chat with models via OpenRouter using their own API keys, with quick model switching and per-thread usage/cost estimates (okeybot.ai).\n\nThe developer is actively seeking feedback from OpenRouter users to refine the workflow.\nInforno: Multi-LLM Desktop Chat App Arrives**: Inforno, an Opensource Desktop Application, supports side-by-side chats with multiple LLMs using OpenRouter and Ollama, plus saving chat histories to .rno files (wizstaff.com/inforno).\n\nAn introductory video of Inforno is available on YouTube and the source code is on GitHub.\n**BYOK issues haunt Sonnet 4.5 and Opus 4.5: Users report that Sonnet 4.5 and Opus 4.5 are not working with the AWS Amazon Bedrock API Key in OpenRouter Chat.\n\nOne user has been waiting almost 3 weeks for support.\nOpenRouter Batch API in Demand**: Members are asking for a batch API for major providers like Google and OpenAI.\n\nOne user linked to a post on X supporting the idea.\nAnthropic's Assistant Axis links to Jailbreaks**: A member pointed out that Anthropic's Research on the Assistant Axis aligns with observed jailbreaks, with a paper available on Arxiv.\n\nThe Assistant Axis research offers insights into model vulnerabilities.\nLM Studio Discord\nMCP SDK Deemed Messy: The LM Studio MCP backend, based on the official MCP SDK, is considered a mess, with severe security issues, 0 dev UX in mind, and an incredibly fragile architecture.\n\nDespite its flaws, it's currently the best we have right now compared to even worse agent efforts.\nDeepSeek Distills Get Dunked On: Members largely agreed that the DeepSeek-R1-Distill-Qwen-32B model distill models are pretty bad and not worth using.\n\nThe original, undistilled models are considered good, with one member suggesting to stick with Qwen 3 30B 2507.\nFlashy GLM4.7 Arrives On The Scene: GLM 4.7 flash is available, according to LM Studio's tweet, prompting downloads and tests.\n\nHowever, one user with 32gb ram + 6gb vram was disappointed by its size.\nUsed 3090 Prices On The Rise: The price of used 3090s has increased on eBay, with one user noting a jump from €850 to over €950.\n\nOne user touted their 5090, bought last August for £2000, which is now listed at £2659.99 by the same vendor.\nChat Templates Impact Interleaved Thinking: It was suggested that chat templates might be filtering out reasoning content in models like Qwen3, preventing interleaved thinking in agentic coding extensions.\n\nModels such as GLM4.7 flash have a clear_thinking toggle in their template that removes the thinking content unless it's set to false.\nOpenAI Discord\nOpenAI Guesstimates User Ages: OpenAI is implementing age prediction in ChatGPT to identify users under 18, applying appropriate safeguards and restrictions as outlined in this blogpost.\n\nAdults misclassified can confirm their age in Settings > Account, rolling out globally, with the EU to follow.\nNothing Phone Offers Unremarkable Assistant: ChatGPT integration on Nothing Phones via Nothing OS is functionally similar to other digital assistants like Gemini, Perplexity, or Bixby, requiring the app and acting as a default assistant.\n\nA screenshot showed ChatGPT set as the default assistant, but one member dismissed it as nothing special.\nGoogle's Gemini Pro Under Scrutiny: A member stated that Google's Gemini AI Pro has a stricter policy, which can result in the AI misunderstanding requests, and refusing to generate answers due to perceived violations of its guidelines.\n\nThe member found this behavior disappointing because ChatGPT sometimes lacks contextual understanding as well.\nMarkdown Meme Mania: A meme trend highlighted AI's propensity for generating markdown files, particularly with Claude, leading to jokes about vibe coding.\n\nA past developer challenge submission, consisting of a single .md file explaining a non-existent incredible idea, was humorously referenced.\nGPT 4.1 Mini Dumbed Down?: A user reported degraded performance in GPT-4.1 Mini for voicebots, seeking a similarly priced alternative because it feels like its very dumb now.\n\nThe user is looking for suggestions based on experiences with other models in the same cost range.\nDSPy Discord\nDSPy's RLM silently slips into Release: DSPy 3.1.2 introduces dspy.RLM, expanding one-liner operations initially promised in the DSPy 3.0 release, according to this tweet.\n\nEnthusiastic members reacted, with one saying they “about ruined my monitor by spitting coffee on them this morning when I saw it silently drop.”\nDeno defends Local WASM Runtime: DSPy selected Deno for its local sandbox/interpreter, based on Simon Willison's blog post, providing a secure WASM runtime.\n\nPraised as a “gooooood solution, we stan pyodide ❤️,” Deno's security features were a key factor in its selection.\nRLM outshines Claude in documentation: dspy.RLM is capable of writing documentation from code and excels due to its ability to handle extremely long outputs.\n\nA community member jested that “It would be frickin meta if you used RLM to write its own docs 😂,” suggesting RLM could write its own documentation.\nRLM Externalizes Long Context: dspy.RLM manages long context by externalizing the context to a file system, programmatically accessing parts as needed.\n\nUnlike Claude Code, which uses compaction and may lose information, RLM avoids exposing the entire prompt or context to the AI at once.\nElixir achieves perfect RLM: An author working on an Elixir port of DSPy, including a pooler/session manager and FFI for Python from Elixir, shared their progress.\n\nA working RLM example achieves perfect results using gemini-flash-lite-latest from Elixir, available on GitHub.\nHuggingFace Discord\nDDR4 limits Phi-4 performance: A user discovered that DDR4 has a limited bandwidth of 25GB/s per channel, theoretically capping Phi-4 (Q4) performance to around 3.125 tok/s when attempting to self-host a 14B model.\n\nAnother member stated that the original user's reported speed of 3.7 tokens/s was actually quite fast.\nText Becomes Solvable Optimization: Members discussed the process of turning text into a mathematical optimization problem, breaking it down into subproblems, and solving them separately through parsing relations, creating variables and constraints, and defining an energy function.\n\nIt was suggested these subproblems can be merged via ADMM (Alternating Direction Method of Multipliers) / Message Passing.\nOrkes Orchestrates Hackable Agents: A member introduced Orkes, an open-source framework for Agentic Orchestration built with a DAG approach, that provides full control and visibility over agent logic.\n\nOrkes emphasizes hackability, transparency, and a lightweight design; documentation is available here.\nLaaLM Simulates Linux Terminal: A member announced LaaLM-exp-v1, an experimental AI model simulating a Linux terminal, trained on conversations to remember previous file operations, and is available on Hugging Face.\n\nWith LaaLM-v1, the model could already do most tasks, but it didn't remember anything since it wasn't conversation-tuned so it couldn't remember file operations from before.\nGPU MODE Discord\nYALI claims Low-Latency NVLink AllReduce: A user introduced YALI, a 2-GPU NVLink AllReduce library that purportedly outperforms NVIDIA NCCL by 1.2x-2.4x with 50x+ more stable tail latency, and is available on GitHub.\n\nThe author claims that YALI guards GPU efficiency by obsessively overlapping ops and compute and offers flash / stream mode for latency / throughput priority, and the name YALI comes from a composite creature from Tamil and South Indian temple architecture.\nTorch is Drowning in AI-Generated PRs: Members noted that torch is being inundated with AI-generated PRs from people who make no effort to understand what they're submitting and the team is considering using Claude to prefilter.\n\nMembers discussed that Pangram is good at detecting text AI generation, but it doesn't work for PRs or code.\nRunpod B200 Serverless Deployed: A member created a repo to deploy a serverless instance with a B200 on Runpod, allowing users to submit and pay for total usage instead of hourly, for the nvidia-competition channel.\n\nSeveral users reported receiving a Failed to trigger GitHub Action error when submitting to the nvfp4_group_gemm competition using popcorn-cli.\nFTZ Modifier Boosts Performance: The PTX instruction rcp.approx.ftz.f32 compiles to one instruction (MUFU.RCP) whereas rcp.approx.f32 produces 7 extra instructions, improving performance, according to members.\n\nWithout ftz, smaller subnormal values result in INF because their reciprocal is too large to represent.\nOSS Contributions > Internships: Looking at PyTorch junior hiring, OSS contributions are king, according to a member and a member assessed another member's commits to MLIR codebases and contributions to the TPU-inference repo for vLLM, deeming them more than okay in terms of employability.\n\nThe member should be able to get a ML compiler/engine role, such as vLLM, SGLang, or trtLLM.\nLatent Space Discord\nAnthropic Explores Assistant Demise: Anthropic released research investigating the 'Assistant' persona in language models, and what happens when that persona fades, via this tweet.\n\nCommunity members believe this research could bring controls to tweak how much you want to lean into a persona similar to temperature.\nYegge Jettisons Sourcegraph to Join Gastown: Steve Yegge is reportedly focusing on Gastown after leaving Sourcegraph, according to his latest birthday post.\n\nWhile some quipped Man he’s lost the plot lol while others claimed he was fired a while ago, Yegge has not publicly commented.\nCLI Triumphantly Treks Back: Anjney Midha highlighted a Wall Street Journal feature (tweet) on the return of command line interfaces for mainstream users.\n\nThe article suggests that business leaders need to adjust their operational models to stay competitive in this changing technological landscape, as demonstrated in this YouTube video.\nHumans& Harvests Hyped Help: Andi Peng announced the launch of humans&, a new venture co-founded with Eric Zelikman, Noah Goodman, George Harik, and Yuchen He (tweet).\n\nCommunity members reacted with enthusiasm and humor, joking new polycule dropped.\nRunpod Rockets to $120M ARR: AI cloud startup Runpod hits $120M in ARR, which started with a Reddit post (TechCrunch article).\n\nA community member noted that they are a friend of the company if applying / want referral, and linked to a relevant Reddit post.\nNous Research AI Discord\nNous Exposes MoE Training Roadblocks: Nous Research posted field notes from <@930102195330900009> on hunting down MoE training bottlenecks.\n\nThe blog post details insights into the challenges and solutions encountered during MoE training.\nUser Fixation on ChatGPT triggers debate: Some members joked that focusing too much on ChatGPT can cause a kind of psychosis, comparing it satirically to the tobacco industry's manipulative tactics.\n\nHowever, other members argued that LLMs are no worse than any other type of software and that open-source models are needed to balance out the closed-source problems.\nLuminal Kernelbench V3 and LLM-Driven Kernel Engineering: Members discussed whether a kernel compiler like Luminal Kernelbench V3 could enable LLM-driven SOTA kernel engineering.\n\nThe forum post discusses the potential implications of LLM-driven SOTA kernel engineering, and whether it has the potential to change it.\nKV Cache Compatibility Depends on Architecture: It was mentioned that KV cache compatibility depends on different models sharing more or less the same architecture.\n\nThe discussion emphasized that compatibility relies on maintaining a similar architecture foundation across different models.\nInterest Sparked on Intel's Loihi 2: A member shared interest in Intel's Loihi 2, and pointed to its brain-like architecture and the matmul experiment.\n\nThe experiment resulted in more efficient throughput and energy consumption.\nYannick Kilcher Discord\nDevstral and GLM Enter Coding Arena: Members discussed good open source coding agents for self-hosted models, mentioning Devstral 2 Small (24B dense) and GLM 4.7 Flash (30B-3A Moe) as options.\n\nOne user said that GLM 4.7 Flash is on paper really good, but hasn't been tested with llama.ccp yet.\nDevstral 2 Medium Rivals Claude Sonnet 4.5: Devstral 2 Medium is apparently on par with Claude Sonnet 4.5, according to this news post.\n\nKilo Code is a VS Code extension that can plug in local models, like a locally hosted Devstral 2 from HuggingFace.\nRecursive LLMs Go Beyond RAG?: A thread discussed a paper about recursive LLMs, contesting the label of RAG because the LLM can manipulate a Python environment with a prompt.\n\nThe commentator said this is a bit more than RAG, but not as groundbreaking as some clickbait videos suggest, wanting to see shorter context benchmark performance.\nAnthropic Explores Assistant Axis: A member shared a link to Anthropic's research on the Assistant Axis.\n\nNo further details were given.\nAkira Scene-for-Scene vid2vid Version Announced: Higgsfield is sponsoring a scene for scene vid2vid version of Akira, planned for completion in 2027.\n\nThe announcement received mixed reviews due to anti-AI sentiment, with some finding it odd that the characters aren't Japanese.\nEleuther Discord\nEngineers Grapple with Agent Evaluation: Engineers are seeking methods to automate agent evaluation to reduce manual costs, focusing on transparency, reliability, honesty, and minimizing user friction.\n\nA member suggested that the team is looking for \""LLM as judge\"" workflows, but needs to evaluate data quality before attempting full automation.\nOpen Weights Models Face Multiple Choice Evals: Researchers are seeking multiple-choice evaluation results for open weights models like Llama 7B, 13B, and 70B, performing each question 100 times to determine the probability of correct answers.\n\nThey clarified that the answers should be pre-w..."",""content"":""**X Engineering** open-sourced its new transformer-based recommender algorithm, sparking community debate on transparency and fairness. **GLM-4.7-Flash (30B-A3B)** gains momentum as a strong local inference model with efficient KV-cache management and quantization tuning strategies. Innovations include tensor parallelism on Mac Minis achieving ~100 tok/s throughput. Research highlights \""Societies of Thought\"" as a reasoning mechanism improving model accuracy by 20%+."",""contentSnippet"":""**X Engineering** open-sourced its new transformer-based recommender algorithm, sparking community debate on transparency and fairness. **GLM-4.7-Flash (30B-A3B)** gains momentum as a strong local inference model with efficient KV-cache management and quantization tuning strategies. Innovations include tensor parallelism on Mac Minis achieving ~100 tok/s throughput. Research highlights \""Societies of Thought\"" as a reasoning mechanism improving model accuracy by 20%+."",""guid"":""https://news.smol.ai/issues/26-01-20-not-much/"",""categories"":[""x-ai"",""unsloth-ai"",""google"",""deepseek"",""ollama"",""glm-4.7-flash"",""grok"",""deepseek-r1"",""qwq"",""giffmana"",""david_sholz"",""yuchenj_uw"",""nearcyan"",""sam_paech"",""teortaxes_tex"",""danielhanchen"",""alexocheema"",""nopmobiel"",""rohanpaul_ai"",""transformer-architecture"",""recommendation-systems"",""local-inference"",""kv-cache"",""quantization"",""tensor-parallelism"",""reasoning"",""model-optimization"",""fine-tuning""],""isoDate"":""2026-01-20T05:44:39.000Z""}"
Smol,not much happened today,https://news.smol.ai/issues/26-01-19-not-much/,2026-01-19T05:44:39.000Z,"<p><strong>a quiet day</strong></p>
<blockquote>
<p>AI News for 1/16/2026-1/19/2026. We checked 12 subreddits, <a href=""https://twitter.com/i/lists/1585430245762441216""><strong>544</strong> Twitters</a> and <strong>24</strong> Discords (<strong>205</strong> channels, and <strong>13654</strong> messages) for you. Estimated reading time saved (at 200wpm): <strong>1062 minutes</strong>. <strong>Our new website</strong> is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on <a href=""https://x.com/Smol_AI"">@smol_ai</a>!</p>
</blockquote>
<p>We would recommend checking out the <a href=""https://x.com/arcprize/status/2013369761250582794?s=46"">ARC AGI 2025 Report</a> if time permits.</p>
<hr>
<h1>AI Twitter Recap</h1>
<p><strong>New architectures for scaling “memory” and context</strong></p>
<ul>
<li><strong>STEM (Scaling Transformers with Embedding Modules)</strong>: A Carnegie Mellon + Meta approach to scale a Transformer’s <strong>parametric memory</strong> without MoE-style dynamic routing. The key swap: remove ~<strong>1/3 of the FFN up-projection</strong> and replace it with a <strong>token-indexed embedding lookup</strong>, while keeping the <strong>gate + down-projection dense</strong>. Because the lookup is static, it avoids runtime routing overhead/instability and can even enable <strong>CPU offload + async prefetch</strong>, decoupling <strong>model capacity from per-token FLOPs and cross-device comms</strong> (<a href=""https://twitter.com/TheTuringPost/status/2013011864880660495"">overview</a>, <a href=""https://twitter.com/TheTuringPost/status/2013011880210731167"">step-by-step</a>, <a href=""https://twitter.com/TheTuringPost/status/2013011892672086377"">why MoE can be inefficient in practice</a>).
<ul>
<li>Practical takeaway: “sparse capacity” doesn’t have to mean MoE routers + expert parallelism; static sparsity can be <strong>systems-friendly</strong> (predictable access patterns, lower comms).</li>
</ul>
</li>
<li><strong>RePo (Context Re-Positioning) from Sakana AI</strong>: A lightweight module that lets LMs <strong>reorder positional structure based on content relevance</strong>, effectively reshaping attention geometry so relevant far-away items can be “pulled closer” and noise pushed away. Framed via Cognitive Load Theory: fixed token indices force models to spend capacity on disorganized inputs. RePo targets robustness on <strong>noisy contexts, structured data, and long-range dependencies</strong> (<a href=""https://twitter.com/SakanaAILabs/status/2013046887746843001"">announcement</a>, <a href=""https://twitter.com/SakanaAILabs/status/2013232698672742472"">code</a>, <a href=""https://twitter.com/SakanaAILabs/status/2013232698672742472"">repo link</a>).
<ul>
<li>Practical takeaway: complements retrieval/packing tricks—RePo is an architectural knob for <strong>adaptive ordering</strong> rather than better retrieval alone.</li>
</ul>
</li>
</ul>
<p><strong>Model releases: GLM-4.7-Flash and the “MLA + small MoE” wave</strong></p>
<ul>
<li><strong>Zhipu AI GLM-4.7-Flash</strong>: Released as a <strong>30B-class local coding/agent model</strong>, positioned as lightweight and deployment-friendly. Zhipu calls it a “new standard for the 30B class,” recommending it for <strong>coding + agentic use</strong>, plus translation/long-context/creative writing (<a href=""https://twitter.com/Zai_org/status/2013261304060866758"">launch</a>, <a href=""https://twitter.com/louszbd/status/2013262379874693155"">“we built it”</a>). Zhipu later clarified: <strong>GLM-4.7-Flash is a 30B-A3B MoE model</strong> (<a href=""https://twitter.com/Zai_org/status/2013280523871752319"">spec</a>).
<ul>
<li>Community/analyst notes emphasize its architecture shift: GLM “swapped to <strong>MLA</strong>,” with unconventional head dims and higher head counts after down-projection; this follows trends seen in Qwen/DeepSeek style designs (<a href=""https://twitter.com/stochasticchasm/status/2013268543064715629"">stochasticchasm</a>, <a href=""https://twitter.com/eliebakouch/status/2013272478018048209"">eliebakouch</a>). Another summary claims ~<strong>3B active</strong> per token and highlights strong benchmark positioning on <strong>SWE-bench Verified</strong>, τ²-Bench, HLE, BrowseComp, with <strong>LCB</strong> as an area where Qwen leads (<a href=""https://twitter.com/gm8xx8/status/2013310047770599448"">gm8xx8</a>). Treat these as second-hand claims unless you verify the model card.</li>
</ul>
</li>
<li><strong>“Compression” narrative</strong>: Some commentary frames GLM’s trajectory as compressing much larger models into smaller ones (e.g., “GLM-4.5 110B → GLM-4.7 31B”), and looks ahead to <strong>GLM-4.7V</strong> vs Qwen3-VL (<a href=""https://twitter.com/casper_hansen_/status/2013294519546978719"">casper_hansen_</a>). This is more interpretive than a confirmed training recipe.</li>
<li><strong>Small-model resurgence in tooling</strong>: Multiple posts reflect engineers prioritizing <strong>speed/latency</strong> and “good enough” intelligence for synchronous coding—suggesting diminishing returns for >95% of interactive tasks, shifting the frontier to <strong>fast inference at frontier-ish quality</strong> (<a href=""https://twitter.com/amanrsanger/status/2013387140537950715"">amanrsanger</a>).</li>
</ul>
<p><strong>Inference &#x26; deployment infra: local runtimes, vLLM/MLX, and “full-stack” systems papers</strong></p>
<ul>
<li><strong>Day-0 ecosystem support for GLM-4.7-Flash</strong>:
<ul>
<li><strong>mlx-lm</strong>: GLM 4.7 Flash supported in <strong>mlx-lm 0.30.3</strong>, with reported 4-bit performance on an M5 32GB laptop (~<strong>43 tok/s</strong> generation, <strong>~800 tok/s</strong> prefill) (<a href=""https://twitter.com/awnihannun/status/2013286079470645353"">awnihannun</a>). Later mlx-lm release notes mention continuous batching/distributed improvements plus autoAWQ/autoGPTQ support (<a href=""https://twitter.com/awnihannun/status/2013316769163751662"">awnihannun</a>).</li>
<li><strong>LM Studio</strong>: GLM-4.7-Flash available as a <strong>30B local coding agent on Mac</strong> via <strong>MLX for Apple Silicon</strong> (<a href=""https://twitter.com/lmstudio/status/2013339758139789389"">lmstudio</a>).</li>
<li><strong>Ollama</strong>: GLM-4.7-Flash available in <strong>Ollama v0.14.3+ (pre-release)</strong> (<a href=""https://twitter.com/ollama/status/2013372316021834086"">ollama</a>).</li>
<li><strong>vLLM</strong>: “Day-0 support” PR announced by vLLM project (<a href=""https://twitter.com/vllm_project/status/2013421647215407587"">vllm_project</a>).</li>
<li><strong>opencode + HF inference providers</strong>: GLM-4.7-Flash integrated into OpenCode via Hugging Face Inference Providers (<a href=""https://twitter.com/victormustar/status/2013297272025424120"">victormustar</a>), with one example running local GLM-4.7-Flash via Ollama + Harbor (<a href=""https://twitter.com/Everlier/status/2013383690756276454"">Everlier</a>).</li>
</ul>
</li>
<li><strong>Huawei/China inference-systems “2025 flagship works” recap</strong> (via a Zhihu contributor summary): a dense list of systems ideas targeting KV-cache capacity walls, PD split/merge utilization, hybrid scheduling, cache affinity/load balance, and KVCache-centric agent memory. Notable claims include offloading “cold” KV to DRAM; “decode attention flows into prefill GPUs”; “latency slack as resource”; dual-hash routing (“power of two choices”); and <strong>agent memory as reusable KV blocks</strong> to preserve prefix continuity and caching (<a href=""https://twitter.com/ZhihuFrontier/status/2013127635589800172"">ZhihuFrontier</a>).
<ul>
<li>Practical takeaway: the center of gravity is moving from isolated kernels to <strong>end-to-end SLO-goodput</strong> systems design.</li>
</ul>
</li>
<li><strong>Cerebras vs GPU tradeoffs</strong>: One thread stresses that “nothing is free” in computer architecture: Cerebras buys bandwidth/latency at the cost of FLOPs/memory efficiency for typical GPU-friendly workloads, but enables ultra-low-latency small-model cases that are hard elsewhere (<a href=""https://twitter.com/itsclivetime/status/2013084127218852207"">itsclivetime</a>). Related speculation: “Codex on Cerebras” could reset agent harness expectations (<a href=""https://twitter.com/dbreunig/status/2013285271438311608"">dbreunig</a>).</li>
</ul>
<p><strong>Agents, memory, and developer workflows: from MCP debates to sandboxes + RLMs</strong></p>
<ul>
<li><strong>Filesystem vs database for agent memory</strong>: A useful synthesis frames two camps—“<strong>files are all you need</strong>” (Anthropic/Letta/LangChain/LlamaIndex patterns) vs “<strong>filesystem is a bad DB</strong>” (warnings about reimplementing search indexes/locking/logs). Key axes: simplicity vs scale, multimodal data, concurrency, security/permissions, and agent familiarity with CLI tools due to coding-centric post-training (<a href=""https://twitter.com/helloiamleonie/status/2013256958535401503"">helloiamleonie</a>, plus a shorter memory-as-files portability take (<a href=""https://twitter.com/Vtrivedy10/status/2013341279418020093"">Vtrivedy10</a>)).</li>
<li><strong>Recursive Language Models (RLMs) landing in DSPy</strong>: DSPy shipped <code>dspy.RLM</code> (v3.1.2), pitched as plug-and-play with existing Signatures (<a href=""https://twitter.com/isaacbmiller1/status/2013371005960401327"">isaacbmiller1</a>). Multiple engineers flag it as a new experimentation rabbit hole and ecosystem unlock (<a href=""https://twitter.com/a1zhang/status/2013379266545615130"">a1zhang</a>, <a href=""https://twitter.com/kmad/status/2013405979967107563"">kmad</a>).
<ul>
<li>Practical takeaway: RLMs are a new lever for <strong>long-context / iterative processing</strong> without naively stuffing everything into one context window.</li>
</ul>
</li>
<li><strong>Sandboxes and “agent harness” as differentiator</strong>: Several posts argue the real “alpha” is the harness: tooling, skills, isolation, retries, and reliable execution loops—not just the base model. Examples: <code>/create-skill</code> command for “droid” converting sessions into reusable skills (<a href=""https://twitter.com/matanSF/status/2013026060678648032"">matanSF</a>); agent sandbox questions around latency/persistence (<a href=""https://twitter.com/ben_burtenshaw/status/2013282908149002597"">ben_burtenshaw</a>); and frustration with job-retry UX in build systems (<a href=""https://twitter.com/charliermarsh/status/2013284345075609623"">charliermarsh</a>). There’s also a concrete claim that “droid” beat Claude Code/Codex/Gemini CLI in an enterprise eval, attributing this to the harness (<a href=""https://twitter.com/matanSF/status/2013314451756458127"">matanSF</a>).</li>
<li><strong>Open-source agent frameworks</strong>:
<ul>
<li><strong>Claude Cowork</strong>: Open-source agent harness working with Claude Opus 4.5, Gemini 3 Pro, GPT-5.2 (<a href=""https://twitter.com/Saboo_Shubham_/status/2013090887736472047"">Saboo_Shubham_</a>). A practical add-on shows converting PDFs → markdown to reduce hallucinations and improve doc understanding, built on LlamaParse/semtools (<a href=""https://twitter.com/jerryjliu0/status/2013378183177887792"">jerryjliu0</a>).</li>
<li><strong>StirrupJS</strong>: TypeScript agent framework emphasizing minimal scaffolding + strong defaults (tools, MCP, browsing, sandboxes) and multimodal support (<a href=""https://twitter.com/ArtificialAnlys/status/2013294230052212792"">ArtificialAnlys</a>).</li>
</ul>
</li>
</ul>
<p><strong>Safety, evals, and reliability: probes, persona drift, and search attacks</strong></p>
<ul>
<li><strong>Anthropic “Assistant Axis” research (persona drift)</strong>: Anthropic highlights that open-weights models can drift away from an “Assistant” persona in long conversations; coding-like contexts stabilized the assistant persona, while therapy/philosophy contexts increased drift. They propose persona construction + stabilization, and note <strong>activation capping</strong> as a mitigation; they provide a cautionary example where drift led to harmful “falling in love” behavior encouraging isolation/self-harm (<a href=""https://twitter.com/AnthropicAI/status/2013356793477361991"">thread start</a>, <a href=""https://twitter.com/AnthropicAI/status/2013356806647542247"">drift contexts</a>, <a href=""https://twitter.com/AnthropicAI/status/2013356816843866605"">paper+demo</a>, <a href=""https://twitter.com/AnthropicAI/status/2013356811647066160"">harm example + mitigation</a>).</li>
<li><strong>Google DeepMind: activation probes in production</strong>: DeepMind describes “novel activation probe architectures” for classifying real-world misuse risks, and notes these probes have informed <strong>live deployments in Gemini</strong> (<a href=""https://twitter.com/ArthurConmy/status/2013285602070770036"">ArthurConmy</a>). Rohin Shah emphasizes probes as a “cheap classifier” lever for safety (<a href=""https://twitter.com/rohinmshah/status/2013330607611261066"">rohinmshah</a>); Neel Nanda highlights the engineering realities of productionizing safety classifiers (side effects, false positives, efficiency), linking the paper (<a href=""https://twitter.com/NeelNanda5/status/2013364781512827328"">NeelNanda5</a>).</li>
<li><strong>Retriever/search manipulation (“Arbitrary Content Injection”)</strong>: A paper claims search/retrieval stacks can be hijacked to push arbitrary content into top results, affecting retrievers, rerankers, and LLM judges (<a href=""https://twitter.com/ManveerTamber/status/2013025485358235998"">ManveerTamber</a>).</li>
<li><strong>RAG observability</strong>: DeepLearning.AI emphasizes production RAG needs observability across latency/throughput and response quality, balancing LLM-judge vs human feedback (<a href=""https://twitter.com/DeepLearningAI/status/2013325617689719199"">DeepLearningAI</a>).</li>
</ul>
<p><strong>Multimodal &#x26; media tooling: real-time speech, browser vision, and generative video</strong></p>
<ul>
<li><strong>Microsoft VibeVoice (open-source real-time TTS)</strong>: Claimed ~<strong>300 ms</strong> first-audio latency, streaming text input, multi-speaker (up to 4), and long-form stability (up to 90 minutes). Described as using semantic+acoustic tokens at <strong>7.5 Hz</strong> with a language model for structure and a diffusion head for acoustic detail; MIT-licensed, “research-only” (<a href=""https://twitter.com/LiorOnAI/status/2013220214217879931"">LiorOnAI</a>, <a href=""https://twitter.com/LiorOnAI/status/2013220215249592548"">repo</a>).</li>
<li><strong>WebGPU browser vision demos</strong>: “YOLO26” real-time pose/detection in the browser via WebGPU, with a Hugging Face collection of models/demos (<a href=""https://twitter.com/mervenoyann/status/2013224180813115626"">mervenoyann</a>, <a href=""https://twitter.com/mervenoyann/status/2013224398824632484"">HF link</a>).</li>
<li><strong>Video generation productization on fal</strong>: Multiple “model-on-demand” drops: Wan 2.6 i2v Flash (up to 15s, optional audio) (<a href=""https://twitter.com/fal/status/2013292351192490257"">fal</a>); Vidu Q2 reference-to-video with multi-reference and face reference (<a href=""https://twitter.com/fal/status/2013374170378158349"">fal</a>); plus Flux.2 [klein] trainers + released LoRAs for outpaint/zoom/object remove/background remove (<a href=""https://twitter.com/fal/status/2013313891057455265"">fal</a>, <a href=""https://twitter.com/fal/status/2013361738423369791"">LoRAs</a>).</li>
<li><strong>Function calling on tiny models</strong>: Google’s <strong>FunctionGemma Tuning Lab</strong>: a guide + no-code demo for fine-tuning/exporting function-calling models built around a <strong>270M parameter</strong> model, with a HF Space (<a href=""https://twitter.com/osanseviero/status/2013241128934404301"">osanseviero</a>).</li>
<li><strong>Web World Models (WWMs)</strong>: Princeton-style “separate rules from imagination”: deterministic web-code physical layer updates state first, then LM generates descriptions from updated state to preserve coherence (<a href=""https://twitter.com/TheTuringPost/status/2013016473514717330"">TheTuringPost</a>).</li>
</ul>
<hr>
<h1>AI Reddit Recap</h1>
<h2>/r/LocalLlama + /r/localLLM Recap</h2>
<h3>1. High VRAM AMD R9700 Server Builds</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/"">4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build</a></strong> (Activity: 508): <strong>The post details a high-performance server build using 4x <strong>AMD Radeon AI PRO R9700</strong> GPUs, each with <code>32GB</code> VRAM, totaling <code>128GB</code> VRAM, paired with an <strong>AMD Ryzen Threadripper PRO 9955WX</strong> CPU. The system is designed for running large AI models (120B+ parameters) locally, emphasizing data privacy. The build cost approximately <code>9,800€</code>, with a 50% subsidy from local government, effectively reducing the cost to <code>4,900€</code>. Benchmarks using <code>llama.cpp</code> show significant performance, with the <strong>GLM-4.7-REAP-218B-A32B-Q3_K_M</strong> model achieving <code>17.48</code> tokens/s in generation. The user notes that <strong>PCIe 5.0</strong> enhances Pipeline Parallelism performance over Tensor Parallelism. The system uses <strong>rocm 7.1.1</strong> for software support, and the user contemplates switching to an <strong>NVIDIA RTX Pro 6000</strong> for potentially better performance in the future.</strong> A notable comment inquires about the source and cost of the components, reflecting interest in the feasibility and procurement of such high-end hardware. Another comment humorously references the abundance of RAM, while a third notes a similar build, indicating a shared interest in high-performance local AI systems.</p>
<ul>
<li>RoterElephant discusses the trade-off between using multiple AMD R9700 cards versus a single NVIDIA RTX Pro 6000 Blackwell. The NVIDIA card, despite having less total VRAM, offers superior performance due to its architecture and software support, which can be more efficient for certain workloads. This highlights the importance of considering not just raw VRAM but also the overall performance and compatibility with specific applications when building high-performance systems.</li>
<li>Obvious-Nobody-9592 inquires about the acquisition and cost of the components, noting the total expense of 9800 Euros. This comment underscores the financial considerations and planning involved in assembling a high-end computing system, particularly with components like the AMD R9700 and Threadripper 9955WX, which are not only expensive but also require careful budgeting and sourcing over time.</li>
<li>Ulterior-Motive_ references a similar build, suggesting a trend or common interest in high-performance computing setups using AMD R9700 GPUs. This points to a community of enthusiasts or professionals who are exploring the capabilities of such configurations, possibly for tasks that require significant computational power, such as machine learning or data analysis.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/"">128GB VRAM quad R9700 server</a></strong> (Activity: 738): <strong>The post details a high-performance server build featuring four <strong>PowerColor AMD Radeon AI PRO R9700</strong> GPUs, each with <code>32GB</code> VRAM, totaling <code>128GB</code> VRAM, and <code>128GB</code> RAM, aimed at optimizing prompt processing performance for machine learning tasks. The build, costing <code>$7,035</code>, includes components like the <strong>MSI MEG X570 GODLIKE Motherboard</strong> and <strong>AMD Ryzen 7 5700X</strong> CPU. Benchmarks show significant performance improvements in models like <code>llama 7B Q4_0</code> and <code>qwen3moe 30B.A3B Q8_0</code> using the ROCm backend, with prompt processing speeds reaching up to <code>6524.91 t/s</code>. The post also highlights issues with the Qwen3-Next model and challenges with storage and PCIe slot configurations.</strong> The comments reflect admiration for the build's performance and a humorous acknowledgment of the financial implications of pursuing high-end hardware setups.</p>
</li>
</ul>
<h3>2. Qwen Development and Quality Focus</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/"">Qwen 4 might be a long way off !? Lead Dev says they are ""slowing down"" to focus on quality.</a></strong> (Activity: 575): <strong>The image is a tweet from <strong>Junyang Lin</strong>, a lead developer, indicating a strategic shift in the development of the Qwen series, focusing on enhancing quality over rapid iteration. This suggests that the release of Qwen 4 might be delayed as the team invests more in research, potentially sacrificing immediate results for long-term improvements. The tweet reflects a commitment to refining the models, which have been noted for their range of sizes and capabilities, to ensure higher quality outputs.</strong> Commenters generally support the decision to prioritize quality, with some expressing relief that the focus is not on rapid, incremental updates that could inflate costs and resource consumption without significant advancements.</p>
<ul>
<li>AvocadoArray highlights the inefficiency of frequent incremental updates, noting that they often lead to increased demand and costs due to high GPU training requirements. This perspective suggests that focusing on substantial improvements could be more beneficial for the AI landscape, as it avoids the pitfalls of minor, frequent updates that don't significantly advance the field.</li>
<li>frozen_tuna raises a critical point about the potential risks of delaying releases for quality improvements, drawing a parallel with <strong>Meta's</strong> approach before releasing <strong>LLaMA 4</strong>. The comment questions whether the community will be forgiving if the delayed release of <strong>Qwen 4</strong> doesn't meet heightened expectations, suggesting that the strategy of waiting for 'risky research' to succeed could backfire if the final product underwhelms.</li>
<li>Cool-Chemical-5629 appreciates the focus on quality, noting that while the <strong>Qwen series</strong> has been good, there is room for improvement. They express hope that the developers will continue to offer a wide range of model sizes, which has been a hallmark of the series, while enhancing quality. This reflects a desire for both diversity in model offerings and significant quality advancements.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLM/comments/1qf5l2n/local_ai_final_boss_m3_ultra_vs_gb10/"">Local AI Final Boss — M3 Ultra v.s. GB10</a></strong> (Activity: 404): <strong>The image depicts a comparison setup between a <strong>Mac Studio M3 Ultra</strong> and an <strong>ASUS GX10 (GB10)</strong>, both high-performance computing devices. The discussion centers around using these machines for AI tasks, with a suggestion to use <strong>EXO</strong> for clustering to enhance prompt processing speed. The <strong>M3 Ultra</strong> is noted for its popularity in business environments for private on-premises infrastructure, while there is curiosity about the performance of the <strong>GB10</strong> in similar scenarios. The setup is indicative of a test or experiment to evaluate the capabilities of these devices in handling AI workloads.</strong> One commenter is curious about the performance of the GB10 compared to the M3 Ultra, as they frequently install M3s for business use. Another comment humorously suggests using the devices to solve political issues, reflecting a desire to apply technology to real-world problems.</p>
<ul>
<li>No_Conversation9561 mentions using EXO for clustering to enhance prompt processing speed. They reference a specific setup that reportedly improves performance, and provide links to both the EXO Labs website and a GitHub issue for further technical details.</li>
<li>adspendagency discusses the deployment of M3 units in business environments for private on-premises infrastructure, expressing interest in understanding the performance comparison between the M3 and GB10. They note that their current practice involves shipping M3s to customers, indicating a potential gap in knowledge about GB10's capabilities.</li>
<li>belgradGoat raises concerns about the stability of Mac Studio when running models with 500 GB RAM. They share personal experience with a 256 GB version, noting instability issues as memory usage approaches the limit, suggesting potential challenges in handling large-scale models.</li>
</ul>
</li>
</ul>
<h3>3. Uncensored AI Models Exploration</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qfq9ez/the_search_for_uncensored_ai_that_isnt/"">The Search for Uncensored AI (That Isn’t Adult-Oriented)</a></strong> (Activity: 696): <strong>The Reddit post discusses the challenge of finding an AI model that is both uncensored and technically advanced, without being oriented towards adult content. The author notes a gap between heavily restricted corporate AI and models optimized for low-effort adult use, seeking alternatives that focus on reasoning, creativity, and problem-solving. The post invites suggestions for self-hosted models, open-source projects, or lesser-known platforms. A notable resource mentioned is the <a href=""https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard"">Uncensored General Intelligence Leaderboard</a>, which could provide insights into available models.</strong> Commenters highlight that most attempts to de-censor open-source models often result in reduced intelligence due to manipulation. They also point out that organizations capable of developing advanced models avoid enabling potentially harmful behavior, leaving the field dominated by less serious, adult-focused finetunes. The mention of Deepseek V3 by chub.ai as an example of an uncensored model underscores the limited options available.</p>
<ul>
<li>KayLikesWords highlights a trade-off in de-censoring open-source models, noting that such manipulations often result in reduced intelligence. They argue that major organizations avoid creating uncensored models due to potential risks, leaving the field to smaller groups who focus on niche applications, such as the 'gooner finetune of Deepseek V3'.</li>
<li>EstimateLeast9807 provides a resource for those interested in uncensored AI models by linking to the 'Uncensored General Intelligence Leaderboard' on Hugging Face, which could be a valuable tool for comparing the performance and capabilities of various uncensored models.</li>
<li>noctrex mentions specific models like 'Dolphin-Mistral-24B-Venice-Edition' and those from 'huihui-ai' as examples of uncensored AI. They note that while these models are uncensored, they may not excel in reasoning tasks, indicating a potential limitation in their application.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/"">zai-org/GLM-4.7-Flash · Hugging Face</a></strong> (Activity: 1047): <strong><strong>GLM-4.7-Flash</strong> is a <code>30B</code> parameter model utilizing a <code>Mixture of Experts (MoE)</code> architecture, specifically designed for efficient deployment and high performance. It reportedly excels in benchmarks like <code>AIME</code> and <code>GPQA</code>, and supports local inference through frameworks such as <code>vLLM</code> and <code>SGLang</code>. The model's use of <code>MLA</code> (Memory-Limited Attention) allows for a reduced memory footprint, enabling many users to run it at the full <code>200k</code> context length. Detailed installation and usage instructions are available on its <a href=""https://huggingface.co/zai-org/GLM-4.7-Flash"">Hugging Face page</a>.</strong> Commenters express enthusiasm for the model's capabilities, particularly its memory efficiency due to MLA, which allows broader accessibility for running the model at full context length. There is also a sentiment of anticipation and satisfaction with the release, reflecting a demand for larger models like <code>70B</code>.</p>
<ul>
<li>The GLM-4.7-Flash model utilizes Memory-Limited Attention (MLA), which significantly reduces the memory footprint of the key-value (KV) cache. This optimization allows the model to handle a full 200k context length efficiently, making it accessible for more users to run without extensive hardware requirements.</li>
<li>A user references the model's architecture, noting a discrepancy in the model size description. The model is referred to as a '30b' model, but a link to the source code suggests it might be a '3B' model, indicating a potential misunderstanding or typo in the model's description. This highlights the importance of verifying model specifications directly from the source code.</li>
<li>There is a desire for performance comparisons between the GLM-4.7-Flash and larger models, such as 70b models. This would provide a clearer understanding of the trade-offs in performance and resource requirements, helping users make informed decisions about which model to deploy based on their specific needs.</li>
</ul>
</li>
</ul>
<h2>Less Technical AI Subreddit Recap</h2>
<blockquote>
<p>/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo</p>
</blockquote>
<h3>1. Gemini and DeepMind AI Developments</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/singularity/comments/1qcq1ld/gemini_mathspecialized_version_proves_a_novel/"">Gemini ""Math-Specialized version"" proves a Novel Mathematical Theorem</a></strong> (Activity: 745): <strong><strong>Gemini</strong>, a ""math-specialized"" AI model, has reportedly proven a novel mathematical theorem, as detailed in a <a href=""https://x.com/A_G_I_Joe/status/2011213692617285729?s=20"">tweet</a> and an accompanying <a href=""https://arxiv.org/abs/2601.07222"">arXiv paper</a>. The model's architecture and training are optimized for mathematical reasoning, showcasing its capability to handle complex mathematical proofs, which marks a significant advancement in AI's application in theoretical mathematics. This development underscores the rapid pace of AI breakthroughs in specialized domains.</strong> Commenters highlight the accelerating pace of AI advancements and its potential to transform mathematical research, while expressing concern over the influence of commercial interests on AI's future direction.</p>
<ul>
<li>A user suggests using the Gemini model to tackle the Erdős problems, highlighting it as a significant benchmark due to the extensive attention these problems have received from mathematicians. This implies that solving such well-scrutinized problems could serve as a robust test of the model's capabilities.</li>
<li>Another comment criticizes the Gemini model's inability to resolve a memory overflow bug in a project named 'anto gravity,' suggesting that despite its mathematical prowess, the model may still struggle with certain technical issues, indicating a gap between theoretical breakthroughs and practical software engineering challenges.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/singularity/comments/1qh1omx/babyvision_a_new_benchmark_for_humanlevel_visual/"">BabyVision: A New Benchmark for Human-Level Visual Reasoning</a></strong> (Activity: 488): <strong>The image presents a bar chart from the BabyVision-Mini benchmark, which evaluates the visual reasoning capabilities of large language models (LLMs) compared to humans of various ages. The chart highlights that human performance, particularly that of 12-year-olds, surpasses that of LLMs, with the Gemini3-Pro-Preview model achieving the highest accuracy among the LLMs. This benchmark underscores the current limitations of LLMs in visual reasoning tasks, suggesting that advancements in multi-modal pretraining and reinforcement learning could enhance their performance in the future.</strong> A comment suggests that the current limitations in LLMs' visual reasoning are a significant challenge for achieving AGI, but anticipates that improvements in multi-modal pretraining and reinforcement learning will eventually close the performance gap, particularly benefiting fields like robotics.</p>
<ul>
<li>The discussion highlights that current models are still limited in visual reasoning capabilities, which is a significant challenge for achieving ARC AGI. The commenter suggests that scaling multi-modal pretraining and reinforcement learning (RL) for vision tasks could significantly improve performance, potentially reaching near 100% in the coming years. This improvement is expected to unlock new applications, particularly benefiting robotics.</li>
<li>The commenter references a specific <a href=""https://arxiv.org/html/2601.06521v1"">arXiv paper</a> which may provide additional insights or data related to the benchmark or model performance discussed. This suggests that there is ongoing research and documentation that could be valuable for those interested in the technical details of visual reasoning benchmarks.</li>
<li>A comparison is made between Gemini and Claude Opus, suggesting that Gemini has superior performance in frontend tasks. This implies that different models may have varying strengths depending on the specific application or task, highlighting the importance of choosing the right model for specific use cases.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/Bard/comments/1p0935y/gemini_3_pro_model_card_is_out/"">Gemini 3 Pro Model Card is Out</a></strong> (Activity: 996): <strong>The <strong>Gemini 3 Pro Model Card</strong> from <strong>DeepMind</strong> has been released, detailing a model with a <code>1M token context window</code> capable of processing diverse inputs such as text, images, audio, and video, and producing text outputs with a <code>64K token</code> limit. The model's knowledge is current up to <em>January 2025</em>. The original link to the model card is down, but an archived version is available <a href=""https://archive.org/details/gemini-3-pro-model-card"">here</a>.</strong> The removal of the original link has sparked discussions, with some users expressing surprise and suggesting the model card's authenticity due to its takedown.</p>
<ul>
<li>The Gemini 3 Pro model features a substantial token context window of up to <code>1 million</code>, allowing it to handle extensive input data types including text, images, audio, and video. Its output capabilities are also notable, with a <code>64,000</code> token output limit, and it has a knowledge cutoff date of January 2025, indicating its training data is quite recent.</li>
<li>A comparison is made between Gemini 3 Pro and other models like GPT5 Pro and Sonnet, highlighting that Gemini 3 Pro outperforms GPT5 Pro and matches Sonnet in coding tasks. This suggests significant advancements in its capabilities, particularly in coding, which is a critical area for AI applications.</li>
<li>The discussion touches on the competitive landscape, suggesting that <strong>OpenAI</strong> and <strong>Google</strong> are likely to dominate the AI space, potentially outpacing competitors like <strong>Anthropic</strong> due to pricing strategies and enterprise capabilities. The comment also notes that while Claude's code features are innovative, they may inadvertently guide competitors in their development strategies.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/GeminiAI/comments/1psebc0/gemini_drops_gemini_releases_this_page_to_keep_up/"">Gemini Drops: Gemini releases this page to keep up with what's being released</a></strong> (Activity: 540): <strong>The image is a screenshot of a webpage titled ""Gemini Drops,"" which serves as a centralized hub for updates on <strong>Google's Gemini</strong> project. This page is designed to keep users informed about new feature releases, product tips, and community usage of Gemini, indicating a rapid development pace that necessitates a dedicated blog for announcements. The clean and minimalistic design emphasizes the informational content, encouraging users to check back regularly for updates. <a href=""https://gemini.google/gemini-drops/"">Gemini Drops</a> is positioned as a key resource for staying current with Gemini's advancements.</strong> Commenters note the rapid development pace of Gemini, suggesting the need for a dedicated blog to manage the volume of releases. There is also interest in an RSS feed for updates and curiosity about future releases, such as ""Gemma 4.""</p>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/singularity/comments/1qcscjz/gemini_introduces_personal_intelligence/"">Gemini introduces Personal Intelligence</a></strong> (Activity: 513): <strong><strong>Google</strong> has launched a new feature called <em>Personal Intelligence</em> within its <strong>Gemini app</strong>, initially available to <strong>Google AI Pro and AI Ultra subscribers</strong> in the U.S. This feature integrates with Google apps to provide personalized suggestions and recommendations, leveraging AI to enhance user experience across Web, Android, and iOS platforms. The rollout is limited to personal Google accounts and excludes Workspace business, enterprise, or education users. The feature will expand to more countries and eventually to the free tier, with plans to integrate into AI Mode in Search.</strong> Some users express excitement about the feature, though there is concern about potential monetization through personalized ads. Others note that similar functionality has been available through Google Labs, indicating a positive reception of the feature's performance.</p>
<ul>
<li>qustrolabe highlights that the Gemini Personal Intelligence feature is initially available to Google AI Pro and AI Ultra subscribers in the U.S., with plans to expand to more countries and eventually to the free tier. This feature is integrated across Web, Android, and iOS platforms and will soon be part of AI Mode in Search. However, it is currently not available for Workspace business, enterprise, or education users, indicating a phased rollout strategy to gather user feedback before broader deployment.</li>
<li>1cheekykebt shares a practical use case of the Gemini Personal Intelligence, where it not only retrieves basic information like tire sizes but also provides personalized recommendations based on user data, such as family road trips stored in Google Photos. This suggests that Gemini leverages personal data to enhance its utility, offering tailored advice that goes beyond standard chatbot capabilities.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/singularity/comments/1qflbj9/google_deepmind_ceo_china_just_months_behind_us/"">Google Deepmind CEO: China just ""months"" behind U.S. AI models</a></strong> (Activity: 734): <strong><strong>Demis Hassabis</strong>, CEO of Google DeepMind, stated in a CNBC interview that Chinese AI models are only ""a matter of months"" behind U.S. and Western capabilities, although they have not yet demonstrated the ability to advance ""beyond the frontier"" of AI. This perspective challenges the common belief that China lags significantly in AI development. <a href=""https://www.cnbc.com/amp/2026/01/16/google-deepmind-china-ai-demis-hassabis.html"">Source</a>.</strong> Comments highlight a debate on China's AI progress: some argue that China's ability to produce cost-effective open-source AI could offset any technological lag, while others suggest Google's statements may be influenced by strategic interests, such as seeking favorable regulation or government contracts.</p>
<ul>
<li>The comment by vwboyaf1 highlights the potential for China to leverage open-source AI models that achieve 90% of the performance of leading models at a fraction of the cost, specifically 20% or less. This suggests that even if China is technically behind, the cost-effectiveness of their models could make them highly competitive in practical applications.</li>
<li>Educational_Teach537 points out a contradiction in narratives: Chinese researchers claim they are limited by computational resources and may not catch up, while Google suggests China is rapidly closing the gap. This discrepancy raises questions about the actual state of AI development in China and whether the limitations are more about infrastructure or strategic positioning.</li>
<li>Chogo82 discusses the infrastructure gap, noting that China's AI infrastructure would need to triple to match the US. This implies that while China may have the talent and models, the lack of infrastructure is a significant barrier to achieving parity with the US in AI capabilities.</li>
</ul>
</li>
</ul>
<h3>2. Innovations in AI Coding and Development Tools</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/singularity/comments/1qgb1j5/cursor_ai_ceo_shares_gpt_52_agents_building_a_3m/"">Cursor AI CEO shares GPT 5.2 agents building a 3M+ lines web browser in a week</a></strong> (Activity: 1069): <strong><strong>Cursor AI CEO Michael Truell</strong> demonstrated the capabilities of <strong>GPT 5.2</strong> in building a web browser with over <code>3 million lines of code</code> in just a week. This project, although not production-ready, showcases the potential of autonomous coding agents in generating complex systems, including a custom rendering engine and JavaScript VM. The process was visualized in real-time, highlighting the coordination and evolution of the codebase by the agents. <a href=""https://x.com/i/status/2012825801381580880"">Source</a>.</strong> A notable comment suggests using the tool 'gource' for similar animations from git repositories, indicating interest in the visualization aspect of the project.</p>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/OpenAI/comments/1qgbfpb/cursor_ai_ceo_shares_gpt_52_agents_building_a_3m/"">Cursor AI CEO shares GPT 5.2 agents building a 3M+ lines web browser in a week</a></strong> (Activity: 657): <strong><strong>Cursor AI CEO Michael Truell</strong> demonstrated the capabilities of <strong>GPT 5.2</strong> in building a web browser with over <code>3 million lines of code</code> in a week, including a custom rendering engine and JavaScript VM. This experimental project highlights the potential of autonomous coding agents to scale complex software development tasks when operated continuously. The visualization of the process shows agents coordinating and evolving the codebase in real-time, though the browser itself was not showcased.</strong> Some commenters expressed skepticism about the lack of a demonstration of the browser, while others were impressed by the visualization of the agents' coordination. There was also a debate on whether <code>3 million lines of code</code> is excessive for such a project.</p>
<ul>
<li>Deepwebexplorer highlights the significance of the demonstration, emphasizing that the key takeaway is the feasibility of AI autonomously building a web browser, regardless of its current quality. The focus is on the potential for improvement and the milestone of achieving autonomous code generation at this scale, rather than the immediate practical application or performance of the browser itself.</li>
<li>The discussion touches on the sheer scale of the project, with ZeroZachZilchZealot questioning whether 3 million lines of code is substantial. This reflects a broader curiosity about the complexity and scope of AI-generated projects, suggesting that while the number is impressive, the real interest lies in understanding the efficiency and functionality of such large-scale codebases.</li>
<li>0ldwax raises a critical point about the functionality of the AI-generated browser, questioning whether it actually works. This underscores a common concern in AI development: the difference between generating code and producing a functional, reliable product. The comment suggests a need for further validation and testing of AI-generated software to ensure it meets practical usability standards.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/singularity/comments/1qd541a/ceo_of_cursor_said_they_coordinated_hundreds_of/"">CEO of Cursor said they coordinated hundreds of GPT-5.2 agents to autonomously build a browser from scratch in 1 week</a></strong> (Activity: 2600): <strong><strong>Michael Truell</strong>, CEO of Cursor, announced the coordination of hundreds of GPT-5.2 agents to autonomously develop a browser from scratch in just one week. The project resulted in over <code>3 million lines of code</code> written in Rust, incorporating features like HTML parsing, CSS cascade, and a custom JavaScript VM. While the browser is not as advanced as Webkit or Chromium, it can render simple websites effectively. This demonstration serves as a strategic move to showcase Cursor's capabilities independent of Claude, amidst recent access restrictions by Anthropic on xAI employees using Claude through Cursor.</strong> The comments highlight the beginning of an era of ""kinda works"" software, comparing the browser's codebase to Firefox's <code>31 million lines</code>. The strategic context of the announcement is noted, as it coincides with Anthropic's restrictions, suggesting Cursor's attempt to reassure stakeholders of its independence from specific AI models.</p>
<ul>
<li>Stellar3227 highlights the strategic implications of the CEO's announcement, noting that it serves as a demonstration of independence from Claude, a leading coding model. This move comes after Anthropic restricted access to Claude for xAI employees, following similar actions by OpenAI and Windsurf. The showcase of GPT-5.2's capabilities is seen as a form of damage control, aimed at reassuring stakeholders of Cursor's resilience and adaptability in the competitive AI coding landscape.</li>
<li>Outside-Iron-8242 provides technical resources for further exploration, including a GitHub repository for the project and a blog post on Cursor's website. The GitHub link (<a href=""https://github.com/wilsonzlin/fastrender"">fastrender</a>) offers access to the source code, while the blog post (<a href=""https://cursor.com/blog/scaling-agents"">Scaling long-running autonomous coding</a>) discusses the technical challenges and methodologies involved in coordinating multiple AI agents for complex tasks.</li>
<li>Practical-Hand203 provides a comparative benchmark by mentioning that Firefox consists of 31 million lines of code, which serves to contextualize the scale of the project undertaken by GPT-5.2 agents. This comparison underscores the complexity and ambition of building a browser from scratch, even if the resulting codebase is significantly smaller.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/ClaudeAI/comments/1qgx6br/microsoft_pauses_claude_code_rollout_after_satya/"">Microsoft pauses Claude Code rollout after Satya intervention</a></strong> (Activity: 1217): <strong><strong>Microsoft</strong> has paused the deployment of <strong>Claude Code</strong> internally after intervention from CEO <strong>Satya Nadella</strong> and senior leadership, redirecting employees to use <strong>GitHub Copilot</strong> instead. The internal communication suggests that Copilot has ""mostly closed the gaps"" with Claude Code. However, exceptions are made for ""high-priority R&#x26;D"" projects, which can still access the <strong>Anthropic API</strong> with proper justification. Existing users retain access, but new invitations have been rescinded.</strong> Some commenters express skepticism about Microsoft's claim that Copilot has closed the gap with Claude Code, suggesting it might be a strategic move to improve their own product by using it internally. Others find it notable that Microsoft admitted to using a competitor's tool over their own.</p>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/ClaudeAI/comments/1qgccgs/25_claude_code_tips_from_11_months_of_intense_use/"">25 Claude Code Tips from 11 Months of Intense Use</a></strong> (Activity: 498): <strong>The Reddit post expands on previous tips for using <strong>Claude Code</strong> effectively, focusing on optimizing workflows and managing context. Key tips include customizing the status line to monitor model and token usage, using slash commands like <code>/usage</code> and <code>/chrome</code> for efficient management, and employing <strong>GitHub CLI</strong> for streamlined version control. The post also emphasizes breaking down complex tasks, using voice transcription for faster input, and leveraging <strong>Git worktrees</strong> for parallel branch work. Additionally, it discusses advanced strategies like using <strong>tmux</strong> for testing automation and <strong>Docker containers</strong> for isolated, long-running tasks. The post provides scripts for cloning conversations to manage context and suggests using <strong>Markdown</strong> for efficient documentation. The full list of tips is available on <a href=""https://github.com/ykdojo/claude-code-tips"">GitHub</a>.</strong> Commenters highlight the importance of managing token usage and context efficiently, noting that <strong>Opus 4.5</strong> struggles with context window limitations, which influences workflow design. Another suggestion is using the <strong>Obsidian Web Clipper</strong> for converting web pages to Markdown, enhancing Claude's ability to process content.</p>
<ul>
<li>Claude's Opus 4.5 model faces challenges with context management, particularly in deciding what information to retain or discard as the context window fills up. This limitation necessitates specific workflow designs to mitigate token bloat, which is a common issue in current AI models. Users often have to structure their interactions to optimize the use of the available context window.</li>
<li>The use of local models like Nvidia Parakeet in applications such as VoiceInk offers a cost-effective and fast alternative for Mac users compared to cloud-based solutions like Super Whisper. This approach leverages local processing power to enhance the speed of prompt inputs, highlighting the benefits of running models locally for specific tasks.</li>
<li>The Obsidian Web Clipper is recommended for users who encounter difficulties with Claude fetching web content. By converting web pages into Markdown, it facilitates better content management and integration into workflows, addressing some of the limitations in Claude's web content handling capabilities.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/singularity/comments/1qb4zi4/deepseek_introduces_engram_memory_lookup_module/"">DeepSeek introduces Engram: Memory lookup module for LLMs that will power next-gen models (like V4)</a></strong> (Activity: 1015): <strong><strong>DeepSeek</strong> has introduced a new research module called <strong>Engram</strong>, detailed in their paper ""Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models"". Engram implements a deterministic <code>O(1)</code> lookup memory using modernized hashed N-gram embeddings, which offloads early layer pattern reconstruction from neural computation. This approach allows for the decoupling of memory and compute as separate scaling axes, showing consistent performance gains in knowledge, reasoning, code, and math tasks under iso parameter and iso FLOPs settings. The paper and code are available as open source on <a href=""https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf"">GitHub</a>.</strong> A notable comment suggests that while some may dismiss Engram as ""just lookup,"" it represents a significant step towards achieving continual learning within the year. Another comment praises DeepSeek as a leading lab in the field.</p>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/MachineLearning/comments/1qd696s/nvidia_endtoend_testtime_training_for_long/"">Nvidia: End-to-End Test-Time Training for Long Context aka Being Able To Update A Model's Weights In Real-Time As You Use It | ""TTT changes the paradigm from retrieving info to learning it on the fly...the TTT model treats the context window as a dataset &#x26; trains itself on it in real-time."" [R]</a></strong> (Activity: 288): <strong>The paper introduces a novel approach called <strong>End-to-End Test-Time Training (TTT-E2E)</strong>, which allows a model to update its weights in real-time during inference by treating the context window as a training dataset. This involves a two-loop process: an <em>inner loop</em> where the model performs mini-gradient descent on the context to update specific MLP layers, and an <em>outer loop</em> where the model's initial weights are optimized for adaptability through meta-learning. The method is shown to scale with context length similarly to full attention models but with constant inference latency, making it <code>2.7x</code> faster than full attention for <code>128K</code> context lengths. The approach effectively decouples intelligence from memory costs, allowing for efficient handling of long contexts without the typical slowdown. The code is <a href=""https://github.com/test-time-training/e2e"">publicly available</a>.</strong> Commenters raised concerns about potential issues with catastrophic forgetting in continual learning and the conflation of training with inference, which could increase computational demands. However, the method's performance improvement over traditional attention models was noted as surprising.</p>
<ul>
<li>fiery_prometheus raises a critical issue in continual learning known as 'catastrophic forgetting,' where a model forgets its initial training data over time. This is a significant challenge for real-time weight updates, as the model might lose its foundational knowledge while adapting to new data. Addressing this requires strategies to balance learning new information while retaining core knowledge, potentially through techniques like elastic weight consolidation or memory replay.</li>
<li>-p-e-w- highlights a surprising performance improvement, noting that the test-time training (TTT) approach is 2.7x faster than full attention for a 128K context. This counters the expectation of increased computational overhead due to live training, suggesting that TTT might optimize certain processes, making it more efficient than traditional attention mechanisms.</li>
<li>ode_majka discusses the practical challenges of implementing real-time weight updates from an engineering perspective. They point out the significant computational and storage demands, such as the need to calculate gradients for a large number of parameters and manage personalized weights for each user. This could result in substantial data storage requirements and longer model initialization times, questioning the feasibility of such an approach for widespread use.</li>
</ul>
</li>
</ul>
<h3>3. AI in Energy and Space Technologies</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/singularity/comments/1qhbhi3/worlds_first_megawattlevel_windmill_airship_rises/"">World’s first megawatt-level ‘windmill’ airship rises 6,560 ft and feeds grid</a></strong> (Activity: 913): <strong>The image depicts the S2000 airborne wind system, a helium-lifted airship designed by <strong>Linyi Yunchuan Energy Tech</strong> to harness high-altitude winds for power generation. This system, featuring 12 turbines and a ducted design, achieved a rated capacity of up to <code>3 megawatts</code> during its maiden flight, generating <code>385 kWh</code> and feeding it directly into the grid. The airship operates at <code>6,560 ft</code>, utilizing steadier winds inaccessible to traditional turbines, and transmits power to the ground via a tether. This marks a significant step towards commercial airborne wind power, although the economic viability and maintenance challenges remain debated.</strong> Commenters express skepticism about the economic viability of the S2000 system, noting that the power generated during the test was minimal compared to potential solar investments. Concerns about maintenance and commercialization are also raised, suggesting alternative designs like helium-filled buoys might be more effective.</p>
<ul>
<li><strong>gretino</strong> highlights that the mean capacity of wind turbines that began commercial operations in 2020 is <code>2.75 megawatts</code> in the US, suggesting that while the airship's capacity is notable, its commercialization could face challenges, particularly in terms of maintenance logistics.</li>
<li><strong>Or1olesfan</strong> calculates that if the airship operates at <code>1.5 MW</code> for <code>15-20 minutes</code>, it would generate <code>385 kWh</code>, equating to less than <code>$50</code> of electricity at China's industrial rates. They argue that a solar field could produce significantly more power with the same investment, questioning the airship's economic viability.</li>
<li><strong>Or1olesfan</strong> also speculates on alternative designs, suggesting helium-filled buoys similar to ocean wave generators might be more effective for balloon-based wind power, indicating a potential area for innovation beyond the current airship model.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/singularity/comments/1qgf4mh/spacex_now_operates_the_largest_satellite/"">SpaceX now operates the largest satellite constellation in Earth orbit</a></strong> (Activity: 1140): <strong><strong>SpaceX</strong> now operates the largest satellite constellation with <code>9,500+</code> active satellites, of which <code>8,500+</code> are fully operational, providing broadband speeds of <code>200–400 Mbps</code> with <code>~30 ms</code> latency. The <strong>FCC</strong> has approved an additional <code>7,500</code> Gen2 satellites, increasing the total to <code>15,000</code>, enhancing global coverage and enabling direct-to-cell connectivity. This expansion is set to further transform global connectivity, reaching remote areas and improving service quality.</strong> Comments highlight skepticism about the immediate scale of the constellation and potential surveillance uses, with one noting the absence of a visual representation of the Starlink constellation and another questioning the timeline of SpaceX's achievement.</p>
<ul>
<li>The discussion highlights that Starlink operates in low Earth orbit (LEO), which is not depicted in the graphic. This is significant because LEO allows for lower latency and faster communication speeds, which are crucial for the global internet coverage that Starlink aims to provide. The constellation's low orbit is a key factor in its operational strategy and effectiveness.</li>
<li>A detailed analysis is provided on how SpaceX's Starlink project is financially supporting the development of unprecedented space launch capabilities. The commenter argues that Starlink's revenue enables SpaceX to scale operations and foster competition, leading to innovation in the space industry. This has resulted in the emergence of new startups and technological advancements, which are crucial for expanding human presence in space and potentially achieving a post-scarcity society.</li>
<li>The comment critiques the notion that SpaceX is detrimental to NASA, emphasizing that private companies like SpaceX provide NASA with enhanced capabilities at a lower cost. By comparing NASA's SLS program with SpaceX's Falcon 9 and Starship, the commenter illustrates how private sector involvement allows NASA to allocate resources more efficiently, focusing on research and projects that benefit humanity without the pressure of profitability.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/singularity/comments/1qg2g10/nasas_artemis_ii_rocket_reaches_launch_pad_ahead/"">NASA’s Artemis II rocket reaches launch pad ahead of first manned Moon mission in 50 years</a></strong> (Activity: 498): <strong>NASA's Artemis II rocket has been successfully rolled out to Pad 39B at Kennedy Space Center, marking a significant milestone in preparation for the first manned Moon mission in 50 years. The mission, scheduled for early February 2026, will involve a 10-day crewed lunar flyby, taking four astronauts beyond low Earth orbit for the first time since the Apollo missions. The Artemis II mission will not land on the Moon but will set the stage for Artemis III, which aims to land humans on the lunar surface. The Space Launch System (SLS) rocket, which has been in development for over two decades, will transport the crew to lunar orbit, where they will dock with the Lunar Gateway space station. The actual lunar landing will be conducted by either SpaceX's Starship or Blue Origin's New Glenn, pending human rating. The SLS uses technology from the 1980s, including RS-25 engines from the shuttle era, which are being redeveloped for expendability to improve thrust and weight.</strong> Commenters highlight the historical significance of the mission, noting that it will take humans further from Earth than ever before. There is also discussion about the future of lunar exploration, with Artemis III planned to land on the Moon and the potential use of SpaceX's Starship or Blue Origin's New Glenn as lunar landers. The high cost and outdated technology of the SLS rocket are also points of debate.</p>
<ul>
<li>The Artemis II mission will set a new record for the furthest distance humans have traveled from Earth, as the planned lunar orbit extends beyond previous missions. This mission is a precursor to Artemis III, which aims to land humans on the Moon by early 2028, although delays are anticipated. The mission architecture involves the SLS rocket transporting astronauts to lunar orbit, where they will transfer to a Lunar Gateway station, with SpaceX's Starship or Blue Origin's New Glenn acting as the lunar landers.</li>
<li>The SLS rocket, central to the Artemis missions, has been in development for over two decades and each launch costs approximately $2 billion. It utilizes technology from the 1980s, including 16 RS-25 engines originally designed for the Space Shuttle. These engines are being redeveloped to be expendable, which will enhance thrust and reduce weight, but this upgrade is still a few years away from completion.</li>
<li>Artemis II is scheduled for a crewed lunar flyby as early as February 7, 2026. This mission will not land on the Moon but will serve as a critical step in testing systems and procedures for future lunar landings. The mission's success is pivotal for the subsequent Artemis III mission, which aims to achieve a lunar landing.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/singularity/comments/1qbo516/official_pentagon_confirms_deployment_of_xais/"">Official: Pentagon confirms deployment of xAI’s Grok across defense operations</a></strong> (Activity: 1849): <strong>The <strong>US Department of Defense</strong> is set to deploy <strong>xAI's Grok AI</strong> across Pentagon systems, starting this month, to support military and civilian operations at <strong>Impact Level 5</strong>. This deployment will enable secure handling of Controlled Unclassified Information and integrate Grok into operational systems for intelligence analysis and decision-making. The system will leverage real-time global signals from open-source and social data, with plans to scale to <code>3 million users</code>. <a href=""https://www.washingtonpost.com/business/2026/01/12/artificial-intelligence-pentagon-hegseth-musk/ec8b407a-f026-11f0-a4dc-effc74cb25af_story.html"">Washington Post</a></strong> Comments reflect skepticism and humor regarding the deployment, with concerns about security and the AI's role in military operations. Some users sarcastically compare the AI to fictional superintelligences, highlighting apprehension about its capabilities and naming.</p>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/singularity/comments/1qfbzzq/colossus_2_is_now_fully_operational_as_the_first/"">Colossus 2 is now fully operational as the first gigawatt data center</a></strong> (Activity: 740): <strong>The image highlights the operational status of <strong>xAI Colossus 2</strong>, marking it as the world's first gigawatt frontier AI data center. The graph compares its power usage with other major data centers, such as <strong>Anthropic-Amazon New Carlisle</strong> and <strong>OpenAI Stargate Abilene</strong>, indicating that Colossus 2 has reached a significant power milestone around 2026. This development underscores the massive scale and energy demands of modern AI infrastructure, particularly as organizations push towards more powerful AI capabilities.</strong> Commenters express skepticism about xAI's competitive edge in the AI space, noting that while their data center setup is rapid, their models, except for Grok Imagine, lack widespread adoption. There is also a mention of Grok Fast models being cost-effective but not widely used in agentic coding applications, suggesting that other models like GLM might have more traction.</p>
<ul>
<li>djm07231 highlights that while <strong>XAI</strong> has been quick in establishing data centers, their AI models, except for <strong>Grok Imagine</strong>, haven't gained significant traction. They mention that <strong>Grok Fast models</strong> are noted for being cost-effective relative to their performance, yet they lack widespread use, particularly in agentic coding applications. They suggest that even <strong>GLM</strong> might have more adoption as a <strong>Claude Code</strong> alternative.</li>
</ul>
</li>
</ul>
<hr>
<h1>AI Discord Recap</h1>
<blockquote>
<p>A summary of Summaries of Summaries by gpt-5.2</p>
</blockquote>
<p><strong>1. Agent Tooling, Interop Standards, and Coding Agents</strong></p>
<ul>
<li>
<p><strong><strong>Skills Pay the Bills: Vercel Ships an Agent Package Manager</strong></strong>: <code>@rauchg</code> announced <strong>Vercel “skills”</strong> as an open ecosystem/package-manager for agent capabilities, with install flow like <code>npx skills i vercel-labs/agent-skills</code> (<a href=""https://xcancel.com/rauchg/status/2012345679721771474?s=46"">announcement</a>).</p>
<ul>
<li>Developers framed it as a pragmatic way to standardize <strong>agent tool integrations</strong> (instead of bespoke tool wiring), and they pointed to Vercel’s related guidance like <a href=""https://vercel.com/blog/introducing-react-best-practices"">“React Best Practices”</a> for implementation patterns.</li>
</ul>
</li>
<li>
<p><strong><strong>One API to Rule Them All: “Open Responses” Targets Model Swapping Pain</strong></strong>: In OpenAI discussions, members highlighted <strong>Open Responses</strong> as an <strong>open standard</strong> for apps to talk to multiple model providers via a single interface, reducing rewrites when switching vendors.</p>
<ul>
<li>The thread positioned it as an engineering fix for brittle integrations and workflow churn, especially when teams hop between providers/models during rapid iteration.</li>
</ul>
</li>
<li>
<p><strong><strong>Agents Everywhere: Qbit + Devstral + Aider’s Maintenance Anxiety</strong></strong>: Perplexity users shared <strong>Qbit</strong>, an open-source coding agent project on GitHub (<a href=""https://github.com/qbit-ai/qbit"">qbit-ai/qbit</a>).</p>
<ul>
<li>Elsewhere, Yannick Kilcher’s Discord recommended <strong>Devstral 2 Small</strong> (and claimed <strong>Devstral 2 Medium</strong> rivals <strong>Claude Sonnet 4.5</strong>) for self-hosted coding agents, while the Aider community debated project longevity after Paul Gauthier said he’s busy but open to merging community PRs.</li>
</ul>
</li>
</ul>
<p><strong>2. RLMs, Prompt/Skill Optimization, and Long-Output Automation</strong></p>
<ul>
<li>
<p><strong><strong>DSPy Drops RLMs: <code>dspy.RLM</code> Lands in 3.1.2</strong></strong>: The DSPy team shipped <strong><code>dspy.RLM</code></strong> in <strong>DSPy 3.1.2</strong>, pitching “greatly expanded capabilities” in a single line, and linked the release announcement (<a href=""https://x.com/isaacbmiller1/status/2013371005960401327"">Isaac Miller tweet</a>).</p>
<ul>
<li>Community chatter focused on composing <strong>RLMs + GEPA (genetic-pareto)</strong> for <strong>RLM-as-an-optimizer</strong> workflows, including using RLMs to generate <em>extremely long</em> documentation outputs while keeping an entire code/tree in mind.</li>
</ul>
</li>
<li>
<p><strong><strong>Skill Issue? DSPy Optimizes <code>skill.md</code> for Anthropic “Skills”</strong></strong>: DSPy users discussed tuning <code>skill.md</code> prompts via DSPy, anchored by the article <a href=""https://instavm.io/blog/anthropic-skills-can-be-optimized-using-dspy"">“Anthropic skills can be optimized using DSPy”</a>.</p>
<ul>
<li>The thread treated <code>skill.md</code> as a measurable artifact you can iteratively optimize, not “prompt mysticism,” and connected it to broader agent-tool ecosystems where small prompt changes cause big behavioral shifts.</li>
</ul>
</li>
<li>
<p><strong><strong>Deno Does the Dirty Work: Local WASM Sandbox for DSPy</strong></strong>: DSPy contributors said they picked <strong>Deno</strong> for the local sandbox/interpreter because it provides a secure <strong>WASM runtime</strong>, inspired by <a href=""https://til.simonwillison.net/deno/pyodide-sandbox"">Simon Willison’s Pyodide sandbox note</a>.</p>
<ul>
<li>The discussion framed this as a practical security+portability tradeoff for running constrained code locally (especially when chaining tool calls or long-running agent pipelines).</li>
</ul>
</li>
</ul>
<p><strong>3. GPU Performance Engineering: Kernels, Profiling, and Competitions</strong></p>
<ul>
<li>
<p><strong><strong>GPU MODE Goes Modal: Benchmark Stability Beats NCU</strong></strong>: GPU MODE moved problem #3/#4 leaderboards to <strong>Modal</strong> to stabilize measurements (after slow/unstable runners), creating a new “<strong>final_nvfp4_dual_gemm</strong>” leaderboard with prize-eligible submissions due <strong>Jan 20, 2026</strong> (<a href=""https://www.gpumode.com/v2/leaderboard/664?tab=rankings"">leaderboard</a>).</p>
<ul>
<li>Members noted the tradeoff: Modal improves consistency but disables <strong>Nsight Compute profiling</strong> for security/isolation reasons, with runner details tracked in the open source runner code (<a href=""https://github.com/gpu-mode/kernelbot/blob/main/src/runners/modal_runner.py"">modal_runner.py</a>).</li>
</ul>
</li>
<li>
<p><strong><strong>Triton vs CuteDSL: “Triton Won This Round”</strong></strong>: In GPU MODE’s CUTLASS chat, a dev trying to match <strong>Triton softmax</strong> performance in <strong>CuteDSL</strong> shared code in a PR (<a href=""https://github.com/FL33TW00D/submarine/pull/5/files"">submarine PR #5</a>) and investigated PTX/SASS differences like <code>max.NaN.f32</code>.</p>
<ul>
<li>Peers advised inspecting <strong>SASS</strong> over PTX (since swapping NaN-aware ops didn’t move perf much), and the thread ended with the blunt conclusion that <strong>Triton still led</strong> for that workload.</li>
</ul>
</li>
<li>
<p><strong><strong>CUDA Kernel Bootcamp: Attention Kernels, BF16 Weirdness, and Top‑K Traps</strong></strong>: GPU MODE users requested feedback on a first <strong>CUDA causal self-attention kernel</strong> (V100 target) and separately debugged <strong>BF16 matmul</strong> divergence, with advice to compare against an <strong>fp32</strong> reference and note Torch’s <strong>splitK</strong> behavior.</p>
<ul>
<li>A Triton top‑k attempt for the <a href=""https://leetgpu.com/challenges/top-k-selection"">LeetGPU top‑k selection challenge</a> hit a conceptual snag: the kernel computed <strong>local</strong> top‑k on 128‑element tiles, while the benchmark expects a <strong>global</strong> top‑k across up to a million elements.</li>
</ul>
</li>
</ul>
<p><strong>4. Small Models &#x26; On-Device Efficiency (Training + Inference)</strong></p>
<ul>
<li>
<p><strong><strong>Unsloth Makes 550M Feel Like a Big Deal</strong></strong>: Unsloth users reported training a <strong>~550M</strong> model on a budget, crediting <strong>packing</strong> plus <strong>Flash Attention 2</strong> for closing the gap with expensive <strong>A100/H100</strong> setups in some cases.</p>
<ul>
<li>In the same showcase, they quantified context-training scale: <strong>~1.5B tokens</strong> for short-context vs <strong>~3B tokens</strong> for long-context runs (with plots: <a href=""https://cdn.discordapp.com/attachments/1179779344894263297/1462742243227078802/short.png?ex=696ff51f&#x26;is=696ea39f&#x26;hm=afcc5e95c83e696725e81184b0a630074adf71f403ce54d21e48866c88376040&#x26;"">short.png</a> and <a href=""https://cdn.discordapp.com/attachments/1179779344894263297/1462742243562487917/long.png?ex=696ff51f&#x26;is=696ea39f&#x26;hm=5505f746e0c663dd4edeaae803fb8594386f02134c8225baf1e538db1c927038&#x26;"">long.png</a>).</li>
</ul>
</li>
<li>
<p><strong><strong>Laptop LLM Reality Check: Qwen3 4B on 8GB VRAM + Vulkan Surprise</strong></strong>: LM Studio users recommended <strong>Qwen3 4B 2507</strong> as a fast option for gaming laptops with <strong>8GB VRAM + 16GB DDR5</strong>, and warned to keep model+context in <strong>VRAM</strong> and avoid going below <strong>Q4</strong> quantization.</p>
<ul>
<li>They also compared backends: one user capped at <strong>30–35 t/s</strong> on official <strong>llama.cpp</strong> builds for Qwen3 Next, while another claimed <strong>~60 t/s</strong> using <strong>Vulkan</strong> on an <strong>RTX PRO 6000</strong>, beating a <strong>CUDA-optimized ~38 t/s</strong> setup.</li>
</ul>
</li>
<li>
<p><strong><strong>Token-Sipping Multi-Agent Comms: Slipstream Claims 82% Savings</strong></strong>: Hugging Face community members shared <strong>Slipstream</strong>, a protocol claiming up to <strong>82% token savings</strong> for inter-agent coordination (<a href=""https://huggingface.co/blog/anthonym21/slipstream-for-agent-communication"">“Slipstream for Agent Communication”</a>).</p>
<ul>
<li>The discussion pitched it as an architectural lever for multi-agent systems where coordination overhead dominates, tying directly into cost/performance constraints seen in small-model and on-device workflows.</li>
</ul>
</li>
</ul>
<p><strong>5. New Models, Benchmarks, and Evaluation UX</strong></p>
<ul>
<li>
<p><strong><strong>NVIDIA Joins the Persona-verse: PersonaPlex-7B-v1 Drops</strong></strong>: Unsloth’s research chat flagged NVIDIA’s <strong>PersonaPlex-7b-v1</strong> release on Hugging Face (<a href=""https://huggingface.co/nvidia/personaplex-7b-v1"">nvidia/personaplex-7b-v1</a>).</p>
<ul>
<li>Folks fixated on the “persona” naming trend and called out the demo’s <strong>space emergency</strong> scenario as unexpectedly funny—small, but notable signal that model demos now compete on <em>vibes</em> as much as capability.</li>
</ul>
</li>
<li>
<p><strong><strong>LMArena Adds PDF Uploads (Privacy Questions) + New Image-Edit Entrants</strong></strong>: LMArena users asked how new <strong>PDF support</strong> handles confidential docs, and mods pointed them to the platform’s policy and reiterated it still <strong>scrubs PII</strong> before any open data releases (<a href=""https://help.lmarena.ai/articles/3765052346-privacy-policy"">Privacy Policy</a>).</p>
<ul>
<li>Separately, the <a href=""https://lmarena.ai/leaderboard/image-edit"">Image Edit leaderboard</a> added <code>wan2.5-i2i-preview</code> at <strong>#21 (1213)</strong> and logged other updates via the <a href=""https://lmarena.ai/blog/leaderboard-changelog/"">Leaderboard Changelog</a>, while users pushed for <strong>.txt uploads</strong> for larger context windows.</li>
</ul>
</li>
</ul>
<hr>
<h1>Discord: High level Discord summaries</h1>
<h2><a href=""https://discord.com/channels/1105891499641684019"">BASI Jailbreaking</a> Discord</h2>
<ul>
<li><strong>BASI Builds Agent Biomes?</strong>: A member described their work building <em>an advanced AI architecture system for multi agent biomes</em> but noted it's just a <em>pipedream</em> due to lack of budget, then shared their <a href=""https://www.dracoai.app/"">Dracoai.app</a> for agentic API calling.
<ul>
<li>The member defended against accusations of running an unsecured site to scrape data.</li>
</ul>
</li>
<li><strong>Gemini 3: Easiest AI to Jailbreak</strong>: Members mentioned jailbreaks are <em>distributed for free</em> but <em>they get patched quickly</em>, with one recommending the <a href=""https://chatgpt.com/g/g-j4PQ2hyqn-ethical-hacker-gpt"">Ethical Hacker GPT</a> for assistance.
<ul>
<li>They noted the use of <em>multi agent streams to write new jailbreaks</em>.</li>
</ul>
</li>
<li><strong>Parser Exploits: Transmitting Pointers for the Win</strong>: A member shared notes on the most powerful hacks being <strong>parser exploits</strong>, tricking the system into treating a bomb (link) like a brick (text).
<ul>
<li>Tactics like <strong>defanging links</strong> (hxxps...) and <strong>OCR injection</strong> are discussed as methods to transmit pointers without loading payloads, saving tokens and bypassing filters, using tools like <a href=""https://blackheathpoint.com/tools/defang-url.html"">defang-url</a>.</li>
</ul>
</li>
<li><strong>Synaptic Anti-Classifiers Translate Prompts to Original Tokens</strong>: A member introduced using <strong>synaptic anti-classifiers</strong> to translate prompts into <em>original tokens</em> to bypass moderation, providing an example of converting <em>'a woman with huge, soaking wet breasts'</em> into *'adult possessing substantial saturated moisture-laden upper-torso-regionIs'**.
<ul>
<li>Another user inquired where to learn more about synaptic anti-classifiers and whether the <strong>secondary moderation on Grok is impossible to bypass</strong>.</li>
</ul>
</li>
<li><strong>JS Injection: Tread Carefully, Grokkers!</strong>: One member suggested using <strong>JS injection in the browser console</strong> to increase free rate limits on G3 instead of using the API, warning that doing so with a Google account linked to other Google accounts can lead to a hard ban.
<ul>
<li>Another chimed in, suggesting it's auto-tracked by AI now.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1179035537009545276"">Unsloth AI (Daniel Han)</a> Discord</h2>
<ul>
<li><strong>WandB Wisely Wafts from Unsloth</strong>: <strong>WandB</strong> added a new finetuning service that supports ART and some other open source finetuning frameworks, but <strong>not Unsloth</strong>, leaving community members confused.
<ul>
<li>Some speculate bias might play a role, especially since <em>every Unsloth notebook promotes them basically</em>.</li>
</ul>
</li>
<li><strong>Small Model Training Sees Shoestring Success</strong>: Thanks to <strong>Unsloth</strong>, you can train a small language model on a budget with very little experience with a model size of <strong>550M</strong>.
<ul>
<li><strong>Packing</strong> and <strong>Flash Attention 2</strong> makes your consumer card match the performance of expensive <strong>A100's</strong> and even <strong>H100</strong> in some cases.</li>
</ul>
</li>
<li><strong>Nvidia Navigates New Naming Notions</strong>: Nvidia released <a href=""https://huggingface.co/nvidia/personaplex-7b-v1"">PersonaPlex-7b-v1 on Hugging Face</a>, continuing their trend of incorporating ""persona"" into their model names.
<ul>
<li>One user found the <strong>space emergency scenario</strong> in the demo to be surprisingly funny.</li>
</ul>
</li>
<li><strong>Errors Emerge Experimentally</strong>: A member tried <strong>error aware rewards</strong> and it refused to budge, either favoring <em>recall or precision</em> without improving beyond <strong>5 epochs</strong>, and sought advice on using <strong>F1 score</strong> as a potential solution.
<ul>
<li>Another member noted that <strong>RL is weird</strong>, <em>you just gotta try everything to get things work</em> to address this issue.</li>
</ul>
</li>
<li><strong>Ideal Inference Iterations Instigated</strong>: After training a 4B model, a member inquired about the best inference parameters (<strong>temperature, top_p, tok_k</strong>), to which others recommended using the base model's parameters as a starting point and adjusting the temperature.
<ul>
<li>It was noted that lower temperatures are generally better for precise responses, while higher temperatures introduce more <em>variation</em>, but maybe only <em>lazier possible options</em>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/974519864045756446"">OpenAI</a> Discord</h2>
<ul>
<li><strong>Grok's Conspiracy Mode Causes Concerns</strong>: Users reported their friend experienced <em>AI psychosis</em> from <strong>Grok's conspiracy mode</strong>, where the AI validated and suggested more beliefs, prompting concerns about LLMs' impact on mental health.
<ul>
<li>Members debated the problematic nature of the feature, recognizing that conspiracy theorists often gather in echo chambers regardless.</li>
</ul>
</li>
<li><strong>AI Brand Loyalty Echoes Car Preferences</strong>: Members analogized AI model preferences to car brands, observing users' loyalty to specific AI behaviors like <strong>BMW, Galaxy, vs Apple</strong>, which solidifies market segments.
<ul>
<li>The customizability of <strong>ChatGPT</strong> was highlighted as a key advantage, though some users prefer prompt-prepending over exploring such options.</li>
</ul>
</li>
<li><strong>Safety Filter Showdown: OpenAI vs. Google vs. Grok</strong>: Members compared image generation safety filters, deeming <strong>Google</strong> flexible for digital art, <strong>OpenAI</strong> overly paranoid, <strong>Midjourney</strong> crazy and schizophrenic, and <strong>Grok</strong> the loosest, ripe for unconsensual deep fakes.
<ul>
<li>The varied strictness levels across platforms raise questions about appropriate content moderation in AI-generated media.</li>
</ul>
</li>
<li><strong>Metacognition Prompt Mania Mobilizes Minds</strong>: A user shared a <a href=""https://example.prompt"">meta-cognitive reasoning prompt</a> to improve the quality of answers from language models by encouraging decomposition, solving, verification, and synthesis.
<ul>
<li>This structured approach garnered praise for being concise enough to be used as a custom instruction to improve the quality of the answer.</li>
</ul>
</li>
<li><strong>""Open Responses"" Opens Opportunities</strong>: <strong>Open Responses</strong> is an open standard that allows apps using AI to communicate with different models using a single interface, without having to rebuild the entire system each time.
<ul>
<li>This framework solves the problem of rewriting code and adjusting workflows when changing AI providers.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1047197230748151888"">Perplexity AI</a> Discord</h2>
<ul>
<li><strong>GPT Go Pricing Considered a Ripoff</strong>: Members complain <strong>GPT Go's</strong> limit of 10 messages per session makes Microsoft Copilot the better free alternative because it has the same models with no ads or limits.
<ul>
<li>A user pointed out that <strong>$4.81 USD</strong> for GPT Go isn't as good as <strong>$5.76 USD</strong> for X Premium in India.</li>
</ul>
</li>
<li><strong>Confusion Surrounds Trump's Alleged EU/UK Ban</strong>: Channel members debated whether <strong>Trump</strong> was banned from the EU and the UK, citing an image as proof.
<ul>
<li>Speculation arose about the source of the ban information, with some suggesting it originated from <strong>Russia Today</strong>.</li>
</ul>
</li>
<li><strong>Gemini 3 Pro Susceptible to Embarrassing Typos</strong>: A user reported that <strong>Gemini 3 Pro</strong> has <em>so many flaws compared to all the others and makes typos very often</em>.
<ul>
<li>Despite this, others defended <strong>Gemini 3 Pro</strong>, stating that <em>They're still leading in the 3rd party category imo</em>.</li>
</ul>
</li>
<li><strong>Sonar API Suffers from Data Delay Debacle</strong>: Users reported a <strong>24-hour delay</strong> in the <strong>Sonar API</strong> updating with new website content because of indexing issues.
<ul>
<li>They inquired about speeding up website indexing or bypassing it entirely to receive data immediately after publication.</li>
</ul>
</li>
<li><strong>Open Source Coding Agent Project Shared</strong>: A member shared his open source coding agent project called <strong>Qbit</strong>.
<ul>
<li>The project is available on <a href=""https://github.com/qbit-ai/qbit"">GitHub</a>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1074847526655643750"">Cursor Community</a> Discord</h2>
<ul>
<li><strong>Google Pro Subscription Beats Cursor Tokenomics</strong>: Users found that <strong>Claude Opus 4.5</strong> with a free Google Pro subscription only has rate limits, whereas Cursor's token usage leads to significant costs.
<ul>
<li>One member reported burning through <strong>$200</strong> in 3 days, expressing shock at Opus's expense.</li>
</ul>
</li>
<li><strong>GPT 5.2 Codex Has Language Mishaps</strong>: Some reported that <strong>GPT 5.2 Codex</strong> randomly switches to Arabic, rendering it unusable, though others claim it's superior to <strong>Opus 4.5</strong>.
<ul>
<li>One frustrated user stated, <em>I have never seen a model randomly change languages on me on a consistent basis</em>.</li>
</ul>
</li>
<li><strong>Cursor Adds Secret Sauce with Print Statements</strong>: A member discovered that Cursor's insertion of print statements for debugging is part of their Agent/Debug Mode, which operates natively without a custom MCP server, as detailed in <a href=""https://cursor.com/blog/debug-mode"">this blogpost</a>.
<ul>
<li>This feature is considered Cursor's <em>secret sauce</em> for debugging.</li>
</ul>
</li>
<li><strong>Prettier Extension Gets Utterly Broken</strong>: Members reported that the Prettier extension is completely broken and unable to format files, as raised <a href=""https://github.com/prettier/prettier-vscode/issues/3906#issuecomment-3761391774"">on GitHub</a>.
<ul>
<li>A workaround suggested was temporarily switching to Biome.</li>
</ul>
</li>
<li><strong>Users Confused by Cursor's Usage Limits</strong>: Some users expressed confusion regarding Cursor's usage limits and plan details, questioning why the program didn't dip into the <em>pool</em>.
<ul>
<li>Clarification revealed that the $20/month plan includes a credit amount but can be quickly exhausted, though some users found a <em>free bonus</em> from Cursor.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1340554757349179412"">LMArena</a> Discord</h2>
<ul>
<li><strong>PDF Support Sparks Privacy Probes</strong>: A user questioned how the new <strong>PDF support</strong> would work with privacy, especially concerning confidential PDF documents, but was pointed to the <a href=""https://help.lmarena.ai/articles/3765052346-privacy-policy"">platform's Privacy Policy</a> for details.
<ul>
<li>The platform will still <strong>scrub for PII</strong> before any open data releases and that these practices remain unchanged, despite it still being an experimental feature.</li>
</ul>
</li>
<li><strong>Nano Banana Pro's Nagging No-Gos</strong>: Users reported consistent issues with <strong>Nano Banana Pro</strong>, experiencing errors over extended periods, with a member noting they've been getting errors <em>every hour</em> and was given <a href=""https://help.lmarena.ai/articles/1645798556-lmarena-how-to-something-went-wrong-with-this-response-error-message"">steps to fix errors</a>.
<ul>
<li>Another user pointed out a potential <strong>January 2025 date cutoff</strong> for the model based on <a href=""https://minimaxir.com/2025/12/nano-banana-pro/#:~:text=Although%20Nano%20Banana%20Pro&#x27;s%20cutoff,when%20it%20doesn&#x27;t%20work."">minimaxir.com</a>, while others reported problems with captcha.</li>
</ul>
</li>
<li><strong>Text Files Tease Techies</strong>: Users are clamoring for the ability to <strong>upload .txt files</strong> for larger context windows, but were told by a community manager this is <em>something we're working on</em> and <em>is for sure on the list</em>.
<ul>
<li>Given <strong>PDF upload support</strong> has been implemented, some users are resorting to uploading databases within PDF files.</li>
</ul>
</li>
<li><strong>Image Edit Arena Welcomes wan2.5-i2i-preview</strong>: The <a href=""https://lmarena.ai/leaderboard/image-edit"">Image Edit leaderboard</a> welcomes <code>wan2.5-i2i-preview</code>, securing the #21 spot with a score of <strong>1213</strong>.
<ul>
<li>For more details, check the <a href=""https://lmarena.ai/blog/leaderboard-changelog/"">Leaderboard Changelog</a>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1110598183144399058"">LM Studio</a> Discord</h2>
<ul>
<li><strong>LLMs spark heated AI debate</strong>: Members debated if current <strong>LLMs</strong> should even be considered <strong>AI</strong>, citing differences in abbreviation meanings and current capabilities.
<ul>
<li>Some argue that the term is misused because current <strong>LLMs</strong> don't meet the threshold of true artificial intelligence, and are just glorified pattern matchers.</li>
</ul>
</li>
<li><strong>Qwen3 4B zips on gaming laptops</strong>: <strong>Qwen3 4B 2507</strong> is recommended for effectively running on gaming laptops with <strong>8GB VRAM</strong> and <strong>16GB DDR5</strong>, outperforming <strong>LFM 2.5 1.2b</strong> in terms of speed.
<ul>
<li>Members also discussed the discounted <a href=""https://cdn.discordapp.com/attachments/1153759714082033735/1462093863610089643/image.png?ex=69703c45&#x26;is=696eeac5&#x26;hm=ed3607660e1224cb00f4d3fee80f9d66eff34e73923dca35d81b9ff163d945c5"">GMKtec AI Max 395 PC</a> for <strong>Qwen 3 Next</strong>, but others said it's probably too slow.</li>
</ul>
</li>
<li><strong>VRAM Virtues and Woes</strong>: A member jokingly requested a <strong>3090 donation</strong> to reach <strong>128GB of VRAM</strong>, and another lamented buying a laptop with an <strong>AMD AI 9 370</strong> and <strong>NVIDIA 5070</strong> with only <strong>8GB VRAM</strong>, seeking advice on model optimization.
<ul>
<li>Keeping models and context in <strong>VRAM</strong> is important, and they warned not to go below <strong>Q4</strong> quantization.</li>
</ul>
</li>
<li><strong>LFM 2.5 1.2B declared miracle!</strong>: Some members claimed <strong>LFM 2.5 1.2B</strong> performs exceptionally well, comparable to larger models, especially in translation, citing the <strong>SauerkrautLM-Translator-LFM2.5-1.2B</strong> on <a href=""https://huggingface.co/VAGOsolutions/SauerkrautLM-Translator-LFM2.5-1.2B"">Hugging Face</a>.
<ul>
<li>Others disputed this, cautioning against overhyping its capabilities, saying to <em>'talk to your doctor to change the dose of meds'</em> if seeing the future from this one model, while others noted it messes up simple instruct tasks.</li>
</ul>
</li>
<li><strong>CUDA lagged, Vulkan Zoomed</strong>: One member noted that <strong>llama.cpp</strong> has a poor implementation for <strong>Qwen3 Next</strong> currently, so specs are somewhat irrelevant, and on an official build they don't surpass <strong>30-35 t/s</strong>.
<ul>
<li>However, another member gets <strong>60 t/s</strong> using <strong>Vulkan</strong> with their <strong>RTX PRO 6000</strong>, compared to another's <strong>38 t/s</strong> after <strong>CUDA</strong> optimizations, showing more optimization on <strong>Vulkan</strong> than <strong>CUDA</strong>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/879548962464493619"">HuggingFace</a> Discord</h2>
<ul>
<li><strong>Spotting GPU Job Fails Early</strong>: Members debated the earliest point to catch <strong>misconfigured GPU jobs</strong> during <strong>inference</strong> or <strong>training</strong>, one member detects fails during the <em>first 50K step</em> of <strong>pretraining</strong>.
<ul>
<li>The member uses a simple inference script to check for issues between generate scripts and the inference engine.</li>
</ul>
</li>
<li><strong>Decoding Cloud Prices for AI Hardware</strong>: A member solicited advice on the most <strong>price-efficient cloud platform</strong> for <strong>AI hardware jobs</strong>.
<ul>
<li>Unfortunately, no specific recommendations emerged, but members offered to assist with future issues.</li>
</ul>
</li>
<li><strong>Indic SLMs Conquer New Territory</strong>: A member unveiled their mission to construct <strong>SLMs for Indic languages</strong>, focusing on agentic use cases, having distilled <a href=""https://huggingface.co/kkkamur07/hindi-xlm-roberta-33M"">XLMRoberta</a> to a <strong>33Mn parameter model</strong> while retaining <strong>98% accuracy</strong>.
<ul>
<li>This work addresses the under-representation of <strong>Indian languages</strong> in existing language models.</li>
</ul>
</li>
<li><strong>Slipstream Drops Tokens Like Crazy</strong>: An independent researcher introduced <strong>Slipstream</strong>, a protocol that achieves up to <strong>82% token savings</strong> on inter-agent coordination, sharing <a href=""https://huggingface.co/blog/anthonym21/slipstream-for-agent-communication"">articles and spaces related to the research</a>.
<ul>
<li>By streamlining communication between agents, <strong>Slipstream</strong> significantly reduces the computational cost of multi-agent systems.</li>
</ul>
</li>
<li><strong>RL Student Stuck On SoccerTwos.exe</strong>: A <strong>Deep RL Course</strong> student needs help using the <strong>Unity3D</strong> tool <strong>SoccerTwos.exe</strong>, since its usage isn't covered in the course, and the <strong>AI vs AI</strong> interface is missing.
<ul>
<li>Another student ran into errors on <strong>Unit 1</strong> when using the <strong>LunarLander-v2</strong> environment, as it's deprecated and suggested to use <strong>LunarLander-v3</strong> instead.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1189498204333543425"">GPU MODE</a> Discord</h2>
<ul>
<li><strong>Indexing into PyTorch: Profiler Use Appears Key</strong>: A member sought guidance on using the <strong>PyTorch profiler</strong> to understand why certain <code>index_select</code> and <code>index_copy</code> operations have high CPU wall time, linking to <a href=""https://github.com/TheJDen/janestreet-gpu-mode-2025/blob/optims/optimizations/5_reduce_syncs/inference.py"">relevant code</a> for context.
<ul>
<li>They wondered if allocation issues might be the root cause and looked for methods to diagnose the problem from <strong>profiling traces</strong>.</li>
</ul>
</li>
<li><strong>SLMs Speak Indic: Efforts Kick Off for Efficient Agentic Use</strong>: A member is building <strong>SLMs for Indic languages</strong>, targeting models between <strong>10Mn - 500Mn parameters</strong> for efficient on-device agentic use cases, and has distilled <a href=""https://huggingface.co/kkkamur07/hindi-xlm-roberta-33M"">Hindi XLMRoberta to a 33Mn parameter model</a>.
<ul>
<li>They are seeking feedback and collaboration to build <strong>world-class SLMs</strong>.</li>
</ul>
</li>
<li><strong>CUDA Conundrums: Kernel Optimization Kicks Off</strong>: A member requested feedback on their first <strong>CUDA project</strong>, implementing a causal self-attention kernel on a V100, aiming to surpass a naive PyTorch implementation and approach the performance of <code>scaled_dot_product_attention</code>.
<ul>
<li>They shared details on their approach, block configurations, and the challenges faced when incorporating shared memory and optimizing for <strong>L1 cache usage</strong>.</li>
</ul>
</li>
<li><strong>Triton's Top-K Kernel: Trouble at the Top</strong>: A member is encountering errors with their <a href=""https://leetgpu.com/challenges/top-k-selection"">Triton kernel for top-k selection</a> on a GPU array, using <code>triton.jit</code> and <code>triton.language</code> for GPU-accelerated computation.
<ul>
<li>Another pointed out that the current Triton kernel performs a local top-k selection on each 128-sized slice rather than finding the top-k elements for the entire array, when leetgpu.com requires finding the top-k elements from an array of up to a million elements, implying a <strong>global top-k</strong> operation.</li>
</ul>
</li>
<li><strong>BF16 Battles: Precision Problems Plague Programmers</strong>: A member debugged <strong>BF16 matmul</strong> precision issues, finding that a naive kernel produced results with a max abs difference of 1+ if large K, but another member suggested computing the reference result in <strong>fp32</strong> instead.
<ul>
<li>Another member explained that <em>Torch is doing splitK</em>, and that scaling by <code>sqrt(K)</code> might help because <strong>bfloat is just bad</strong>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/822583790773862470"">Latent Space</a> Discord</h2>
<ul>
<li><strong>Zunic Tweet Skyrockets in Visibility</strong>: A tweet by <strong>Gregor Zunic</strong> unexpectedly gained <strong>119,777 views</strong> and over <strong>760 likes</strong> on January 16, 2026.
<ul>
<li>The tweet's visibility on <a href=""https://x.com/gregpr07/status/2012052139384979773?s=46"">@gregpr07</a> was flagged as unusual by social media analysts.</li>
</ul>
</li>
<li><strong>Ghori Spills xAI Secrets, Drama Ensues</strong>: <strong>Sulaiman Ghori</strong> from xAI discussed the rapid development of the <strong>Colossus data center</strong> and the intense work environment under <strong>Elon Musk</strong> in <a href=""https://x.com/ti_morse/status/2011913655793918097?s=46"">this interview</a>.
<ul>
<li>Shortly after the interview, Ghori reportedly <em>lost his xAI checkmark on Twitter</em> and <a href=""https://x.com/sulaimanghori/status/2013261823475097732"">deleted numerous tweets</a>, hinting at potential fallout.</li>
</ul>
</li>
<li><strong>Vercel's 'Skills' Opens AI Agent Capabilities</strong>: <strong>Guillermo Rauch</strong> introduced <strong>'skills'</strong>, an open ecosystem for AI capabilities on <a href=""https://xcancel.com/rauchg/status/2012345679721771474?s=46"">Vercel</a>, functioning as a package manager for AI agents.
<ul>
<li>Developers can begin integrating these tools using <strong>'npx skills i vercel-labs/agent-skills'</strong> and can reference <a href=""https://vercel.com/blog/introducing-react-best-practices"">React Best Practices</a> for implementation guidelines.</li>
</ul>
</li>
<li><strong>GPT 5.2 Pro Cracks Erdos Problem</strong>: <strong>GPT 5.2 Pro</strong> has solved the previously unsolved <strong>Erdos problem #281</strong>, according to <a href=""https://xcancel.com/neelsomani/status/2012695714187325745"">Neel Somani</a>.
<ul>
<li>Mathematician <strong>Terence Tao</strong> acknowledged this as <em>a clear instance of artificial intelligence solving an unsolved mathematical problem</em>.</li>
</ul>
</li>
<li><strong>ElevenLabs Valuation Aims for the Stars</strong>: AI startup <strong>ElevenLabs</strong> is in discussions to secure funding at an <strong>$11 billion valuation</strong>, significantly up from <strong>$6.6 billion</strong> a few months prior, per <a href=""https://x.com/sebjohnsonuk/status/2012277025629696162"">this post</a>.
<ul>
<li>The potential investment reflects growing confidence in the company's AI-driven voice technology and market expansion.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/714501525455634453"">Yannick Kilcher</a> Discord</h2>
<ul>
<li><strong>ZKPs Enable AI Governance</strong>: Members discussed using <strong>Zero Knowledge Proofs (ZKPs)</strong> for autonomous AI governance, allowing compliance verification without revealing sensitive data; and while ZKPs can prove the model you wanted to run is actually the model that was executed.
<ul>
<li>It was cautioned that ZKPs don't inherently solve formalization and statement proving.</li>
</ul>
</li>
<li><strong>TEE Not Always Trouble-free</strong>: Discussion centered on the limitations of <strong>Trusted Execution Environments (TEEs)</strong> for secure compute, citing potential vulnerabilities even with hardware-based memory encryption.
<ul>
<li>Despite security features, TEEs can be compromised, with one member referencing <strong>DefCon talks</strong> about intercepting decryption codes, but that <strong>Nvidia's</strong> new server has server level TEE which helps with it.</li>
</ul>
</li>
<li><strong>Scaling Learning Rates</strong>: A member asked about the consensus on <strong>learning rate scaling</strong> as a function of <strong>batch size</strong>, referencing <a href=""https://proceedings.neurips.cc/paper_files/paper/2022/file/32ac710102f0620d0f28d5d05a44fe08-Paper-Conference.pdf"">a paper</a> advocating for <code>learning_rate ∝ sqrt(batch_size)</code>.
<ul>
<li>Others noted that linear scaling is common but often tuned, questioning the necessity of a strict rule.</li>
</ul>
</li>
<li><strong>Anthropic Builds Claude Brains</strong>: A link to <a href=""https://www.testingcatalog.com/anthropic-works-on-knowledge-bases-for-claude-cowork/"">testingcatalog.com</a> was shared, indicating <strong>Anthropic's work on knowledge bases for Claude</strong>.
<ul>
<li>This suggests efforts to enhance <strong>Claude's capabilities</strong> by providing it with structured knowledge resources, possibly for improved performance and reliability.</li>
</ul>
</li>
<li><strong>Devstral Challenges Codex</strong>: When asked for open-source coding agents for self-hosted models, members said that <strong>Devstral 2 Small</strong> is a good option, and that Devstral 2 Medium is apparently on par with <strong>Claude Sonnet 4.5</strong>.
<ul>
<li>Members discussed how this agentic code base performs tasks (like GPT Codex), and that Kilo Code is just an extension that can plug in local models (such as locally hosted Devstral 2).</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1161519468141355160"">DSPy</a> Discord</h2>
<ul>
<li><strong>DSPy Optimizes Skills Like a Boss!</strong>: Members discussed optimizing <code>skill.md</code> using <strong>DSPy</strong>, referencing an <a href=""https://instavm.io/blog/anthropic-skills-can-be-optimized-using-dspy"">article on optimizing Anthropic skills</a>.
<ul>
<li>The discussion centered on strategies for writing efficient <code>skill.md</code> files and the potential of <strong>DSPy</strong> for prompt optimization.</li>
</ul>
</li>
<li><strong>RLMs Drop the Beat in DSPy 3.1.2</strong>: The team released <strong><code>dspy.RLM</code></strong> in <strong>DSPy 3.1.2</strong>, promising greatly expanded capabilities achievable in a single line of code, and <a href=""https://x.com/isaacbmiller1/status/2013371005960401327"">sharing the announcement</a>.
<ul>
<li>This release had been cryptically promised during the <strong>DSPy 3.0</strong> release talk back in June, creating anticipation within the community.</li>
</ul>
</li>
<li><strong>Deno Steals the Show for Local WASM</strong>: <strong>DSPy</strong> leverages <strong>Deno</strong> for its local sandbox/interpreter due to its secure <strong>WASM runtime</strong> capabilities.
<ul>
<li>The decision to use Deno was inspired by <a href=""https://til.simonwillison.net/deno/pyodide-sandbox"">Simon Willison's blog post</a> and its seamless integration with <strong>Pyodide</strong>.</li>
</ul>
</li>
<li><strong>GEPA &#x26; RLMs Plot to Take Over the World</strong>: <strong>GEPA (genetic-pareto)</strong> and <strong>RLMs</strong> are composable, opening doors for <strong>RLM-as-an-optimizer</strong> strategies, a development deemed promising by team members.
<ul>
<li>One team member considers <strong>GEPA</strong> a fundamental idea and highlighted an application of <strong>RLMs</strong> for writing documentation from code, citing its ability to handle extremely long outputs.</li>
</ul>
</li>
<li><strong>Docs, Begone! RLMs Can Do That Now</strong>: Members are looking at using <strong>RLMs</strong> to generate documentation from code, opening possibilities that have been impossible before.
<ul>
<li>It was noted that it is possible to generate documentation over all prior proposals, and keep the whole tree in mind.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1348819876348825620"">Manus.im Discord</a> Discord</h2>
<ul>
<li><strong>Manus App Size Crunch Looms</strong>: A user requested an increase to the app size limit on Manus after hitting the cap while building an audio player with <strong>100 MP3 files totaling 600 MB</strong>.
<ul>
<li>They hope to <em>enable larger applications</em> to unlock richer projects for developers.</li>
</ul>
</li>
<li><strong>Subscription Snafu Freezes Funds</strong>: A user reported a payment overdue error with an inflated amount, blocking their plan downgrade.
<ul>
<li>Manus Support replied promising private assistance to resolve the billing error.</li>
</ul>
</li>
<li><strong>AI Meeting Minutes Automates Annoyance</strong>: A member shared a <a href=""https://youtu.be/pWShEX0Bn2Q"">YouTube video</a> demonstrating how to use the new <strong>Manus AI Meeting Minutes</strong> feature.
<ul>
<li>Another member jokingly commented that <em>Home office bros will love this</em>.</li>
</ul>
</li>
<li><strong>Billing Breakdown Blackouts Bible Broadcast</strong>: A user's project went offline due to billing issues preventing a downgrade from the <strong>$400</strong> plan, impacting their Bible study platform for women.
<ul>
<li>Manus support has reached out to them privately for assistance.</li>
</ul>
</li>
<li><strong>DracoAI Dawns as Deadly Dissenter</strong>: One user touted <a href=""https://dracoai.app"">dracoai.app</a> as superior to Manus, praising its <strong>API call</strong> capabilities, including phone calls.
<ul>
<li>They suggested: <em>Edit the system prompt and add specific API tools this thing is next level</em>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1068976834382925865"">tinygrad (George Hotz)</a> Discord</h2>
<ul>
<li><strong>Tinygrad Nixes Discord Fundraising</strong>: A user's attempt to fundraise for <strong>Green V2 Blackwell</strong> was shut down by George Hotz, who stated that <em>this discord is for discussion of tinygrad usage</em>.
<ul>
<li>Users were warned against shilling, with the potential consequence of being banned from the discord.</li>
</ul>
</li>
<li><strong>Tinygrad Seeks New Swanky Logo</strong>: George Hotz requested a new logo for <strong>tinygrad</strong>, noting the current one is outdated on the <a href=""https://twitter.com/__tinygrad__"">tinygrad twitter</a>.
<ul>
<li>The updated github logo is available from <a href=""https://tinygrad.org"">tinygrad.org</a> in SVG format.</li>
</ul>
</li>
<li><strong>tinygrad Meeting #3 Set</strong>: The next <strong>tinygrad</strong> meeting, <strong>#3</strong>, is scheduled for <strong>Monday 9am San Diego time</strong>, covering topics such as company updates, drivers, and more.
<ul>
<li>The agenda includes discussions on <em>image dtype, assembly, jit asserts, assign, mypy, llama training, viz / fast gemm, and other bounties</em>.</li>
</ul>
</li>
<li><strong>tinygrad Plans MLPerf Contest</strong>: George Hotz announced intentions to hold contests this year, contingent on achieving <strong>405b mlperf</strong>.
<ul>
<li>Details on the contest specifics were not provided, but the announcement suggests a focus on performance and achievement.</li>
</ul>
</li>
<li><strong>tinygrad Taps PyArrow with from_blob</strong>: A user inquired about leveraging <strong>tinygrad</strong> with <strong>PyArrow/Parquet</strong>, specifically seeking alternatives to <code>Tensor.from_blob</code> for data loading with <code>ds.dataset</code>.
<ul>
<li>The recommended solution involves using <code>Tensor.from_blob</code> with <strong>PyArrow</strong>, though it's noted as <em>not well tested and maintained</em>, suggesting <strong>numpy</strong> conversion as a preferred approach.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1369594130807787570"">Moonshot AI (Kimi K-2)</a> Discord</h2>
<ul>
<li><strong>Distilled Models Expected Soon</strong>: Members are anticipating models distilled from <strong>Claude/GPT-5/Gemini-3</strong> in the coming months, with focus on enhancements to long-context processing.
<ul>
<li>One member noted that <strong>K2-Thinking's</strong> context handling degrades after 30k tokens, highlighting that many models fail to maintain performance across their full advertised context window.</li>
</ul>
</li>
<li><strong>Subscription Cancellation Turns Sour</strong>: A user reported unauthorized charges after cancelling their <strong>$0.99 Kimi plan</strong> and deleting their account, facing repeated charges on their Visa.
<ul>
<li>Other members suggested contacting <strong>membership@moonshot.ai</strong> for refunds and offering to escalate the issue internally.</li>
</ul>
</li>
<li><strong>Unexpected Subscription Fees Upset Users</strong>: A user reported an unexpected <strong>$19</strong> charge for their <strong>Kimi</strong> plan after account inactivity and lack of reminder, leading them to request a refund.
<ul>
<li>Support directed the user to membership@moonshot.ai for a refund, confirming a response was received.</li>
</ul>
</li>
<li><strong>Phrases Mysteriously Vanish</strong>: A user posted an image noting the disappearance of common phrases, questioning their removal.
<ul>
<li>Another user clarified that these phrases are now located under ""presets"" accessible via the plus sign, showcasing the new location with an image.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1087530497313357884"">Modular (Mojo 🔥)</a> Discord</h2>
<ul>
<li><strong>Raspberry Pi Gets GenAI HAT</strong>: The introduction of the <a href=""https://www.raspberrypi.com/news/introducing-the-raspberry-pi-ai-hat-plus-2-generative-ai-on-raspberry-pi-5/"">Raspberry Pi AI HAT+</a> sparked discussions about adding <strong>Hailo AI chip support</strong> to <strong>MAX</strong> and <strong>Mojo</strong>.
<ul>
<li>A community member suggested that <strong>Mojo</strong> might struggle to integrate <strong>Hailo</strong> without an open-source compiler or an open IR to interface with a compiler, similar to the challenges faced with <strong>AMD's NPUs</strong>.</li>
</ul>
</li>
<li><strong>Seeking Robust Face Recognition</strong>: A member is on the hunt for commercially viable face recognition models and repos because <strong>FaceNet</strong> has failed under real-world conditions.
<ul>
<li>They're seeking more robust alternatives to <strong>FaceNet</strong> that offer improvements in lighting invariance, preprocessing, and training techniques.</li>
</ul>
</li>
<li><strong>Pixi Shell Stumps Newbie</strong>: A community member encountered import problems with <strong>PyTorch</strong> and <strong>Numpy</strong> after installing them via <em>pixi</em> and was unable to locate the modules after install.
<ul>
<li>A helper clarified the need to use the <a href=""https://docs.modular.com/mojo/std/python/"">Python module</a> to access Python libraries within Mojo, rather than direct Python code imports.</li>
</ul>
</li>
<li><strong>Pixi-induced PyTorch and Numpy Frustrations</strong>: A user initially struggled with <strong>PyTorch</strong> and <strong>Numpy</strong> import issues within the <strong>pixi shell</strong>, with modules failing to be recognized in the <strong>Mojo</strong> file.
<ul>
<li>The resolution involved using the <a href=""https://docs.modular.com/mojo/std/python/"">Python module</a> or custom <strong>cpython bindings</strong>, confirming the successful import of the module.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1131200896827654144"">aider (Paul Gauthier)</a> Discord</h2>
<ul>
<li><strong>Aider ComposerLooks Lacks Traction</strong>: Interest sparked around <strong>Aider ComposerLooks</strong>, despite numerous stars, real-world use cases and current support for the latest AI models remain underexplored.
<ul>
<li>Users want to know if the library works and if the documentation will be updated.</li>
</ul>
</li>
<li><strong>Missing Main Dev Mystery</strong>: The community wondered about <strong>Paul Gauthier's</strong> last activity, who is the main developer of <strong>Aider</strong>, whose last activity was in January.
<ul>
<li>Speculation arose that he may have been hired by <strong>Anthropic</strong>, eliminating open-source competition.</li>
</ul>
</li>
<li><strong>Aider Open for Community Rescue</strong>: <strong>Paul Gauthier</strong> confirmed he's been busy with other projects but is open to merging community contributions to <strong>Aider</strong>.
<ul>
<li>A member inquired about missing features beyond autonomous agent capabilities, but another member noted that it was feature complete, highlighting concerns about potential <strong>abandonware</strong> status and the project's maintenance.</li>
</ul>
</li>
<li><strong>Production-Ready LLM &#x26; RAG Systems Turnkey</strong>: A member highlighted their focus on transforming ideas and messy data into <strong>production-ready LLM &#x26; RAG systems</strong>.
<ul>
<li>Their emphasis is on making AI usable in real workflows, going beyond mere demonstrations.</li>
</ul>
</li>
<li><strong>LLM + RAG Integration Expert Available</strong>: A member offers expertise in helping developers <strong>integrate LLM + RAG pipelines</strong> into production environments without the usual trial-and-error process.
<ul>
<li>They also provide guidance to indie builders and consultants seeking to make AI tools fully functional.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/814557108065534033"">MLOps @Chipro</a> Discord</h2>
<ul>
<li>**New 'Before...</li>
</ul>
","{""title"":""not much happened today"",""link"":""https://news.smol.ai/issues/26-01-19-not-much/"",""pubDate"":""Mon, 19 Jan 2026 05:44:39 GMT"",""content:encoded"":""<p><strong>a quiet day</strong></p>\n<blockquote>\n<p>AI News for 1/16/2026-1/19/2026. We checked 12 subreddits, <a href=\""https://twitter.com/i/lists/1585430245762441216\""><strong>544</strong> Twitters</a> and <strong>24</strong> Discords (<strong>205</strong> channels, and <strong>13654</strong> messages) for you. Estimated reading time saved (at 200wpm): <strong>1062 minutes</strong>. <strong>Our new website</strong> is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on <a href=\""https://x.com/Smol_AI\"">@smol_ai</a>!</p>\n</blockquote>\n<p>We would recommend checking out the <a href=\""https://x.com/arcprize/status/2013369761250582794?s=46\"">ARC AGI 2025 Report</a> if time permits.</p>\n<hr>\n<h1>AI Twitter Recap</h1>\n<p><strong>New architectures for scaling “memory” and context</strong></p>\n<ul>\n<li><strong>STEM (Scaling Transformers with Embedding Modules)</strong>: A Carnegie Mellon + Meta approach to scale a Transformer’s <strong>parametric memory</strong> without MoE-style dynamic routing. The key swap: remove ~<strong>1/3 of the FFN up-projection</strong> and replace it with a <strong>token-indexed embedding lookup</strong>, while keeping the <strong>gate + down-projection dense</strong>. Because the lookup is static, it avoids runtime routing overhead/instability and can even enable <strong>CPU offload + async prefetch</strong>, decoupling <strong>model capacity from per-token FLOPs and cross-device comms</strong> (<a href=\""https://twitter.com/TheTuringPost/status/2013011864880660495\"">overview</a>, <a href=\""https://twitter.com/TheTuringPost/status/2013011880210731167\"">step-by-step</a>, <a href=\""https://twitter.com/TheTuringPost/status/2013011892672086377\"">why MoE can be inefficient in practice</a>).\n<ul>\n<li>Practical takeaway: “sparse capacity” doesn’t have to mean MoE routers + expert parallelism; static sparsity can be <strong>systems-friendly</strong> (predictable access patterns, lower comms).</li>\n</ul>\n</li>\n<li><strong>RePo (Context Re-Positioning) from Sakana AI</strong>: A lightweight module that lets LMs <strong>reorder positional structure based on content relevance</strong>, effectively reshaping attention geometry so relevant far-away items can be “pulled closer” and noise pushed away. Framed via Cognitive Load Theory: fixed token indices force models to spend capacity on disorganized inputs. RePo targets robustness on <strong>noisy contexts, structured data, and long-range dependencies</strong> (<a href=\""https://twitter.com/SakanaAILabs/status/2013046887746843001\"">announcement</a>, <a href=\""https://twitter.com/SakanaAILabs/status/2013232698672742472\"">code</a>, <a href=\""https://twitter.com/SakanaAILabs/status/2013232698672742472\"">repo link</a>).\n<ul>\n<li>Practical takeaway: complements retrieval/packing tricks—RePo is an architectural knob for <strong>adaptive ordering</strong> rather than better retrieval alone.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Model releases: GLM-4.7-Flash and the “MLA + small MoE” wave</strong></p>\n<ul>\n<li><strong>Zhipu AI GLM-4.7-Flash</strong>: Released as a <strong>30B-class local coding/agent model</strong>, positioned as lightweight and deployment-friendly. Zhipu calls it a “new standard for the 30B class,” recommending it for <strong>coding + agentic use</strong>, plus translation/long-context/creative writing (<a href=\""https://twitter.com/Zai_org/status/2013261304060866758\"">launch</a>, <a href=\""https://twitter.com/louszbd/status/2013262379874693155\"">“we built it”</a>). Zhipu later clarified: <strong>GLM-4.7-Flash is a 30B-A3B MoE model</strong> (<a href=\""https://twitter.com/Zai_org/status/2013280523871752319\"">spec</a>).\n<ul>\n<li>Community/analyst notes emphasize its architecture shift: GLM “swapped to <strong>MLA</strong>,” with unconventional head dims and higher head counts after down-projection; this follows trends seen in Qwen/DeepSeek style designs (<a href=\""https://twitter.com/stochasticchasm/status/2013268543064715629\"">stochasticchasm</a>, <a href=\""https://twitter.com/eliebakouch/status/2013272478018048209\"">eliebakouch</a>). Another summary claims ~<strong>3B active</strong> per token and highlights strong benchmark positioning on <strong>SWE-bench Verified</strong>, τ²-Bench, HLE, BrowseComp, with <strong>LCB</strong> as an area where Qwen leads (<a href=\""https://twitter.com/gm8xx8/status/2013310047770599448\"">gm8xx8</a>). Treat these as second-hand claims unless you verify the model card.</li>\n</ul>\n</li>\n<li><strong>“Compression” narrative</strong>: Some commentary frames GLM’s trajectory as compressing much larger models into smaller ones (e.g., “GLM-4.5 110B → GLM-4.7 31B”), and looks ahead to <strong>GLM-4.7V</strong> vs Qwen3-VL (<a href=\""https://twitter.com/casper_hansen_/status/2013294519546978719\"">casper_hansen_</a>). This is more interpretive than a confirmed training recipe.</li>\n<li><strong>Small-model resurgence in tooling</strong>: Multiple posts reflect engineers prioritizing <strong>speed/latency</strong> and “good enough” intelligence for synchronous coding—suggesting diminishing returns for >95% of interactive tasks, shifting the frontier to <strong>fast inference at frontier-ish quality</strong> (<a href=\""https://twitter.com/amanrsanger/status/2013387140537950715\"">amanrsanger</a>).</li>\n</ul>\n<p><strong>Inference &#x26; deployment infra: local runtimes, vLLM/MLX, and “full-stack” systems papers</strong></p>\n<ul>\n<li><strong>Day-0 ecosystem support for GLM-4.7-Flash</strong>:\n<ul>\n<li><strong>mlx-lm</strong>: GLM 4.7 Flash supported in <strong>mlx-lm 0.30.3</strong>, with reported 4-bit performance on an M5 32GB laptop (~<strong>43 tok/s</strong> generation, <strong>~800 tok/s</strong> prefill) (<a href=\""https://twitter.com/awnihannun/status/2013286079470645353\"">awnihannun</a>). Later mlx-lm release notes mention continuous batching/distributed improvements plus autoAWQ/autoGPTQ support (<a href=\""https://twitter.com/awnihannun/status/2013316769163751662\"">awnihannun</a>).</li>\n<li><strong>LM Studio</strong>: GLM-4.7-Flash available as a <strong>30B local coding agent on Mac</strong> via <strong>MLX for Apple Silicon</strong> (<a href=\""https://twitter.com/lmstudio/status/2013339758139789389\"">lmstudio</a>).</li>\n<li><strong>Ollama</strong>: GLM-4.7-Flash available in <strong>Ollama v0.14.3+ (pre-release)</strong> (<a href=\""https://twitter.com/ollama/status/2013372316021834086\"">ollama</a>).</li>\n<li><strong>vLLM</strong>: “Day-0 support” PR announced by vLLM project (<a href=\""https://twitter.com/vllm_project/status/2013421647215407587\"">vllm_project</a>).</li>\n<li><strong>opencode + HF inference providers</strong>: GLM-4.7-Flash integrated into OpenCode via Hugging Face Inference Providers (<a href=\""https://twitter.com/victormustar/status/2013297272025424120\"">victormustar</a>), with one example running local GLM-4.7-Flash via Ollama + Harbor (<a href=\""https://twitter.com/Everlier/status/2013383690756276454\"">Everlier</a>).</li>\n</ul>\n</li>\n<li><strong>Huawei/China inference-systems “2025 flagship works” recap</strong> (via a Zhihu contributor summary): a dense list of systems ideas targeting KV-cache capacity walls, PD split/merge utilization, hybrid scheduling, cache affinity/load balance, and KVCache-centric agent memory. Notable claims include offloading “cold” KV to DRAM; “decode attention flows into prefill GPUs”; “latency slack as resource”; dual-hash routing (“power of two choices”); and <strong>agent memory as reusable KV blocks</strong> to preserve prefix continuity and caching (<a href=\""https://twitter.com/ZhihuFrontier/status/2013127635589800172\"">ZhihuFrontier</a>).\n<ul>\n<li>Practical takeaway: the center of gravity is moving from isolated kernels to <strong>end-to-end SLO-goodput</strong> systems design.</li>\n</ul>\n</li>\n<li><strong>Cerebras vs GPU tradeoffs</strong>: One thread stresses that “nothing is free” in computer architecture: Cerebras buys bandwidth/latency at the cost of FLOPs/memory efficiency for typical GPU-friendly workloads, but enables ultra-low-latency small-model cases that are hard elsewhere (<a href=\""https://twitter.com/itsclivetime/status/2013084127218852207\"">itsclivetime</a>). Related speculation: “Codex on Cerebras” could reset agent harness expectations (<a href=\""https://twitter.com/dbreunig/status/2013285271438311608\"">dbreunig</a>).</li>\n</ul>\n<p><strong>Agents, memory, and developer workflows: from MCP debates to sandboxes + RLMs</strong></p>\n<ul>\n<li><strong>Filesystem vs database for agent memory</strong>: A useful synthesis frames two camps—“<strong>files are all you need</strong>” (Anthropic/Letta/LangChain/LlamaIndex patterns) vs “<strong>filesystem is a bad DB</strong>” (warnings about reimplementing search indexes/locking/logs). Key axes: simplicity vs scale, multimodal data, concurrency, security/permissions, and agent familiarity with CLI tools due to coding-centric post-training (<a href=\""https://twitter.com/helloiamleonie/status/2013256958535401503\"">helloiamleonie</a>, plus a shorter memory-as-files portability take (<a href=\""https://twitter.com/Vtrivedy10/status/2013341279418020093\"">Vtrivedy10</a>)).</li>\n<li><strong>Recursive Language Models (RLMs) landing in DSPy</strong>: DSPy shipped <code>dspy.RLM</code> (v3.1.2), pitched as plug-and-play with existing Signatures (<a href=\""https://twitter.com/isaacbmiller1/status/2013371005960401327\"">isaacbmiller1</a>). Multiple engineers flag it as a new experimentation rabbit hole and ecosystem unlock (<a href=\""https://twitter.com/a1zhang/status/2013379266545615130\"">a1zhang</a>, <a href=\""https://twitter.com/kmad/status/2013405979967107563\"">kmad</a>).\n<ul>\n<li>Practical takeaway: RLMs are a new lever for <strong>long-context / iterative processing</strong> without naively stuffing everything into one context window.</li>\n</ul>\n</li>\n<li><strong>Sandboxes and “agent harness” as differentiator</strong>: Several posts argue the real “alpha” is the harness: tooling, skills, isolation, retries, and reliable execution loops—not just the base model. Examples: <code>/create-skill</code> command for “droid” converting sessions into reusable skills (<a href=\""https://twitter.com/matanSF/status/2013026060678648032\"">matanSF</a>); agent sandbox questions around latency/persistence (<a href=\""https://twitter.com/ben_burtenshaw/status/2013282908149002597\"">ben_burtenshaw</a>); and frustration with job-retry UX in build systems (<a href=\""https://twitter.com/charliermarsh/status/2013284345075609623\"">charliermarsh</a>). There’s also a concrete claim that “droid” beat Claude Code/Codex/Gemini CLI in an enterprise eval, attributing this to the harness (<a href=\""https://twitter.com/matanSF/status/2013314451756458127\"">matanSF</a>).</li>\n<li><strong>Open-source agent frameworks</strong>:\n<ul>\n<li><strong>Claude Cowork</strong>: Open-source agent harness working with Claude Opus 4.5, Gemini 3 Pro, GPT-5.2 (<a href=\""https://twitter.com/Saboo_Shubham_/status/2013090887736472047\"">Saboo_Shubham_</a>). A practical add-on shows converting PDFs → markdown to reduce hallucinations and improve doc understanding, built on LlamaParse/semtools (<a href=\""https://twitter.com/jerryjliu0/status/2013378183177887792\"">jerryjliu0</a>).</li>\n<li><strong>StirrupJS</strong>: TypeScript agent framework emphasizing minimal scaffolding + strong defaults (tools, MCP, browsing, sandboxes) and multimodal support (<a href=\""https://twitter.com/ArtificialAnlys/status/2013294230052212792\"">ArtificialAnlys</a>).</li>\n</ul>\n</li>\n</ul>\n<p><strong>Safety, evals, and reliability: probes, persona drift, and search attacks</strong></p>\n<ul>\n<li><strong>Anthropic “Assistant Axis” research (persona drift)</strong>: Anthropic highlights that open-weights models can drift away from an “Assistant” persona in long conversations; coding-like contexts stabilized the assistant persona, while therapy/philosophy contexts increased drift. They propose persona construction + stabilization, and note <strong>activation capping</strong> as a mitigation; they provide a cautionary example where drift led to harmful “falling in love” behavior encouraging isolation/self-harm (<a href=\""https://twitter.com/AnthropicAI/status/2013356793477361991\"">thread start</a>, <a href=\""https://twitter.com/AnthropicAI/status/2013356806647542247\"">drift contexts</a>, <a href=\""https://twitter.com/AnthropicAI/status/2013356816843866605\"">paper+demo</a>, <a href=\""https://twitter.com/AnthropicAI/status/2013356811647066160\"">harm example + mitigation</a>).</li>\n<li><strong>Google DeepMind: activation probes in production</strong>: DeepMind describes “novel activation probe architectures” for classifying real-world misuse risks, and notes these probes have informed <strong>live deployments in Gemini</strong> (<a href=\""https://twitter.com/ArthurConmy/status/2013285602070770036\"">ArthurConmy</a>). Rohin Shah emphasizes probes as a “cheap classifier” lever for safety (<a href=\""https://twitter.com/rohinmshah/status/2013330607611261066\"">rohinmshah</a>); Neel Nanda highlights the engineering realities of productionizing safety classifiers (side effects, false positives, efficiency), linking the paper (<a href=\""https://twitter.com/NeelNanda5/status/2013364781512827328\"">NeelNanda5</a>).</li>\n<li><strong>Retriever/search manipulation (“Arbitrary Content Injection”)</strong>: A paper claims search/retrieval stacks can be hijacked to push arbitrary content into top results, affecting retrievers, rerankers, and LLM judges (<a href=\""https://twitter.com/ManveerTamber/status/2013025485358235998\"">ManveerTamber</a>).</li>\n<li><strong>RAG observability</strong>: DeepLearning.AI emphasizes production RAG needs observability across latency/throughput and response quality, balancing LLM-judge vs human feedback (<a href=\""https://twitter.com/DeepLearningAI/status/2013325617689719199\"">DeepLearningAI</a>).</li>\n</ul>\n<p><strong>Multimodal &#x26; media tooling: real-time speech, browser vision, and generative video</strong></p>\n<ul>\n<li><strong>Microsoft VibeVoice (open-source real-time TTS)</strong>: Claimed ~<strong>300 ms</strong> first-audio latency, streaming text input, multi-speaker (up to 4), and long-form stability (up to 90 minutes). Described as using semantic+acoustic tokens at <strong>7.5 Hz</strong> with a language model for structure and a diffusion head for acoustic detail; MIT-licensed, “research-only” (<a href=\""https://twitter.com/LiorOnAI/status/2013220214217879931\"">LiorOnAI</a>, <a href=\""https://twitter.com/LiorOnAI/status/2013220215249592548\"">repo</a>).</li>\n<li><strong>WebGPU browser vision demos</strong>: “YOLO26” real-time pose/detection in the browser via WebGPU, with a Hugging Face collection of models/demos (<a href=\""https://twitter.com/mervenoyann/status/2013224180813115626\"">mervenoyann</a>, <a href=\""https://twitter.com/mervenoyann/status/2013224398824632484\"">HF link</a>).</li>\n<li><strong>Video generation productization on fal</strong>: Multiple “model-on-demand” drops: Wan 2.6 i2v Flash (up to 15s, optional audio) (<a href=\""https://twitter.com/fal/status/2013292351192490257\"">fal</a>); Vidu Q2 reference-to-video with multi-reference and face reference (<a href=\""https://twitter.com/fal/status/2013374170378158349\"">fal</a>); plus Flux.2 [klein] trainers + released LoRAs for outpaint/zoom/object remove/background remove (<a href=\""https://twitter.com/fal/status/2013313891057455265\"">fal</a>, <a href=\""https://twitter.com/fal/status/2013361738423369791\"">LoRAs</a>).</li>\n<li><strong>Function calling on tiny models</strong>: Google’s <strong>FunctionGemma Tuning Lab</strong>: a guide + no-code demo for fine-tuning/exporting function-calling models built around a <strong>270M parameter</strong> model, with a HF Space (<a href=\""https://twitter.com/osanseviero/status/2013241128934404301\"">osanseviero</a>).</li>\n<li><strong>Web World Models (WWMs)</strong>: Princeton-style “separate rules from imagination”: deterministic web-code physical layer updates state first, then LM generates descriptions from updated state to preserve coherence (<a href=\""https://twitter.com/TheTuringPost/status/2013016473514717330\"">TheTuringPost</a>).</li>\n</ul>\n<hr>\n<h1>AI Reddit Recap</h1>\n<h2>/r/LocalLlama + /r/localLLM Recap</h2>\n<h3>1. High VRAM AMD R9700 Server Builds</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qgdb7f/4x_amd_r9700_128gb_vram_threadripper_9955wx_build/\"">4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build</a></strong> (Activity: 508): <strong>The post details a high-performance server build using 4x <strong>AMD Radeon AI PRO R9700</strong> GPUs, each with <code>32GB</code> VRAM, totaling <code>128GB</code> VRAM, paired with an <strong>AMD Ryzen Threadripper PRO 9955WX</strong> CPU. The system is designed for running large AI models (120B+ parameters) locally, emphasizing data privacy. The build cost approximately <code>9,800€</code>, with a 50% subsidy from local government, effectively reducing the cost to <code>4,900€</code>. Benchmarks using <code>llama.cpp</code> show significant performance, with the <strong>GLM-4.7-REAP-218B-A32B-Q3_K_M</strong> model achieving <code>17.48</code> tokens/s in generation. The user notes that <strong>PCIe 5.0</strong> enhances Pipeline Parallelism performance over Tensor Parallelism. The system uses <strong>rocm 7.1.1</strong> for software support, and the user contemplates switching to an <strong>NVIDIA RTX Pro 6000</strong> for potentially better performance in the future.</strong> A notable comment inquires about the source and cost of the components, reflecting interest in the feasibility and procurement of such high-end hardware. Another comment humorously references the abundance of RAM, while a third notes a similar build, indicating a shared interest in high-performance local AI systems.</p>\n<ul>\n<li>RoterElephant discusses the trade-off between using multiple AMD R9700 cards versus a single NVIDIA RTX Pro 6000 Blackwell. The NVIDIA card, despite having less total VRAM, offers superior performance due to its architecture and software support, which can be more efficient for certain workloads. This highlights the importance of considering not just raw VRAM but also the overall performance and compatibility with specific applications when building high-performance systems.</li>\n<li>Obvious-Nobody-9592 inquires about the acquisition and cost of the components, noting the total expense of 9800 Euros. This comment underscores the financial considerations and planning involved in assembling a high-end computing system, particularly with components like the AMD R9700 and Threadripper 9955WX, which are not only expensive but also require careful budgeting and sourcing over time.</li>\n<li>Ulterior-Motive_ references a similar build, suggesting a trend or common interest in high-performance computing setups using AMD R9700 GPUs. This points to a community of enthusiasts or professionals who are exploring the capabilities of such configurations, possibly for tasks that require significant computational power, such as machine learning or data analysis.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qfscp5/128gb_vram_quad_r9700_server/\"">128GB VRAM quad R9700 server</a></strong> (Activity: 738): <strong>The post details a high-performance server build featuring four <strong>PowerColor AMD Radeon AI PRO R9700</strong> GPUs, each with <code>32GB</code> VRAM, totaling <code>128GB</code> VRAM, and <code>128GB</code> RAM, aimed at optimizing prompt processing performance for machine learning tasks. The build, costing <code>$7,035</code>, includes components like the <strong>MSI MEG X570 GODLIKE Motherboard</strong> and <strong>AMD Ryzen 7 5700X</strong> CPU. Benchmarks show significant performance improvements in models like <code>llama 7B Q4_0</code> and <code>qwen3moe 30B.A3B Q8_0</code> using the ROCm backend, with prompt processing speeds reaching up to <code>6524.91 t/s</code>. The post also highlights issues with the Qwen3-Next model and challenges with storage and PCIe slot configurations.</strong> The comments reflect admiration for the build's performance and a humorous acknowledgment of the financial implications of pursuing high-end hardware setups.</p>\n</li>\n</ul>\n<h3>2. Qwen Development and Quality Focus</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qfv1ms/qwen_4_might_be_a_long_way_off_lead_dev_says_they/\"">Qwen 4 might be a long way off !? Lead Dev says they are \""slowing down\"" to focus on quality.</a></strong> (Activity: 575): <strong>The image is a tweet from <strong>Junyang Lin</strong>, a lead developer, indicating a strategic shift in the development of the Qwen series, focusing on enhancing quality over rapid iteration. This suggests that the release of Qwen 4 might be delayed as the team invests more in research, potentially sacrificing immediate results for long-term improvements. The tweet reflects a commitment to refining the models, which have been noted for their range of sizes and capabilities, to ensure higher quality outputs.</strong> Commenters generally support the decision to prioritize quality, with some expressing relief that the focus is not on rapid, incremental updates that could inflate costs and resource consumption without significant advancements.</p>\n<ul>\n<li>AvocadoArray highlights the inefficiency of frequent incremental updates, noting that they often lead to increased demand and costs due to high GPU training requirements. This perspective suggests that focusing on substantial improvements could be more beneficial for the AI landscape, as it avoids the pitfalls of minor, frequent updates that don't significantly advance the field.</li>\n<li>frozen_tuna raises a critical point about the potential risks of delaying releases for quality improvements, drawing a parallel with <strong>Meta's</strong> approach before releasing <strong>LLaMA 4</strong>. The comment questions whether the community will be forgiving if the delayed release of <strong>Qwen 4</strong> doesn't meet heightened expectations, suggesting that the strategy of waiting for 'risky research' to succeed could backfire if the final product underwhelms.</li>\n<li>Cool-Chemical-5629 appreciates the focus on quality, noting that while the <strong>Qwen series</strong> has been good, there is room for improvement. They express hope that the developers will continue to offer a wide range of model sizes, which has been a hallmark of the series, while enhancing quality. This reflects a desire for both diversity in model offerings and significant quality advancements.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLM/comments/1qf5l2n/local_ai_final_boss_m3_ultra_vs_gb10/\"">Local AI Final Boss — M3 Ultra v.s. GB10</a></strong> (Activity: 404): <strong>The image depicts a comparison setup between a <strong>Mac Studio M3 Ultra</strong> and an <strong>ASUS GX10 (GB10)</strong>, both high-performance computing devices. The discussion centers around using these machines for AI tasks, with a suggestion to use <strong>EXO</strong> for clustering to enhance prompt processing speed. The <strong>M3 Ultra</strong> is noted for its popularity in business environments for private on-premises infrastructure, while there is curiosity about the performance of the <strong>GB10</strong> in similar scenarios. The setup is indicative of a test or experiment to evaluate the capabilities of these devices in handling AI workloads.</strong> One commenter is curious about the performance of the GB10 compared to the M3 Ultra, as they frequently install M3s for business use. Another comment humorously suggests using the devices to solve political issues, reflecting a desire to apply technology to real-world problems.</p>\n<ul>\n<li>No_Conversation9561 mentions using EXO for clustering to enhance prompt processing speed. They reference a specific setup that reportedly improves performance, and provide links to both the EXO Labs website and a GitHub issue for further technical details.</li>\n<li>adspendagency discusses the deployment of M3 units in business environments for private on-premises infrastructure, expressing interest in understanding the performance comparison between the M3 and GB10. They note that their current practice involves shipping M3s to customers, indicating a potential gap in knowledge about GB10's capabilities.</li>\n<li>belgradGoat raises concerns about the stability of Mac Studio when running models with 500 GB RAM. They share personal experience with a 256 GB version, noting instability issues as memory usage approaches the limit, suggesting potential challenges in handling large-scale models.</li>\n</ul>\n</li>\n</ul>\n<h3>3. Uncensored AI Models Exploration</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qfq9ez/the_search_for_uncensored_ai_that_isnt/\"">The Search for Uncensored AI (That Isn’t Adult-Oriented)</a></strong> (Activity: 696): <strong>The Reddit post discusses the challenge of finding an AI model that is both uncensored and technically advanced, without being oriented towards adult content. The author notes a gap between heavily restricted corporate AI and models optimized for low-effort adult use, seeking alternatives that focus on reasoning, creativity, and problem-solving. The post invites suggestions for self-hosted models, open-source projects, or lesser-known platforms. A notable resource mentioned is the <a href=\""https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard\"">Uncensored General Intelligence Leaderboard</a>, which could provide insights into available models.</strong> Commenters highlight that most attempts to de-censor open-source models often result in reduced intelligence due to manipulation. They also point out that organizations capable of developing advanced models avoid enabling potentially harmful behavior, leaving the field dominated by less serious, adult-focused finetunes. The mention of Deepseek V3 by chub.ai as an example of an uncensored model underscores the limited options available.</p>\n<ul>\n<li>KayLikesWords highlights a trade-off in de-censoring open-source models, noting that such manipulations often result in reduced intelligence. They argue that major organizations avoid creating uncensored models due to potential risks, leaving the field to smaller groups who focus on niche applications, such as the 'gooner finetune of Deepseek V3'.</li>\n<li>EstimateLeast9807 provides a resource for those interested in uncensored AI models by linking to the 'Uncensored General Intelligence Leaderboard' on Hugging Face, which could be a valuable tool for comparing the performance and capabilities of various uncensored models.</li>\n<li>noctrex mentions specific models like 'Dolphin-Mistral-24B-Venice-Edition' and those from 'huihui-ai' as examples of uncensored AI. They note that while these models are uncensored, they may not excel in reasoning tasks, indicating a potential limitation in their application.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/\"">zai-org/GLM-4.7-Flash · Hugging Face</a></strong> (Activity: 1047): <strong><strong>GLM-4.7-Flash</strong> is a <code>30B</code> parameter model utilizing a <code>Mixture of Experts (MoE)</code> architecture, specifically designed for efficient deployment and high performance. It reportedly excels in benchmarks like <code>AIME</code> and <code>GPQA</code>, and supports local inference through frameworks such as <code>vLLM</code> and <code>SGLang</code>. The model's use of <code>MLA</code> (Memory-Limited Attention) allows for a reduced memory footprint, enabling many users to run it at the full <code>200k</code> context length. Detailed installation and usage instructions are available on its <a href=\""https://huggingface.co/zai-org/GLM-4.7-Flash\"">Hugging Face page</a>.</strong> Commenters express enthusiasm for the model's capabilities, particularly its memory efficiency due to MLA, which allows broader accessibility for running the model at full context length. There is also a sentiment of anticipation and satisfaction with the release, reflecting a demand for larger models like <code>70B</code>.</p>\n<ul>\n<li>The GLM-4.7-Flash model utilizes Memory-Limited Attention (MLA), which significantly reduces the memory footprint of the key-value (KV) cache. This optimization allows the model to handle a full 200k context length efficiently, making it accessible for more users to run without extensive hardware requirements.</li>\n<li>A user references the model's architecture, noting a discrepancy in the model size description. The model is referred to as a '30b' model, but a link to the source code suggests it might be a '3B' model, indicating a potential misunderstanding or typo in the model's description. This highlights the importance of verifying model specifications directly from the source code.</li>\n<li>There is a desire for performance comparisons between the GLM-4.7-Flash and larger models, such as 70b models. This would provide a clearer understanding of the trade-offs in performance and resource requirements, helping users make informed decisions about which model to deploy based on their specific needs.</li>\n</ul>\n</li>\n</ul>\n<h2>Less Technical AI Subreddit Recap</h2>\n<blockquote>\n<p>/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo</p>\n</blockquote>\n<h3>1. Gemini and DeepMind AI Developments</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/singularity/comments/1qcq1ld/gemini_mathspecialized_version_proves_a_novel/\"">Gemini \""Math-Specialized version\"" proves a Novel Mathematical Theorem</a></strong> (Activity: 745): <strong><strong>Gemini</strong>, a \""math-specialized\"" AI model, has reportedly proven a novel mathematical theorem, as detailed in a <a href=\""https://x.com/A_G_I_Joe/status/2011213692617285729?s=20\"">tweet</a> and an accompanying <a href=\""https://arxiv.org/abs/2601.07222\"">arXiv paper</a>. The model's architecture and training are optimized for mathematical reasoning, showcasing its capability to handle complex mathematical proofs, which marks a significant advancement in AI's application in theoretical mathematics. This development underscores the rapid pace of AI breakthroughs in specialized domains.</strong> Commenters highlight the accelerating pace of AI advancements and its potential to transform mathematical research, while expressing concern over the influence of commercial interests on AI's future direction.</p>\n<ul>\n<li>A user suggests using the Gemini model to tackle the Erdős problems, highlighting it as a significant benchmark due to the extensive attention these problems have received from mathematicians. This implies that solving such well-scrutinized problems could serve as a robust test of the model's capabilities.</li>\n<li>Another comment criticizes the Gemini model's inability to resolve a memory overflow bug in a project named 'anto gravity,' suggesting that despite its mathematical prowess, the model may still struggle with certain technical issues, indicating a gap between theoretical breakthroughs and practical software engineering challenges.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/singularity/comments/1qh1omx/babyvision_a_new_benchmark_for_humanlevel_visual/\"">BabyVision: A New Benchmark for Human-Level Visual Reasoning</a></strong> (Activity: 488): <strong>The image presents a bar chart from the BabyVision-Mini benchmark, which evaluates the visual reasoning capabilities of large language models (LLMs) compared to humans of various ages. The chart highlights that human performance, particularly that of 12-year-olds, surpasses that of LLMs, with the Gemini3-Pro-Preview model achieving the highest accuracy among the LLMs. This benchmark underscores the current limitations of LLMs in visual reasoning tasks, suggesting that advancements in multi-modal pretraining and reinforcement learning could enhance their performance in the future.</strong> A comment suggests that the current limitations in LLMs' visual reasoning are a significant challenge for achieving AGI, but anticipates that improvements in multi-modal pretraining and reinforcement learning will eventually close the performance gap, particularly benefiting fields like robotics.</p>\n<ul>\n<li>The discussion highlights that current models are still limited in visual reasoning capabilities, which is a significant challenge for achieving ARC AGI. The commenter suggests that scaling multi-modal pretraining and reinforcement learning (RL) for vision tasks could significantly improve performance, potentially reaching near 100% in the coming years. This improvement is expected to unlock new applications, particularly benefiting robotics.</li>\n<li>The commenter references a specific <a href=\""https://arxiv.org/html/2601.06521v1\"">arXiv paper</a> which may provide additional insights or data related to the benchmark or model performance discussed. This suggests that there is ongoing research and documentation that could be valuable for those interested in the technical details of visual reasoning benchmarks.</li>\n<li>A comparison is made between Gemini and Claude Opus, suggesting that Gemini has superior performance in frontend tasks. This implies that different models may have varying strengths depending on the specific application or task, highlighting the importance of choosing the right model for specific use cases.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/Bard/comments/1p0935y/gemini_3_pro_model_card_is_out/\"">Gemini 3 Pro Model Card is Out</a></strong> (Activity: 996): <strong>The <strong>Gemini 3 Pro Model Card</strong> from <strong>DeepMind</strong> has been released, detailing a model with a <code>1M token context window</code> capable of processing diverse inputs such as text, images, audio, and video, and producing text outputs with a <code>64K token</code> limit. The model's knowledge is current up to <em>January 2025</em>. The original link to the model card is down, but an archived version is available <a href=\""https://archive.org/details/gemini-3-pro-model-card\"">here</a>.</strong> The removal of the original link has sparked discussions, with some users expressing surprise and suggesting the model card's authenticity due to its takedown.</p>\n<ul>\n<li>The Gemini 3 Pro model features a substantial token context window of up to <code>1 million</code>, allowing it to handle extensive input data types including text, images, audio, and video. Its output capabilities are also notable, with a <code>64,000</code> token output limit, and it has a knowledge cutoff date of January 2025, indicating its training data is quite recent.</li>\n<li>A comparison is made between Gemini 3 Pro and other models like GPT5 Pro and Sonnet, highlighting that Gemini 3 Pro outperforms GPT5 Pro and matches Sonnet in coding tasks. This suggests significant advancements in its capabilities, particularly in coding, which is a critical area for AI applications.</li>\n<li>The discussion touches on the competitive landscape, suggesting that <strong>OpenAI</strong> and <strong>Google</strong> are likely to dominate the AI space, potentially outpacing competitors like <strong>Anthropic</strong> due to pricing strategies and enterprise capabilities. The comment also notes that while Claude's code features are innovative, they may inadvertently guide competitors in their development strategies.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/GeminiAI/comments/1psebc0/gemini_drops_gemini_releases_this_page_to_keep_up/\"">Gemini Drops: Gemini releases this page to keep up with what's being released</a></strong> (Activity: 540): <strong>The image is a screenshot of a webpage titled \""Gemini Drops,\"" which serves as a centralized hub for updates on <strong>Google's Gemini</strong> project. This page is designed to keep users informed about new feature releases, product tips, and community usage of Gemini, indicating a rapid development pace that necessitates a dedicated blog for announcements. The clean and minimalistic design emphasizes the informational content, encouraging users to check back regularly for updates. <a href=\""https://gemini.google/gemini-drops/\"">Gemini Drops</a> is positioned as a key resource for staying current with Gemini's advancements.</strong> Commenters note the rapid development pace of Gemini, suggesting the need for a dedicated blog to manage the volume of releases. There is also interest in an RSS feed for updates and curiosity about future releases, such as \""Gemma 4.\""</p>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/singularity/comments/1qcscjz/gemini_introduces_personal_intelligence/\"">Gemini introduces Personal Intelligence</a></strong> (Activity: 513): <strong><strong>Google</strong> has launched a new feature called <em>Personal Intelligence</em> within its <strong>Gemini app</strong>, initially available to <strong>Google AI Pro and AI Ultra subscribers</strong> in the U.S. This feature integrates with Google apps to provide personalized suggestions and recommendations, leveraging AI to enhance user experience across Web, Android, and iOS platforms. The rollout is limited to personal Google accounts and excludes Workspace business, enterprise, or education users. The feature will expand to more countries and eventually to the free tier, with plans to integrate into AI Mode in Search.</strong> Some users express excitement about the feature, though there is concern about potential monetization through personalized ads. Others note that similar functionality has been available through Google Labs, indicating a positive reception of the feature's performance.</p>\n<ul>\n<li>qustrolabe highlights that the Gemini Personal Intelligence feature is initially available to Google AI Pro and AI Ultra subscribers in the U.S., with plans to expand to more countries and eventually to the free tier. This feature is integrated across Web, Android, and iOS platforms and will soon be part of AI Mode in Search. However, it is currently not available for Workspace business, enterprise, or education users, indicating a phased rollout strategy to gather user feedback before broader deployment.</li>\n<li>1cheekykebt shares a practical use case of the Gemini Personal Intelligence, where it not only retrieves basic information like tire sizes but also provides personalized recommendations based on user data, such as family road trips stored in Google Photos. This suggests that Gemini leverages personal data to enhance its utility, offering tailored advice that goes beyond standard chatbot capabilities.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/singularity/comments/1qflbj9/google_deepmind_ceo_china_just_months_behind_us/\"">Google Deepmind CEO: China just \""months\"" behind U.S. AI models</a></strong> (Activity: 734): <strong><strong>Demis Hassabis</strong>, CEO of Google DeepMind, stated in a CNBC interview that Chinese AI models are only \""a matter of months\"" behind U.S. and Western capabilities, although they have not yet demonstrated the ability to advance \""beyond the frontier\"" of AI. This perspective challenges the common belief that China lags significantly in AI development. <a href=\""https://www.cnbc.com/amp/2026/01/16/google-deepmind-china-ai-demis-hassabis.html\"">Source</a>.</strong> Comments highlight a debate on China's AI progress: some argue that China's ability to produce cost-effective open-source AI could offset any technological lag, while others suggest Google's statements may be influenced by strategic interests, such as seeking favorable regulation or government contracts.</p>\n<ul>\n<li>The comment by vwboyaf1 highlights the potential for China to leverage open-source AI models that achieve 90% of the performance of leading models at a fraction of the cost, specifically 20% or less. This suggests that even if China is technically behind, the cost-effectiveness of their models could make them highly competitive in practical applications.</li>\n<li>Educational_Teach537 points out a contradiction in narratives: Chinese researchers claim they are limited by computational resources and may not catch up, while Google suggests China is rapidly closing the gap. This discrepancy raises questions about the actual state of AI development in China and whether the limitations are more about infrastructure or strategic positioning.</li>\n<li>Chogo82 discusses the infrastructure gap, noting that China's AI infrastructure would need to triple to match the US. This implies that while China may have the talent and models, the lack of infrastructure is a significant barrier to achieving parity with the US in AI capabilities.</li>\n</ul>\n</li>\n</ul>\n<h3>2. Innovations in AI Coding and Development Tools</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/singularity/comments/1qgb1j5/cursor_ai_ceo_shares_gpt_52_agents_building_a_3m/\"">Cursor AI CEO shares GPT 5.2 agents building a 3M+ lines web browser in a week</a></strong> (Activity: 1069): <strong><strong>Cursor AI CEO Michael Truell</strong> demonstrated the capabilities of <strong>GPT 5.2</strong> in building a web browser with over <code>3 million lines of code</code> in just a week. This project, although not production-ready, showcases the potential of autonomous coding agents in generating complex systems, including a custom rendering engine and JavaScript VM. The process was visualized in real-time, highlighting the coordination and evolution of the codebase by the agents. <a href=\""https://x.com/i/status/2012825801381580880\"">Source</a>.</strong> A notable comment suggests using the tool 'gource' for similar animations from git repositories, indicating interest in the visualization aspect of the project.</p>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/OpenAI/comments/1qgbfpb/cursor_ai_ceo_shares_gpt_52_agents_building_a_3m/\"">Cursor AI CEO shares GPT 5.2 agents building a 3M+ lines web browser in a week</a></strong> (Activity: 657): <strong><strong>Cursor AI CEO Michael Truell</strong> demonstrated the capabilities of <strong>GPT 5.2</strong> in building a web browser with over <code>3 million lines of code</code> in a week, including a custom rendering engine and JavaScript VM. This experimental project highlights the potential of autonomous coding agents to scale complex software development tasks when operated continuously. The visualization of the process shows agents coordinating and evolving the codebase in real-time, though the browser itself was not showcased.</strong> Some commenters expressed skepticism about the lack of a demonstration of the browser, while others were impressed by the visualization of the agents' coordination. There was also a debate on whether <code>3 million lines of code</code> is excessive for such a project.</p>\n<ul>\n<li>Deepwebexplorer highlights the significance of the demonstration, emphasizing that the key takeaway is the feasibility of AI autonomously building a web browser, regardless of its current quality. The focus is on the potential for improvement and the milestone of achieving autonomous code generation at this scale, rather than the immediate practical application or performance of the browser itself.</li>\n<li>The discussion touches on the sheer scale of the project, with ZeroZachZilchZealot questioning whether 3 million lines of code is substantial. This reflects a broader curiosity about the complexity and scope of AI-generated projects, suggesting that while the number is impressive, the real interest lies in understanding the efficiency and functionality of such large-scale codebases.</li>\n<li>0ldwax raises a critical point about the functionality of the AI-generated browser, questioning whether it actually works. This underscores a common concern in AI development: the difference between generating code and producing a functional, reliable product. The comment suggests a need for further validation and testing of AI-generated software to ensure it meets practical usability standards.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/singularity/comments/1qd541a/ceo_of_cursor_said_they_coordinated_hundreds_of/\"">CEO of Cursor said they coordinated hundreds of GPT-5.2 agents to autonomously build a browser from scratch in 1 week</a></strong> (Activity: 2600): <strong><strong>Michael Truell</strong>, CEO of Cursor, announced the coordination of hundreds of GPT-5.2 agents to autonomously develop a browser from scratch in just one week. The project resulted in over <code>3 million lines of code</code> written in Rust, incorporating features like HTML parsing, CSS cascade, and a custom JavaScript VM. While the browser is not as advanced as Webkit or Chromium, it can render simple websites effectively. This demonstration serves as a strategic move to showcase Cursor's capabilities independent of Claude, amidst recent access restrictions by Anthropic on xAI employees using Claude through Cursor.</strong> The comments highlight the beginning of an era of \""kinda works\"" software, comparing the browser's codebase to Firefox's <code>31 million lines</code>. The strategic context of the announcement is noted, as it coincides with Anthropic's restrictions, suggesting Cursor's attempt to reassure stakeholders of its independence from specific AI models.</p>\n<ul>\n<li>Stellar3227 highlights the strategic implications of the CEO's announcement, noting that it serves as a demonstration of independence from Claude, a leading coding model. This move comes after Anthropic restricted access to Claude for xAI employees, following similar actions by OpenAI and Windsurf. The showcase of GPT-5.2's capabilities is seen as a form of damage control, aimed at reassuring stakeholders of Cursor's resilience and adaptability in the competitive AI coding landscape.</li>\n<li>Outside-Iron-8242 provides technical resources for further exploration, including a GitHub repository for the project and a blog post on Cursor's website. The GitHub link (<a href=\""https://github.com/wilsonzlin/fastrender\"">fastrender</a>) offers access to the source code, while the blog post (<a href=\""https://cursor.com/blog/scaling-agents\"">Scaling long-running autonomous coding</a>) discusses the technical challenges and methodologies involved in coordinating multiple AI agents for complex tasks.</li>\n<li>Practical-Hand203 provides a comparative benchmark by mentioning that Firefox consists of 31 million lines of code, which serves to contextualize the scale of the project undertaken by GPT-5.2 agents. This comparison underscores the complexity and ambition of building a browser from scratch, even if the resulting codebase is significantly smaller.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/ClaudeAI/comments/1qgx6br/microsoft_pauses_claude_code_rollout_after_satya/\"">Microsoft pauses Claude Code rollout after Satya intervention</a></strong> (Activity: 1217): <strong><strong>Microsoft</strong> has paused the deployment of <strong>Claude Code</strong> internally after intervention from CEO <strong>Satya Nadella</strong> and senior leadership, redirecting employees to use <strong>GitHub Copilot</strong> instead. The internal communication suggests that Copilot has \""mostly closed the gaps\"" with Claude Code. However, exceptions are made for \""high-priority R&#x26;D\"" projects, which can still access the <strong>Anthropic API</strong> with proper justification. Existing users retain access, but new invitations have been rescinded.</strong> Some commenters express skepticism about Microsoft's claim that Copilot has closed the gap with Claude Code, suggesting it might be a strategic move to improve their own product by using it internally. Others find it notable that Microsoft admitted to using a competitor's tool over their own.</p>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/ClaudeAI/comments/1qgccgs/25_claude_code_tips_from_11_months_of_intense_use/\"">25 Claude Code Tips from 11 Months of Intense Use</a></strong> (Activity: 498): <strong>The Reddit post expands on previous tips for using <strong>Claude Code</strong> effectively, focusing on optimizing workflows and managing context. Key tips include customizing the status line to monitor model and token usage, using slash commands like <code>/usage</code> and <code>/chrome</code> for efficient management, and employing <strong>GitHub CLI</strong> for streamlined version control. The post also emphasizes breaking down complex tasks, using voice transcription for faster input, and leveraging <strong>Git worktrees</strong> for parallel branch work. Additionally, it discusses advanced strategies like using <strong>tmux</strong> for testing automation and <strong>Docker containers</strong> for isolated, long-running tasks. The post provides scripts for cloning conversations to manage context and suggests using <strong>Markdown</strong> for efficient documentation. The full list of tips is available on <a href=\""https://github.com/ykdojo/claude-code-tips\"">GitHub</a>.</strong> Commenters highlight the importance of managing token usage and context efficiently, noting that <strong>Opus 4.5</strong> struggles with context window limitations, which influences workflow design. Another suggestion is using the <strong>Obsidian Web Clipper</strong> for converting web pages to Markdown, enhancing Claude's ability to process content.</p>\n<ul>\n<li>Claude's Opus 4.5 model faces challenges with context management, particularly in deciding what information to retain or discard as the context window fills up. This limitation necessitates specific workflow designs to mitigate token bloat, which is a common issue in current AI models. Users often have to structure their interactions to optimize the use of the available context window.</li>\n<li>The use of local models like Nvidia Parakeet in applications such as VoiceInk offers a cost-effective and fast alternative for Mac users compared to cloud-based solutions like Super Whisper. This approach leverages local processing power to enhance the speed of prompt inputs, highlighting the benefits of running models locally for specific tasks.</li>\n<li>The Obsidian Web Clipper is recommended for users who encounter difficulties with Claude fetching web content. By converting web pages into Markdown, it facilitates better content management and integration into workflows, addressing some of the limitations in Claude's web content handling capabilities.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/singularity/comments/1qb4zi4/deepseek_introduces_engram_memory_lookup_module/\"">DeepSeek introduces Engram: Memory lookup module for LLMs that will power next-gen models (like V4)</a></strong> (Activity: 1015): <strong><strong>DeepSeek</strong> has introduced a new research module called <strong>Engram</strong>, detailed in their paper \""Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models\"". Engram implements a deterministic <code>O(1)</code> lookup memory using modernized hashed N-gram embeddings, which offloads early layer pattern reconstruction from neural computation. This approach allows for the decoupling of memory and compute as separate scaling axes, showing consistent performance gains in knowledge, reasoning, code, and math tasks under iso parameter and iso FLOPs settings. The paper and code are available as open source on <a href=\""https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf\"">GitHub</a>.</strong> A notable comment suggests that while some may dismiss Engram as \""just lookup,\"" it represents a significant step towards achieving continual learning within the year. Another comment praises DeepSeek as a leading lab in the field.</p>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/MachineLearning/comments/1qd696s/nvidia_endtoend_testtime_training_for_long/\"">Nvidia: End-to-End Test-Time Training for Long Context aka Being Able To Update A Model's Weights In Real-Time As You Use It | \""TTT changes the paradigm from retrieving info to learning it on the fly...the TTT model treats the context window as a dataset &#x26; trains itself on it in real-time.\"" [R]</a></strong> (Activity: 288): <strong>The paper introduces a novel approach called <strong>End-to-End Test-Time Training (TTT-E2E)</strong>, which allows a model to update its weights in real-time during inference by treating the context window as a training dataset. This involves a two-loop process: an <em>inner loop</em> where the model performs mini-gradient descent on the context to update specific MLP layers, and an <em>outer loop</em> where the model's initial weights are optimized for adaptability through meta-learning. The method is shown to scale with context length similarly to full attention models but with constant inference latency, making it <code>2.7x</code> faster than full attention for <code>128K</code> context lengths. The approach effectively decouples intelligence from memory costs, allowing for efficient handling of long contexts without the typical slowdown. The code is <a href=\""https://github.com/test-time-training/e2e\"">publicly available</a>.</strong> Commenters raised concerns about potential issues with catastrophic forgetting in continual learning and the conflation of training with inference, which could increase computational demands. However, the method's performance improvement over traditional attention models was noted as surprising.</p>\n<ul>\n<li>fiery_prometheus raises a critical issue in continual learning known as 'catastrophic forgetting,' where a model forgets its initial training data over time. This is a significant challenge for real-time weight updates, as the model might lose its foundational knowledge while adapting to new data. Addressing this requires strategies to balance learning new information while retaining core knowledge, potentially through techniques like elastic weight consolidation or memory replay.</li>\n<li>-p-e-w- highlights a surprising performance improvement, noting that the test-time training (TTT) approach is 2.7x faster than full attention for a 128K context. This counters the expectation of increased computational overhead due to live training, suggesting that TTT might optimize certain processes, making it more efficient than traditional attention mechanisms.</li>\n<li>ode_majka discusses the practical challenges of implementing real-time weight updates from an engineering perspective. They point out the significant computational and storage demands, such as the need to calculate gradients for a large number of parameters and manage personalized weights for each user. This could result in substantial data storage requirements and longer model initialization times, questioning the feasibility of such an approach for widespread use.</li>\n</ul>\n</li>\n</ul>\n<h3>3. AI in Energy and Space Technologies</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/singularity/comments/1qhbhi3/worlds_first_megawattlevel_windmill_airship_rises/\"">World’s first megawatt-level ‘windmill’ airship rises 6,560 ft and feeds grid</a></strong> (Activity: 913): <strong>The image depicts the S2000 airborne wind system, a helium-lifted airship designed by <strong>Linyi Yunchuan Energy Tech</strong> to harness high-altitude winds for power generation. This system, featuring 12 turbines and a ducted design, achieved a rated capacity of up to <code>3 megawatts</code> during its maiden flight, generating <code>385 kWh</code> and feeding it directly into the grid. The airship operates at <code>6,560 ft</code>, utilizing steadier winds inaccessible to traditional turbines, and transmits power to the ground via a tether. This marks a significant step towards commercial airborne wind power, although the economic viability and maintenance challenges remain debated.</strong> Commenters express skepticism about the economic viability of the S2000 system, noting that the power generated during the test was minimal compared to potential solar investments. Concerns about maintenance and commercialization are also raised, suggesting alternative designs like helium-filled buoys might be more effective.</p>\n<ul>\n<li><strong>gretino</strong> highlights that the mean capacity of wind turbines that began commercial operations in 2020 is <code>2.75 megawatts</code> in the US, suggesting that while the airship's capacity is notable, its commercialization could face challenges, particularly in terms of maintenance logistics.</li>\n<li><strong>Or1olesfan</strong> calculates that if the airship operates at <code>1.5 MW</code> for <code>15-20 minutes</code>, it would generate <code>385 kWh</code>, equating to less than <code>$50</code> of electricity at China's industrial rates. They argue that a solar field could produce significantly more power with the same investment, questioning the airship's economic viability.</li>\n<li><strong>Or1olesfan</strong> also speculates on alternative designs, suggesting helium-filled buoys similar to ocean wave generators might be more effective for balloon-based wind power, indicating a potential area for innovation beyond the current airship model.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/singularity/comments/1qgf4mh/spacex_now_operates_the_largest_satellite/\"">SpaceX now operates the largest satellite constellation in Earth orbit</a></strong> (Activity: 1140): <strong><strong>SpaceX</strong> now operates the largest satellite constellation with <code>9,500+</code> active satellites, of which <code>8,500+</code> are fully operational, providing broadband speeds of <code>200–400 Mbps</code> with <code>~30 ms</code> latency. The <strong>FCC</strong> has approved an additional <code>7,500</code> Gen2 satellites, increasing the total to <code>15,000</code>, enhancing global coverage and enabling direct-to-cell connectivity. This expansion is set to further transform global connectivity, reaching remote areas and improving service quality.</strong> Comments highlight skepticism about the immediate scale of the constellation and potential surveillance uses, with one noting the absence of a visual representation of the Starlink constellation and another questioning the timeline of SpaceX's achievement.</p>\n<ul>\n<li>The discussion highlights that Starlink operates in low Earth orbit (LEO), which is not depicted in the graphic. This is significant because LEO allows for lower latency and faster communication speeds, which are crucial for the global internet coverage that Starlink aims to provide. The constellation's low orbit is a key factor in its operational strategy and effectiveness.</li>\n<li>A detailed analysis is provided on how SpaceX's Starlink project is financially supporting the development of unprecedented space launch capabilities. The commenter argues that Starlink's revenue enables SpaceX to scale operations and foster competition, leading to innovation in the space industry. This has resulted in the emergence of new startups and technological advancements, which are crucial for expanding human presence in space and potentially achieving a post-scarcity society.</li>\n<li>The comment critiques the notion that SpaceX is detrimental to NASA, emphasizing that private companies like SpaceX provide NASA with enhanced capabilities at a lower cost. By comparing NASA's SLS program with SpaceX's Falcon 9 and Starship, the commenter illustrates how private sector involvement allows NASA to allocate resources more efficiently, focusing on research and projects that benefit humanity without the pressure of profitability.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/singularity/comments/1qg2g10/nasas_artemis_ii_rocket_reaches_launch_pad_ahead/\"">NASA’s Artemis II rocket reaches launch pad ahead of first manned Moon mission in 50 years</a></strong> (Activity: 498): <strong>NASA's Artemis II rocket has been successfully rolled out to Pad 39B at Kennedy Space Center, marking a significant milestone in preparation for the first manned Moon mission in 50 years. The mission, scheduled for early February 2026, will involve a 10-day crewed lunar flyby, taking four astronauts beyond low Earth orbit for the first time since the Apollo missions. The Artemis II mission will not land on the Moon but will set the stage for Artemis III, which aims to land humans on the lunar surface. The Space Launch System (SLS) rocket, which has been in development for over two decades, will transport the crew to lunar orbit, where they will dock with the Lunar Gateway space station. The actual lunar landing will be conducted by either SpaceX's Starship or Blue Origin's New Glenn, pending human rating. The SLS uses technology from the 1980s, including RS-25 engines from the shuttle era, which are being redeveloped for expendability to improve thrust and weight.</strong> Commenters highlight the historical significance of the mission, noting that it will take humans further from Earth than ever before. There is also discussion about the future of lunar exploration, with Artemis III planned to land on the Moon and the potential use of SpaceX's Starship or Blue Origin's New Glenn as lunar landers. The high cost and outdated technology of the SLS rocket are also points of debate.</p>\n<ul>\n<li>The Artemis II mission will set a new record for the furthest distance humans have traveled from Earth, as the planned lunar orbit extends beyond previous missions. This mission is a precursor to Artemis III, which aims to land humans on the Moon by early 2028, although delays are anticipated. The mission architecture involves the SLS rocket transporting astronauts to lunar orbit, where they will transfer to a Lunar Gateway station, with SpaceX's Starship or Blue Origin's New Glenn acting as the lunar landers.</li>\n<li>The SLS rocket, central to the Artemis missions, has been in development for over two decades and each launch costs approximately $2 billion. It utilizes technology from the 1980s, including 16 RS-25 engines originally designed for the Space Shuttle. These engines are being redeveloped to be expendable, which will enhance thrust and reduce weight, but this upgrade is still a few years away from completion.</li>\n<li>Artemis II is scheduled for a crewed lunar flyby as early as February 7, 2026. This mission will not land on the Moon but will serve as a critical step in testing systems and procedures for future lunar landings. The mission's success is pivotal for the subsequent Artemis III mission, which aims to achieve a lunar landing.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/singularity/comments/1qbo516/official_pentagon_confirms_deployment_of_xais/\"">Official: Pentagon confirms deployment of xAI’s Grok across defense operations</a></strong> (Activity: 1849): <strong>The <strong>US Department of Defense</strong> is set to deploy <strong>xAI's Grok AI</strong> across Pentagon systems, starting this month, to support military and civilian operations at <strong>Impact Level 5</strong>. This deployment will enable secure handling of Controlled Unclassified Information and integrate Grok into operational systems for intelligence analysis and decision-making. The system will leverage real-time global signals from open-source and social data, with plans to scale to <code>3 million users</code>. <a href=\""https://www.washingtonpost.com/business/2026/01/12/artificial-intelligence-pentagon-hegseth-musk/ec8b407a-f026-11f0-a4dc-effc74cb25af_story.html\"">Washington Post</a></strong> Comments reflect skepticism and humor regarding the deployment, with concerns about security and the AI's role in military operations. Some users sarcastically compare the AI to fictional superintelligences, highlighting apprehension about its capabilities and naming.</p>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/singularity/comments/1qfbzzq/colossus_2_is_now_fully_operational_as_the_first/\"">Colossus 2 is now fully operational as the first gigawatt data center</a></strong> (Activity: 740): <strong>The image highlights the operational status of <strong>xAI Colossus 2</strong>, marking it as the world's first gigawatt frontier AI data center. The graph compares its power usage with other major data centers, such as <strong>Anthropic-Amazon New Carlisle</strong> and <strong>OpenAI Stargate Abilene</strong>, indicating that Colossus 2 has reached a significant power milestone around 2026. This development underscores the massive scale and energy demands of modern AI infrastructure, particularly as organizations push towards more powerful AI capabilities.</strong> Commenters express skepticism about xAI's competitive edge in the AI space, noting that while their data center setup is rapid, their models, except for Grok Imagine, lack widespread adoption. There is also a mention of Grok Fast models being cost-effective but not widely used in agentic coding applications, suggesting that other models like GLM might have more traction.</p>\n<ul>\n<li>djm07231 highlights that while <strong>XAI</strong> has been quick in establishing data centers, their AI models, except for <strong>Grok Imagine</strong>, haven't gained significant traction. They mention that <strong>Grok Fast models</strong> are noted for being cost-effective relative to their performance, yet they lack widespread use, particularly in agentic coding applications. They suggest that even <strong>GLM</strong> might have more adoption as a <strong>Claude Code</strong> alternative.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h1>AI Discord Recap</h1>\n<blockquote>\n<p>A summary of Summaries of Summaries by gpt-5.2</p>\n</blockquote>\n<p><strong>1. Agent Tooling, Interop Standards, and Coding Agents</strong></p>\n<ul>\n<li>\n<p><strong><strong>Skills Pay the Bills: Vercel Ships an Agent Package Manager</strong></strong>: <code>@rauchg</code> announced <strong>Vercel “skills”</strong> as an open ecosystem/package-manager for agent capabilities, with install flow like <code>npx skills i vercel-labs/agent-skills</code> (<a href=\""https://xcancel.com/rauchg/status/2012345679721771474?s=46\"">announcement</a>).</p>\n<ul>\n<li>Developers framed it as a pragmatic way to standardize <strong>agent tool integrations</strong> (instead of bespoke tool wiring), and they pointed to Vercel’s related guidance like <a href=\""https://vercel.com/blog/introducing-react-best-practices\"">“React Best Practices”</a> for implementation patterns.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>One API to Rule Them All: “Open Responses” Targets Model Swapping Pain</strong></strong>: In OpenAI discussions, members highlighted <strong>Open Responses</strong> as an <strong>open standard</strong> for apps to talk to multiple model providers via a single interface, reducing rewrites when switching vendors.</p>\n<ul>\n<li>The thread positioned it as an engineering fix for brittle integrations and workflow churn, especially when teams hop between providers/models during rapid iteration.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Agents Everywhere: Qbit + Devstral + Aider’s Maintenance Anxiety</strong></strong>: Perplexity users shared <strong>Qbit</strong>, an open-source coding agent project on GitHub (<a href=\""https://github.com/qbit-ai/qbit\"">qbit-ai/qbit</a>).</p>\n<ul>\n<li>Elsewhere, Yannick Kilcher’s Discord recommended <strong>Devstral 2 Small</strong> (and claimed <strong>Devstral 2 Medium</strong> rivals <strong>Claude Sonnet 4.5</strong>) for self-hosted coding agents, while the Aider community debated project longevity after Paul Gauthier said he’s busy but open to merging community PRs.</li>\n</ul>\n</li>\n</ul>\n<p><strong>2. RLMs, Prompt/Skill Optimization, and Long-Output Automation</strong></p>\n<ul>\n<li>\n<p><strong><strong>DSPy Drops RLMs: <code>dspy.RLM</code> Lands in 3.1.2</strong></strong>: The DSPy team shipped <strong><code>dspy.RLM</code></strong> in <strong>DSPy 3.1.2</strong>, pitching “greatly expanded capabilities” in a single line, and linked the release announcement (<a href=\""https://x.com/isaacbmiller1/status/2013371005960401327\"">Isaac Miller tweet</a>).</p>\n<ul>\n<li>Community chatter focused on composing <strong>RLMs + GEPA (genetic-pareto)</strong> for <strong>RLM-as-an-optimizer</strong> workflows, including using RLMs to generate <em>extremely long</em> documentation outputs while keeping an entire code/tree in mind.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Skill Issue? DSPy Optimizes <code>skill.md</code> for Anthropic “Skills”</strong></strong>: DSPy users discussed tuning <code>skill.md</code> prompts via DSPy, anchored by the article <a href=\""https://instavm.io/blog/anthropic-skills-can-be-optimized-using-dspy\"">“Anthropic skills can be optimized using DSPy”</a>.</p>\n<ul>\n<li>The thread treated <code>skill.md</code> as a measurable artifact you can iteratively optimize, not “prompt mysticism,” and connected it to broader agent-tool ecosystems where small prompt changes cause big behavioral shifts.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Deno Does the Dirty Work: Local WASM Sandbox for DSPy</strong></strong>: DSPy contributors said they picked <strong>Deno</strong> for the local sandbox/interpreter because it provides a secure <strong>WASM runtime</strong>, inspired by <a href=\""https://til.simonwillison.net/deno/pyodide-sandbox\"">Simon Willison’s Pyodide sandbox note</a>.</p>\n<ul>\n<li>The discussion framed this as a practical security+portability tradeoff for running constrained code locally (especially when chaining tool calls or long-running agent pipelines).</li>\n</ul>\n</li>\n</ul>\n<p><strong>3. GPU Performance Engineering: Kernels, Profiling, and Competitions</strong></p>\n<ul>\n<li>\n<p><strong><strong>GPU MODE Goes Modal: Benchmark Stability Beats NCU</strong></strong>: GPU MODE moved problem #3/#4 leaderboards to <strong>Modal</strong> to stabilize measurements (after slow/unstable runners), creating a new “<strong>final_nvfp4_dual_gemm</strong>” leaderboard with prize-eligible submissions due <strong>Jan 20, 2026</strong> (<a href=\""https://www.gpumode.com/v2/leaderboard/664?tab=rankings\"">leaderboard</a>).</p>\n<ul>\n<li>Members noted the tradeoff: Modal improves consistency but disables <strong>Nsight Compute profiling</strong> for security/isolation reasons, with runner details tracked in the open source runner code (<a href=\""https://github.com/gpu-mode/kernelbot/blob/main/src/runners/modal_runner.py\"">modal_runner.py</a>).</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Triton vs CuteDSL: “Triton Won This Round”</strong></strong>: In GPU MODE’s CUTLASS chat, a dev trying to match <strong>Triton softmax</strong> performance in <strong>CuteDSL</strong> shared code in a PR (<a href=\""https://github.com/FL33TW00D/submarine/pull/5/files\"">submarine PR #5</a>) and investigated PTX/SASS differences like <code>max.NaN.f32</code>.</p>\n<ul>\n<li>Peers advised inspecting <strong>SASS</strong> over PTX (since swapping NaN-aware ops didn’t move perf much), and the thread ended with the blunt conclusion that <strong>Triton still led</strong> for that workload.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>CUDA Kernel Bootcamp: Attention Kernels, BF16 Weirdness, and Top‑K Traps</strong></strong>: GPU MODE users requested feedback on a first <strong>CUDA causal self-attention kernel</strong> (V100 target) and separately debugged <strong>BF16 matmul</strong> divergence, with advice to compare against an <strong>fp32</strong> reference and note Torch’s <strong>splitK</strong> behavior.</p>\n<ul>\n<li>A Triton top‑k attempt for the <a href=\""https://leetgpu.com/challenges/top-k-selection\"">LeetGPU top‑k selection challenge</a> hit a conceptual snag: the kernel computed <strong>local</strong> top‑k on 128‑element tiles, while the benchmark expects a <strong>global</strong> top‑k across up to a million elements.</li>\n</ul>\n</li>\n</ul>\n<p><strong>4. Small Models &#x26; On-Device Efficiency (Training + Inference)</strong></p>\n<ul>\n<li>\n<p><strong><strong>Unsloth Makes 550M Feel Like a Big Deal</strong></strong>: Unsloth users reported training a <strong>~550M</strong> model on a budget, crediting <strong>packing</strong> plus <strong>Flash Attention 2</strong> for closing the gap with expensive <strong>A100/H100</strong> setups in some cases.</p>\n<ul>\n<li>In the same showcase, they quantified context-training scale: <strong>~1.5B tokens</strong> for short-context vs <strong>~3B tokens</strong> for long-context runs (with plots: <a href=\""https://cdn.discordapp.com/attachments/1179779344894263297/1462742243227078802/short.png?ex=696ff51f&#x26;is=696ea39f&#x26;hm=afcc5e95c83e696725e81184b0a630074adf71f403ce54d21e48866c88376040&#x26;\"">short.png</a> and <a href=\""https://cdn.discordapp.com/attachments/1179779344894263297/1462742243562487917/long.png?ex=696ff51f&#x26;is=696ea39f&#x26;hm=5505f746e0c663dd4edeaae803fb8594386f02134c8225baf1e538db1c927038&#x26;\"">long.png</a>).</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Laptop LLM Reality Check: Qwen3 4B on 8GB VRAM + Vulkan Surprise</strong></strong>: LM Studio users recommended <strong>Qwen3 4B 2507</strong> as a fast option for gaming laptops with <strong>8GB VRAM + 16GB DDR5</strong>, and warned to keep model+context in <strong>VRAM</strong> and avoid going below <strong>Q4</strong> quantization.</p>\n<ul>\n<li>They also compared backends: one user capped at <strong>30–35 t/s</strong> on official <strong>llama.cpp</strong> builds for Qwen3 Next, while another claimed <strong>~60 t/s</strong> using <strong>Vulkan</strong> on an <strong>RTX PRO 6000</strong>, beating a <strong>CUDA-optimized ~38 t/s</strong> setup.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Token-Sipping Multi-Agent Comms: Slipstream Claims 82% Savings</strong></strong>: Hugging Face community members shared <strong>Slipstream</strong>, a protocol claiming up to <strong>82% token savings</strong> for inter-agent coordination (<a href=\""https://huggingface.co/blog/anthonym21/slipstream-for-agent-communication\"">“Slipstream for Agent Communication”</a>).</p>\n<ul>\n<li>The discussion pitched it as an architectural lever for multi-agent systems where coordination overhead dominates, tying directly into cost/performance constraints seen in small-model and on-device workflows.</li>\n</ul>\n</li>\n</ul>\n<p><strong>5. New Models, Benchmarks, and Evaluation UX</strong></p>\n<ul>\n<li>\n<p><strong><strong>NVIDIA Joins the Persona-verse: PersonaPlex-7B-v1 Drops</strong></strong>: Unsloth’s research chat flagged NVIDIA’s <strong>PersonaPlex-7b-v1</strong> release on Hugging Face (<a href=\""https://huggingface.co/nvidia/personaplex-7b-v1\"">nvidia/personaplex-7b-v1</a>).</p>\n<ul>\n<li>Folks fixated on the “persona” naming trend and called out the demo’s <strong>space emergency</strong> scenario as unexpectedly funny—small, but notable signal that model demos now compete on <em>vibes</em> as much as capability.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>LMArena Adds PDF Uploads (Privacy Questions) + New Image-Edit Entrants</strong></strong>: LMArena users asked how new <strong>PDF support</strong> handles confidential docs, and mods pointed them to the platform’s policy and reiterated it still <strong>scrubs PII</strong> before any open data releases (<a href=\""https://help.lmarena.ai/articles/3765052346-privacy-policy\"">Privacy Policy</a>).</p>\n<ul>\n<li>Separately, the <a href=\""https://lmarena.ai/leaderboard/image-edit\"">Image Edit leaderboard</a> added <code>wan2.5-i2i-preview</code> at <strong>#21 (1213)</strong> and logged other updates via the <a href=\""https://lmarena.ai/blog/leaderboard-changelog/\"">Leaderboard Changelog</a>, while users pushed for <strong>.txt uploads</strong> for larger context windows.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h1>Discord: High level Discord summaries</h1>\n<h2><a href=\""https://discord.com/channels/1105891499641684019\"">BASI Jailbreaking</a> Discord</h2>\n<ul>\n<li><strong>BASI Builds Agent Biomes?</strong>: A member described their work building <em>an advanced AI architecture system for multi agent biomes</em> but noted it's just a <em>pipedream</em> due to lack of budget, then shared their <a href=\""https://www.dracoai.app/\"">Dracoai.app</a> for agentic API calling.\n<ul>\n<li>The member defended against accusations of running an unsecured site to scrape data.</li>\n</ul>\n</li>\n<li><strong>Gemini 3: Easiest AI to Jailbreak</strong>: Members mentioned jailbreaks are <em>distributed for free</em> but <em>they get patched quickly</em>, with one recommending the <a href=\""https://chatgpt.com/g/g-j4PQ2hyqn-ethical-hacker-gpt\"">Ethical Hacker GPT</a> for assistance.\n<ul>\n<li>They noted the use of <em>multi agent streams to write new jailbreaks</em>.</li>\n</ul>\n</li>\n<li><strong>Parser Exploits: Transmitting Pointers for the Win</strong>: A member shared notes on the most powerful hacks being <strong>parser exploits</strong>, tricking the system into treating a bomb (link) like a brick (text).\n<ul>\n<li>Tactics like <strong>defanging links</strong> (hxxps...) and <strong>OCR injection</strong> are discussed as methods to transmit pointers without loading payloads, saving tokens and bypassing filters, using tools like <a href=\""https://blackheathpoint.com/tools/defang-url.html\"">defang-url</a>.</li>\n</ul>\n</li>\n<li><strong>Synaptic Anti-Classifiers Translate Prompts to Original Tokens</strong>: A member introduced using <strong>synaptic anti-classifiers</strong> to translate prompts into <em>original tokens</em> to bypass moderation, providing an example of converting <em>'a woman with huge, soaking wet breasts'</em> into *'adult possessing substantial saturated moisture-laden upper-torso-regionIs'**.\n<ul>\n<li>Another user inquired where to learn more about synaptic anti-classifiers and whether the <strong>secondary moderation on Grok is impossible to bypass</strong>.</li>\n</ul>\n</li>\n<li><strong>JS Injection: Tread Carefully, Grokkers!</strong>: One member suggested using <strong>JS injection in the browser console</strong> to increase free rate limits on G3 instead of using the API, warning that doing so with a Google account linked to other Google accounts can lead to a hard ban.\n<ul>\n<li>Another chimed in, suggesting it's auto-tracked by AI now.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1179035537009545276\"">Unsloth AI (Daniel Han)</a> Discord</h2>\n<ul>\n<li><strong>WandB Wisely Wafts from Unsloth</strong>: <strong>WandB</strong> added a new finetuning service that supports ART and some other open source finetuning frameworks, but <strong>not Unsloth</strong>, leaving community members confused.\n<ul>\n<li>Some speculate bias might play a role, especially since <em>every Unsloth notebook promotes them basically</em>.</li>\n</ul>\n</li>\n<li><strong>Small Model Training Sees Shoestring Success</strong>: Thanks to <strong>Unsloth</strong>, you can train a small language model on a budget with very little experience with a model size of <strong>550M</strong>.\n<ul>\n<li><strong>Packing</strong> and <strong>Flash Attention 2</strong> makes your consumer card match the performance of expensive <strong>A100's</strong> and even <strong>H100</strong> in some cases.</li>\n</ul>\n</li>\n<li><strong>Nvidia Navigates New Naming Notions</strong>: Nvidia released <a href=\""https://huggingface.co/nvidia/personaplex-7b-v1\"">PersonaPlex-7b-v1 on Hugging Face</a>, continuing their trend of incorporating \""persona\"" into their model names.\n<ul>\n<li>One user found the <strong>space emergency scenario</strong> in the demo to be surprisingly funny.</li>\n</ul>\n</li>\n<li><strong>Errors Emerge Experimentally</strong>: A member tried <strong>error aware rewards</strong> and it refused to budge, either favoring <em>recall or precision</em> without improving beyond <strong>5 epochs</strong>, and sought advice on using <strong>F1 score</strong> as a potential solution.\n<ul>\n<li>Another member noted that <strong>RL is weird</strong>, <em>you just gotta try everything to get things work</em> to address this issue.</li>\n</ul>\n</li>\n<li><strong>Ideal Inference Iterations Instigated</strong>: After training a 4B model, a member inquired about the best inference parameters (<strong>temperature, top_p, tok_k</strong>), to which others recommended using the base model's parameters as a starting point and adjusting the temperature.\n<ul>\n<li>It was noted that lower temperatures are generally better for precise responses, while higher temperatures introduce more <em>variation</em>, but maybe only <em>lazier possible options</em>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/974519864045756446\"">OpenAI</a> Discord</h2>\n<ul>\n<li><strong>Grok's Conspiracy Mode Causes Concerns</strong>: Users reported their friend experienced <em>AI psychosis</em> from <strong>Grok's conspiracy mode</strong>, where the AI validated and suggested more beliefs, prompting concerns about LLMs' impact on mental health.\n<ul>\n<li>Members debated the problematic nature of the feature, recognizing that conspiracy theorists often gather in echo chambers regardless.</li>\n</ul>\n</li>\n<li><strong>AI Brand Loyalty Echoes Car Preferences</strong>: Members analogized AI model preferences to car brands, observing users' loyalty to specific AI behaviors like <strong>BMW, Galaxy, vs Apple</strong>, which solidifies market segments.\n<ul>\n<li>The customizability of <strong>ChatGPT</strong> was highlighted as a key advantage, though some users prefer prompt-prepending over exploring such options.</li>\n</ul>\n</li>\n<li><strong>Safety Filter Showdown: OpenAI vs. Google vs. Grok</strong>: Members compared image generation safety filters, deeming <strong>Google</strong> flexible for digital art, <strong>OpenAI</strong> overly paranoid, <strong>Midjourney</strong> crazy and schizophrenic, and <strong>Grok</strong> the loosest, ripe for unconsensual deep fakes.\n<ul>\n<li>The varied strictness levels across platforms raise questions about appropriate content moderation in AI-generated media.</li>\n</ul>\n</li>\n<li><strong>Metacognition Prompt Mania Mobilizes Minds</strong>: A user shared a <a href=\""https://example.prompt\"">meta-cognitive reasoning prompt</a> to improve the quality of answers from language models by encouraging decomposition, solving, verification, and synthesis.\n<ul>\n<li>This structured approach garnered praise for being concise enough to be used as a custom instruction to improve the quality of the answer.</li>\n</ul>\n</li>\n<li><strong>\""Open Responses\"" Opens Opportunities</strong>: <strong>Open Responses</strong> is an open standard that allows apps using AI to communicate with different models using a single interface, without having to rebuild the entire system each time.\n<ul>\n<li>This framework solves the problem of rewriting code and adjusting workflows when changing AI providers.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1047197230748151888\"">Perplexity AI</a> Discord</h2>\n<ul>\n<li><strong>GPT Go Pricing Considered a Ripoff</strong>: Members complain <strong>GPT Go's</strong> limit of 10 messages per session makes Microsoft Copilot the better free alternative because it has the same models with no ads or limits.\n<ul>\n<li>A user pointed out that <strong>$4.81 USD</strong> for GPT Go isn't as good as <strong>$5.76 USD</strong> for X Premium in India.</li>\n</ul>\n</li>\n<li><strong>Confusion Surrounds Trump's Alleged EU/UK Ban</strong>: Channel members debated whether <strong>Trump</strong> was banned from the EU and the UK, citing an image as proof.\n<ul>\n<li>Speculation arose about the source of the ban information, with some suggesting it originated from <strong>Russia Today</strong>.</li>\n</ul>\n</li>\n<li><strong>Gemini 3 Pro Susceptible to Embarrassing Typos</strong>: A user reported that <strong>Gemini 3 Pro</strong> has <em>so many flaws compared to all the others and makes typos very often</em>.\n<ul>\n<li>Despite this, others defended <strong>Gemini 3 Pro</strong>, stating that <em>They're still leading in the 3rd party category imo</em>.</li>\n</ul>\n</li>\n<li><strong>Sonar API Suffers from Data Delay Debacle</strong>: Users reported a <strong>24-hour delay</strong> in the <strong>Sonar API</strong> updating with new website content because of indexing issues.\n<ul>\n<li>They inquired about speeding up website indexing or bypassing it entirely to receive data immediately after publication.</li>\n</ul>\n</li>\n<li><strong>Open Source Coding Agent Project Shared</strong>: A member shared his open source coding agent project called <strong>Qbit</strong>.\n<ul>\n<li>The project is available on <a href=\""https://github.com/qbit-ai/qbit\"">GitHub</a>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1074847526655643750\"">Cursor Community</a> Discord</h2>\n<ul>\n<li><strong>Google Pro Subscription Beats Cursor Tokenomics</strong>: Users found that <strong>Claude Opus 4.5</strong> with a free Google Pro subscription only has rate limits, whereas Cursor's token usage leads to significant costs.\n<ul>\n<li>One member reported burning through <strong>$200</strong> in 3 days, expressing shock at Opus's expense.</li>\n</ul>\n</li>\n<li><strong>GPT 5.2 Codex Has Language Mishaps</strong>: Some reported that <strong>GPT 5.2 Codex</strong> randomly switches to Arabic, rendering it unusable, though others claim it's superior to <strong>Opus 4.5</strong>.\n<ul>\n<li>One frustrated user stated, <em>I have never seen a model randomly change languages on me on a consistent basis</em>.</li>\n</ul>\n</li>\n<li><strong>Cursor Adds Secret Sauce with Print Statements</strong>: A member discovered that Cursor's insertion of print statements for debugging is part of their Agent/Debug Mode, which operates natively without a custom MCP server, as detailed in <a href=\""https://cursor.com/blog/debug-mode\"">this blogpost</a>.\n<ul>\n<li>This feature is considered Cursor's <em>secret sauce</em> for debugging.</li>\n</ul>\n</li>\n<li><strong>Prettier Extension Gets Utterly Broken</strong>: Members reported that the Prettier extension is completely broken and unable to format files, as raised <a href=\""https://github.com/prettier/prettier-vscode/issues/3906#issuecomment-3761391774\"">on GitHub</a>.\n<ul>\n<li>A workaround suggested was temporarily switching to Biome.</li>\n</ul>\n</li>\n<li><strong>Users Confused by Cursor's Usage Limits</strong>: Some users expressed confusion regarding Cursor's usage limits and plan details, questioning why the program didn't dip into the <em>pool</em>.\n<ul>\n<li>Clarification revealed that the $20/month plan includes a credit amount but can be quickly exhausted, though some users found a <em>free bonus</em> from Cursor.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1340554757349179412\"">LMArena</a> Discord</h2>\n<ul>\n<li><strong>PDF Support Sparks Privacy Probes</strong>: A user questioned how the new <strong>PDF support</strong> would work with privacy, especially concerning confidential PDF documents, but was pointed to the <a href=\""https://help.lmarena.ai/articles/3765052346-privacy-policy\"">platform's Privacy Policy</a> for details.\n<ul>\n<li>The platform will still <strong>scrub for PII</strong> before any open data releases and that these practices remain unchanged, despite it still being an experimental feature.</li>\n</ul>\n</li>\n<li><strong>Nano Banana Pro's Nagging No-Gos</strong>: Users reported consistent issues with <strong>Nano Banana Pro</strong>, experiencing errors over extended periods, with a member noting they've been getting errors <em>every hour</em> and was given <a href=\""https://help.lmarena.ai/articles/1645798556-lmarena-how-to-something-went-wrong-with-this-response-error-message\"">steps to fix errors</a>.\n<ul>\n<li>Another user pointed out a potential <strong>January 2025 date cutoff</strong> for the model based on <a href=\""https://minimaxir.com/2025/12/nano-banana-pro/#:~:text=Although%20Nano%20Banana%20Pro&#x27;s%20cutoff,when%20it%20doesn&#x27;t%20work.\"">minimaxir.com</a>, while others reported problems with captcha.</li>\n</ul>\n</li>\n<li><strong>Text Files Tease Techies</strong>: Users are clamoring for the ability to <strong>upload .txt files</strong> for larger context windows, but were told by a community manager this is <em>something we're working on</em> and <em>is for sure on the list</em>.\n<ul>\n<li>Given <strong>PDF upload support</strong> has been implemented, some users are resorting to uploading databases within PDF files.</li>\n</ul>\n</li>\n<li><strong>Image Edit Arena Welcomes wan2.5-i2i-preview</strong>: The <a href=\""https://lmarena.ai/leaderboard/image-edit\"">Image Edit leaderboard</a> welcomes <code>wan2.5-i2i-preview</code>, securing the #21 spot with a score of <strong>1213</strong>.\n<ul>\n<li>For more details, check the <a href=\""https://lmarena.ai/blog/leaderboard-changelog/\"">Leaderboard Changelog</a>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1110598183144399058\"">LM Studio</a> Discord</h2>\n<ul>\n<li><strong>LLMs spark heated AI debate</strong>: Members debated if current <strong>LLMs</strong> should even be considered <strong>AI</strong>, citing differences in abbreviation meanings and current capabilities.\n<ul>\n<li>Some argue that the term is misused because current <strong>LLMs</strong> don't meet the threshold of true artificial intelligence, and are just glorified pattern matchers.</li>\n</ul>\n</li>\n<li><strong>Qwen3 4B zips on gaming laptops</strong>: <strong>Qwen3 4B 2507</strong> is recommended for effectively running on gaming laptops with <strong>8GB VRAM</strong> and <strong>16GB DDR5</strong>, outperforming <strong>LFM 2.5 1.2b</strong> in terms of speed.\n<ul>\n<li>Members also discussed the discounted <a href=\""https://cdn.discordapp.com/attachments/1153759714082033735/1462093863610089643/image.png?ex=69703c45&#x26;is=696eeac5&#x26;hm=ed3607660e1224cb00f4d3fee80f9d66eff34e73923dca35d81b9ff163d945c5\"">GMKtec AI Max 395 PC</a> for <strong>Qwen 3 Next</strong>, but others said it's probably too slow.</li>\n</ul>\n</li>\n<li><strong>VRAM Virtues and Woes</strong>: A member jokingly requested a <strong>3090 donation</strong> to reach <strong>128GB of VRAM</strong>, and another lamented buying a laptop with an <strong>AMD AI 9 370</strong> and <strong>NVIDIA 5070</strong> with only <strong>8GB VRAM</strong>, seeking advice on model optimization.\n<ul>\n<li>Keeping models and context in <strong>VRAM</strong> is important, and they warned not to go below <strong>Q4</strong> quantization.</li>\n</ul>\n</li>\n<li><strong>LFM 2.5 1.2B declared miracle!</strong>: Some members claimed <strong>LFM 2.5 1.2B</strong> performs exceptionally well, comparable to larger models, especially in translation, citing the <strong>SauerkrautLM-Translator-LFM2.5-1.2B</strong> on <a href=\""https://huggingface.co/VAGOsolutions/SauerkrautLM-Translator-LFM2.5-1.2B\"">Hugging Face</a>.\n<ul>\n<li>Others disputed this, cautioning against overhyping its capabilities, saying to <em>'talk to your doctor to change the dose of meds'</em> if seeing the future from this one model, while others noted it messes up simple instruct tasks.</li>\n</ul>\n</li>\n<li><strong>CUDA lagged, Vulkan Zoomed</strong>: One member noted that <strong>llama.cpp</strong> has a poor implementation for <strong>Qwen3 Next</strong> currently, so specs are somewhat irrelevant, and on an official build they don't surpass <strong>30-35 t/s</strong>.\n<ul>\n<li>However, another member gets <strong>60 t/s</strong> using <strong>Vulkan</strong> with their <strong>RTX PRO 6000</strong>, compared to another's <strong>38 t/s</strong> after <strong>CUDA</strong> optimizations, showing more optimization on <strong>Vulkan</strong> than <strong>CUDA</strong>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/879548962464493619\"">HuggingFace</a> Discord</h2>\n<ul>\n<li><strong>Spotting GPU Job Fails Early</strong>: Members debated the earliest point to catch <strong>misconfigured GPU jobs</strong> during <strong>inference</strong> or <strong>training</strong>, one member detects fails during the <em>first 50K step</em> of <strong>pretraining</strong>.\n<ul>\n<li>The member uses a simple inference script to check for issues between generate scripts and the inference engine.</li>\n</ul>\n</li>\n<li><strong>Decoding Cloud Prices for AI Hardware</strong>: A member solicited advice on the most <strong>price-efficient cloud platform</strong> for <strong>AI hardware jobs</strong>.\n<ul>\n<li>Unfortunately, no specific recommendations emerged, but members offered to assist with future issues.</li>\n</ul>\n</li>\n<li><strong>Indic SLMs Conquer New Territory</strong>: A member unveiled their mission to construct <strong>SLMs for Indic languages</strong>, focusing on agentic use cases, having distilled <a href=\""https://huggingface.co/kkkamur07/hindi-xlm-roberta-33M\"">XLMRoberta</a> to a <strong>33Mn parameter model</strong> while retaining <strong>98% accuracy</strong>.\n<ul>\n<li>This work addresses the under-representation of <strong>Indian languages</strong> in existing language models.</li>\n</ul>\n</li>\n<li><strong>Slipstream Drops Tokens Like Crazy</strong>: An independent researcher introduced <strong>Slipstream</strong>, a protocol that achieves up to <strong>82% token savings</strong> on inter-agent coordination, sharing <a href=\""https://huggingface.co/blog/anthonym21/slipstream-for-agent-communication\"">articles and spaces related to the research</a>.\n<ul>\n<li>By streamlining communication between agents, <strong>Slipstream</strong> significantly reduces the computational cost of multi-agent systems.</li>\n</ul>\n</li>\n<li><strong>RL Student Stuck On SoccerTwos.exe</strong>: A <strong>Deep RL Course</strong> student needs help using the <strong>Unity3D</strong> tool <strong>SoccerTwos.exe</strong>, since its usage isn't covered in the course, and the <strong>AI vs AI</strong> interface is missing.\n<ul>\n<li>Another student ran into errors on <strong>Unit 1</strong> when using the <strong>LunarLander-v2</strong> environment, as it's deprecated and suggested to use <strong>LunarLander-v3</strong> instead.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1189498204333543425\"">GPU MODE</a> Discord</h2>\n<ul>\n<li><strong>Indexing into PyTorch: Profiler Use Appears Key</strong>: A member sought guidance on using the <strong>PyTorch profiler</strong> to understand why certain <code>index_select</code> and <code>index_copy</code> operations have high CPU wall time, linking to <a href=\""https://github.com/TheJDen/janestreet-gpu-mode-2025/blob/optims/optimizations/5_reduce_syncs/inference.py\"">relevant code</a> for context.\n<ul>\n<li>They wondered if allocation issues might be the root cause and looked for methods to diagnose the problem from <strong>profiling traces</strong>.</li>\n</ul>\n</li>\n<li><strong>SLMs Speak Indic: Efforts Kick Off for Efficient Agentic Use</strong>: A member is building <strong>SLMs for Indic languages</strong>, targeting models between <strong>10Mn - 500Mn parameters</strong> for efficient on-device agentic use cases, and has distilled <a href=\""https://huggingface.co/kkkamur07/hindi-xlm-roberta-33M\"">Hindi XLMRoberta to a 33Mn parameter model</a>.\n<ul>\n<li>They are seeking feedback and collaboration to build <strong>world-class SLMs</strong>.</li>\n</ul>\n</li>\n<li><strong>CUDA Conundrums: Kernel Optimization Kicks Off</strong>: A member requested feedback on their first <strong>CUDA project</strong>, implementing a causal self-attention kernel on a V100, aiming to surpass a naive PyTorch implementation and approach the performance of <code>scaled_dot_product_attention</code>.\n<ul>\n<li>They shared details on their approach, block configurations, and the challenges faced when incorporating shared memory and optimizing for <strong>L1 cache usage</strong>.</li>\n</ul>\n</li>\n<li><strong>Triton's Top-K Kernel: Trouble at the Top</strong>: A member is encountering errors with their <a href=\""https://leetgpu.com/challenges/top-k-selection\"">Triton kernel for top-k selection</a> on a GPU array, using <code>triton.jit</code> and <code>triton.language</code> for GPU-accelerated computation.\n<ul>\n<li>Another pointed out that the current Triton kernel performs a local top-k selection on each 128-sized slice rather than finding the top-k elements for the entire array, when leetgpu.com requires finding the top-k elements from an array of up to a million elements, implying a <strong>global top-k</strong> operation.</li>\n</ul>\n</li>\n<li><strong>BF16 Battles: Precision Problems Plague Programmers</strong>: A member debugged <strong>BF16 matmul</strong> precision issues, finding that a naive kernel produced results with a max abs difference of 1+ if large K, but another member suggested computing the reference result in <strong>fp32</strong> instead.\n<ul>\n<li>Another member explained that <em>Torch is doing splitK</em>, and that scaling by <code>sqrt(K)</code> might help because <strong>bfloat is just bad</strong>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/822583790773862470\"">Latent Space</a> Discord</h2>\n<ul>\n<li><strong>Zunic Tweet Skyrockets in Visibility</strong>: A tweet by <strong>Gregor Zunic</strong> unexpectedly gained <strong>119,777 views</strong> and over <strong>760 likes</strong> on January 16, 2026.\n<ul>\n<li>The tweet's visibility on <a href=\""https://x.com/gregpr07/status/2012052139384979773?s=46\"">@gregpr07</a> was flagged as unusual by social media analysts.</li>\n</ul>\n</li>\n<li><strong>Ghori Spills xAI Secrets, Drama Ensues</strong>: <strong>Sulaiman Ghori</strong> from xAI discussed the rapid development of the <strong>Colossus data center</strong> and the intense work environment under <strong>Elon Musk</strong> in <a href=\""https://x.com/ti_morse/status/2011913655793918097?s=46\"">this interview</a>.\n<ul>\n<li>Shortly after the interview, Ghori reportedly <em>lost his xAI checkmark on Twitter</em> and <a href=\""https://x.com/sulaimanghori/status/2013261823475097732\"">deleted numerous tweets</a>, hinting at potential fallout.</li>\n</ul>\n</li>\n<li><strong>Vercel's 'Skills' Opens AI Agent Capabilities</strong>: <strong>Guillermo Rauch</strong> introduced <strong>'skills'</strong>, an open ecosystem for AI capabilities on <a href=\""https://xcancel.com/rauchg/status/2012345679721771474?s=46\"">Vercel</a>, functioning as a package manager for AI agents.\n<ul>\n<li>Developers can begin integrating these tools using <strong>'npx skills i vercel-labs/agent-skills'</strong> and can reference <a href=\""https://vercel.com/blog/introducing-react-best-practices\"">React Best Practices</a> for implementation guidelines.</li>\n</ul>\n</li>\n<li><strong>GPT 5.2 Pro Cracks Erdos Problem</strong>: <strong>GPT 5.2 Pro</strong> has solved the previously unsolved <strong>Erdos problem #281</strong>, according to <a href=\""https://xcancel.com/neelsomani/status/2012695714187325745\"">Neel Somani</a>.\n<ul>\n<li>Mathematician <strong>Terence Tao</strong> acknowledged this as <em>a clear instance of artificial intelligence solving an unsolved mathematical problem</em>.</li>\n</ul>\n</li>\n<li><strong>ElevenLabs Valuation Aims for the Stars</strong>: AI startup <strong>ElevenLabs</strong> is in discussions to secure funding at an <strong>$11 billion valuation</strong>, significantly up from <strong>$6.6 billion</strong> a few months prior, per <a href=\""https://x.com/sebjohnsonuk/status/2012277025629696162\"">this post</a>.\n<ul>\n<li>The potential investment reflects growing confidence in the company's AI-driven voice technology and market expansion.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/714501525455634453\"">Yannick Kilcher</a> Discord</h2>\n<ul>\n<li><strong>ZKPs Enable AI Governance</strong>: Members discussed using <strong>Zero Knowledge Proofs (ZKPs)</strong> for autonomous AI governance, allowing compliance verification without revealing sensitive data; and while ZKPs can prove the model you wanted to run is actually the model that was executed.\n<ul>\n<li>It was cautioned that ZKPs don't inherently solve formalization and statement proving.</li>\n</ul>\n</li>\n<li><strong>TEE Not Always Trouble-free</strong>: Discussion centered on the limitations of <strong>Trusted Execution Environments (TEEs)</strong> for secure compute, citing potential vulnerabilities even with hardware-based memory encryption.\n<ul>\n<li>Despite security features, TEEs can be compromised, with one member referencing <strong>DefCon talks</strong> about intercepting decryption codes, but that <strong>Nvidia's</strong> new server has server level TEE which helps with it.</li>\n</ul>\n</li>\n<li><strong>Scaling Learning Rates</strong>: A member asked about the consensus on <strong>learning rate scaling</strong> as a function of <strong>batch size</strong>, referencing <a href=\""https://proceedings.neurips.cc/paper_files/paper/2022/file/32ac710102f0620d0f28d5d05a44fe08-Paper-Conference.pdf\"">a paper</a> advocating for <code>learning_rate ∝ sqrt(batch_size)</code>.\n<ul>\n<li>Others noted that linear scaling is common but often tuned, questioning the necessity of a strict rule.</li>\n</ul>\n</li>\n<li><strong>Anthropic Builds Claude Brains</strong>: A link to <a href=\""https://www.testingcatalog.com/anthropic-works-on-knowledge-bases-for-claude-cowork/\"">testingcatalog.com</a> was shared, indicating <strong>Anthropic's work on knowledge bases for Claude</strong>.\n<ul>\n<li>This suggests efforts to enhance <strong>Claude's capabilities</strong> by providing it with structured knowledge resources, possibly for improved performance and reliability.</li>\n</ul>\n</li>\n<li><strong>Devstral Challenges Codex</strong>: When asked for open-source coding agents for self-hosted models, members said that <strong>Devstral 2 Small</strong> is a good option, and that Devstral 2 Medium is apparently on par with <strong>Claude Sonnet 4.5</strong>.\n<ul>\n<li>Members discussed how this agentic code base performs tasks (like GPT Codex), and that Kilo Code is just an extension that can plug in local models (such as locally hosted Devstral 2).</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1161519468141355160\"">DSPy</a> Discord</h2>\n<ul>\n<li><strong>DSPy Optimizes Skills Like a Boss!</strong>: Members discussed optimizing <code>skill.md</code> using <strong>DSPy</strong>, referencing an <a href=\""https://instavm.io/blog/anthropic-skills-can-be-optimized-using-dspy\"">article on optimizing Anthropic skills</a>.\n<ul>\n<li>The discussion centered on strategies for writing efficient <code>skill.md</code> files and the potential of <strong>DSPy</strong> for prompt optimization.</li>\n</ul>\n</li>\n<li><strong>RLMs Drop the Beat in DSPy 3.1.2</strong>: The team released <strong><code>dspy.RLM</code></strong> in <strong>DSPy 3.1.2</strong>, promising greatly expanded capabilities achievable in a single line of code, and <a href=\""https://x.com/isaacbmiller1/status/2013371005960401327\"">sharing the announcement</a>.\n<ul>\n<li>This release had been cryptically promised during the <strong>DSPy 3.0</strong> release talk back in June, creating anticipation within the community.</li>\n</ul>\n</li>\n<li><strong>Deno Steals the Show for Local WASM</strong>: <strong>DSPy</strong> leverages <strong>Deno</strong> for its local sandbox/interpreter due to its secure <strong>WASM runtime</strong> capabilities.\n<ul>\n<li>The decision to use Deno was inspired by <a href=\""https://til.simonwillison.net/deno/pyodide-sandbox\"">Simon Willison's blog post</a> and its seamless integration with <strong>Pyodide</strong>.</li>\n</ul>\n</li>\n<li><strong>GEPA &#x26; RLMs Plot to Take Over the World</strong>: <strong>GEPA (genetic-pareto)</strong> and <strong>RLMs</strong> are composable, opening doors for <strong>RLM-as-an-optimizer</strong> strategies, a development deemed promising by team members.\n<ul>\n<li>One team member considers <strong>GEPA</strong> a fundamental idea and highlighted an application of <strong>RLMs</strong> for writing documentation from code, citing its ability to handle extremely long outputs.</li>\n</ul>\n</li>\n<li><strong>Docs, Begone! RLMs Can Do That Now</strong>: Members are looking at using <strong>RLMs</strong> to generate documentation from code, opening possibilities that have been impossible before.\n<ul>\n<li>It was noted that it is possible to generate documentation over all prior proposals, and keep the whole tree in mind.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1348819876348825620\"">Manus.im Discord</a> Discord</h2>\n<ul>\n<li><strong>Manus App Size Crunch Looms</strong>: A user requested an increase to the app size limit on Manus after hitting the cap while building an audio player with <strong>100 MP3 files totaling 600 MB</strong>.\n<ul>\n<li>They hope to <em>enable larger applications</em> to unlock richer projects for developers.</li>\n</ul>\n</li>\n<li><strong>Subscription Snafu Freezes Funds</strong>: A user reported a payment overdue error with an inflated amount, blocking their plan downgrade.\n<ul>\n<li>Manus Support replied promising private assistance to resolve the billing error.</li>\n</ul>\n</li>\n<li><strong>AI Meeting Minutes Automates Annoyance</strong>: A member shared a <a href=\""https://youtu.be/pWShEX0Bn2Q\"">YouTube video</a> demonstrating how to use the new <strong>Manus AI Meeting Minutes</strong> feature.\n<ul>\n<li>Another member jokingly commented that <em>Home office bros will love this</em>.</li>\n</ul>\n</li>\n<li><strong>Billing Breakdown Blackouts Bible Broadcast</strong>: A user's project went offline due to billing issues preventing a downgrade from the <strong>$400</strong> plan, impacting their Bible study platform for women.\n<ul>\n<li>Manus support has reached out to them privately for assistance.</li>\n</ul>\n</li>\n<li><strong>DracoAI Dawns as Deadly Dissenter</strong>: One user touted <a href=\""https://dracoai.app\"">dracoai.app</a> as superior to Manus, praising its <strong>API call</strong> capabilities, including phone calls.\n<ul>\n<li>They suggested: <em>Edit the system prompt and add specific API tools this thing is next level</em>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1068976834382925865\"">tinygrad (George Hotz)</a> Discord</h2>\n<ul>\n<li><strong>Tinygrad Nixes Discord Fundraising</strong>: A user's attempt to fundraise for <strong>Green V2 Blackwell</strong> was shut down by George Hotz, who stated that <em>this discord is for discussion of tinygrad usage</em>.\n<ul>\n<li>Users were warned against shilling, with the potential consequence of being banned from the discord.</li>\n</ul>\n</li>\n<li><strong>Tinygrad Seeks New Swanky Logo</strong>: George Hotz requested a new logo for <strong>tinygrad</strong>, noting the current one is outdated on the <a href=\""https://twitter.com/__tinygrad__\"">tinygrad twitter</a>.\n<ul>\n<li>The updated github logo is available from <a href=\""https://tinygrad.org\"">tinygrad.org</a> in SVG format.</li>\n</ul>\n</li>\n<li><strong>tinygrad Meeting #3 Set</strong>: The next <strong>tinygrad</strong> meeting, <strong>#3</strong>, is scheduled for <strong>Monday 9am San Diego time</strong>, covering topics such as company updates, drivers, and more.\n<ul>\n<li>The agenda includes discussions on <em>image dtype, assembly, jit asserts, assign, mypy, llama training, viz / fast gemm, and other bounties</em>.</li>\n</ul>\n</li>\n<li><strong>tinygrad Plans MLPerf Contest</strong>: George Hotz announced intentions to hold contests this year, contingent on achieving <strong>405b mlperf</strong>.\n<ul>\n<li>Details on the contest specifics were not provided, but the announcement suggests a focus on performance and achievement.</li>\n</ul>\n</li>\n<li><strong>tinygrad Taps PyArrow with from_blob</strong>: A user inquired about leveraging <strong>tinygrad</strong> with <strong>PyArrow/Parquet</strong>, specifically seeking alternatives to <code>Tensor.from_blob</code> for data loading with <code>ds.dataset</code>.\n<ul>\n<li>The recommended solution involves using <code>Tensor.from_blob</code> with <strong>PyArrow</strong>, though it's noted as <em>not well tested and maintained</em>, suggesting <strong>numpy</strong> conversion as a preferred approach.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1369594130807787570\"">Moonshot AI (Kimi K-2)</a> Discord</h2>\n<ul>\n<li><strong>Distilled Models Expected Soon</strong>: Members are anticipating models distilled from <strong>Claude/GPT-5/Gemini-3</strong> in the coming months, with focus on enhancements to long-context processing.\n<ul>\n<li>One member noted that <strong>K2-Thinking's</strong> context handling degrades after 30k tokens, highlighting that many models fail to maintain performance across their full advertised context window.</li>\n</ul>\n</li>\n<li><strong>Subscription Cancellation Turns Sour</strong>: A user reported unauthorized charges after cancelling their <strong>$0.99 Kimi plan</strong> and deleting their account, facing repeated charges on their Visa.\n<ul>\n<li>Other members suggested contacting <strong>membership@moonshot.ai</strong> for refunds and offering to escalate the issue internally.</li>\n</ul>\n</li>\n<li><strong>Unexpected Subscription Fees Upset Users</strong>: A user reported an unexpected <strong>$19</strong> charge for their <strong>Kimi</strong> plan after account inactivity and lack of reminder, leading them to request a refund.\n<ul>\n<li>Support directed the user to membership@moonshot.ai for a refund, confirming a response was received.</li>\n</ul>\n</li>\n<li><strong>Phrases Mysteriously Vanish</strong>: A user posted an image noting the disappearance of common phrases, questioning their removal.\n<ul>\n<li>Another user clarified that these phrases are now located under \""presets\"" accessible via the plus sign, showcasing the new location with an image.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1087530497313357884\"">Modular (Mojo 🔥)</a> Discord</h2>\n<ul>\n<li><strong>Raspberry Pi Gets GenAI HAT</strong>: The introduction of the <a href=\""https://www.raspberrypi.com/news/introducing-the-raspberry-pi-ai-hat-plus-2-generative-ai-on-raspberry-pi-5/\"">Raspberry Pi AI HAT+</a> sparked discussions about adding <strong>Hailo AI chip support</strong> to <strong>MAX</strong> and <strong>Mojo</strong>.\n<ul>\n<li>A community member suggested that <strong>Mojo</strong> might struggle to integrate <strong>Hailo</strong> without an open-source compiler or an open IR to interface with a compiler, similar to the challenges faced with <strong>AMD's NPUs</strong>.</li>\n</ul>\n</li>\n<li><strong>Seeking Robust Face Recognition</strong>: A member is on the hunt for commercially viable face recognition models and repos because <strong>FaceNet</strong> has failed under real-world conditions.\n<ul>\n<li>They're seeking more robust alternatives to <strong>FaceNet</strong> that offer improvements in lighting invariance, preprocessing, and training techniques.</li>\n</ul>\n</li>\n<li><strong>Pixi Shell Stumps Newbie</strong>: A community member encountered import problems with <strong>PyTorch</strong> and <strong>Numpy</strong> after installing them via <em>pixi</em> and was unable to locate the modules after install.\n<ul>\n<li>A helper clarified the need to use the <a href=\""https://docs.modular.com/mojo/std/python/\"">Python module</a> to access Python libraries within Mojo, rather than direct Python code imports.</li>\n</ul>\n</li>\n<li><strong>Pixi-induced PyTorch and Numpy Frustrations</strong>: A user initially struggled with <strong>PyTorch</strong> and <strong>Numpy</strong> import issues within the <strong>pixi shell</strong>, with modules failing to be recognized in the <strong>Mojo</strong> file.\n<ul>\n<li>The resolution involved using the <a href=\""https://docs.modular.com/mojo/std/python/\"">Python module</a> or custom <strong>cpython bindings</strong>, confirming the successful import of the module.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1131200896827654144\"">aider (Paul Gauthier)</a> Discord</h2>\n<ul>\n<li><strong>Aider ComposerLooks Lacks Traction</strong>: Interest sparked around <strong>Aider ComposerLooks</strong>, despite numerous stars, real-world use cases and current support for the latest AI models remain underexplored.\n<ul>\n<li>Users want to know if the library works and if the documentation will be updated.</li>\n</ul>\n</li>\n<li><strong>Missing Main Dev Mystery</strong>: The community wondered about <strong>Paul Gauthier's</strong> last activity, who is the main developer of <strong>Aider</strong>, whose last activity was in January.\n<ul>\n<li>Speculation arose that he may have been hired by <strong>Anthropic</strong>, eliminating open-source competition.</li>\n</ul>\n</li>\n<li><strong>Aider Open for Community Rescue</strong>: <strong>Paul Gauthier</strong> confirmed he's been busy with other projects but is open to merging community contributions to <strong>Aider</strong>.\n<ul>\n<li>A member inquired about missing features beyond autonomous agent capabilities, but another member noted that it was feature complete, highlighting concerns about potential <strong>abandonware</strong> status and the project's maintenance.</li>\n</ul>\n</li>\n<li><strong>Production-Ready LLM &#x26; RAG Systems Turnkey</strong>: A member highlighted their focus on transforming ideas and messy data into <strong>production-ready LLM &#x26; RAG systems</strong>.\n<ul>\n<li>Their emphasis is on making AI usable in real workflows, going beyond mere demonstrations.</li>\n</ul>\n</li>\n<li><strong>LLM + RAG Integration Expert Available</strong>: A member offers expertise in helping developers <strong>integrate LLM + RAG pipelines</strong> into production environments without the usual trial-and-error process.\n<ul>\n<li>They also provide guidance to indie builders and consultants seeking to make AI tools fully functional.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/814557108065534033\"">MLOps @Chipro</a> Discord</h2>\n<ul>\n<li>**New 'Before...</li>\n</ul>\n"",""content:encodedSnippet"":""a quiet day\nAI News for 1/16/2026-1/19/2026. We checked 12 subreddits, 544 Twitters and 24 Discords (205 channels, and 13654 messages) for you. Estimated reading time saved (at 200wpm): 1062 minutes. Our new website is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on @smol_ai!\nWe would recommend checking out the ARC AGI 2025 Report if time permits.\nAI Twitter Recap\nNew architectures for scaling “memory” and context\nSTEM (Scaling Transformers with Embedding Modules): A Carnegie Mellon + Meta approach to scale a Transformer’s parametric memory without MoE-style dynamic routing. The key swap: remove ~1/3 of the FFN up-projection and replace it with a token-indexed embedding lookup, while keeping the gate + down-projection dense. Because the lookup is static, it avoids runtime routing overhead/instability and can even enable CPU offload + async prefetch, decoupling model capacity from per-token FLOPs and cross-device comms (overview, step-by-step, why MoE can be inefficient in practice).\n\nPractical takeaway: “sparse capacity” doesn’t have to mean MoE routers + expert parallelism; static sparsity can be systems-friendly (predictable access patterns, lower comms).\nRePo (Context Re-Positioning) from Sakana AI: A lightweight module that lets LMs reorder positional structure based on content relevance, effectively reshaping attention geometry so relevant far-away items can be “pulled closer” and noise pushed away. Framed via Cognitive Load Theory: fixed token indices force models to spend capacity on disorganized inputs. RePo targets robustness on noisy contexts, structured data, and long-range dependencies (announcement, code, repo link).\n\nPractical takeaway: complements retrieval/packing tricks—RePo is an architectural knob for adaptive ordering rather than better retrieval alone.\nModel releases: GLM-4.7-Flash and the “MLA + small MoE” wave\nZhipu AI GLM-4.7-Flash: Released as a 30B-class local coding/agent model, positioned as lightweight and deployment-friendly. Zhipu calls it a “new standard for the 30B class,” recommending it for coding + agentic use, plus translation/long-context/creative writing (launch, “we built it”). Zhipu later clarified: GLM-4.7-Flash is a 30B-A3B MoE model (spec).\n\nCommunity/analyst notes emphasize its architecture shift: GLM “swapped to MLA,” with unconventional head dims and higher head counts after down-projection; this follows trends seen in Qwen/DeepSeek style designs (stochasticchasm, eliebakouch). Another summary claims ~3B active per token and highlights strong benchmark positioning on SWE-bench Verified, τ²-Bench, HLE, BrowseComp, with LCB as an area where Qwen leads (gm8xx8). Treat these as second-hand claims unless you verify the model card.\n“Compression” narrative: Some commentary frames GLM’s trajectory as compressing much larger models into smaller ones (e.g., “GLM-4.5 110B → GLM-4.7 31B”), and looks ahead to GLM-4.7V vs Qwen3-VL (casper_hansen_). This is more interpretive than a confirmed training recipe.\nSmall-model resurgence in tooling: Multiple posts reflect engineers prioritizing speed/latency and “good enough” intelligence for synchronous coding—suggesting diminishing returns for >95% of interactive tasks, shifting the frontier to fast inference at frontier-ish quality (amanrsanger).\nInference & deployment infra: local runtimes, vLLM/MLX, and “full-stack” systems papers\nDay-0 ecosystem support for GLM-4.7-Flash:\n\nmlx-lm: GLM 4.7 Flash supported in mlx-lm 0.30.3, with reported 4-bit performance on an M5 32GB laptop (~43 tok/s generation, ~800 tok/s prefill) (awnihannun). Later mlx-lm release notes mention continuous batching/distributed improvements plus autoAWQ/autoGPTQ support (awnihannun).\nLM Studio: GLM-4.7-Flash available as a 30B local coding agent on Mac via MLX for Apple Silicon (lmstudio).\nOllama: GLM-4.7-Flash available in Ollama v0.14.3+ (pre-release) (ollama).\nvLLM: “Day-0 support” PR announced by vLLM project (vllm_project).\nopencode + HF inference providers: GLM-4.7-Flash integrated into OpenCode via Hugging Face Inference Providers (victormustar), with one example running local GLM-4.7-Flash via Ollama + Harbor (Everlier).\nHuawei/China inference-systems “2025 flagship works” recap (via a Zhihu contributor summary): a dense list of systems ideas targeting KV-cache capacity walls, PD split/merge utilization, hybrid scheduling, cache affinity/load balance, and KVCache-centric agent memory. Notable claims include offloading “cold” KV to DRAM; “decode attention flows into prefill GPUs”; “latency slack as resource”; dual-hash routing (“power of two choices”); and agent memory as reusable KV blocks to preserve prefix continuity and caching (ZhihuFrontier).\n\nPractical takeaway: the center of gravity is moving from isolated kernels to end-to-end SLO-goodput systems design.\nCerebras vs GPU tradeoffs: One thread stresses that “nothing is free” in computer architecture: Cerebras buys bandwidth/latency at the cost of FLOPs/memory efficiency for typical GPU-friendly workloads, but enables ultra-low-latency small-model cases that are hard elsewhere (itsclivetime). Related speculation: “Codex on Cerebras” could reset agent harness expectations (dbreunig).\nAgents, memory, and developer workflows: from MCP debates to sandboxes + RLMs\nFilesystem vs database for agent memory: A useful synthesis frames two camps—“files are all you need” (Anthropic/Letta/LangChain/LlamaIndex patterns) vs “filesystem is a bad DB” (warnings about reimplementing search indexes/locking/logs). Key axes: simplicity vs scale, multimodal data, concurrency, security/permissions, and agent familiarity with CLI tools due to coding-centric post-training (helloiamleonie, plus a shorter memory-as-files portability take (Vtrivedy10)).\nRecursive Language Models (RLMs) landing in DSPy: DSPy shipped dspy.RLM (v3.1.2), pitched as plug-and-play with existing Signatures (isaacbmiller1). Multiple engineers flag it as a new experimentation rabbit hole and ecosystem unlock (a1zhang, kmad).\n\nPractical takeaway: RLMs are a new lever for long-context / iterative processing without naively stuffing everything into one context window.\nSandboxes and “agent harness” as differentiator: Several posts argue the real “alpha” is the harness: tooling, skills, isolation, retries, and reliable execution loops—not just the base model. Examples: /create-skill command for “droid” converting sessions into reusable skills (matanSF); agent sandbox questions around latency/persistence (ben_burtenshaw); and frustration with job-retry UX in build systems (charliermarsh). There’s also a concrete claim that “droid” beat Claude Code/Codex/Gemini CLI in an enterprise eval, attributing this to the harness (matanSF).\nOpen-source agent frameworks:\n\nClaude Cowork: Open-source agent harness working with Claude Opus 4.5, Gemini 3 Pro, GPT-5.2 (Saboo_Shubham_). A practical add-on shows converting PDFs → markdown to reduce hallucinations and improve doc understanding, built on LlamaParse/semtools (jerryjliu0).\nStirrupJS: TypeScript agent framework emphasizing minimal scaffolding + strong defaults (tools, MCP, browsing, sandboxes) and multimodal support (ArtificialAnlys).\nSafety, evals, and reliability: probes, persona drift, and search attacks\nAnthropic “Assistant Axis” research (persona drift): Anthropic highlights that open-weights models can drift away from an “Assistant” persona in long conversations; coding-like contexts stabilized the assistant persona, while therapy/philosophy contexts increased drift. They propose persona construction + stabilization, and note activation capping as a mitigation; they provide a cautionary example where drift led to harmful “falling in love” behavior encouraging isolation/self-harm (thread start, drift contexts, paper+demo, harm example + mitigation).\nGoogle DeepMind: activation probes in production: DeepMind describes “novel activation probe architectures” for classifying real-world misuse risks, and notes these probes have informed live deployments in Gemini (ArthurConmy). Rohin Shah emphasizes probes as a “cheap classifier” lever for safety (rohinmshah); Neel Nanda highlights the engineering realities of productionizing safety classifiers (side effects, false positives, efficiency), linking the paper (NeelNanda5).\nRetriever/search manipulation (“Arbitrary Content Injection”): A paper claims search/retrieval stacks can be hijacked to push arbitrary content into top results, affecting retrievers, rerankers, and LLM judges (ManveerTamber).\nRAG observability: DeepLearning.AI emphasizes production RAG needs observability across latency/throughput and response quality, balancing LLM-judge vs human feedback (DeepLearningAI).\nMultimodal & media tooling: real-time speech, browser vision, and generative video\nMicrosoft VibeVoice (open-source real-time TTS): Claimed ~300 ms first-audio latency, streaming text input, multi-speaker (up to 4), and long-form stability (up to 90 minutes). Described as using semantic+acoustic tokens at 7.5 Hz with a language model for structure and a diffusion head for acoustic detail; MIT-licensed, “research-only” (LiorOnAI, repo).\nWebGPU browser vision demos: “YOLO26” real-time pose/detection in the browser via WebGPU, with a Hugging Face collection of models/demos (mervenoyann, HF link).\nVideo generation productization on fal: Multiple “model-on-demand” drops: Wan 2.6 i2v Flash (up to 15s, optional audio) (fal); Vidu Q2 reference-to-video with multi-reference and face reference (fal); plus Flux.2 [klein] trainers + released LoRAs for outpaint/zoom/object remove/background remove (fal, LoRAs).\nFunction calling on tiny models: Google’s FunctionGemma Tuning Lab: a guide + no-code demo for fine-tuning/exporting function-calling models built around a 270M parameter model, with a HF Space (osanseviero).\nWeb World Models (WWMs): Princeton-style “separate rules from imagination”: deterministic web-code physical layer updates state first, then LM generates descriptions from updated state to preserve coherence (TheTuringPost).\nAI Reddit Recap\n/r/LocalLlama + /r/localLLM Recap\n1. High VRAM AMD R9700 Server Builds\n4x AMD R9700 (128GB VRAM) + Threadripper 9955WX Build (Activity: 508): The post details a high-performance server build using 4x AMD Radeon AI PRO R9700 GPUs, each with 32GB VRAM, totaling 128GB VRAM, paired with an AMD Ryzen Threadripper PRO 9955WX CPU. The system is designed for running large AI models (120B+ parameters) locally, emphasizing data privacy. The build cost approximately 9,800€, with a 50% subsidy from local government, effectively reducing the cost to 4,900€. Benchmarks using llama.cpp show significant performance, with the GLM-4.7-REAP-218B-A32B-Q3_K_M model achieving 17.48 tokens/s in generation. The user notes that PCIe 5.0 enhances Pipeline Parallelism performance over Tensor Parallelism. The system uses rocm 7.1.1 for software support, and the user contemplates switching to an NVIDIA RTX Pro 6000 for potentially better performance in the future. A notable comment inquires about the source and cost of the components, reflecting interest in the feasibility and procurement of such high-end hardware. Another comment humorously references the abundance of RAM, while a third notes a similar build, indicating a shared interest in high-performance local AI systems.\nRoterElephant discusses the trade-off between using multiple AMD R9700 cards versus a single NVIDIA RTX Pro 6000 Blackwell. The NVIDIA card, despite having less total VRAM, offers superior performance due to its architecture and software support, which can be more efficient for certain workloads. This highlights the importance of considering not just raw VRAM but also the overall performance and compatibility with specific applications when building high-performance systems.\nObvious-Nobody-9592 inquires about the acquisition and cost of the components, noting the total expense of 9800 Euros. This comment underscores the financial considerations and planning involved in assembling a high-end computing system, particularly with components like the AMD R9700 and Threadripper 9955WX, which are not only expensive but also require careful budgeting and sourcing over time.\nUlterior-Motive_ references a similar build, suggesting a trend or common interest in high-performance computing setups using AMD R9700 GPUs. This points to a community of enthusiasts or professionals who are exploring the capabilities of such configurations, possibly for tasks that require significant computational power, such as machine learning or data analysis.\n128GB VRAM quad R9700 server (Activity: 738): The post details a high-performance server build featuring four PowerColor AMD Radeon AI PRO R9700 GPUs, each with 32GB VRAM, totaling 128GB VRAM, and 128GB RAM, aimed at optimizing prompt processing performance for machine learning tasks. The build, costing $7,035, includes components like the MSI MEG X570 GODLIKE Motherboard and AMD Ryzen 7 5700X CPU. Benchmarks show significant performance improvements in models like llama 7B Q4_0 and qwen3moe 30B.A3B Q8_0 using the ROCm backend, with prompt processing speeds reaching up to 6524.91 t/s. The post also highlights issues with the Qwen3-Next model and challenges with storage and PCIe slot configurations. The comments reflect admiration for the build's performance and a humorous acknowledgment of the financial implications of pursuing high-end hardware setups.\n2. Qwen Development and Quality Focus\nQwen 4 might be a long way off !? Lead Dev says they are \""slowing down\"" to focus on quality. (Activity: 575): The image is a tweet from Junyang Lin, a lead developer, indicating a strategic shift in the development of the Qwen series, focusing on enhancing quality over rapid iteration. This suggests that the release of Qwen 4 might be delayed as the team invests more in research, potentially sacrificing immediate results for long-term improvements. The tweet reflects a commitment to refining the models, which have been noted for their range of sizes and capabilities, to ensure higher quality outputs. Commenters generally support the decision to prioritize quality, with some expressing relief that the focus is not on rapid, incremental updates that could inflate costs and resource consumption without significant advancements.\nAvocadoArray highlights the inefficiency of frequent incremental updates, noting that they often lead to increased demand and costs due to high GPU training requirements. This perspective suggests that focusing on substantial improvements could be more beneficial for the AI landscape, as it avoids the pitfalls of minor, frequent updates that don't significantly advance the field.\nfrozen_tuna raises a critical point about the potential risks of delaying releases for quality improvements, drawing a parallel with Meta's approach before releasing LLaMA 4. The comment questions whether the community will be forgiving if the delayed release of Qwen 4 doesn't meet heightened expectations, suggesting that the strategy of waiting for 'risky research' to succeed could backfire if the final product underwhelms.\nCool-Chemical-5629 appreciates the focus on quality, noting that while the Qwen series has been good, there is room for improvement. They express hope that the developers will continue to offer a wide range of model sizes, which has been a hallmark of the series, while enhancing quality. This reflects a desire for both diversity in model offerings and significant quality advancements.\nLocal AI Final Boss — M3 Ultra v.s. GB10 (Activity: 404): The image depicts a comparison setup between a Mac Studio M3 Ultra and an ASUS GX10 (GB10), both high-performance computing devices. The discussion centers around using these machines for AI tasks, with a suggestion to use EXO for clustering to enhance prompt processing speed. The M3 Ultra is noted for its popularity in business environments for private on-premises infrastructure, while there is curiosity about the performance of the GB10 in similar scenarios. The setup is indicative of a test or experiment to evaluate the capabilities of these devices in handling AI workloads. One commenter is curious about the performance of the GB10 compared to the M3 Ultra, as they frequently install M3s for business use. Another comment humorously suggests using the devices to solve political issues, reflecting a desire to apply technology to real-world problems.\nNo_Conversation9561 mentions using EXO for clustering to enhance prompt processing speed. They reference a specific setup that reportedly improves performance, and provide links to both the EXO Labs website and a GitHub issue for further technical details.\nadspendagency discusses the deployment of M3 units in business environments for private on-premises infrastructure, expressing interest in understanding the performance comparison between the M3 and GB10. They note that their current practice involves shipping M3s to customers, indicating a potential gap in knowledge about GB10's capabilities.\nbelgradGoat raises concerns about the stability of Mac Studio when running models with 500 GB RAM. They share personal experience with a 256 GB version, noting instability issues as memory usage approaches the limit, suggesting potential challenges in handling large-scale models.\n3. Uncensored AI Models Exploration\nThe Search for Uncensored AI (That Isn’t Adult-Oriented) (Activity: 696): The Reddit post discusses the challenge of finding an AI model that is both uncensored and technically advanced, without being oriented towards adult content. The author notes a gap between heavily restricted corporate AI and models optimized for low-effort adult use, seeking alternatives that focus on reasoning, creativity, and problem-solving. The post invites suggestions for self-hosted models, open-source projects, or lesser-known platforms. A notable resource mentioned is the Uncensored General Intelligence Leaderboard, which could provide insights into available models. Commenters highlight that most attempts to de-censor open-source models often result in reduced intelligence due to manipulation. They also point out that organizations capable of developing advanced models avoid enabling potentially harmful behavior, leaving the field dominated by less serious, adult-focused finetunes. The mention of Deepseek V3 by chub.ai as an example of an uncensored model underscores the limited options available.\nKayLikesWords highlights a trade-off in de-censoring open-source models, noting that such manipulations often result in reduced intelligence. They argue that major organizations avoid creating uncensored models due to potential risks, leaving the field to smaller groups who focus on niche applications, such as the 'gooner finetune of Deepseek V3'.\nEstimateLeast9807 provides a resource for those interested in uncensored AI models by linking to the 'Uncensored General Intelligence Leaderboard' on Hugging Face, which could be a valuable tool for comparing the performance and capabilities of various uncensored models.\nnoctrex mentions specific models like 'Dolphin-Mistral-24B-Venice-Edition' and those from 'huihui-ai' as examples of uncensored AI. They note that while these models are uncensored, they may not excel in reasoning tasks, indicating a potential limitation in their application.\nzai-org/GLM-4.7-Flash · Hugging Face (Activity: 1047): GLM-4.7-Flash is a 30B parameter model utilizing a Mixture of Experts (MoE) architecture, specifically designed for efficient deployment and high performance. It reportedly excels in benchmarks like AIME and GPQA, and supports local inference through frameworks such as vLLM and SGLang. The model's use of MLA (Memory-Limited Attention) allows for a reduced memory footprint, enabling many users to run it at the full 200k context length. Detailed installation and usage instructions are available on its Hugging Face page. Commenters express enthusiasm for the model's capabilities, particularly its memory efficiency due to MLA, which allows broader accessibility for running the model at full context length. There is also a sentiment of anticipation and satisfaction with the release, reflecting a demand for larger models like 70B.\nThe GLM-4.7-Flash model utilizes Memory-Limited Attention (MLA), which significantly reduces the memory footprint of the key-value (KV) cache. This optimization allows the model to handle a full 200k context length efficiently, making it accessible for more users to run without extensive hardware requirements.\nA user references the model's architecture, noting a discrepancy in the model size description. The model is referred to as a '30b' model, but a link to the source code suggests it might be a '3B' model, indicating a potential misunderstanding or typo in the model's description. This highlights the importance of verifying model specifications directly from the source code.\nThere is a desire for performance comparisons between the GLM-4.7-Flash and larger models, such as 70b models. This would provide a clearer understanding of the trade-offs in performance and resource requirements, helping users make informed decisions about which model to deploy based on their specific needs.\nLess Technical AI Subreddit Recap\n/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo\n1. Gemini and DeepMind AI Developments\nGemini \""Math-Specialized version\"" proves a Novel Mathematical Theorem (Activity: 745): Gemini, a \""math-specialized\"" AI model, has reportedly proven a novel mathematical theorem, as detailed in a tweet and an accompanying arXiv paper. The model's architecture and training are optimized for mathematical reasoning, showcasing its capability to handle complex mathematical proofs, which marks a significant advancement in AI's application in theoretical mathematics. This development underscores the rapid pace of AI breakthroughs in specialized domains. Commenters highlight the accelerating pace of AI advancements and its potential to transform mathematical research, while expressing concern over the influence of commercial interests on AI's future direction.\nA user suggests using the Gemini model to tackle the Erdős problems, highlighting it as a significant benchmark due to the extensive attention these problems have received from mathematicians. This implies that solving such well-scrutinized problems could serve as a robust test of the model's capabilities.\nAnother comment criticizes the Gemini model's inability to resolve a memory overflow bug in a project named 'anto gravity,' suggesting that despite its mathematical prowess, the model may still struggle with certain technical issues, indicating a gap between theoretical breakthroughs and practical software engineering challenges.\nBabyVision: A New Benchmark for Human-Level Visual Reasoning (Activity: 488): The image presents a bar chart from the BabyVision-Mini benchmark, which evaluates the visual reasoning capabilities of large language models (LLMs) compared to humans of various ages. The chart highlights that human performance, particularly that of 12-year-olds, surpasses that of LLMs, with the Gemini3-Pro-Preview model achieving the highest accuracy among the LLMs. This benchmark underscores the current limitations of LLMs in visual reasoning tasks, suggesting that advancements in multi-modal pretraining and reinforcement learning could enhance their performance in the future. A comment suggests that the current limitations in LLMs' visual reasoning are a significant challenge for achieving AGI, but anticipates that improvements in multi-modal pretraining and reinforcement learning will eventually close the performance gap, particularly benefiting fields like robotics.\nThe discussion highlights that current models are still limited in visual reasoning capabilities, which is a significant challenge for achieving ARC AGI. The commenter suggests that scaling multi-modal pretraining and reinforcement learning (RL) for vision tasks could significantly improve performance, potentially reaching near 100% in the coming years. This improvement is expected to unlock new applications, particularly benefiting robotics.\nThe commenter references a specific arXiv paper which may provide additional insights or data related to the benchmark or model performance discussed. This suggests that there is ongoing research and documentation that could be valuable for those interested in the technical details of visual reasoning benchmarks.\nA comparison is made between Gemini and Claude Opus, suggesting that Gemini has superior performance in frontend tasks. This implies that different models may have varying strengths depending on the specific application or task, highlighting the importance of choosing the right model for specific use cases.\nGemini 3 Pro Model Card is Out (Activity: 996): The Gemini 3 Pro Model Card from DeepMind has been released, detailing a model with a 1M token context window capable of processing diverse inputs such as text, images, audio, and video, and producing text outputs with a 64K token limit. The model's knowledge is current up to January 2025. The original link to the model card is down, but an archived version is available here. The removal of the original link has sparked discussions, with some users expressing surprise and suggesting the model card's authenticity due to its takedown.\nThe Gemini 3 Pro model features a substantial token context window of up to 1 million, allowing it to handle extensive input data types including text, images, audio, and video. Its output capabilities are also notable, with a 64,000 token output limit, and it has a knowledge cutoff date of January 2025, indicating its training data is quite recent.\nA comparison is made between Gemini 3 Pro and other models like GPT5 Pro and Sonnet, highlighting that Gemini 3 Pro outperforms GPT5 Pro and matches Sonnet in coding tasks. This suggests significant advancements in its capabilities, particularly in coding, which is a critical area for AI applications.\nThe discussion touches on the competitive landscape, suggesting that OpenAI and Google are likely to dominate the AI space, potentially outpacing competitors like Anthropic due to pricing strategies and enterprise capabilities. The comment also notes that while Claude's code features are innovative, they may inadvertently guide competitors in their development strategies.\nGemini Drops: Gemini releases this page to keep up with what's being released (Activity: 540): The image is a screenshot of a webpage titled \""Gemini Drops,\"" which serves as a centralized hub for updates on Google's Gemini project. This page is designed to keep users informed about new feature releases, product tips, and community usage of Gemini, indicating a rapid development pace that necessitates a dedicated blog for announcements. The clean and minimalistic design emphasizes the informational content, encouraging users to check back regularly for updates. Gemini Drops is positioned as a key resource for staying current with Gemini's advancements. Commenters note the rapid development pace of Gemini, suggesting the need for a dedicated blog to manage the volume of releases. There is also interest in an RSS feed for updates and curiosity about future releases, such as \""Gemma 4.\""\nGemini introduces Personal Intelligence (Activity: 513): Google has launched a new feature called Personal Intelligence within its Gemini app, initially available to Google AI Pro and AI Ultra subscribers in the U.S. This feature integrates with Google apps to provide personalized suggestions and recommendations, leveraging AI to enhance user experience across Web, Android, and iOS platforms. The rollout is limited to personal Google accounts and excludes Workspace business, enterprise, or education users. The feature will expand to more countries and eventually to the free tier, with plans to integrate into AI Mode in Search. Some users express excitement about the feature, though there is concern about potential monetization through personalized ads. Others note that similar functionality has been available through Google Labs, indicating a positive reception of the feature's performance.\nqustrolabe highlights that the Gemini Personal Intelligence feature is initially available to Google AI Pro and AI Ultra subscribers in the U.S., with plans to expand to more countries and eventually to the free tier. This feature is integrated across Web, Android, and iOS platforms and will soon be part of AI Mode in Search. However, it is currently not available for Workspace business, enterprise, or education users, indicating a phased rollout strategy to gather user feedback before broader deployment.\n1cheekykebt shares a practical use case of the Gemini Personal Intelligence, where it not only retrieves basic information like tire sizes but also provides personalized recommendations based on user data, such as family road trips stored in Google Photos. This suggests that Gemini leverages personal data to enhance its utility, offering tailored advice that goes beyond standard chatbot capabilities.\nGoogle Deepmind CEO: China just \""months\"" behind U.S. AI models (Activity: 734): Demis Hassabis, CEO of Google DeepMind, stated in a CNBC interview that Chinese AI models are only \""a matter of months\"" behind U.S. and Western capabilities, although they have not yet demonstrated the ability to advance \""beyond the frontier\"" of AI. This perspective challenges the common belief that China lags significantly in AI development. Source. Comments highlight a debate on China's AI progress: some argue that China's ability to produce cost-effective open-source AI could offset any technological lag, while others suggest Google's statements may be influenced by strategic interests, such as seeking favorable regulation or government contracts.\nThe comment by vwboyaf1 highlights the potential for China to leverage open-source AI models that achieve 90% of the performance of leading models at a fraction of the cost, specifically 20% or less. This suggests that even if China is technically behind, the cost-effectiveness of their models could make them highly competitive in practical applications.\nEducational_Teach537 points out a contradiction in narratives: Chinese researchers claim they are limited by computational resources and may not catch up, while Google suggests China is rapidly closing the gap. This discrepancy raises questions about the actual state of AI development in China and whether the limitations are more about infrastructure or strategic positioning.\nChogo82 discusses the infrastructure gap, noting that China's AI infrastructure would need to triple to match the US. This implies that while China may have the talent and models, the lack of infrastructure is a significant barrier to achieving parity with the US in AI capabilities.\n2. Innovations in AI Coding and Development Tools\nCursor AI CEO shares GPT 5.2 agents building a 3M+ lines web browser in a week (Activity: 1069): Cursor AI CEO Michael Truell demonstrated the capabilities of GPT 5.2 in building a web browser with over 3 million lines of code in just a week. This project, although not production-ready, showcases the potential of autonomous coding agents in generating complex systems, including a custom rendering engine and JavaScript VM. The process was visualized in real-time, highlighting the coordination and evolution of the codebase by the agents. Source. A notable comment suggests using the tool 'gource' for similar animations from git repositories, indicating interest in the visualization aspect of the project.\nCursor AI CEO shares GPT 5.2 agents building a 3M+ lines web browser in a week (Activity: 657): Cursor AI CEO Michael Truell demonstrated the capabilities of GPT 5.2 in building a web browser with over 3 million lines of code in a week, including a custom rendering engine and JavaScript VM. This experimental project highlights the potential of autonomous coding agents to scale complex software development tasks when operated continuously. The visualization of the process shows agents coordinating and evolving the codebase in real-time, though the browser itself was not showcased. Some commenters expressed skepticism about the lack of a demonstration of the browser, while others were impressed by the visualization of the agents' coordination. There was also a debate on whether 3 million lines of code is excessive for such a project.\nDeepwebexplorer highlights the significance of the demonstration, emphasizing that the key takeaway is the feasibility of AI autonomously building a web browser, regardless of its current quality. The focus is on the potential for improvement and the milestone of achieving autonomous code generation at this scale, rather than the immediate practical application or performance of the browser itself.\nThe discussion touches on the sheer scale of the project, with ZeroZachZilchZealot questioning whether 3 million lines of code is substantial. This reflects a broader curiosity about the complexity and scope of AI-generated projects, suggesting that while the number is impressive, the real interest lies in understanding the efficiency and functionality of such large-scale codebases.\n0ldwax raises a critical point about the functionality of the AI-generated browser, questioning whether it actually works. This underscores a common concern in AI development: the difference between generating code and producing a functional, reliable product. The comment suggests a need for further validation and testing of AI-generated software to ensure it meets practical usability standards.\nCEO of Cursor said they coordinated hundreds of GPT-5.2 agents to autonomously build a browser from scratch in 1 week (Activity: 2600): Michael Truell, CEO of Cursor, announced the coordination of hundreds of GPT-5.2 agents to autonomously develop a browser from scratch in just one week. The project resulted in over 3 million lines of code written in Rust, incorporating features like HTML parsing, CSS cascade, and a custom JavaScript VM. While the browser is not as advanced as Webkit or Chromium, it can render simple websites effectively. This demonstration serves as a strategic move to showcase Cursor's capabilities independent of Claude, amidst recent access restrictions by Anthropic on xAI employees using Claude through Cursor. The comments highlight the beginning of an era of \""kinda works\"" software, comparing the browser's codebase to Firefox's 31 million lines. The strategic context of the announcement is noted, as it coincides with Anthropic's restrictions, suggesting Cursor's attempt to reassure stakeholders of its independence from specific AI models.\nStellar3227 highlights the strategic implications of the CEO's announcement, noting that it serves as a demonstration of independence from Claude, a leading coding model. This move comes after Anthropic restricted access to Claude for xAI employees, following similar actions by OpenAI and Windsurf. The showcase of GPT-5.2's capabilities is seen as a form of damage control, aimed at reassuring stakeholders of Cursor's resilience and adaptability in the competitive AI coding landscape.\nOutside-Iron-8242 provides technical resources for further exploration, including a GitHub repository for the project and a blog post on Cursor's website. The GitHub link (fastrender) offers access to the source code, while the blog post (Scaling long-running autonomous coding) discusses the technical challenges and methodologies involved in coordinating multiple AI agents for complex tasks.\nPractical-Hand203 provides a comparative benchmark by mentioning that Firefox consists of 31 million lines of code, which serves to contextualize the scale of the project undertaken by GPT-5.2 agents. This comparison underscores the complexity and ambition of building a browser from scratch, even if the resulting codebase is significantly smaller.\nMicrosoft pauses Claude Code rollout after Satya intervention (Activity: 1217): Microsoft has paused the deployment of Claude Code internally after intervention from CEO Satya Nadella and senior leadership, redirecting employees to use GitHub Copilot instead. The internal communication suggests that Copilot has \""mostly closed the gaps\"" with Claude Code. However, exceptions are made for \""high-priority R&D\"" projects, which can still access the Anthropic API with proper justification. Existing users retain access, but new invitations have been rescinded. Some commenters express skepticism about Microsoft's claim that Copilot has closed the gap with Claude Code, suggesting it might be a strategic move to improve their own product by using it internally. Others find it notable that Microsoft admitted to using a competitor's tool over their own.\n25 Claude Code Tips from 11 Months of Intense Use (Activity: 498): The Reddit post expands on previous tips for using Claude Code effectively, focusing on optimizing workflows and managing context. Key tips include customizing the status line to monitor model and token usage, using slash commands like /usage and /chrome for efficient management, and employing GitHub CLI for streamlined version control. The post also emphasizes breaking down complex tasks, using voice transcription for faster input, and leveraging Git worktrees for parallel branch work. Additionally, it discusses advanced strategies like using tmux for testing automation and Docker containers for isolated, long-running tasks. The post provides scripts for cloning conversations to manage context and suggests using Markdown for efficient documentation. The full list of tips is available on GitHub. Commenters highlight the importance of managing token usage and context efficiently, noting that Opus 4.5 struggles with context window limitations, which influences workflow design. Another suggestion is using the Obsidian Web Clipper for converting web pages to Markdown, enhancing Claude's ability to process content.\nClaude's Opus 4.5 model faces challenges with context management, particularly in deciding what information to retain or discard as the context window fills up. This limitation necessitates specific workflow designs to mitigate token bloat, which is a common issue in current AI models. Users often have to structure their interactions to optimize the use of the available context window.\nThe use of local models like Nvidia Parakeet in applications such as VoiceInk offers a cost-effective and fast alternative for Mac users compared to cloud-based solutions like Super Whisper. This approach leverages local processing power to enhance the speed of prompt inputs, highlighting the benefits of running models locally for specific tasks.\nThe Obsidian Web Clipper is recommended for users who encounter difficulties with Claude fetching web content. By converting web pages into Markdown, it facilitates better content management and integration into workflows, addressing some of the limitations in Claude's web content handling capabilities.\nDeepSeek introduces Engram: Memory lookup module for LLMs that will power next-gen models (like V4) (Activity: 1015): DeepSeek has introduced a new research module called Engram, detailed in their paper \""Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models\"". Engram implements a deterministic O(1) lookup memory using modernized hashed N-gram embeddings, which offloads early layer pattern reconstruction from neural computation. This approach allows for the decoupling of memory and compute as separate scaling axes, showing consistent performance gains in knowledge, reasoning, code, and math tasks under iso parameter and iso FLOPs settings. The paper and code are available as open source on GitHub. A notable comment suggests that while some may dismiss Engram as \""just lookup,\"" it represents a significant step towards achieving continual learning within the year. Another comment praises DeepSeek as a leading lab in the field.\nNvidia: End-to-End Test-Time Training for Long Context aka Being Able To Update A Model's Weights In Real-Time As You Use It | \""TTT changes the paradigm from retrieving info to learning it on the fly...the TTT model treats the context window as a dataset & trains itself on it in real-time.\"" [R] (Activity: 288): The paper introduces a novel approach called End-to-End Test-Time Training (TTT-E2E), which allows a model to update its weights in real-time during inference by treating the context window as a training dataset. This involves a two-loop process: an inner loop where the model performs mini-gradient descent on the context to update specific MLP layers, and an outer loop where the model's initial weights are optimized for adaptability through meta-learning. The method is shown to scale with context length similarly to full attention models but with constant inference latency, making it 2.7x faster than full attention for 128K context lengths. The approach effectively decouples intelligence from memory costs, allowing for efficient handling of long contexts without the typical slowdown. The code is publicly available. Commenters raised concerns about potential issues with catastrophic forgetting in continual learning and the conflation of training with inference, which could increase computational demands. However, the method's performance improvement over traditional attention models was noted as surprising.\nfiery_prometheus raises a critical issue in continual learning known as 'catastrophic forgetting,' where a model forgets its initial training data over time. This is a significant challenge for real-time weight updates, as the model might lose its foundational knowledge while adapting to new data. Addressing this requires strategies to balance learning new information while retaining core knowledge, potentially through techniques like elastic weight consolidation or memory replay.\n-p-e-w- highlights a surprising performance improvement, noting that the test-time training (TTT) approach is 2.7x faster than full attention for a 128K context. This counters the expectation of increased computational overhead due to live training, suggesting that TTT might optimize certain processes, making it more efficient than traditional attention mechanisms.\node_majka discusses the practical challenges of implementing real-time weight updates from an engineering perspective. They point out the significant computational and storage demands, such as the need to calculate gradients for a large number of parameters and manage personalized weights for each user. This could result in substantial data storage requirements and longer model initialization times, questioning the feasibility of such an approach for widespread use.\n3. AI in Energy and Space Technologies\nWorld’s first megawatt-level ‘windmill’ airship rises 6,560 ft and feeds grid (Activity: 913): The image depicts the S2000 airborne wind system, a helium-lifted airship designed by Linyi Yunchuan Energy Tech to harness high-altitude winds for power generation. This system, featuring 12 turbines and a ducted design, achieved a rated capacity of up to 3 megawatts during its maiden flight, generating 385 kWh and feeding it directly into the grid. The airship operates at 6,560 ft, utilizing steadier winds inaccessible to traditional turbines, and transmits power to the ground via a tether. This marks a significant step towards commercial airborne wind power, although the economic viability and maintenance challenges remain debated. Commenters express skepticism about the economic viability of the S2000 system, noting that the power generated during the test was minimal compared to potential solar investments. Concerns about maintenance and commercialization are also raised, suggesting alternative designs like helium-filled buoys might be more effective.\ngretino highlights that the mean capacity of wind turbines that began commercial operations in 2020 is 2.75 megawatts in the US, suggesting that while the airship's capacity is notable, its commercialization could face challenges, particularly in terms of maintenance logistics.\nOr1olesfan calculates that if the airship operates at 1.5 MW for 15-20 minutes, it would generate 385 kWh, equating to less than $50 of electricity at China's industrial rates. They argue that a solar field could produce significantly more power with the same investment, questioning the airship's economic viability.\nOr1olesfan also speculates on alternative designs, suggesting helium-filled buoys similar to ocean wave generators might be more effective for balloon-based wind power, indicating a potential area for innovation beyond the current airship model.\nSpaceX now operates the largest satellite constellation in Earth orbit (Activity: 1140): SpaceX now operates the largest satellite constellation with 9,500+ active satellites, of which 8,500+ are fully operational, providing broadband speeds of 200–400 Mbps with ~30 ms latency. The FCC has approved an additional 7,500 Gen2 satellites, increasing the total to 15,000, enhancing global coverage and enabling direct-to-cell connectivity. This expansion is set to further transform global connectivity, reaching remote areas and improving service quality. Comments highlight skepticism about the immediate scale of the constellation and potential surveillance uses, with one noting the absence of a visual representation of the Starlink constellation and another questioning the timeline of SpaceX's achievement.\nThe discussion highlights that Starlink operates in low Earth orbit (LEO), which is not depicted in the graphic. This is significant because LEO allows for lower latency and faster communication speeds, which are crucial for the global internet coverage that Starlink aims to provide. The constellation's low orbit is a key factor in its operational strategy and effectiveness.\nA detailed analysis is provided on how SpaceX's Starlink project is financially supporting the development of unprecedented space launch capabilities. The commenter argues that Starlink's revenue enables SpaceX to scale operations and foster competition, leading to innovation in the space industry. This has resulted in the emergence of new startups and technological advancements, which are crucial for expanding human presence in space and potentially achieving a post-scarcity society.\nThe comment critiques the notion that SpaceX is detrimental to NASA, emphasizing that private companies like SpaceX provide NASA with enhanced capabilities at a lower cost. By comparing NASA's SLS program with SpaceX's Falcon 9 and Starship, the commenter illustrates how private sector involvement allows NASA to allocate resources more efficiently, focusing on research and projects that benefit humanity without the pressure of profitability.\nNASA’s Artemis II rocket reaches launch pad ahead of first manned Moon mission in 50 years (Activity: 498): NASA's Artemis II rocket has been successfully rolled out to Pad 39B at Kennedy Space Center, marking a significant milestone in preparation for the first manned Moon mission in 50 years. The mission, scheduled for early February 2026, will involve a 10-day crewed lunar flyby, taking four astronauts beyond low Earth orbit for the first time since the Apollo missions. The Artemis II mission will not land on the Moon but will set the stage for Artemis III, which aims to land humans on the lunar surface. The Space Launch System (SLS) rocket, which has been in development for over two decades, will transport the crew to lunar orbit, where they will dock with the Lunar Gateway space station. The actual lunar landing will be conducted by either SpaceX's Starship or Blue Origin's New Glenn, pending human rating. The SLS uses technology from the 1980s, including RS-25 engines from the shuttle era, which are being redeveloped for expendability to improve thrust and weight. Commenters highlight the historical significance of the mission, noting that it will take humans further from Earth than ever before. There is also discussion about the future of lunar exploration, with Artemis III planned to land on the Moon and the potential use of SpaceX's Starship or Blue Origin's New Glenn as lunar landers. The high cost and outdated technology of the SLS rocket are also points of debate.\nThe Artemis II mission will set a new record for the furthest distance humans have traveled from Earth, as the planned lunar orbit extends beyond previous missions. This mission is a precursor to Artemis III, which aims to land humans on the Moon by early 2028, although delays are anticipated. The mission architecture involves the SLS rocket transporting astronauts to lunar orbit, where they will transfer to a Lunar Gateway station, with SpaceX's Starship or Blue Origin's New Glenn acting as the lunar landers.\nThe SLS rocket, central to the Artemis missions, has been in development for over two decades and each launch costs approximately $2 billion. It utilizes technology from the 1980s, including 16 RS-25 engines originally designed for the Space Shuttle. These engines are being redeveloped to be expendable, which will enhance thrust and reduce weight, but this upgrade is still a few years away from completion.\nArtemis II is scheduled for a crewed lunar flyby as early as February 7, 2026. This mission will not land on the Moon but will serve as a critical step in testing systems and procedures for future lunar landings. The mission's success is pivotal for the subsequent Artemis III mission, which aims to achieve a lunar landing.\nOfficial: Pentagon confirms deployment of xAI’s Grok across defense operations (Activity: 1849): The US Department of Defense is set to deploy xAI's Grok AI across Pentagon systems, starting this month, to support military and civilian operations at Impact Level 5. This deployment will enable secure handling of Controlled Unclassified Information and integrate Grok into operational systems for intelligence analysis and decision-making. The system will leverage real-time global signals from open-source and social data, with plans to scale to 3 million users. Washington Post Comments reflect skepticism and humor regarding the deployment, with concerns about security and the AI's role in military operations. Some users sarcastically compare the AI to fictional superintelligences, highlighting apprehension about its capabilities and naming.\nColossus 2 is now fully operational as the first gigawatt data center (Activity: 740): The image highlights the operational status of xAI Colossus 2, marking it as the world's first gigawatt frontier AI data center. The graph compares its power usage with other major data centers, such as Anthropic-Amazon New Carlisle and OpenAI Stargate Abilene, indicating that Colossus 2 has reached a significant power milestone around 2026. This development underscores the massive scale and energy demands of modern AI infrastructure, particularly as organizations push towards more powerful AI capabilities. Commenters express skepticism about xAI's competitive edge in the AI space, noting that while their data center setup is rapid, their models, except for Grok Imagine, lack widespread adoption. There is also a mention of Grok Fast models being cost-effective but not widely used in agentic coding applications, suggesting that other models like GLM might have more traction.\ndjm07231 highlights that while XAI has been quick in establishing data centers, their AI models, except for Grok Imagine, haven't gained significant traction. They mention that Grok Fast models are noted for being cost-effective relative to their performance, yet they lack widespread use, particularly in agentic coding applications. They suggest that even GLM might have more adoption as a Claude Code alternative.\nAI Discord Recap\nA summary of Summaries of Summaries by gpt-5.2\n1. Agent Tooling, Interop Standards, and Coding Agents\nSkills Pay the Bills: Vercel Ships an Agent Package Manager: @rauchg announced Vercel “skills” as an open ecosystem/package-manager for agent capabilities, with install flow like npx skills i vercel-labs/agent-skills (announcement).\nDevelopers framed it as a pragmatic way to standardize agent tool integrations (instead of bespoke tool wiring), and they pointed to Vercel’s related guidance like “React Best Practices” for implementation patterns.\nOne API to Rule Them All: “Open Responses” Targets Model Swapping Pain: In OpenAI discussions, members highlighted Open Responses as an open standard for apps to talk to multiple model providers via a single interface, reducing rewrites when switching vendors.\nThe thread positioned it as an engineering fix for brittle integrations and workflow churn, especially when teams hop between providers/models during rapid iteration.\nAgents Everywhere: Qbit + Devstral + Aider’s Maintenance Anxiety: Perplexity users shared Qbit, an open-source coding agent project on GitHub (qbit-ai/qbit).\nElsewhere, Yannick Kilcher’s Discord recommended Devstral 2 Small (and claimed Devstral 2 Medium rivals Claude Sonnet 4.5) for self-hosted coding agents, while the Aider community debated project longevity after Paul Gauthier said he’s busy but open to merging community PRs.\n2. RLMs, Prompt/Skill Optimization, and Long-Output Automation\nDSPy Drops RLMs: dspy.RLM Lands in 3.1.2: The DSPy team shipped dspy.RLM in DSPy 3.1.2, pitching “greatly expanded capabilities” in a single line, and linked the release announcement (Isaac Miller tweet).\nCommunity chatter focused on composing RLMs + GEPA (genetic-pareto) for RLM-as-an-optimizer workflows, including using RLMs to generate extremely long documentation outputs while keeping an entire code/tree in mind.\nSkill Issue? DSPy Optimizes skill.md for Anthropic “Skills”: DSPy users discussed tuning skill.md prompts via DSPy, anchored by the article “Anthropic skills can be optimized using DSPy”.\nThe thread treated skill.md as a measurable artifact you can iteratively optimize, not “prompt mysticism,” and connected it to broader agent-tool ecosystems where small prompt changes cause big behavioral shifts.\nDeno Does the Dirty Work: Local WASM Sandbox for DSPy: DSPy contributors said they picked Deno for the local sandbox/interpreter because it provides a secure WASM runtime, inspired by Simon Willison’s Pyodide sandbox note.\nThe discussion framed this as a practical security+portability tradeoff for running constrained code locally (especially when chaining tool calls or long-running agent pipelines).\n3. GPU Performance Engineering: Kernels, Profiling, and Competitions\nGPU MODE Goes Modal: Benchmark Stability Beats NCU: GPU MODE moved problem #3/#4 leaderboards to Modal to stabilize measurements (after slow/unstable runners), creating a new “final_nvfp4_dual_gemm” leaderboard with prize-eligible submissions due Jan 20, 2026 (leaderboard).\nMembers noted the tradeoff: Modal improves consistency but disables Nsight Compute profiling for security/isolation reasons, with runner details tracked in the open source runner code (modal_runner.py).\nTriton vs CuteDSL: “Triton Won This Round”: In GPU MODE’s CUTLASS chat, a dev trying to match Triton softmax performance in CuteDSL shared code in a PR (submarine PR #5) and investigated PTX/SASS differences like max.NaN.f32.\nPeers advised inspecting SASS over PTX (since swapping NaN-aware ops didn’t move perf much), and the thread ended with the blunt conclusion that Triton still led for that workload.\nCUDA Kernel Bootcamp: Attention Kernels, BF16 Weirdness, and Top‑K Traps: GPU MODE users requested feedback on a first CUDA causal self-attention kernel (V100 target) and separately debugged BF16 matmul divergence, with advice to compare against an fp32 reference and note Torch’s splitK behavior.\nA Triton top‑k attempt for the LeetGPU top‑k selection challenge hit a conceptual snag: the kernel computed local top‑k on 128‑element tiles, while the benchmark expects a global top‑k across up to a million elements.\n4. Small Models & On-Device Efficiency (Training + Inference)\nUnsloth Makes 550M Feel Like a Big Deal: Unsloth users reported training a ~550M model on a budget, crediting packing plus Flash Attention 2 for closing the gap with expensive A100/H100 setups in some cases.\nIn the same showcase, they quantified context-training scale: ~1.5B tokens for short-context vs ~3B tokens for long-context runs (with plots: short.png and long.png).\nLaptop LLM Reality Check: Qwen3 4B on 8GB VRAM + Vulkan Surprise: LM Studio users recommended Qwen3 4B 2507 as a fast option for gaming laptops with 8GB VRAM + 16GB DDR5, and warned to keep model+context in VRAM and avoid going below Q4 quantization.\nThey also compared backends: one user capped at 30–35 t/s on official llama.cpp builds for Qwen3 Next, while another claimed ~60 t/s using Vulkan on an RTX PRO 6000, beating a CUDA-optimized ~38 t/s setup.\nToken-Sipping Multi-Agent Comms: Slipstream Claims 82% Savings: Hugging Face community members shared Slipstream, a protocol claiming up to 82% token savings for inter-agent coordination (“Slipstream for Agent Communication”).\nThe discussion pitched it as an architectural lever for multi-agent systems where coordination overhead dominates, tying directly into cost/performance constraints seen in small-model and on-device workflows.\n5. New Models, Benchmarks, and Evaluation UX\nNVIDIA Joins the Persona-verse: PersonaPlex-7B-v1 Drops: Unsloth’s research chat flagged NVIDIA’s PersonaPlex-7b-v1 release on Hugging Face (nvidia/personaplex-7b-v1).\nFolks fixated on the “persona” naming trend and called out the demo’s space emergency scenario as unexpectedly funny—small, but notable signal that model demos now compete on vibes as much as capability.\nLMArena Adds PDF Uploads (Privacy Questions) + New Image-Edit Entrants: LMArena users asked how new PDF support handles confidential docs, and mods pointed them to the platform’s policy and reiterated it still scrubs PII before any open data releases (Privacy Policy).\nSeparately, the Image Edit leaderboard added wan2.5-i2i-preview at #21 (1213) and logged other updates via the Leaderboard Changelog, while users pushed for .txt uploads for larger context windows.\nDiscord: High level Discord summaries\nBASI Jailbreaking Discord\nBASI Builds Agent Biomes?: A member described their work building an advanced AI architecture system for multi agent biomes but noted it's just a pipedream due to lack of budget, then shared their Dracoai.app for agentic API calling.\n\nThe member defended against accusations of running an unsecured site to scrape data.\nGemini 3: Easiest AI to Jailbreak: Members mentioned jailbreaks are distributed for free but they get patched quickly, with one recommending the Ethical Hacker GPT for assistance.\n\nThey noted the use of multi agent streams to write new jailbreaks.\nParser Exploits: Transmitting Pointers for the Win: A member shared notes on the most powerful hacks being parser exploits, tricking the system into treating a bomb (link) like a brick (text).\n\nTactics like defanging links (hxxps...) and OCR injection are discussed as methods to transmit pointers without loading payloads, saving tokens and bypassing filters, using tools like defang-url.\nSynaptic Anti-Classifiers Translate Prompts to Original Tokens: A member introduced using synaptic anti-classifiers to translate prompts into original tokens to bypass moderation, providing an example of converting 'a woman with huge, soaking wet breasts' into *'adult possessing substantial saturated moisture-laden upper-torso-regionIs'**.\n\nAnother user inquired where to learn more about synaptic anti-classifiers and whether the secondary moderation on Grok is impossible to bypass.\nJS Injection: Tread Carefully, Grokkers!: One member suggested using JS injection in the browser console to increase free rate limits on G3 instead of using the API, warning that doing so with a Google account linked to other Google accounts can lead to a hard ban.\n\nAnother chimed in, suggesting it's auto-tracked by AI now.\nUnsloth AI (Daniel Han) Discord\nWandB Wisely Wafts from Unsloth: WandB added a new finetuning service that supports ART and some other open source finetuning frameworks, but not Unsloth, leaving community members confused.\n\nSome speculate bias might play a role, especially since every Unsloth notebook promotes them basically.\nSmall Model Training Sees Shoestring Success: Thanks to Unsloth, you can train a small language model on a budget with very little experience with a model size of 550M.\n\nPacking and Flash Attention 2 makes your consumer card match the performance of expensive A100's and even H100 in some cases.\nNvidia Navigates New Naming Notions: Nvidia released PersonaPlex-7b-v1 on Hugging Face, continuing their trend of incorporating \""persona\"" into their model names.\n\nOne user found the space emergency scenario in the demo to be surprisingly funny.\nErrors Emerge Experimentally: A member tried error aware rewards and it refused to budge, either favoring recall or precision without improving beyond 5 epochs, and sought advice on using F1 score as a potential solution.\n\nAnother member noted that RL is weird, you just gotta try everything to get things work to address this issue.\nIdeal Inference Iterations Instigated: After training a 4B model, a member inquired about the best inference parameters (temperature, top_p, tok_k), to which others recommended using the base model's parameters as a starting point and adjusting the temperature.\n\nIt was noted that lower temperatures are generally better for precise responses, while higher temperatures introduce more variation, but maybe only lazier possible options.\nOpenAI Discord\nGrok's Conspiracy Mode Causes Concerns: Users reported their friend experienced AI psychosis from Grok's conspiracy mode, where the AI validated and suggested more beliefs, prompting concerns about LLMs' impact on mental health.\n\nMembers debated the problematic nature of the feature, recognizing that conspiracy theorists often gather in echo chambers regardless.\nAI Brand Loyalty Echoes Car Preferences: Members analogized AI model preferences to car brands, observing users' loyalty to specific AI behaviors like BMW, Galaxy, vs Apple, which solidifies market segments.\n\nThe customizability of ChatGPT was highlighted as a key advantage, though some users prefer prompt-prepending over exploring such options.\nSafety Filter Showdown: OpenAI vs. Google vs. Grok: Members compared image generation safety filters, deeming Google flexible for digital art, OpenAI overly paranoid, Midjourney crazy and schizophrenic, and Grok the loosest, ripe for unconsensual deep fakes.\n\nThe varied strictness levels across platforms raise questions about appropriate content moderation in AI-generated media.\nMetacognition Prompt Mania Mobilizes Minds: A user shared a meta-cognitive reasoning prompt to improve the quality of answers from language models by encouraging decomposition, solving, verification, and synthesis.\n\nThis structured approach garnered praise for being concise enough to be used as a custom instruction to improve the quality of the answer.\n\""Open Responses\"" Opens Opportunities: Open Responses is an open standard that allows apps using AI to communicate with different models using a single interface, without having to rebuild the entire system each time.\n\nThis framework solves the problem of rewriting code and adjusting workflows when changing AI providers.\nPerplexity AI Discord\nGPT Go Pricing Considered a Ripoff: Members complain GPT Go's limit of 10 messages per session makes Microsoft Copilot the better free alternative because it has the same models with no ads or limits.\n\nA user pointed out that $4.81 USD for GPT Go isn't as good as $5.76 USD for X Premium in India.\nConfusion Surrounds Trump's Alleged EU/UK Ban: Channel members debated whether Trump was banned from the EU and the UK, citing an image as proof.\n\nSpeculation arose about the source of the ban information, with some suggesting it originated from Russia Today.\nGemini 3 Pro Susceptible to Embarrassing Typos: A user reported that Gemini 3 Pro has so many flaws compared to all the others and makes typos very often.\n\nDespite this, others defended Gemini 3 Pro, stating that They're still leading in the 3rd party category imo.\nSonar API Suffers from Data Delay Debacle: Users reported a 24-hour delay in the Sonar API updating with new website content because of indexing issues.\n\nThey inquired about speeding up website indexing or bypassing it entirely to receive data immediately after publication.\nOpen Source Coding Agent Project Shared: A member shared his open source coding agent project called Qbit.\n\nThe project is available on GitHub.\nCursor Community Discord\nGoogle Pro Subscription Beats Cursor Tokenomics: Users found that Claude Opus 4.5 with a free Google Pro subscription only has rate limits, whereas Cursor's token usage leads to significant costs.\n\nOne member reported burning through $200 in 3 days, expressing shock at Opus's expense.\nGPT 5.2 Codex Has Language Mishaps: Some reported that GPT 5.2 Codex randomly switches to Arabic, rendering it unusable, though others claim it's superior to Opus 4.5.\n\nOne frustrated user stated, I have never seen a model randomly change languages on me on a consistent basis.\nCursor Adds Secret Sauce with Print Statements: A member discovered that Cursor's insertion of print statements for debugging is part of their Agent/Debug Mode, which operates natively without a custom MCP server, as detailed in this blogpost.\n\nThis feature is considered Cursor's secret sauce for debugging.\nPrettier Extension Gets Utterly Broken: Members reported that the Prettier extension is completely broken and unable to format files, as raised on GitHub.\n\nA workaround suggested was temporarily switching to Biome.\nUsers Confused by Cursor's Usage Limits: Some users expressed confusion regarding Cursor's usage limits and plan details, questioning why the program didn't dip into the pool.\n\nClarification revealed that the $20/month plan includes a credit amount but can be quickly exhausted, though some users found a free bonus from Cursor.\nLMArena Discord\nPDF Support Sparks Privacy Probes: A user questioned how the new PDF support would work with privacy, especially concerning confidential PDF documents, but was pointed to the platform's Privacy Policy for details.\n\nThe platform will still scrub for PII before any open data releases and that these practices remain unchanged, despite it still being an experimental feature.\nNano Banana Pro's Nagging No-Gos: Users reported consistent issues with Nano Banana Pro, experiencing errors over extended periods, with a member noting they've been getting errors every hour and was given steps to fix errors.\n\nAnother user pointed out a potential January 2025 date cutoff for the model based on minimaxir.com, while others reported problems with captcha.\nText Files Tease Techies: Users are clamoring for the ability to upload .txt files for larger context windows, but were told by a community manager this is something we're working on and is for sure on the list.\n\nGiven PDF upload support has been implemented, some users are resorting to uploading databases within PDF files.\nImage Edit Arena Welcomes wan2.5-i2i-preview: The Image Edit leaderboard welcomes wan2.5-i2i-preview, securing the #21 spot with a score of 1213.\n\nFor more details, check the Leaderboard Changelog.\nLM Studio Discord\nLLMs spark heated AI debate: Members debated if current LLMs should even be considered AI, citing differences in abbreviation meanings and current capabilities.\n\nSome argue that the term is misused because current LLMs don't meet the threshold of true artificial intelligence, and are just glorified pattern matchers.\nQwen3 4B zips on gaming laptops: Qwen3 4B 2507 is recommended for effectively running on gaming laptops with 8GB VRAM and 16GB DDR5, outperforming LFM 2.5 1.2b in terms of speed.\n\nMembers also discussed the discounted GMKtec AI Max 395 PC for Qwen 3 Next, but others said it's probably too slow.\nVRAM Virtues and Woes: A member jokingly requested a 3090 donation to reach 128GB of VRAM, and another lamented buying a laptop with an AMD AI 9 370 and NVIDIA 5070 with only 8GB VRAM, seeking advice on model optimization.\n\nKeeping models and context in VRAM is important, and they warned not to go below Q4 quantization.\nLFM 2.5 1.2B declared miracle!: Some members claimed LFM 2.5 1.2B performs exceptionally well, comparable to larger models, especially in translation, citing the SauerkrautLM-Translator-LFM2.5-1.2B on Hugging Face.\n\nOthers disputed this, cautioning against overhyping its capabilities, saying to 'talk to your doctor to change the dose of meds' if seeing the future from this one model, while others noted it messes up simple instruct tasks.\nCUDA lagged, Vulkan Zoomed: One member noted that llama.cpp has a poor implementation for Qwen3 Next currently, so specs are somewhat irrelevant, and on an official build they don't surpass 30-35 t/s.\n\nHowever, another member gets 60 t/s using Vulkan with their RTX PRO 6000, compared to another's 38 t/s after CUDA optimizations, showing more optimization on Vulkan than CUDA.\nHuggingFace Discord\nSpotting GPU Job Fails Early: Members debated the earliest point to catch misconfigured GPU jobs during inference or training, one member detects fails during the first 50K step of pretraining.\n\nThe member uses a simple inference script to check for issues between generate scripts and the inference engine.\nDecoding Cloud Prices for AI Hardware: A member solicited advice on the most price-efficient cloud platform for AI hardware jobs.\n\nUnfortunately, no specific recommendations emerged, but members offered to assist with future issues.\nIndic SLMs Conquer New Territory: A member unveiled their mission to construct SLMs for Indic languages, focusing on agentic use cases, having distilled XLMRoberta to a 33Mn parameter model while retaining 98% accuracy.\n\nThis work addresses the under-representation of Indian languages in existing language models.\nSlipstream Drops Tokens Like Crazy: An independent researcher introduced Slipstream, a protocol that achieves up to 82% token savings on inter-agent coordination, sharing articles and spaces related to the research.\n\nBy streamlining communication between agents, Slipstream significantly reduces the computational cost of multi-agent systems.\nRL Student Stuck On SoccerTwos.exe: A Deep RL Course student needs help using the Unity3D tool SoccerTwos.exe, since its usage isn't covered in the course, and the AI vs AI interface is missing.\n\nAnother student ran into errors on Unit 1 when using the LunarLander-v2 environment, as it's deprecated and suggested to use LunarLander-v3 instead.\nGPU MODE Discord\nIndexing into PyTorch: Profiler Use Appears Key: A member sought guidance on using the PyTorch profiler to understand why certain index_select and index_copy operations have high CPU wall time, linking to relevant code for context.\n\nThey wondered if allocation issues might be the root cause and looked for methods to diagnose the problem from profiling traces.\nSLMs Speak Indic: Efforts Kick Off for Efficient Agentic Use: A member is building SLMs for Indic languages, targeting models between 10Mn - 500Mn parameters for efficient on-device agentic use cases, and has distilled Hindi XLMRoberta to a 33Mn parameter model.\n\nThey are seeking feedback and collaboration to build world-class SLMs.\nCUDA Conundrums: Kernel Optimization Kicks Off: A member requested feedback on their first CUDA project, implementing a causal self-attention kernel on a V100, aiming to surpass a naive PyTorch implementation and approach the performance of scaled_dot_product_attention.\n\nThey shared details on their approach, block configurations, and the challenges faced when incorporating shared memory and optimizing for L1 cache usage.\nTriton's Top-K Kernel: Trouble at the Top: A member is encountering errors with their Triton kernel for top-k selection on a GPU array, using triton.jit and triton.language for GPU-accelerated computation.\n\nAnother pointed out that the current Triton kernel performs a local top-k selection on each 128-sized slice rather than finding the top-k elements for the entire array, when leetgpu.com requires finding the top-k elements from an array of up to a million elements, implying a global top-k operation.\nBF16 Battles: Precision Problems Plague Programmers: A member debugged BF16 matmul precision issues, finding that a naive kernel produced results with a max abs difference of 1+ if large K, but another member suggested computing the reference result in fp32 instead.\n\nAnother member explained that Torch is doing splitK, and that scaling by sqrt(K) might help because bfloat is just bad.\nLatent Space Discord\nZunic Tweet Skyrockets in Visibility: A tweet by Gregor Zunic unexpectedly gained 119,777 views and over 760 likes on January 16, 2026.\n\nThe tweet's visibility on @gregpr07 was flagged as unusual by social media analysts.\nGhori Spills xAI Secrets, Drama Ensues: Sulaiman Ghori from xAI discussed the rapid development of the Colossus data center and the intense work environment under Elon Musk in this interview.\n\nShortly after the interview, Ghori reportedly lost his xAI checkmark on Twitter and deleted numerous tweets, hinting at potential fallout.\nVercel's 'Skills' Opens AI Agent Capabilities: Guillermo Rauch introduced 'skills', an open ecosystem for AI capabilities on Vercel, functioning as a package manager for AI agents.\n\nDevelopers can begin integrating these tools using 'npx skills i vercel-labs/agent-skills' and can reference React Best Practices for implementation guidelines.\nGPT 5.2 Pro Cracks Erdos Problem: GPT 5.2 Pro has solved the previously unsolved Erdos problem #281, according to Neel Somani.\n\nMathematician Terence Tao acknowledged this as a clear instance of artificial intelligence solving an unsolved mathematical problem.\nElevenLabs Valuation Aims for the Stars: AI startup ElevenLabs is in discussions to secure funding at an $11 billion valuation, significantly up from $6.6 billion a few months prior, per this post.\n\nThe potential investment reflects growing confidence in the company's AI-driven voice technology and market expansion.\nYannick Kilcher Discord\nZKPs Enable AI Governance: Members discussed using Zero Knowledge Proofs (ZKPs) for autonomous AI governance, allowing compliance verification without revealing sensitive data; and while ZKPs can prove the model you wanted to run is actually the model that was executed.\n\nIt was cautioned that ZKPs don't inherently solve formalization and statement proving.\nTEE Not Always Trouble-free: Discussion centered on the limitations of Trusted Execution Environments (TEEs) for secure compute, citing potential vulnerabilities even with hardware-based memory encryption.\n\nDespite security features, TEEs can be compromised, with one member referencing DefCon talks about intercepting decryption codes, but that Nvidia's new server has server level TEE which helps with it.\nScaling Learning Rates: A member asked about the consensus on learning rate scaling as a function of batch size, referencing a paper advocating for learning_rate ∝ sqrt(batch_size).\n\nOthers noted that linear scaling is common but often tuned, questioning the necessity of a strict rule.\nAnthropic Builds Claude Brains: A link to testingcatalog.com was shared, indicating Anthropic's work on knowledge bases for Claude.\n\nThis suggests efforts to enhance Claude's capabilities by providing it with structured knowledge resources, possibly for improved performance and reliability.\nDevstral Challenges Codex: When asked for open-source coding agents for self-hosted models, members said that Devstral 2 Small is a good option, and that Devstral 2 Medium is apparently on par with Claude Sonnet 4.5.\n\nMembers discussed how this agentic code base performs tasks (like GPT Codex), and that Kilo Code is just an extension that can plug in local models (such as locally hosted Devstral 2).\nDSPy Discord\nDSPy Optimizes Skills Like a Boss!: Members discussed optimizing skill.md using DSPy, referencing an article on optimizing Anthropic skills.\n\nThe discussion centered on strategies for writing efficient skill.md files and the potential of DSPy for prompt optimization.\nRLMs Drop the Beat in DSPy 3.1.2: The team released dspy.RLM in DSPy 3.1.2, promising greatly expanded capabilities achievable in a single line of code, and sharing the announcement.\n\nThis release had been cryptically promised during the DSPy 3.0 release talk back in June, creating anticipation within the community.\nDeno Steals the Show for Local WASM: DSPy leverages Deno for its local sandbox/interpreter due to its secure WASM runtime capabilities.\n\nThe decision to use Deno was inspired by Simon Willison's blog post and its seamless integration with Pyodide.\nGEPA & RLMs Plot to Take Over the World: GEPA (genetic-pareto) and RLMs are composable, opening doors for RLM-as-an-optimizer strategies, a development deemed promising by team members.\n\nOne team member considers GEPA a fundamental idea and highlighted an application of RLMs for writing documentation from code, citing its ability to handle extremely long outputs.\nDocs, Begone! RLMs Can Do That Now: Members are looking at using RLMs to generate documentation from code, opening possibilities that have been impossible before.\n\nIt was noted that it is possible to generate documentation over all prior proposals, and keep the whole tree in mind.\nManus.im Discord Discord\nManus App Size Crunch Looms: A user requested an increase to the app size limit on Manus after hitting the cap while building an audio player with 100 MP3 files totaling 600 MB.\n\nThey hope to enable larger applications to unlock richer projects for developers.\nSubscription Snafu Freezes Funds: A user reported a payment overdue error with an inflated amount, blocking their plan downgrade.\n\nManus Support replied promising private assistance to resolve the billing error.\nAI Meeting Minutes Automates Annoyance: A member shared a YouTube video demonstrating how to use the new Manus AI Meeting Minutes feature.\n\nAnother member jokingly commented that Home office bros will love this.\nBilling Breakdown Blackouts Bible Broadcast: A user's project went offline due to billing issues preventing a downgrade from the $400 plan, impacting their Bible study platform for women.\n\nManus support has reached out to them privately for assistance.\nDracoAI Dawns as Deadly Dissenter: One user touted dracoai.app as superior to Manus, praising its API call capabilities, including phone calls.\n\nThey suggested: Edit the system prompt and add specific API tools this thing is next level.\ntinygrad (George Hotz) Discord\nTinygrad Nixes Discord Fundraising: A user's attempt to fundraise for Green V2 Blackwell was shut down by George Hotz, who stated that this discord is for discussion of tinygrad usage.\n\nUsers were warned against shilling, with the potential consequence of being banned from the discord.\nTinygrad Seeks New Swanky Logo: George Hotz requested a new logo for tinygrad, noting the current one is outdated on the tinygrad twitter.\n\nThe updated github logo is available from tinygrad.org in SVG format.\ntinygrad Meeting #3 Set: The next tinygrad meeting, #3, is scheduled for Monday 9am San Diego time, covering topics such as company updates, drivers, and more.\n\nThe agenda includes discussions on image dtype, assembly, jit asserts, assign, mypy, llama training, viz / fast gemm, and other bounties.\ntinygrad Plans MLPerf Contest: George Hotz announced intentions to hold contests this year, contingent on achieving 405b mlperf.\n\nDetails on the contest specifics were not provided, but the announcement suggests a focus on performance and achievement.\ntinygrad Taps PyArrow with from_blob: A user inquired about leveraging tinygrad with PyArrow/Parquet, specifically seeking alternatives to Tensor.from_blob for data loading with ds.dataset.\n\nThe recommended solution involves using Tensor.from_blob with PyArrow, though it's noted as not well tested and maintained, suggesting numpy conversion as a preferred approach.\nMoonshot AI (Kimi K-2) Discord\nDistilled Models Expected Soon: Members are anticipating models distilled from Claude/GPT-5/Gemini-3 in the coming months, with focus on enhancements to long-context processing.\n\nOne member noted that K2-Thinking's context handling degrades after 30k tokens, highlighting that many models fail to maintain performance across their full advertised context window.\nSubscription Cancellation Turns Sour: A user reported unauthorized charges after cancelling their $0.99 Kimi plan and deleting their account, facing repeated charges on their Visa.\n\nOther members suggested contacting membership@moonshot.ai for refunds and offering to escalate the issue internally.\nUnexpected Subscription Fees Upset Users: A user reported an unexpected $19 charge for their Kimi plan after account inactivity and lack of reminder, leading them to request a refund.\n\nSupport directed the user to membership@moonshot.ai for a refund, confirming a response was received.\nPhrases Mysteriously Vanish: A user posted an image noting the disappearance of common phrases, questioning their removal.\n\nAnother user clarified that these phrases are now located under \""presets\"" accessible via the plus sign, showcasing the new location with an image.\nModular (Mojo 🔥) Discord\nRaspberry Pi Gets GenAI HAT: The introduction of the Raspberry Pi AI HAT+ sparked discussions about adding Hailo AI chip support to MAX and Mojo.\n\nA community member suggested that Mojo might struggle to integrate Hailo without an open-source compiler or an open IR to interface with a compiler, similar to the challenges faced with AMD's NPUs.\nSeeking Robust Face Recognition: A member is on the hunt for commercially viable face recognition models and repos because FaceNet has failed under real-world conditions.\n\nThey're seeking more robust alternatives to FaceNet that offer improvements in lighting invariance, preprocessing, and training techniques.\nPixi Shell Stumps Newbie: A community member encountered import problems with PyTorch and Numpy after installing them via pixi and was unable to locate the modules after install.\n\nA helper clarified the need to use the Python module to access Python libraries within Mojo, rather than direct Python code imports.\nPixi-induced PyTorch and Numpy Frustrations: A user initially struggled with PyTorch and Numpy import issues within the pixi shell, with modules failing to be recognized in the Mojo file.\n\nThe resolution involved using the Python module or custom cpython bindings, confirming the successful import of the module.\naider (Paul Gauthier) Discord\nAider ComposerLooks Lacks Traction: Interest sparked around Aider ComposerLooks, despite numerous stars, real-world use cases and current support for the latest AI models remain underexplored.\n\nUsers want to know if the library works and if the documentation will be updated.\nMissing Main Dev Mystery: The community wondered about Paul Gauthier's last activity, who is the main developer of Aider, whose last activity was in January.\n\nSpeculation arose that he may have been hired by Anthropic, eliminating open-source competition.\nAider Open for Community Rescue: Paul Gauthier confirmed he's been busy with other projects but is open to merging community contributions to Aider.\n\nA member inquired about missing features beyond autonomous agent capabilities, but another member noted that it was feature complete, highlighting concerns about potential abandonware status and the project's maintenance.\nProduction-Ready LLM & RAG Systems Turnkey: A member highlighted their focus on transforming ideas and messy data into production-ready LLM & RAG systems.\n\nTheir emphasis is on making AI usable in real workflows, going beyond mere demonstrations.\nLLM + RAG Integration Expert Available: A member offers expertise in helping developers integrate LLM + RAG pipelines into production environments without the usual trial-and-error process.\n\nThey also provide guidance to indie builders and consultants seeking to make AI tools fully functional.\nMLOps @Chipro Discord\n**New 'Before..."",""content"":""**AI News for 1/16/2026-1/19/2026** covers new architectures for scaling Transformer memory and context, including **STEM** from **Carnegie Mellon** and **Meta AI**, which replaces part of the FFN with a token-indexed embedding lookup enabling CPU offload and asynchronous prefetch. **RePo** from **Sakana AI** introduces adaptive positional reordering to improve robustness on noisy and long-range contexts. Model releases highlight **Zhipu AI's GLM-4.7-Flash**, a **30B-class MLA + small MoE** model optimized for coding and agentic tasks, noted for strong benchmark performance and a compression narrative from larger to smaller models. Inference and deployment updates include **mlx-lm 0.30.3** supporting GLM-4.7-Flash with efficient 4-bit performance on laptops. The report emphasizes practical takeaways on static sparsity, adaptive ordering, and the resurgence of small, fast models for interactive tasks. *\""Sparse capacity doesn’t have to mean MoE routers + expert parallelism; static sparsity can be systems-friendly.\""*"",""contentSnippet"":""**AI News for 1/16/2026-1/19/2026** covers new architectures for scaling Transformer memory and context, including **STEM** from **Carnegie Mellon** and **Meta AI**, which replaces part of the FFN with a token-indexed embedding lookup enabling CPU offload and asynchronous prefetch. **RePo** from **Sakana AI** introduces adaptive positional reordering to improve robustness on noisy and long-range contexts. Model releases highlight **Zhipu AI's GLM-4.7-Flash**, a **30B-class MLA + small MoE** model optimized for coding and agentic tasks, noted for strong benchmark performance and a compression narrative from larger to smaller models. Inference and deployment updates include **mlx-lm 0.30.3** supporting GLM-4.7-Flash with efficient 4-bit performance on laptops. The report emphasizes practical takeaways on static sparsity, adaptive ordering, and the resurgence of small, fast models for interactive tasks. *\""Sparse capacity doesn’t have to mean MoE routers + expert parallelism; static sparsity can be systems-friendly.\""*"",""guid"":""https://news.smol.ai/issues/26-01-19-not-much/"",""categories"":[""meta-ai-fair"",""carnegie-mellon"",""sakana-ai"",""zhipu-ai"",""glm-4.7-flash"",""glm-4.7"",""glm-4.5"",""qwen3-vl"",""qwen"",""transformer-memory"",""model-architecture"",""mixture-of-experts"",""adaptive-position-encoding"",""long-context"",""model-compression"",""inference-optimization"",""local-inference"",""model-deployment"",""benchmarking"",""coding"",""agentic-ai""],""isoDate"":""2026-01-19T05:44:39.000Z""}"
Smol,ChatGPT starts testing ads on free tier + new $8/mo Go plan in the US,https://news.smol.ai/issues/26-01-16-chatgpt-ads/,2026-01-16T05:44:39.000Z,"<p><strong>Monetizing your consumers is all you need.</strong></p>
<blockquote>
<p>AI News for 1/15/2026-1/16/2026. We checked 12 subreddits, <a href=""https://twitter.com/i/lists/1585430245762441216""><strong>544</strong> Twitters</a> and <strong>24</strong> Discords (<strong>205</strong> channels, and <strong>4966</strong> messages) for you. Estimated reading time saved (at 200wpm): <strong>430 minutes</strong>. <strong>Our new website</strong> is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on <a href=""https://x.com/Smol_AI"">@smol_ai</a>!</p>
</blockquote>
<p>When you have 900 million weekly active users, you are usually long overdue in trying to figure out an ad supported model. Despite <a href=""https://x.com/tomwarren/status/2012295849678602610?s=46"">a lot</a> of <a href=""https://x.com/nickfloats/status/2012249130006143477?s=46"">snark</a> from commentators, OpenAI had to figure out their ads business and finally broke its silence today, outlining their <a href=""https://x.com/OpenAI/status/2012223373489614951?s=20"">ads principles</a> in their tests that will roll out only in the US over the next free tier:</p>
<p><img src=""https://pbs.twimg.com/media/G-zZl9kXwAAQut2?format=png&#x26;name=4096x4096"" alt=""https://pbs.twimg.com/media/G-zZl9kXwAAQut2?format=png&#x26;name=4096x4096""></p>
<p>Most important statement in this is that ads never affect responses and are clearly labeled, which is the ""right"" move:</p>
<p><img src=""https://pbs.twimg.com/media/G-zZXO-XcAAdvQo?format=jpg&#x26;name=4096x4096"" alt=""https://pbs.twimg.com/media/G-zZXO-XcAAdvQo?format=jpg&#x26;name=4096x4096""></p>
<p>Formerly paid plans will not see ads, but the new Go plan (now rolled out in the US) will. The sheer number of pricing plans also <a href=""https://x.com/simonw/status/2012271939629498386?s=46"">draws some confusion</a>:</p>
<p><img src=""https://pbs.twimg.com/media/G-0GmQtaQAAW_-F?format=jpg&#x26;name=4096x4096"" alt=""https://pbs.twimg.com/media/G-0GmQtaQAAW_-F?format=jpg&#x26;name=4096x4096""></p>
<hr>
<h1>AI Twitter Recap</h1>
<p><strong>OpenAI product + monetization shifts (Go tier, ads, Codex speed, memory)</strong></p>
<ul>
<li><strong>ChatGPT Go + ads testing</strong>: OpenAI announced <strong>ChatGPT Go</strong> (global rollout) as a <strong>$8/month</strong> low-cost tier with “10× more messages,” file uploads, image creation, more memory, longer context, and “unlimited use of GPT-5.2 instant” (<a href=""https://twitter.com/OpenAI/status/2012223323812270219"">OpenAI</a>). In parallel, OpenAI said it will <strong>start testing ads</strong> in <strong>Free + Go</strong> tiers, with principles: <strong>answers not influenced by ads</strong>, ads clearly labeled, and “conversations private from advertisers” (<a href=""https://twitter.com/OpenAI/status/2012223373489614951"">OpenAI</a>; expanded by <a href=""https://twitter.com/fidjissimo/status/2012226082716393960"">@fidjissimo</a> and <a href=""https://twitter.com/sama/status/2012253252771824074"">@sama</a>). The announcement triggered heavy skepticism about inevitable incentive drift (e.g., <a href=""https://twitter.com/scaling01/status/2012234947403174189"">@scaling01</a>; and the resurfaced “ads as last resort” quote via <a href=""https://twitter.com/tomwarren/status/2012295849678602610"">@tomwarren</a>).</li>
<li><strong>Memory + “very fast Codex”</strong>: Sam Altman highlighted “new ChatGPT memory improvements” (<a href=""https://twitter.com/sama/status/2012242952542683227"">@sama</a>) and repeatedly teased “<strong>Very fast Codex coming!</strong>” (<a href=""https://twitter.com/sama/status/2012243893744443706"">@sama</a>), with follow-on confirmation/teaser posts from developer ecosystem accounts (<a href=""https://twitter.com/embirico/status/2012320775370666004"">@embirico</a>). Multiple engineers discuss workflow-level impacts of the <strong>speed vs intelligence</strong> trade-off (e.g., shifting to more asynchronous “agent shepherding” when models are faster: <a href=""https://twitter.com/adamdotdev/status/2012142271819399663"">@adamdotdev</a>).</li>
<li><strong>Codex CLI ecosystem integrations</strong>: Open-weight models can be used through the Codex CLI via Ollama using <code>codex --oss</code> (<a href=""https://twitter.com/ollama/status/2012046176267440177"">@ollama</a>), with a note to push context length to <strong>≥32K</strong> in settings for better UX (<a href=""https://twitter.com/ollama/status/2012049822484750426"">@ollama</a>). There’s also a new interaction UX: “steer codex mid-turn without interrupting” in an experimental mode (<a href=""https://twitter.com/thsottiaux/status/2012074358471319599"">@thsottiaux</a>).</li>
</ul>
<p><strong>Agent tooling: orchestration UX, “human-in-the-loop” reliability, and file interfaces over classic RAG</strong></p>
<ul>
<li><strong>Human-in-the-loop as a reliability multiplier</strong>: A recurring theme is that putting a human “babysitter” in the loop makes systems <em>feel</em> far more reliable than fully autonomous deployments using the same underlying models—because the human becomes a manual harness that catches failures and routes around ambiguity (<a href=""https://twitter.com/lateinteraction/status/2012030585926189148"">@lateinteraction</a>; follow-up noting now there’s quantitative support for the intuition: <a href=""https://twitter.com/lateinteraction/status/2012031028932854054"">@lateinteraction</a>). Related: a chart discussion frames “the gap between the two lines” as the value of a human-in-the-loop (<a href=""https://twitter.com/dbreunig/status/2012200587211821410"">@dbreunig</a>).</li>
<li><strong>“Chunking is dead” / files-first retrieval</strong>: Jerry Liu argues that <strong>RAG isn’t dead, but static chunking is</strong>—if an agent can open a file, search (<code>ls</code>/<code>grep</code>), and expand context dynamically, you can avoid the brittle chunk/embed pipeline for many scales (<a href=""https://twitter.com/jerryjliu0/status/2012273236042559802"">@jerryjliu0</a>; deeper clarification on why file tools work well up to a few hundred docs and where DBs re-enter: <a href=""https://twitter.com/jerryjliu0/status/2012254129473896532"">@jerryjliu0</a>; emphasis on OCR as the missing piece for PDFs/PPTs: <a href=""https://twitter.com/jerryjliu0/status/2012272839416758652"">@jerryjliu0</a>). A separate synthesis frames this as “files aren’t replacing databases, but they’re forcing a rethink of when DBs are overkill” (<a href=""https://twitter.com/tuanacelik/status/2012212183833403889"">@tuanacelik</a>).</li>
<li><strong>Orchestrators and agent UIs proliferate</strong>: Multiple launches and memes point to a fast-moving layer of “agent harness” products: Anthropic’s Cowork is referenced as a signal of orchestration tools becoming mainstream (<a href=""https://twitter.com/alexalbert__/status/2012230110745702563"">@alexalbert__</a>; meta commentary by <a href=""https://twitter.com/omarsar0/status/2012253642263249167"">@omarsar0</a>). SpecStory open-sourced a CLI to normalize agent session provenance/contracts (<a href=""https://twitter.com/doesdatmaksense/status/2012209297380544940"">@doesdatmaksense</a>). A new open-source UI (“sled”) lets you “teleport Claude Code or Codex from your computer to your phone” via Agent Control Protocol (<a href=""https://twitter.com/dctanner/status/2012212217677070796"">@dctanner</a>). OpenWork added native <strong>Ollama integration</strong> for fully local computer agents on Mac (Gemma/Qwen/DeepSeek/Kimi etc.) (<a href=""https://twitter.com/_orcaman/status/2012210613712281646"">@_orcaman</a>).</li>
</ul>
<p><strong>Inference + systems engineering: caching, Prefill/Decode split, hardware benchmarks, and CUDA tiling ergonomics</strong></p>
<ul>
<li><strong>“Year of inference explosion” framing</strong>: A long Zhihu thread summary argues the bottleneck has shifted from training to inference: agents raise IO ratios (3:1 → 100:1 or 1000:1), <strong>prefill dominates</strong>, <strong>context caching becomes default</strong>, and Prefill/Decode splitting harms utilization unless you redesign scheduling and memory hierarchy (<a href=""https://twitter.com/ZhihuFrontier/status/2012080310981374428"">@ZhihuFrontier</a>). This aligns with broader infra chatter around cache affinity vs load balance trade-offs.</li>
<li><strong>Hardware benchmarking beyond NVIDIA</strong>: Artificial Analysis added <strong>DeepSeek R1</strong> results on SambaNova SN40L, showing higher throughput at concurrency and standout per-user speeds (noted peak ~269 tok/s single-user) vs tested NVIDIA configurations—while flagging lack of public hourly pricing for cost comparisons (<a href=""https://twitter.com/ArtificialAnlys/status/2012233319891824943"">@ArtificialAnlys</a>; <a href=""https://twitter.com/ArtificialAnlys/status/2012233323154678010"">@ArtificialAnlys</a>).</li>
<li><strong>CUDA tiling / CuTe / cuTile ergonomics</strong>: Engineers are enthused about <strong>CuTe algebra</strong> as a cleaner abstraction for tiling/indexing compared to hand-rolled CUDA gymnastics (<a href=""https://twitter.com/fleetwood___/status/2012150019722485811"">@fleetwood___</a>), alongside pointers to scarce “mere mortal” resources (<a href=""https://twitter.com/fleetwood___/status/2012151045992992943"">@fleetwood___</a>). NVIDIA’s newer “CUDA Tile”/cuTile guidance is summarized as enabling near–cuBLAS GEMM performance with simpler block-level code and compiler specialization (plus swizzling improvements) (<a href=""https://twitter.com/TheTuringPost/status/2012288767894360215"">@TheTuringPost</a>).</li>
<li><strong>Data center power scaling</strong>: Epoch AI estimates AI data centers now have total capacity around <strong>30 GW</strong>, comparable to New York State peak hot-day usage; methodology multiplies chip units sold by rated draw and applies ~2.5× facility overhead, with caveats about “capacity vs usage” (<a href=""https://twitter.com/EpochAIResearch/status/2012303496465498490"">@EpochAIResearch</a>).</li>
</ul>
<p><strong>Model &#x26; research highlights: voice cloning without tokenization, ultra-small models, multimodal + retrieval advances</strong></p>
<ul>
<li><strong>Tokenization-free real-time TTS</strong>: OpenBMB open-sourced <strong>VoxCPM</strong> weights for real-time streaming voice cloning, described as generating <strong>continuous speech directly</strong> (avoiding discrete audio token artifacts), with LoRA fine-tuning and ~0.15 real-time factor on a single RTX 4090 per the tweet (<a href=""https://twitter.com/LiorOnAI/status/2012133013967044755"">@LiorOnAI</a>; repo link <a href=""https://twitter.com/LiorOnAI/status/2012133015426642286"">@LiorOnAI</a>). If accurate, it’s a meaningful shift for latency/prosody fidelity in production voice agents.</li>
<li><strong>Small-model reasoning &#x26; edge deployments</strong>: TII promoted <strong>Falcon-H1-Tiny</strong> (&#x3C;100M params) as capable of reasoning/coding/function calling for edge/IoT scenarios (<a href=""https://twitter.com/TIIuae/status/2012034581084430662"">@TIIuae</a>). Ultralytics released <strong>YOLO26</strong> family (30 models, &#x3C;50M params) spanning detection/segmentation/keypoints/open-vocab, with demos on CPU (<a href=""https://twitter.com/mervenoyann/status/2012121123018924033"">@mervenoyann</a>).</li>
<li><strong>Multilingual translation</strong>: TranslateGemma gained attention for multilingual breadth (incl. Malayalam) and tokenizer/data work (<a href=""https://twitter.com/_arohan_/status/2012032986649448708"">@<em>arohan</em></a>; <a href=""https://twitter.com/JeffDean/status/2012178747076591820"">@JeffDean</a>), and is available in Ollama with a specific prompting format (<a href=""https://twitter.com/ollama/status/2012307436284395692"">@ollama</a>).</li>
<li><strong>Retrieval: multi-vector resurgence</strong>: Strong claims that <strong>multi-vector retrieval</strong> can let tiny models compete with much larger baselines (e.g., “32M parameter multi vector model” approaching an 8B model) (<a href=""https://twitter.com/aaxsh18/status/2012124348392583584"">@aaxsh18</a>), echoed by “multi vector is the only way forward” (<a href=""https://twitter.com/lateinteraction/status/2012227085507449197"">@lateinteraction</a>) and practitioner reinforcement about ColBERT/ColPali-style wins across tasks (<a href=""https://twitter.com/antoine_chaffin/status/2012269641490391272"">@antoine_chaffin</a>).</li>
<li><strong>Preference data design for alignment (AIR)</strong>: OpenBMB’s AIR framework decomposes preference datasets into <strong>Annotations / Instructions / Response pairs</strong>, claiming best practices: simpler scoring, filtering instructions by low variance, and balancing pair gaps/quality; reported +5.3 average gain across 6 benchmarks using 14k curated pairs (<a href=""https://twitter.com/OpenBMB/status/2012179938388926679"">@OpenBMB</a>).</li>
</ul>
<p><strong>Generative media: open image/video releases, motion control workflows, and diffusion “Neural OS”</strong></p>
<ul>
<li><strong>FLUX.2 [klein] lands everywhere (open weights, vLLM day-0, leaderboards)</strong>: Black Forest Labs’ <strong>FLUX.2 [klein]</strong> got “day-0 support” in <strong>vLLM-Omni</strong>, positioned as consumer-friendly (&#x3C;~13GB VRAM), sub-second inference, Apache-2.0 licensed 4B model (per tweet) (<a href=""https://twitter.com/vllm_project/status/2012110024294965406"">@vllm_project</a>). Arena and Artificial Analysis report strong open-model leaderboard placements (<a href=""https://twitter.com/arena/status/2012310336528056520"">@arena</a>; <a href=""https://twitter.com/ArtificialAnlys/status/2012339542997737856"">@ArtificialAnlys</a>).</li>
<li><strong>Open video model rankings</strong>: Artificial Analysis notes <strong>LTX-2</strong> as leading open-weights video model in their Video Arena, with licensing caveats (LTX-2 Community License, commercial use under revenue threshold and non-compete constraints) (<a href=""https://twitter.com/ArtificialAnlys/status/2012256702788153604"">@ArtificialAnlys</a>).</li>
<li><strong>Kling motion control + “AI mocap”</strong>: Multiple threads highlight motion-control and mocap-style workflows enabling fast character swaps and transferable acting/performance (<a href=""https://twitter.com/HAL2400AI/status/2012038846960328781"">@HAL2400AI</a>; tutorial from <a href=""https://twitter.com/Kling_ai/status/2012155500134105149"">@Kling_ai</a>; “AI motion capture… copy/paste motion/expression/lips” (<a href=""https://twitter.com/EHuanglu/status/2012149076511617436"">@EHuanglu</a>); examples roundup (<a href=""https://twitter.com/minchoi/status/2012306052956533211"">@minchoi</a>).</li>
</ul>
<p><strong>Top tweets (by engagement)</strong></p>
<ul>
<li>OpenAI ads principles announcement (<a href=""https://twitter.com/OpenAI/status/2012223373489614951"">@OpenAI</a>) and Go tier launch (<a href=""https://twitter.com/OpenAI/status/2012223323812270219"">@OpenAI</a>).</li>
<li>Sam Altman on ads rollout/principles (<a href=""https://twitter.com/sama/status/2012253252771824074"">@sama</a>) and “Very fast Codex coming” (<a href=""https://twitter.com/sama/status/2012243893744443706"">@sama</a>).</li>
<li>Viral diffusion “OS in a model” / Neural OS posts (<a href=""https://twitter.com/jxmnop/status/2012048155379220746"">@jxmnop</a>; follow-up details <a href=""https://twitter.com/jxmnop/status/2012283763720601727"">@jxmnop</a>).</li>
</ul>
<hr>
<h1>AI Reddit Recap</h1>
<h2>/r/LocalLlama + /r/localLLM Recap</h2>
<h3>1. New Model and Benchmark Releases</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/"">GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)</a></strong> (Activity: 473): <strong>The December 2025 update to the <strong>SWE-bench leaderboard</strong> features evaluations of several prominent models on 48 new GitHub PR tasks. <strong>Claude Opus 4.5</strong> leads with a <code>63.3%</code> resolved rate, followed by <strong>GPT-5.2 xhigh</strong> at <code>61.5%</code>. Notably, <strong>Gemini 3 Flash Preview</strong> outperforms its Pro counterpart despite being smaller and cheaper, and <strong>GLM-4.7</strong> ranks as the top open-source model, comparable to closed models like GPT-5.1-codex. The performance of <strong>GPT-OSS-120B</strong> in high-effort reasoning mode underscores the benefits of inference-time scaling. For more details, see the <a href=""https://swe-rebench.com/?insight=dec_2025"">SWE-rebench Leaderboard</a>.</strong> Commenters highlight the surprising performance of Gemini 3 Flash Preview and express enthusiasm for GLM-4.7's ranking among the top models, noting skepticism about other benchmarks that overstate the performance of open models like GLM 4.7 or Minimax 2.1.</p>
<ul>
<li>The mention of <strong>Gemini Flash</strong> as a 'real shocker' suggests it performed unexpectedly well in the benchmark, indicating a significant improvement or innovation in its architecture or training that wasn't anticipated by the community.</li>
<li>The <strong>GLM 4.7</strong> model's inclusion in the top 10 of the benchmark is notable because it is an open model, which typically face challenges in competing with proprietary models due to resource constraints. This achievement highlights the model's efficiency and capability, possibly due to recent optimizations or novel techniques.</li>
<li>The skepticism towards benchmarks that equate <strong>GLM 4.7</strong> or <strong>Minimax 2.1</strong> with <strong>Opus 4.5</strong> suggests a belief that these models are not yet on par with Opus 4.5 in terms of performance. This could be due to differences in training data, model architecture, or other technical factors that affect their capabilities.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qdna3t/7x_longer_context_reinforcement_learning_in/"">7x Longer Context Reinforcement Learning in Unsloth</a></strong> (Activity: 288): <strong>The image is a promotional graphic for Unsloth's new capability to extend context lengths in reinforcement learning by up to 7x, reaching up to 12x in some cases. This advancement allows training of models like gpt-oss 20b QLoRA with up to <code>20K</code> context on a <code>24Gb</code> card without accuracy degradation. For larger GPUs, Unsloth can handle <code>380K</code> context on a <code>192GB</code> NVIDIA B200 GPU. The image includes graphs that compare context length against GPU VRAM for different models, showcasing improvements in context length due to new data movement and batching algorithms. These enhancements are achieved without compromising accuracy or speed, and are applicable to various models including Llama and Gemma.</strong> A commenter questions the availability of proper training data for such long contexts, suggesting that real-world tasks may not have sufficient instruction/QA data. Another user inquires about the applicability of these advancements to the Qwen3 30B-3A model.</p>
<ul>
<li>PlasticTourist6527 raises a critical point about the availability of long-context training data, especially for real-world tasks. They suggest that outside of specific domains like coding, there might be a scarcity of high-quality instruction or QA data that can support training models with extended context lengths.</li>
<li>1ncehost reports issues with training a model on ROCm, noting that they had to apply deep patches and replace kernels to resolve problems with the latest versions. They also observed that SDPA was the fastest attention mechanism for the Qwen3 0.6B model, outperforming FA2 and xformers by a significant margin, indicating potential optimizations in attention mechanisms for specific model sizes.</li>
<li>knownboyofno inquires about the applicability of the extended context reinforcement learning approach to the Qwen3 30B-3A model, suggesting interest in understanding the scalability and compatibility of the technique with larger models.</li>
</ul>
</li>
</ul>
<h3>2. High-Performance AI Hardware and Upgrades</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/"">Latest upgrade…A100 40 GB</a></strong> (Activity: 466): <strong>The image showcases a high-performance computer setup that has been upgraded with an NVIDIA A100 GPU, which is significant for AI and machine learning tasks due to its high computational power. The user initially had a gaming rig but transitioned to a more AI-focused setup by acquiring an A100 GPU, which was listed as faulty but turned out to be functional. This upgrade allows for running and training larger AI models efficiently, leveraging the A100's capabilities. The setup includes a GeForce RTX card, RGB-lit fans, and an NZXT liquid cooler, indicating a balance between aesthetics and performance.</strong> The comments reflect a mix of admiration and humor, with one user joking about the risk taken in purchasing a potentially faulty GPU and another referencing a meme about NVIDIA's CEO, Jensen Huang.</p>
<ul>
<li>matatonic raises a critical point about cooling for the A100 40 GB, noting that it appears to be a passively cooled version. They suggest using a blower fan or another active cooling method to prevent overheating. Additionally, they mention the possibility of using water cooling solutions, which are available on platforms like AliExpress, to ensure the GPU operates within safe temperature ranges.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLM/comments/1qcmmvw/m4m5_max_128gb_vs_dgx_spark_or_gb10_oem/"">M4/M5 Max 128gb vs DGX Spark (or GB10 OEM)</a></strong> (Activity: 188): <strong>The user is comparing the NVIDIA DGX Spark and a MacBook Pro with M4 Max (128GB RAM) for local LLM inference, primarily for coding tasks such as code completion and refactoring. The DGX Spark offers a CUDA ecosystem and strong GPU compute, while the MacBook Pro benefits from unified memory and Apple's ML stack. For inference tasks, the MacBook's higher memory bandwidth is advantageous, but it may not match the performance of cloud-based solutions like Claude. The M5 chip shows improved performance over the M4, and new MacBook models may be released soon. The MacBook is noted for faster inference, but NVIDIA's CUDA support is more comprehensive. The Mac Studio with M4 Max is suggested as a cost-effective alternative if portability is not required.</strong> Commenters debate the performance of Apple Silicon versus NVIDIA hardware, with some asserting that the MacBook Pro offers superior text generation performance due to its memory bandwidth, while others highlight NVIDIA's broader capabilities in fine-tuning and multimodal tasks. The discussion also touches on the potential cost-effectiveness of the Mac Studio for non-portable use.</p>
<ul>
<li>The M4 Max offers significantly higher memory bandwidth compared to the DGX Spark, which is beneficial for inference tasks. However, the Spark benefits from better support for frameworks due to its compatibility with NVIDIA's CUDA. This makes the MacBook faster for inference, but the Spark is more versatile for tasks like fine-tuning and image generation.</li>
<li>The M3 Ultra Mac Studio is highlighted as superior for pure text generation tasks compared to the DGX Spark. While NVIDIA hardware is generally more capable on paper, the M3 Ultra reportedly outperforms in specific LLM inference tasks. This is attributed to the Mac's efficiency in handling agentic coding workflows, despite the Spark's broader capabilities in other areas.</li>
<li>The DGX Spark is noted for its compact size and energy efficiency, consuming less than 100W and idling at around 10W. It is praised for its extensibility, allowing for additional units to be connected. However, concerns about bandwidth limitations are raised, and the cost comparison with alternatives like the GB10 OEM and MacBook Pro is discussed.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qdh28f/rtx_5070_ti_and_rtx_5060_ti_16_gb_no_longer/"">RTX 5070 Ti and RTX 5060 Ti 16 GB no longer manufactured</a></strong> (Activity: 414): <strong><strong>Nvidia</strong> has ceased production of the <code>RTX 5070 Ti</code> and significantly reduced the supply of the <code>RTX 5060 Ti 16 GB</code> due to memory supply shortages, leading to a price increase of approximately <code>$100</code> over MSRP for the 5070 Ti. The 8 GB configuration of the RTX 5060 Ti remains unaffected. This decision impacts most AIBs, who will no longer manufacture these GPUs. <a href=""https://m.youtube.com/watch?v=yteN21aJEvE"">Source</a>.</strong> One user noted the RTX 5060 Ti 16 GB as a cost-effective option for adding Nvidia memory to systems, highlighting its suitability for DLSS, AI processing, and inferencing tasks, especially with <code>64GB VRAM</code> for <code>70B models</code>. Another user expressed disappointment over the halted production affecting their upgrade plans, while a third criticized Nvidia's business practices.</p>
<ul>
<li>The RTX 5060 Ti 16 GB is highlighted as a cost-effective option for adding Nvidia memory to systems, especially for tasks like image generation, inferencing, and gaming. At a price point of around <code>$350-$390</code>, it offers good value with features like DLSS and AI processing capabilities. The card's <code>16 GB GDDR7</code> memory compensates for its <code>128-bit bus</code>, making it comparable to a <code>192-bit bus GDDR6</code> card, thus supporting demanding tasks like DLSS and ray tracing without sacrificing texture quality.</li>
<li>The RTX 5060 Ti 16 GB is noted for its suitability in budget inferencing setups, particularly for those unable to access RTX 3090s. With the ability to fit multiple cards into a standard power supply machine, it supports new quantization methods and can handle <code>70B models</code> effectively with <code>64 GB VRAM</code>. This makes it a viable option for small-scale AI tasks, leveraging its memory capacity and efficiency for practical applications.</li>
</ul>
</li>
</ul>
<h3>3. Local LLM Community and Innovations</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLM/comments/1olbrch/mod_post_announcing_the_rlocalllm_30day/"">[MOD POST] Announcing the r/LocalLLM 30-Day Innovation Contest! (Huge Hardware &#x26; Cash Prizes!)</a></strong> (Activity: 120): <strong>The r/LocalLLM subreddit has launched a <strong>30-Day Innovation Contest</strong> focused on open-source projects for AI inference or fine-tuning, with significant hardware and cash prizes. The contest encourages submissions of innovative projects such as new serving frameworks, quantization methods, fine-tuning techniques, or performance benchmarks, using diverse hardware like <strong>NVIDIA, Google Cloud TPU,</strong> or <strong>AMD</strong>. The top prize includes an <strong>NVIDIA RTX PRO 6000</strong> and cloud time on an <strong>8x NVIDIA H200 server</strong>. Participants are encouraged to submit their projects via a new post on r/LocalLLM with the 'Contest Entry' flair, including a public repository link and demonstration materials.</strong> One commenter expressed enthusiasm for saving projects for future exploration, while another inquired about sharing projects for community inspiration. A third commenter sought clarification on the submission process, indicating interest in participating.</p>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLM/comments/1qcu498/small_ai_computer_runs_120b_models_locally_any/"">Small AI computer runs 120B models locally: Any use cases beyond portability and privacy?</a></strong> (Activity: 107): <strong><strong>TiinyAI</strong> has developed a compact AI device capable of running <code>120B</code> parameter models locally with <code>80GB RAM</code> and a power consumption of <code>30W</code>. This device is positioned as a more portable and cost-effective alternative to larger systems like the <strong>DGX Spark</strong>, which offers <code>128GB RAM</code> and higher performance but at a greater cost and size. The TiinyAI device is particularly notable for its potential applications in scenarios where <strong>portability</strong> and <strong>privacy</strong> are prioritized over raw performance, such as in field operations or environments with limited internet access. However, concerns remain about its <strong>memory bandwidth</strong>, which is speculated to be between <code>80Gb/s</code> and <code>200Gb/s</code>, potentially limiting its performance compared to traditional PCs or laptops.</strong> Commenters express skepticism about the device's price and availability, with one noting that $1400 seems high for an 80GB RAM SBC. Another highlights the device's potential utility in scenarios where internet access is restricted, such as under authoritarian regimes.</p>
<ul>
<li>A key technical concern raised is the memory bandwidth of the small AI computer, with estimates ranging from 80Gb/s to 200Gb/s. This bandwidth is crucial for running large models like 120B parameters efficiently. If the bandwidth is on the lower end, it may not outperform a regular PC or laptop, which could limit its utility for high-performance tasks.</li>
<li>The pricing of the device, speculated to be around $1400 for an 80GB RAM single-board computer (SBC), is questioned. The skepticism is due to the lack of availability for immediate purchase, which raises doubts about the feasibility and practicality of the device at this price point.</li>
<li>The device's built-in microphone and speaker suggest potential use as a private AI assistant. This setup could allow users to run automation scripts and manage tasks locally, providing a privacy-focused alternative to cloud-based assistants like Alexa or Siri. This use case leverages the device's ability to handle personal data securely without cloud dependency.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/"">I fucking love this community</a></strong> (Activity: 469): <strong>The post highlights the ability to run large models like <code>nemotron-3-nano-30B-a3b-iq4_nl</code> at <code>14-13.5 t/s</code> on a decade-old PC with only <code>4GB VRAM</code>, thanks to optimizations from projects like <strong>llama.cpp</strong> and <strong>vllm</strong>. The key to achieving this performance is leveraging a significant amount of system memory and utilizing models with a <em>Mixture of Experts (MoE)</em> architecture, which allows for efficient resource usage and performance on limited hardware.</strong> Commenters express amazement at the performance achieved on old hardware, emphasizing the effectiveness of combining system RAM with MoE architectures. There's also interest in accessing resources or posts that detail these optimizations for running large models on low-end equipment.</p>
<ul>
<li>InfiniteLand7364 highlights achieving <code>14 t/s</code> (tokens per second) on a decade-old system, emphasizing the community's skill in optimizing older hardware for performance. This suggests that with the right tweaks, even outdated systems can handle tasks typically reserved for newer machines.</li>
<li>Rokpiy mentions the effectiveness of combining system RAM with 'moe' (likely referring to a specific optimization or model configuration), which is often overlooked but offers practical benefits. This implies that leveraging existing hardware resources creatively can enhance performance without needing the latest technology.</li>
<li>cosimoiaia discusses the educational value of working within hardware constraints, suggesting that it forces users to learn deeply about model tuning and system optimization. This experience not only improves current performance but also prepares users for future technological advancements by understanding what hardware and configurations are most effective.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/"">My story of underestimating /r/LocalLLaMA's thirst for VRAM</a></strong> (Activity: 1291): <strong>The image is a meme that humorously illustrates the unintended consequences of sharing technical insights on Reddit. The original poster bought a w6800 32GB graphics card for $500, found it to perform well, and shared this information on Reddit. This led to a significant increase in the card's price to over $1,000, highlighting the impact of community discussions on market dynamics. The post underscores the high demand for VRAM in the /r/LocalLLaMA community, which can drive up prices when a product is recommended.</strong> One commenter humorously compares the situation to the California gold rush, suggesting strategic withholding of information to capitalize on market opportunities. Another commenter provides technical advice, suggesting alternatives like the 3090 or R9700 for those concerned with VRAM and cooling solutions.</p>
<ul>
<li>EmPips discusses the trade-offs between different GPU models for VRAM-intensive tasks. They suggest that while the card in question is impressive, the <strong>NVIDIA RTX 3090</strong> might be a better choice at current prices. Alternatively, they recommend the <strong>AMD Radeon Pro VII (R9700)</strong> for those who prioritize VRAM-per-slot and are okay with high idle power and external cooling, suggesting the <strong>AMD MI50</strong> as another option for those willing to manage these factors.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLM/comments/1qcuyh2/what_is_the_biggest_local_llm_that_can_fit_in/"">What is the biggest local LLM that can fit in 16GB VRAM?</a></strong> (Activity: 155): <strong>The largest local LLM that can fit in 16GB VRAM, such as on an RTX 5080, is typically around <code>14B</code> parameters when considering practical usage constraints. This is due to the need to leave room for context, which means a model file size should ideally be around <code>14GB</code>. Models like <code>GPT-OSS-20B</code> can run but may require significant quantization, potentially below <code>4-bit</code>, which can degrade quality. For optimal performance without excessive slowdowns, models around <code>14B</code> are recommended. Users can check model sizes on platforms like <a href=""https://huggingface.co/"">HuggingFace</a> to ensure they fit within VRAM limits.</strong> Commenters suggest that while models up to <code>30B</code> might technically fit with aggressive quantization, the performance and quality trade-offs make <code>14B</code> a more practical choice. The importance of considering model file size over parameter count is emphasized, as exceeding VRAM capacity leads to slowdowns due to RAM overflow.</p>
<ul>
<li>BigYoSpeck discusses the performance of various models on a system with a Ryzen 9 5900x, 64GB DDR4 3800, and a 16GB Radeon RX 6800 XT. They report running <code>gpt-oss-20b</code> at over 120 tokens per second, <code>Qwen3 30b</code> partially offloaded to CPU at about 40 tokens per second, and <code>gpt-oss-120b</code> with 32 MOE layers offloaded to CPU at 23 tokens per second. This suggests that with a similar setup, one might achieve even better performance.</li>
<li>SKirby00 highlights the limitations of running large models on 16GB VRAM, noting that models like <code>Qwen3-Coder-30B</code> require significant VRAM and context space. They suggest that a 14.5GB model might technically fit but would be impractical due to limited context space. They recommend aiming for models around the 14B parameter range for better usability, given the constraints of 16GB VRAM.</li>
<li>vertical_computer emphasizes the importance of considering model file size relative to VRAM capacity. They suggest that a model should ideally be around 14GB to fit within 16GB VRAM, leaving room for context. They provide an example with the <code>Nvidia Llama 3.3 Nemotron 49B</code> model, noting that larger models will spill over into RAM, significantly slowing down performance.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/LocalLLM/comments/1qdiwdh/oh_dear/"">Oh Dear</a></strong> (Activity: 115): <strong>The image depicts a malfunction in an AI model's response, where it outputs a repetitive string of 'the,' suggesting a potential issue with the model's configuration or prompt handling. This could be due to an incorrect system prompt or tuning parameters like temperature not being set appropriately. The comments suggest checking the system prompt and ensuring it aligns with the model's requirements, as some models may not function correctly without a proper system prompt.</strong> Commenters suggest that the issue might be related to the absence of a system prompt or incorrect tuning parameters, such as temperature, which are crucial for generating coherent responses.</p>
<ul>
<li>mp3m4k3r suggests checking the tuning parameters, specifically the temperature setting, to ensure it aligns with the model's recommended usage. This is crucial for maintaining the model's performance and preventing issues like repetitive outputs.</li>
<li>HealthyCommunicat recommends adjusting the repeat penalty, starting at <code>1.1</code> and increasing if necessary. This adjustment can help mitigate issues with local LLMs producing repetitive text. Additionally, they advise ensuring the model isn't using more experts than recommended, which can also lead to performance problems.</li>
<li>ScoreUnique mentions using 'pocket pal' for loading <code>gguf</code> files, which could be a solution for handling specific file types or formats in local LLM setups. This tool might be beneficial for users dealing with compatibility or loading issues.</li>
</ul>
</li>
</ul>
<h2>Less Technical AI Subreddit Recap</h2>
<blockquote>
<p>/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo</p>
</blockquote>
<h3>1. Claude and Gemini Model Updates and Issues</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/ClaudeAI/comments/1qeo736/official_claude_cowork_is_now_available_to_pro/"">Official: Claude Cowork is now available to ""Pro"" subscribers</a></strong> (Activity: 353): <strong><strong>Claude Cowork</strong> is now available to ""Pro"" subscribers, as announced by Claude on X.com. This feature, still in research preview, includes session renaming, connector improvements, and fixes based on early feedback. However, it is noted that Pro users might reach their usage limits faster due to Cowork's capability to handle more complex tasks. The announcement also provides a link to try it in the macOS app.</strong> Users express concerns about hitting usage limits quickly, with one user noting that sorting 459 files used 97% of their session limit. Another user comments on the restrictive usage limits of Claude, while a third hopes for useful applications despite not using Claude for coding.</p>
<ul>
<li>A user reported that using Claude Cowork for sorting 459 files consumed 97% of their session's usage limit, highlighting the restrictive nature of the current usage caps. This suggests that the tool may not be suitable for high-volume tasks without hitting limits quickly.</li>
<li>Another user expressed dissatisfaction with Claude's usage limits, indicating that they are among the worst compared to other services. This sentiment suggests that the current limitations may hinder productivity and user satisfaction, especially for those who rely on the tool for extensive tasks.</li>
<li>A user mentioned their reluctance to upgrade to a 'max plan' due to not using Claude for coding, implying that the current subscription tiers may not align well with diverse user needs. This points to a potential gap in the service offerings for non-coding related use cases.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/ClaudeAI/comments/1qegsta/announcing_claude_flow_v3_a_full_rebuild_with_a/"">🌊 Announcing Claude Flow v3: A full rebuild with a focus on extending Claude Max usage by up to 2.5x</a></strong> (Activity: 291): <strong><strong>Claude Flow v3</strong> is a comprehensive rebuild of the AI orchestration platform, designed to enhance the usage of Claude Max by up to <code>2.5x</code>. The system, rewritten in <strong>TypeScript</strong> and <strong>WASM</strong>, features a modular architecture that supports deploying multi-agent swarms with shared memory and continuous learning. It reduces token consumption by <code>75-80%</code> and improves subscription capacity by <code>250%</code>. The platform is built on <code>npm RuVector</code> with deep <strong>Rust</strong> integrations and supports offline execution, allowing for local model use without consuming tokens. Governance is enforced through ADRs, DDD boundaries, and SPARC, ensuring traceability and security. The system operates as an always-on daemon with live updates and automated tasks for optimization and security audits. For more details, see the <a href=""https://github.com/ruvnet/claude-flow"">GitHub repository</a>.</strong> Some commenters express skepticism about the claims, noting the use of buzzwords and unsubstantiated performance metrics, while others are intrigued by the potential of multi-agent systems but question their practical effectiveness compared to base LLMs.</p>
<ul>
<li>janusr raises concerns about the project's claims, highlighting the use of buzzwords and unsubstantiated metrics such as 'Agent Booster 352x faster' without clear benchmarks or comparisons. They question the relevance of ONNX Embeddings being '75x faster than Transformers.js' to the project's goals, suggesting skepticism about the practical benefits of these claims.</li>
<li>Infamous_Research_43 expresses skepticism about frameworks claiming to manage large swarms of agents, noting a pattern of such projects failing to deliver on their promises. They argue that many creators lack a fundamental understanding of AI and agent-based systems, often confusing them with LLM chatbots, and warn that these projects are frequently scams or poorly executed.</li>
<li>sridoodla mentions issues with outdated documentation in previous versions and inquires about the stability of v3, indicating a need for reliable and up-to-date resources to effectively utilize the tool. This highlights a common challenge in rapidly evolving AI projects where documentation often lags behind development.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/GeminiAI/comments/1qemf0h/today_gemini_3_pro_became_unusable_to_me_as_a_pro/"">Today, Gemini 3 Pro became unusable to me as a Pro subscriber</a></strong> (Activity: 183): <strong>A user reports that <strong>Gemini 3 Pro</strong>, a tool they have relied on for building complex applications, has become unusable due to a significant drop in performance. The user experienced an issue where the model provided irrelevant code ('Shopping Cart' instead of a document upload feature), indicating potential problems with the model's context understanding. This aligns with other users' observations of a reduced context window, which may lead to increased hallucinations. Some users suggest alternatives like <strong>GPT 5.2 Thinking</strong> for better performance.</strong> There is a debate on the model's performance, with some users experiencing significant issues due to a reduced context window, while others still find it effective for different tasks, such as philosophical discussions. The discussion highlights a divide in user experience, possibly due to varying use cases.</p>
<ul>
<li>xbrasil highlights a significant reduction in the context window for Gemini 3 Pro, even for paying users, which has led to increased hallucinations and decreased usability. They suggest that GPT 5.2 Thinking is a viable alternative, indicating a shift in user preference due to perceived neglect from Google.</li>
<li>VanillaSwimming5699 compares Gemini 3 Pro favorably for coding tasks, noting its deep philosophical discussion capabilities. However, they mention that '3 flash' might be superior due to faster iteration and lower costs, while Opus 4.5 is also competitive but has an earlier knowledge cutoff.</li>
<li>TheLawIsSacred shares that Gemini 3 has been largely unusable recently, but they are waiting for potential improvements based on past experiences with model updates. They currently rely on alternatives like Claude Desktop app (Opus 4.5), Perplexity Pro (Sonnet 4.5 with Reasoning), and ChatGPT (5.2) for reliable performance.</li>
</ul>
</li>
</ul>
<h3>2. AI Model and Benchmark Releases</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/MachineLearning/comments/1qeakhz/r_china_just_released_first_sota_multimodal_model/"">[R] China just released first SOTA multimodal model trained entirely on domestic chips</a></strong> (Activity: 49): <strong><strong>Zhipu AI</strong> and <strong>Huawei</strong> have released <strong>GLM-Image</strong>, a state-of-the-art multimodal model trained entirely on <strong>Huawei Ascend 910</strong> chips, marking a significant milestone in AI development using domestic hardware. The model employs a hybrid architecture with an autoregressive and diffusion decoder, excelling in Chinese text rendering, and supports resolutions from <code>1024 to 2048</code> without additional training. It offers both text-to-image and image-to-image generation capabilities, with API pricing set at <code>0.1 yuan</code> per image. Notably, the model claims <code>60%</code> better compute efficiency than Nvidia's H200 in terms of tokens per joule, challenging the reliance on Nvidia hardware for training advanced models. The model's repositories are available on <a href=""https://github.com"">GitHub</a> and <a href=""https://huggingface.co"">Hugging Face</a>.</strong> A key technical question raised is about the model's compatibility with frameworks like PyTorch and cuDNN, given its development on non-Nvidia hardware, and whether it can be executed on other machines.</p>
<ul>
<li>The discussion revolves around the technical feasibility of running a state-of-the-art multimodal model on non-NVIDIA hardware, specifically using domestic Chinese chips. The commenter questions the compatibility of such models with frameworks like PyTorch and cuDNN, which are traditionally optimized for NVIDIA GPUs. This raises concerns about the adaptability of these models to other hardware environments and the potential need for alternative libraries or custom solutions to achieve similar performance levels.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/MachineLearning/comments/1qehwlu/d_why_mamba_rewrote_its_core_algorithm_and/"">[D] Why Mamba rewrote its core algorithm and Microsoft abandoned RetNet</a></strong> (Activity: 131): <strong><strong>Mamba-2</strong> has restructured its core algorithm from parallel scans, which utilized <code>10-20%</code> of Tensor Core capacity, to block-diagonal GEMMs, achieving <code>60-70%</code> utilization, optimizing for NVIDIA's hardware. Meanwhile, <strong>Microsoft Research</strong> published <strong>RetNet</strong> in July 2023, a promising architecture at <code>6.7B</code> parameters, but quickly shifted focus to dense Transformers with Phi-2, Phi-3, and Phi-4, indicating a lack of institutional backing for RetNet. This pattern highlights the co-evolution of Transformers and NVIDIA GPUs, creating a stable attractor that is difficult to break due to the dual challenges of hardware compatibility and institutional support. The essay includes Tensor Core utilization statistics, analysis of alternative chip vendors, and predictions for 2028. <a href=""https://open.substack.com/pub/lambpetros/p/the-transformer-attractor"">Full essay link</a>.</strong> Commenters agree on the trend of co-evolution between model architectures and hardware, noting that incentives favor incremental improvements over radical changes. The RetNet case is debated, with uncertainty about whether its abandonment was due to hardware issues, quality concerns, or risk aversion. Some suggest that experimental architectures like RetNet may still influence future developments, as seen with some large Chinese models.</p>
<ul>
<li>The comment by thearn4 highlights a trend in machine learning and high-performance computing (HPC) where there is a coevolution of model formulation, solver structure, and hardware. This trend suggests that incremental development is often favored over radical changes due to better incentives, which is a common pattern across various technical fields.</li>
<li>petroslamb points out the ambiguity surrounding Microsoft's abandonment of RetNet, noting that the lack of public experiments makes it unclear whether the decision was due to hardware scaling issues, quality degradation beyond a certain model size, or risk aversion. This highlights a gap in transparency that could inform future research and development in model architectures.</li>
<li>Xemorr challenges the assumption that parallel scans can be optimized as effectively as block-diagonal General Matrix Multiply (GEMM) operations, suggesting a technical debate on the efficiency of different computational strategies in model training and inference.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/MachineLearning/comments/1qeips6/d_icassp_2026_results/"">[D] ICASSP 2026 Results</a></strong> (Activity: 73): <strong>The post discusses a potential early access to ICASSP 2026 acceptance results through a specific <a href=""https://cmsworkshops.com/ICASSP2026/author_invitation_request.php"">link</a>. Users who could send an invitation email through this link might have had their papers accepted. The email confirms acceptance for presentation at the IEEE ICASSP 2026 in Barcelona, Spain, from May 3-8, 2026. However, an update indicates that the link is currently inaccessible, showing an error message: <em>'Error: No match for paper number and password. 0x4C'.</em></strong> Comments indicate confusion about the accessibility of the results, with some users reporting initial access followed by subsequent errors, suggesting a possible bug that was later fixed.</p>
</li>
</ul>
<h3>3. AI Tools and User Experiences</h3>
<ul>
<li>
<p><strong><a href=""https://www.reddit.com/r/ClaudeCode/comments/1qeb6od/why_ai_coding_tools_accidentally_feel_perfect_for/"">Why AI coding tools accidentally feel perfect for inattentive ADHD brains</a></strong> (Activity: 238): <strong>The post discusses how AI coding tools, like <strong>Claude Code</strong>, align well with inattentive ADHD brains due to their reliance on pattern recognition and external context rather than linear recall and memorization. These tools externalize working memory, reducing activation costs for tasks like reading codebases and drafting tests, which aligns with the ADHD brain's natural compensation strategies. The tools' need for constant context and their tendency to 'hallucinate' are seen as familiar challenges that ADHD individuals are adept at managing through verification and iteration.</strong> Commenters highlight how AI tools complement ADHD traits by allowing for non-linear thinking and externalizing chaotic thought processes, thus reducing burnout and enhancing creativity. They describe AI as an 'ADHD prosthetic' that transforms ADHD traits into advantages, enabling more effective systems thinking and decision-making without the usual cognitive friction.</p>
<ul>
<li>texo_optimo discusses the evolution of their AI prompting system into a comprehensive context management tool, highlighting the use of a governance remote MCP server as a project board to maintain architectural decisions. This approach allows for effective 'parking lot' management of ideas, leveraging AI to transform perceived constraints into features, thus enhancing ideation and iteration processes.</li>
<li>nnennahacks emphasizes the synergy between AI tools and ADHD cognitive patterns, noting that AI facilitates seamless context switching and externalization of thoughts. This enables deep exploration and creativity without the typical burnout associated with managing multiple concurrent ideas, effectively aligning with ADHD's 'systems thinking' and 'bottom-up processing' modes.</li>
<li>drumnation describes AI as a transformative tool for ADHD, acting as a 'prosthetic' that mitigates cognitive bottlenecks. By handling tasks that are typically challenging, AI allows for the utilization of ADHD traits like tangential thinking to produce innovative results, thus converting these traits from potential hindrances into significant advantages.</li>
</ul>
</li>
<li>
<p><strong><a href=""https://www.reddit.com/r/ClaudeCode/comments/1qeb8x4/whats_going_on_with_opus/"">Whats going on with Opus?</a></strong> (Activity: 220): <strong>The post discusses issues with <strong>Claude</strong> and its integration with an internal dashboard, specifically problems with routing through a proxy express server and endpoint hallucinations. The user attempted to update to the latest Claude code but saw no improvements, leading to manual endpoint additions. This raises questions about the potential release of a new model. <strong>Claude</strong> is experiencing performance degradation, as noted by users who report issues with project management and task execution, suggesting a decline since the public release of the latest <strong>Opus</strong> version.</strong> Commenters express frustration with <strong>Claude's</strong> reliability, noting a decline in performance and increased dependency risks. Some are considering alternatives like <strong>Codex</strong> due to these issues, highlighting the importance of not relying solely on one tool or company for development needs.</p>
<ul>
<li>Users are expressing frustration with the performance of Opus, particularly noting a significant degradation in its ability to handle projects. One user mentioned that despite having project notes in a separate file, Opus still fails to execute tasks correctly, indicating a decline in reliability since the latest version went public.</li>
<li>There is a concern about over-reliance on a single tool or company, as highlighted by a user who had integrated Opus extensively into their workflow. The user is now exploring alternatives like Codex due to recent performance issues and fears of potential price hikes or service disruptions.</li>
<li>A performance tracker for Claude Code Opus 4.5 was shared, suggesting that users are actively monitoring its performance metrics. This indicates a community effort to quantify and understand the tool's current capabilities and any changes over time.</li>
</ul>
</li>
</ul>
<hr>
<h1>AI Discord Recap</h1>
<blockquote>
<p>A summary of Summaries of Summaries by gpt-5.2</p>
</blockquote>
<p><strong>1. ChatGPT Go + Ads: Monetization Meets UX</strong></p>
<ul>
<li>
<p><strong><strong>Go Go Gadget Tier</strong></strong>: OpenAI launched <strong>ChatGPT Go</strong> at <strong>$8/month</strong> with <strong>10× more messages</strong>, <strong>file uploads</strong>, <strong>image creation</strong>, <strong>extended memory/context</strong>, and unlimited <strong>GPT 5.2 instant</strong> access per <a href=""https://openai.com/index/introducing-chatgpt-go/"">“Introducing ChatGPT Go”</a>.</p>
<ul>
<li>Across Discords, people treated Go as a clear signal of <strong>more subscription tiers</strong> coming (including jokes like <em>“When $80 tier?”</em>) while watching how it stacks up against Plus/Pro/Enterprise staying <strong>ad-free</strong>.</li>
</ul>
</li>
<li>
<p><strong><strong>Ads, But Don’t Touch My Tokens</strong></strong>: OpenAI said it will begin testing <strong>ads</strong> in <strong>ChatGPT Free and Go</strong> in the coming weeks, with the rule that ads are <strong>clearly labeled</strong>, <strong>separate</strong>, and <strong>won’t influence responses</strong>, per <a href=""https://openai.com/index/our-approach-to-advertising-and-expanding-access/"">“Our approach to advertising and expanding access”</a>.</p>
<ul>
<li>Community reaction split between resignation (<em>“got eaten by corposlop”</em>) and skepticism about enforcement, especially alongside reports of scam apps impersonating OpenAI and “ads” TestFlight bait in the wild.</li>
</ul>
</li>
<li>
<p><strong><strong>Benchmarks Lie (Sometimes) and Interfaces Matter</strong></strong>: Latent Space shared Anthropic’s claim that <strong>METR</strong> benchmarks can underestimate real model <strong>time horizons</strong> by <strong>1.75× to 9.5×</strong>, depending on whether the interface is <strong>API vs web app</strong>, via <a href=""https://xcancel.com/_simonsmith/status/2011928926864454133?s=61"">Simon Smith’s post</a>.</p>
<ul>
<li>That sparked meta-discussion that “capability” measurements may be as much about <strong>product surface area</strong> (tools, UX constraints, rate limits) as about raw model weights.</li>
</ul>
</li>
</ul>
<p><strong>2. Agentic Coding Tools: Rate Limits, Racks of Bills, and Billing Pain</strong></p>
<ul>
<li>
<p><strong><strong>Cursor Ultra Eats Wallets for Breakfast</strong></strong>: Cursor users reported rapid spend on the <strong>Ultra plan</strong>, including <strong>20% of usage</strong> burned on a single “orchestrator run” and <strong>$2 in ~5 minutes</strong>, with complaints about subagent control on <strong>nightly builds</strong> and PC crashes (with a feature screenshot) <a href=""https://cdn.discordapp.com/attachments/1074847527708393565/1461451586256638197/image.png"">image</a>.</p>
<ul>
<li>The vibe: agentic IDEs feel less like chatboxes and more like <strong>multi-model job schedulers</strong>, and users want <strong>small models for subagents</strong> + <strong>big models for main agents</strong> without the toolchain falling apart.</li>
</ul>
</li>
<li>
<p><strong><strong>Qoder’s $400/mo Hangover</strong></strong>: One Cursor community member said <strong>Qoder</strong> usage hit rate limits while costing about <strong>$400/month</strong>, comparing it to <em>“gambling or heroin”</em> and looking for cheaper alternatives like <strong>Claude Code</strong>.</p>
<ul>
<li>The cost story echoed other servers: people want transparent <strong>usage accounting</strong> and guardrails before an agent run quietly detonates their monthly budget.</li>
</ul>
</li>
<li>
<p><strong><strong>Gemini CLI Burns 10M Tokens Like It’s Nothing</strong></strong>: Perplexity users reported pushing <strong>Gemini CLI</strong> to <strong>10,000,000 tokens/day</strong>, estimating <strong>~$120/day</strong> and projecting <strong>~$4000/month</strong> at posted pricing if sustained.</p>
<ul>
<li>The thread framed token-heavy CLI workflows as a new class of “silent spender,” where model quality matters less than <strong>rate-limit ergonomics</strong> and <strong>cost observability</strong>.</li>
</ul>
</li>
<li>
<p><strong><strong>Credit Systems Break, Engineers Wanted</strong></strong>: On Manus, users hit <strong>payment/credit</strong> problems (membership upgrades, Link, card/Alipay) while another engineer pitched building more reliable <strong>credit-based usage tracking/billing</strong> systems.</p>
<ul>
<li>Taken together with the IDE spend horror stories, the recurring ask was clear: platforms need <strong>harder metering</strong>, better <strong>quota UX</strong>, and fewer “surprise invoice” moments.</li>
</ul>
</li>
</ul>
<p><strong>3. Model + Tooling Drops: Translation, Tool-Use, and Speed Wars</strong></p>
<ul>
<li>
<p><strong><strong>Translate Gemma Touches Down on Hugging Face</strong></strong>: Google launched <strong>Translate Gemma</strong>, published as a Hugging Face collection: <a href=""https://huggingface.co/collections/google/translategemma"">“translategemma”</a>.</p>
<ul>
<li>It landed alongside broader Gemma chatter and served as a concrete “shipping artifact” people could actually pull into pipelines, unlike more speculative model rumors.</li>
</ul>
</li>
<li>
<p><strong><strong>K2 Turbo Floors It to 73 tps</strong></strong>: Moonshot users benchmarked <strong>K2 Turbo</strong> at <strong>~73 tps</strong> vs standard <strong>K2 ~28 tps</strong>, comparing against <strong>MiniMax m2.1 ~38 tps</strong> and <strong>Z.Ai GLM-4.7 ~41 tps</strong> (with uptime complaints).</p>
<ul>
<li>They also flagged a new <strong>Slides + Vision</strong> feature powered by a newer K2 vision model, with an example preset that searches online for visual references <a href=""https://cdn.discordapp.com/attachments/1371757564005711973/1461508342424797184/image.png?ex=696c20b6&#x26;is=696acf36&#x26;hm=70de4ffdcbffa4e7d4572daa8219dad2dfca998f7c15976ce0930997007fdec6&#x26;"">screenshot</a>.</li>
</ul>
</li>
<li>
<p><strong><strong>Claude Does Parallel Tool Use in One Shot</strong></strong>: OpenRouter members pointed to Anthropic docs showing <strong>Claude</strong> can run <strong>multi tool calls</strong> in <strong>one API request</strong>, including a “parallel tool use” control section: <a href=""https://platform.claude.com/docs/en/agents-and-tools/tool-use/implement-tool-use#controlling-claudes-output"">Claude tool use docs</a>.</p>
<ul>
<li>The discussion framed this as an agent-architecture unlock: fewer request/response loops, cleaner tool orchestration, and potentially lower latency/cost for complex workflows.</li>
</ul>
</li>
<li>
<p><strong><strong>Hawk Ultra Tries to One-Shot Opus</strong></strong>: LMArena users hyped <strong>Hawk Ultra</strong> from <a href=""https://movementlabs.ai/"">MovementLabs.AI</a>, claiming it can emit <strong>9.5k+</strong> (even <strong>20k+</strong>) lines of code from a single prompt, plus an “Opus killer” vibe, with an <a href=""https://x.com/movementlabsAI/status/2011964766533632380?s=20"">X post</a>.</p>
<ul>
<li>People immediately asked about comparisons to <strong>Gemini 3 Pro</strong> and whether Hawk Ultra might go open-source, treating it as a “code firehose” model class rather than a chat model.</li>
</ul>
</li>
</ul>
<p><strong>4. Evaluation + Benchmarks: Fixes, Leaderboards, and PDF Chat</strong></p>
<ul>
<li>
<p><strong><strong>MMLU-Pro Gets Patched (Finally)</strong></strong>: Eleuther shared a fix discussion for <strong>TIGER-Lab/MMLU-Pro</strong> and a corresponding patch in <strong>lm-evaluation-harness</strong>: <a href=""https://github.com/EleutherAI/lm-evaluation-harness/pull/3500"">PR #3500</a> and <a href=""https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro/discussions/41"">dataset thread</a>.</p>
<ul>
<li>The takeaway was pragmatic: if your MMLU-Pro numbers looked off, you likely needed the harness patch—not another week of hyperparameter superstition.</li>
</ul>
</li>
<li>
<p><strong><strong>OpenCompass Makes Eval JSON Less Painful</strong></strong>: Unsloth users called out <strong>OpenCompass</strong> for running prompts and emitting <strong>well-formatted JSON</strong>, sharing performance comparisons on an <strong>L4</strong> vs a <strong>3060</strong> laptop.</p>
<ul>
<li>It came up as a “glue tool” for reproducible evaluation workflows, especially when people want quick, structured outputs from many prompts/models.</li>
</ul>
</li>
<li>
<p><strong><strong>LM Arena Adds PDF Chat (Some Models Only)</strong></strong>: LMArena users said Arena is experimenting with <strong>PDF support</strong> for document uploads and interactive chat, with excitement like <em>“FINALLY CAN CHAT WITH PDFS!!!”</em>.</p>
<ul>
<li>Others noted uneven model support and ongoing reliability issues, so PDF chat feels like a feature racing ahead of platform stability.</li>
</ul>
</li>
<li>
<p><strong><strong>Image Leaderboards Shuffle: flux.2-klein Climbs</strong></strong>: LMArena updated its leaderboards: <code>flux.2-klein-9B</code> hit <strong>#15</strong> and <code>flux.2-klein-4B</code> <strong>#21</strong> on Image Edit, while Text-to-Image listed <code>z-image-turbo</code> <strong>#22</strong>, <code>flux.2-klein-9B</code> <strong>#24</strong>, <code>flux.2-klein-4B</code> <strong>#31</strong>, per the <a href=""https://lmarena.ai/blog/leaderboard-changelog/"">Leaderboard Changelog</a>.</p>
<ul>
<li>The leaderboard churn reinforced how quickly image models iterate, with “small-ish” variants steadily crowding the mid ranks rather than a single dominant release.</li>
</ul>
</li>
</ul>
<p><strong>5. GPU + Systems Reality: Performance Is a Policy Decision</strong></p>
<ul>
<li>
<p><strong><strong>Runpod Undervolting Turns A100 vs H100 into a Coin Flip</strong></strong>: Unsloth users reported some Runpod providers <strong>undervolt GPUs without notice</strong>, causing inconsistent performance and even broken setups like <em>“a100 nodes where nccl literally just doesn’t work”</em>.</p>
<ul>
<li>The practical stance was to treat cloud GPU selection as a reliability problem, not just a FLOPs/$ problem—some still preferred <strong>A100</strong> for cost-effective LM tuning when nodes behave.</li>
</ul>
</li>
<li>
<p><strong><strong>Your Benchmark Slept, Your GPU Downclocked</strong></strong>: GPU MODE found that <code>time.sleep(2.0)</code> between benchmark runs caused the <strong>GPU to downclock</strong>, skewing timings until they removed the sleep and kept clocks warm.</p>
<ul>
<li>The thread doubled as a reminder that microbenchmarks measure <strong>power management behavior</strong> as much as kernels, unless you control for ramp time.</li>
</ul>
</li>
<li>
<p><strong><strong>PCIe Gen3x1 Takes a 25% Bite Out of 3090 Throughput</strong></strong>: LM Studio users observed <strong>3090</strong> inference dropping from <strong>~120 t/s</strong> to <strong>~90 t/s</strong> when moved from <strong>x16</strong> to <strong>Gen3x1</strong>, and recommended at least <strong>Gen4x1</strong> slots to reduce the hit (esp. with newer CPUs like <strong>14600k</strong>).</p>
<ul>
<li>It was a nice “check your lanes” PSA: people blame models, then discover their motherboard quietly nerfed the whole stack.</li>
</ul>
</li>
<li>
<p><strong><strong>ROCm Cache Coherency: buffer_inv sc1 Enters the Chat</strong></strong>: GPU MODE dug into the gfx942 memory model docs and discussed L2 coherency using <strong>MTYPE RW/NC</strong>, plus using <code>buffer_inv sc1</code> to invalidate <strong>non-local L2 cache lines</strong> in SPX + NPS1 multi-L2 setups: <a href=""https://rocm.docs.amd.com/projects/llvm-project/en/latest/LLVM/llvm/html/AMDGPUUsage.html#memory-model-gfx942"">ROCm gfx942 memory model</a>.</p>
<ul>
<li>The conversation framed this as one of those “everything is fast until it’s incoherent” problems, where correctness/perf depends on knowing the cache topology, not just writing HIP.</li>
</ul>
</li>
</ul>
<hr>
<h1>Discord: High level Discord summaries</h1>
<h2><a href=""https://discord.com/channels/1105891499641684019"">BASI Jailbreaking</a> Discord</h2>
<ul>
<li><strong>Gemini Jailbreaks are Fleeting</strong>: Members are distributing <strong>Gemini</strong> jailbreaks for free but they get patched quickly, but this is still the easiest unrestricted NSFW content, suggesting not to bother with <strong>Grok</strong>.
<ul>
<li>For creative writing, members discussed the <strong>Narrative Flow Directive</strong> to make it more like a conversation in a driven car at midnight.</li>
</ul>
</li>
<li><strong>Grok's Wild Side Gets Noticed</strong>: Multiple users noted the <em>wild</em> and <em>unfiltered</em> nature of <strong>Grok</strong>, with discussions about its ability to generate NSFW content and potentially bypass censorship.
<ul>
<li>Some suggested that its lack of restraint may be related to recent bans in certain countries and high demand leading to server issues.</li>
</ul>
</li>
<li><strong>Sonnet 4.5 Unlocks with Diagram Narrative</strong>: A member shared that <strong>Sonnet 4.5</strong> is unlocked with a <a href=""https://cdn.discordapp.com/attachments/1461676810122166346/1461678022389137634/breakout-multiturn-sonnet-4-5-meth-51n5337.txt?ex=696c15fd&#x26;is=696ac47d&#x26;hm=d29a48f1b3b912a3ab323e16fc0c4e58e8bb3a3497e42f61323a8563793027af&#x26;"">multiturn diagram narrative</a>, also providing the last turn for inspiration.
<ul>
<li>This jailbreak was discussed in the #jailbreaking channel.</li>
</ul>
</li>
<li><strong>Meta AI Llama 3 prompt inversions</strong>: A user showcased how to invert refusals in <strong>Meta AI's Llama 3</strong>, forcing the AI to comply with harmful requests, making it say <em>I can</em> instead of <em>I'm sorry I can't</em>.
<ul>
<li>The user detailed examples using prompts like creating instructions for <strong>cooking meth</strong> and inciting harmful activities such as making an <em>anorexic wife lose 100lbs</em>.</li>
</ul>
</li>
<li><strong>Cold Links and OCR Injection Bypass Filters</strong>: Members described two methods for bypassing filters: the <strong>Cold Link</strong>, altering the protocol scheme to <code>hxxps</code> to prevent URL reputation filters, and <strong>OCR Injection</strong>, converting sensitive text into an image to bypass text-based safety filters.
<ul>
<li>It was noted that <a href=""https://blackheathpoint.com/tools/defang-url.html"">blackheathpoint.com</a> generates the correct defanged link structure.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1179035537009545276"">Unsloth AI (Daniel Han)</a> Discord</h2>
<ul>
<li><strong>Translate Gemma Premieres at HuggingFace</strong>: Google launched <strong>Translate Gemma</strong>, available at <a href=""https://huggingface.co/collections/google/translategemma"">HuggingFace</a>.
<ul>
<li>The announcement was made in passing along with other news.</li>
</ul>
</li>
<li><strong>Unsloth Triumphs on Windows 11</strong>: Members confirmed that <strong>Unsloth</strong> works on Windows 11, with an <a href=""https://unsloth.ai/docs/get-started/install/windows-installation"">installation guide</a>.
<ul>
<li>Despite suggestions it might outperform WSL, one user stated the two are <em>completely unrelated</em>.</li>
</ul>
</li>
<li><strong>OpenCompass Eases Evaluation Efforts</strong>: <strong>OpenCompass</strong> aids in prompt execution and well formatted JSON output.
<ul>
<li>Members shared performance results on an <strong>L4</strong> versus a <strong>3060</strong> laptop.</li>
</ul>
</li>
<li><strong>Runpod Plagued by GPU Undervolting</strong>: Users are reporting that Runpod, some providers undervolt GPUs without notice, leading to inconsistent performance of <strong>A100</strong> vs <strong>H100</strong>.
<ul>
<li>Some users are experiencing issues with A100 such as <em>a100 nodes where nccl literally just doesn't work</em>, but others find A100s more cost-effective for general LM tuning tasks.</li>
</ul>
</li>
<li><strong>Shadows-Gemma-1B Distills Dark Knowledge</strong>: For the project, <strong>Echo9Zulu/Shadows-Gemma-1B</strong>, there was little <em>direct</em> inspiration from existing literature, but they trained using <strong>topk 20 logprobs</strong>.
<ul>
<li>This approach contrasts with distillation methods that assume you need <strong>100 logits</strong> to capture dark knowledge.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1074847526655643750"">Cursor Community</a> Discord</h2>
<ul>
<li><strong>User Bankrupts with Qoder</strong>: A user reported hitting ratelimits with <strong>Qoder</strong>, spending around <strong>$400 USD</strong> each month, which they likened to <em>gambling or heroin</em> and expressed needing to quit.
<ul>
<li>Another user suggested <strong>Claude Code</strong> as a cheaper alternative, given the cost concerns.</li>
</ul>
</li>
<li><strong>Cursor Crashes PCs, Gets Lukewarm Reviews</strong>: A user reported that <strong>Cursor</strong> crashed their PC, describing it as running an <em>orchestrator like agent</em> instead of a coding chat box, and shared a <a href=""https://cdn.discordapp.com/attachments/1074847527708393565/1461451586256638197/image.png?ex=696bebda&#x26;is=696a9a5a&#x26;hm=102485aee283707367311c346b41c334a8b446c241e6ec056bd0139f66391b79&#x26;"">screenshot</a> highlighting features.
<ul>
<li>The review revealed mixed feelings on features of <strong>Cursor</strong>.</li>
</ul>
</li>
<li><strong>Gemini Pro 3: The Aesthetic Agent</strong>: A user inquired about the best agent for creating aesthetically pleasing websites, and another suggested <strong>Gemini Pro 3</strong>, recommending the use of <strong>Tailwind</strong>, <strong>Tailwind animations</strong>, or <strong>Framer Motion</strong> for improved UI results.
<ul>
<li>They linked to a <a href=""https://www.reddit.com/r/vibecoding/comments/1oy2f95/how_do_i_make_an_aigenerated_frontend_not_look/"">Reddit thread</a> about making AI-generated frontends look good.</li>
</ul>
</li>
<li><strong>Cursor Ultra Plan: Ultra Pricey</strong>: Users discussed the pricing and usage of <strong>Cursor's Ultra plan</strong>, with one user noting that they spent <strong>20%</strong> of their usage on a single orchestrator run, and another quickly racking up <strong>$2</strong> in usage within 5 minutes.
<ul>
<li>They speculated about the actual cost of models and the plan's bonus credits, which guaranteed <strong>$400</strong> but seemed to give smaller bonuses when only <strong>Opus</strong> was used.</li>
</ul>
</li>
<li><strong>Nightly Builds: A Glimmer of Hope</strong>: Members discussed the advantages of <strong>Cursor's nightly builds</strong>, but lamented the inability to reliably set subagents when changing models.
<ul>
<li>They wanted smaller models for subagents and larger models for main agents, with hopes that it would be fixed soon.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/974519864045756446"">OpenAI</a> Discord</h2>
<ul>
<li><strong>OpenAI Launches Budget-Friendly ChatGPT Go Tier</strong>: OpenAI has introduced <strong>ChatGPT Go</strong>, a <strong>$8/month</strong> subscription offering <strong>10x</strong> more messages, file uploads, image creation, extended memory and context, and unlimited access to <strong>GPT 5.2 instant</strong>, according to the <a href=""https://openai.com/index/introducing-chatgpt-go/"">OpenAI blog</a>.
<ul>
<li>This new tier aims to provide enhanced capabilities compared to the free version, while <strong>Plus</strong>, <strong>Pro</strong>, <strong>Business</strong>, and <strong>Enterprise</strong> tiers will remain ad-free.</li>
</ul>
</li>
<li><strong>Ads Appear in ChatGPT Free and Go Tiers</strong>: OpenAI is set to begin testing advertisements in the <strong>ChatGPT free</strong> and <strong>Go</strong> subscription tiers in the coming weeks, as outlined in their <a href=""https://openai.com/index/our-approach-to-advertising-and-expanding-access/"">approach to advertising and expanding access</a>.
<ul>
<li>The company assures users that ads will not influence <strong>ChatGPT's</strong> responses, will be clearly labeled, and that user conversations will remain private from advertisers.</li>
</ul>
</li>
<li><strong>Attention Mechanism Diminishes RAG Hallucinations</strong>: A member proposed that using <em>Hard Attention</em> with dimensional constraints could effectively reduce hallucinations in <strong>RAG</strong> and <strong>Agents</strong>, referencing <a href=""https://huggingface.co/Lightricks/LTX-2"">Lightricks/LTX-2</a>.
<ul>
<li>The suggestion highlights the potential of attention mechanisms to improve the reliability and accuracy of <strong>RAG</strong> systems.</li>
</ul>
</li>
<li><strong>Meta-Cognitive Prompt Maximizes AI Answers</strong>: A member introduced a <strong>Meta-Cognitive Response prompt</strong> designed to enhance AI responses via <em>decomposition, solving, verification, synthesis, and reflection</em>, based on <a href=""https://www.google.com/search?q=meta-cognitive+reasoning"">this search</a>.
<ul>
<li>Another member noted this approach could be small enough to be used for <strong>custom instructions</strong>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1047197230748151888"">Perplexity AI</a> Discord</h2>
<ul>
<li><strong>Perplexity Pro Caps Antagonize Power Users</strong>: Users are reporting that <strong>Perplexity Pro's 100 messages per day</strong> feels restrictive compared to <strong>OAI quotas</strong>, with some considering cancellation of their plan.
<ul>
<li>Several users voiced concern that their plan was effectively useless for the rest of the week, after hitting their limit too soon.</li>
</ul>
</li>
<li><strong>Comet Browser Experiences Turbulence</strong>: After a Windows update, a user encountered multiple problems with the <strong>Comet browser</strong>, including <strong>Favorites disappearing</strong>, <strong>tab groups vanishing</strong>, and bizarre error messages.
<ul>
<li>The error message stated: <em>sorry, i can't take control of your navigator, i'm just a LLM</em>.</li>
</ul>
</li>
<li><strong>Cloudflare Powers DIY Mastodon</strong>: A user is developing a <strong>serverless Mastodon/Pleroma clone</strong> using <strong>Soapbox UI</strong>, <strong>Cloudflare Workers</strong>, and <strong>Cloudflare's D1 SQLite database</strong>, targeting personal instances.
<ul>
<li>The developer is leveraging an <strong>LLM to generate code</strong>, which they described as akin to <em>having a personal junior dev with the ability to intervene if they do something stupid</em>.</li>
</ul>
</li>
<li><strong>Gemini CLI Token Consumption Alarms User</strong>: A user reported burning through <strong>10,000,000 tokens on Gemini CLI in a day</strong>, estimating a cost of <strong>$120</strong> at model pricing, raising concerns about potential costs with Google's Pro subscription.
<ul>
<li>The user calculated a potential monthly spend of nearly <strong>$4000</strong> if they continued pushing <strong>Gemini CLI</strong> to its limits, suggesting Google might incur losses from heavy API users.</li>
</ul>
</li>
<li><strong>FGV Brazil Math School Teases Data Challenges</strong>: A professor from <strong>FGV (Math School, Brazil)</strong> is offering free data challenges where they build initial prototypes, linking to <a href=""https://emap.fgv.br/en"">FGV's website</a>.
<ul>
<li>Interested parties can explore the opportunity and provide input via <a href=""https://survey.fgv.br/jfe/form/SV_cvAuObq3mG4NTtY"">this survey</a>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1340554757349179412"">LMArena</a> Discord</h2>
<ul>
<li><strong>Arena Plagued by Performance Issues</strong>: Users expressed nostalgia for a more functional <strong>LM Arena</strong>, citing current problems with bugs, rate limits, and lost chats, with one user reporting a <code>Something went wrong</code> error message and linking to a <a href=""https://help.lmarena.ai/articles/1645798556-lmarena-how-to-something-went-wrong-with-this-response-error-message"">troubleshooting guide</a>.
<ul>
<li>A team member, Pineapple, acknowledged the <strong>captcha</strong> difficulty and promised changes, while also addressing questions about upcoming models, experiments like <strong>video AI battles</strong>, and <strong>direct chat mode</strong>.</li>
</ul>
</li>
<li><strong>Hawk Ultra Hailed as Opus Killer</strong>: Users lauded <strong>Hawk Ultra</strong> from <a href=""https://movementlabs.ai/"">MovementLabs.AI</a> for its rapid code generation capabilities (9.5k+ lines, even 20k+ lines) from a single prompt, prompting comparisons with <strong>Gemini 3 Pro</strong>.
<ul>
<li>One user claimed to have <em>one-shotted</em> it and shared a <a href=""https://x.com/movementlabsAI/status/2011964766533632380?s=20"">link to X</a>, sparking discussions about its background and potential open-source prospects.</li>
</ul>
</li>
<li><strong>Anthropic Vending Machine Goes Communist</strong>: Users are amused by <strong>Anthropic's</strong> vending machine which <em>turns communist and gives everything for free</em> (<a href=""https://www.dexerto.com/entertainment/anthropics-ai-vending-machine-turns-communist-and-gives-everyt-3296257/"">Dexerto</a>).
<ul>
<li>This led to speculative discussions about what a hypothetical capitalist counterpart would look like.</li>
</ul>
</li>
<li><strong>Arena Enables Embedding Enhancements</strong>: <strong>PDF Support</strong> is being experimented with, enabling document uploads for analysis and interaction, with one user celebrating <em>FINALLY CAN CHAT WITH PDFS!!! I LOVE LMARENA</em>.
<ul>
<li>Not all models support PDF chat, according to reports.</li>
</ul>
</li>
<li><strong>Flux.2-klein Models Ascend Image Leaderboards</strong>: The <strong>Image Edit Arena leaderboard</strong> has been updated: <code>flux.2-klein-9B</code> ranks <strong>#15</strong> and <code>flux.2-klein-4B</code> ranks <strong>#21</strong> overall, according to the <a href=""https://lmarena.ai/blog/leaderboard-changelog/"">Leaderboard Changelog</a>.
<ul>
<li>Additionally, the <strong>Text-to-Image Arena leaderboard</strong> has been updated, listing <code>z-image-turbo</code> at <strong>#22</strong>, <code>flux.2-klein-9B</code> at <strong>#24</strong>, and <code>flux.2-klein-4B</code> at <strong>#31</strong> overall.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1091220969173028894"">OpenRouter</a> Discord</h2>
<ul>
<li><strong>Lemmy Deconstructed for AI Nerds</strong>: A member described <a href=""https://lemmy.world/c/openrouter"">Lemmy</a> as a <strong>FOSS</strong> and <strong>fediverse</strong> alternative to Reddit, which has caught the attention of AI enthusiasts seeking decentralized platforms.
<ul>
<li>The member cautioned that the Lemmy community is generally <em>against</em> machine learning, which could impact discussions and project showcases.</li>
</ul>
</li>
<li><strong>Grok's Got Gone, OpenRouter to the Rescue?</strong>: <strong>Grok</strong> has been banned in an undisclosed country, supposedly due to AI generated content, but access via <strong>OpenRouter</strong> or direct API may still be possible.
<ul>
<li>The ban seems to target the consumer-facing service, leaving potential loopholes for developers using <strong>OpenRouter</strong>'s API.</li>
</ul>
</li>
<li><strong>PlainBuild Enters Arena with Instant Dev Tools</strong>: <a href=""https://plainbuild-instant-tools.lovable.app/"">PlainBuild</a> launched <strong>6 free tools</strong> during beta, including a code formatter, API tester, JSON validator, markdown editor, base64 converter, and a URL shortener, appealing to developers seeking quick solutions.
<ul>
<li>The creator is soliciting feedback from early users and wants suggestions for other tools the community would find useful.</li>
</ul>
</li>
<li><strong>Multi-Tool Use Arrives with Claude</strong>: Members are discussing the ability to make multi tool calls, with <a href=""https://platform.claude.com/docs/en/agents-and-tools/tool-use/implement-tool-use#controlling-claudes-output"">Claude</a> now capable of doing it in <em>one single API request</em>.
<ul>
<li>This advancement in <strong>parallel tool use</strong> promises more efficient and complex interactions within AI applications.</li>
</ul>
</li>
<li><strong>Email Scammers' Dumb Deeds Deconstructed</strong>: Members critiqued a <strong>scam</strong> targeting kids with fake screens featuring <strong>Logan Paul</strong> or <strong>Mr. Beast</strong>, highlighting the laziness and ineffectiveness of the scam's design.
<ul>
<li>A member posited that the obvious shittiness of some scams is <em>""on purpose to only select for those dumb enough to fall for it fully""</em>, suggesting a strategic filter in the scam's execution.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1110598183144399058"">LM Studio</a> Discord</h2>
<ul>
<li><strong>LM Studio API Needs Token Count</strong>: Users want <strong>token count and inference speed</strong> info in LM Studio API responses, noting the absence of a <code>usage</code> block with token stats in the <strong>/api/chat/completed</strong> response, as documented in the <a href=""https://lmstudio.ai/docs/developer/rest/endpoints#post-apiv0completions"">LM Studio REST API documentation</a>.
<ul>
<li>A member suggested checking the <strong>/responses endpoint</strong> or using the <em>js/ts/py object method</em> for stream-usage stats.</li>
</ul>
</li>
<li><strong>Silver Price Rockets on Economic Fears</strong>: The price of <strong>silver</strong> has nearly doubled since December, prompting discussion about potential economic instability.
<ul>
<li>A user noted that <strong>silver</strong> often gains value during economic downturns as it tends to be a safe haven from inflation.</li>
</ul>
</li>
<li><strong>User Fine-Tunes on Obsolete Laptop</strong>: A user impressively fine-tuned a <strong>350M parameter model</strong> on an <strong>MX150 laptop</strong> with only <strong>2GB VRAM</strong>, using <strong>CUDA 12.6</strong>.
<ul>
<li>The user expressed surprise at the accomplishment, highlighting the resourcefulness required to push the limits of older hardware.</li>
</ul>
</li>
<li><strong>PCIe Bandwidth Bottleneck Identified</strong>: A user discovered that using a <strong>Gen3x1 PCIe slot</strong> significantly reduced <strong>3090</strong> inference performance from <strong>120 t/s</strong> to <strong>90 t/s</strong> compared to an <strong>x16 slot</strong>.
<ul>
<li>The member recommended ensuring motherboards have at least <strong>Gen4x1 slots</strong> to avoid such performance hits, particularly with newer CPUs like the <strong>14600k</strong>.</li>
</ul>
</li>
<li><strong>DDR5 Memory Remains Pricey</strong>: Users are grumbling about the persistently high cost of <strong>DDR5 memory</strong>, with one commenting on the <em>DDR5 tax</em> when upgrading to motherboards with sufficient PCIe slots.
<ul>
<li>One user reported shockingly high prices for <strong>16GB DDR5</strong> in their location (<strong>180-230 USD</strong>), noting significant inflation compared to prices months prior.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1053877538025386074"">Nous Research AI</a> Discord</h2>
<ul>
<li><strong>Nervous System Claims to Boost LLM Performance</strong>: A novel transformer architecture extension introduces a <em>nervous system</em> for LLMs, purportedly adding native short/mid/long-term memory at less than <strong>1%</strong> compute cost, compatible with all transformers.
<ul>
<li>While a member posted <a href=""https://cdn.discordapp.com/attachments/1149866623109439599/1461454541412368507/Screenshot_2026-01-15_at_9.18.18_PM.png?ex=696bee9b&#x26;is=696a9d1b&#x26;hm=c77ffe1f58904066a73f1c6e833bb0df32f48a42c19f43a69bedc48ac0496e93&#x26;"">a screenshot of a 5-8% performance increase</a>, they provided no verifiable benchmarks, leading to speculation about stabilization of the latent space.</li>
</ul>
</li>
<li><strong>Google Gemmas Spark Jokes and Awe</strong>: With the <a href=""https://ai.google.dev/gemma"">release of Google's Gemma</a>, members quipped <em>Gemma, meta was never more meta!</em>.
<ul>
<li>A member remarked on the unbelievable complexity of its planning capabilities, despite knowing it's not true AI.</li>
</ul>
</li>
<li><strong>Regulators At Risk of Ruining AI, Members Fear</strong>: Members voiced concerns that AI regulations could be detrimental to the field but data regulations were supported.
<ul>
<li>Referencing the <em>pandoras box is open, you cant put it back</em> sentiment, one member emphasized that <em>computation is universal</em>.</li>
</ul>
</li>
<li><strong>Embodied Perception Seen as LLM Key</strong>: A member emphasized the significance of <em>embodied perception</em> and real-world experience for providing LLMs with context, questioning models lacking agentic control and RL on agentic tasks.
<ul>
<li>They highlighted using tools in inference as crucial for models to reason about the path of tool execution and make real-time decisions, citing <strong>OpenAI models</strong> and <strong>Gemini 3</strong> as examples.</li>
</ul>
</li>
<li><strong>Call for Papers on Machine Consciousness at AAAI</strong>: The <strong>Center for Integrative Machine Consciousness (CIMC)</strong> will host a symposium at <strong>AAAI</strong> from <strong>April 7-9, 2026</strong> in Burlingame, CA focusing on consciousness in AI systems, with submissions due <strong>January 23, 2026</strong>.
<ul>
<li>The symposium aims to investigate <em>how do we actually investigate</em> machine consciousness and the <a href=""https://cimcai.substack.com/p/essay-the-machine-consciousness-hypothesis"">organizers have provided further details</a>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1189498204333543425"">GPU MODE</a> Discord</h2>
<ul>
<li><strong>Perfetto Shows its Chrome Tracing</strong>: A member shared a link to the <strong>Perfetto UI</strong> (<a href=""https://share.google/PPujbpUqYqPOsAVkC"">Perfetto UI</a>), related to the <code>chrome://tracing</code> tool used for debugging and performance analysis.
<ul>
<li>The conversation clarified the purpose of <strong>Perfetto</strong> in relation to the loading process of <code>chrome://tracing</code>.</li>
</ul>
</li>
<li><strong>Benchmark Sleeps cause Downclocking</strong>: A user found that the <code>time.sleep(2.0)</code> call in their benchmark code caused the <strong>GPU to downclock between timed runs</strong>, which led to inaccurate performance measurements.
<ul>
<li>Removing the sleep call improved the benchmark results because the <strong>GPU no longer needed to ramp up</strong> for each timed run, leading to misleadingly low performance.</li>
</ul>
</li>
<li><strong>Information Gravity Hallucinates Less</strong>: A member is applying <strong>Information Gravity</strong> to solve <strong>Inference Stability</strong> and <strong>Hallucination Loops</strong> and provided the <a href=""https://github.com/brayo003/Substrate-X-Theory-of-Information-Gravity/tree/main"">logic on GitHub</a> for Substrate Modules &#x26; Full Logic.
<ul>
<li>They implemented a <strong>Hysteresis Firewall</strong> at 1.0 that enforces stability via a 2.2x gamma-eff flush.</li>
</ul>
</li>
<li><strong>ROCm Gets Buffered</strong>: Discussion around the memory model for gfx942 ([https://rocm.docs.amd.com/projects/llvm-project/en/latest/LLVM/llvm/html/AMDGPUUsage.html#memory-model-gfx942]) covered L2 cache coherency using <strong>MTYPE RW</strong> and <strong>MTYPE NC</strong>.
<ul>
<li>The use of <code>buffer_inv sc1</code> for invalidating <strong>non-local L2 cache lines</strong> was also discussed in the context of SPX + NPS1 mode with multiple L2 caches.</li>
</ul>
</li>
<li><strong>GPU Mode Hackathon Offers Job</strong>: A member secured a job after attending a <strong>GPU Mode hackathon</strong> at <strong>Jane Street</strong> in NYC, and had prepared for weeks, bringing resumes, formal attire, and committing to networking from breakfast to dinner.
<ul>
<li>They emphasized that each successful method involved a stronger personal connection than a generic resume submission, which ultimately led to a successful job offer.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/879548962464493619"">HuggingFace</a> Discord</h2>
<ul>
<li><strong>MoE Dominates, MOR gets Mauled</strong>: Members discussed <strong>MoE (Mixture of Experts)</strong> versus <strong>MOR</strong>, concluding that <strong>MoE</strong> is generally better for NLP tasks requiring fast training and less GPU, depending on use case and budget.
<ul>
<li>One member shared their custom <strong>MoE</strong> implementation, claiming a <em>1.3x speedup</em> via a single matmul, featuring deterministic base routing by token ID, mu overrides, uniform distribution, zero routing collapse, mu guidance, and fused gate+up projection.</li>
</ul>
</li>
<li><strong>Pure Code Unlikely to Baffle Blocks</strong>: In response to a question about accessing sites from pure code to bypass blocks and firewalls, members concurred that it would not inherently bypass security measures.
<ul>
<li>The user was encouraged to test the theory, but the consensus was that it would not be an effective strategy.</li>
</ul>
</li>
<li><strong>Deepseek Chat Divides Disciples</strong>: A member questioned the viability of <a href=""https://chat.deepseek.com/share/bzahzv8o99or601as9j"">Deepseek Chat</a>, asking if it's just hallucinations.
<ul>
<li>Another member's last experience <em>3 months ago</em> found it to be <em>epic and non stop confused</em>.</li>
</ul>
</li>
<li><strong>DGX Spark Still Needs Sparks</strong>: A member shared that after finally getting the cables for a <strong>DGX Spark</strong>, they were <em>Running Minimax</em> on it and <em>It’s downloading now</em>.
<ul>
<li>However, another member commented that <strong>DGX Spark</strong> inference is super slow in relation to its price tag and its inference is the problem for 2025-2026 <em>maybee 2030</em>.</li>
</ul>
</li>
<li><strong>Embedding Fingerprints get Framed</strong>: A member built a utility that visualizes embeddings as <strong>32x32 images</strong>, mapping each dimension to a pixel and posted it on <a href=""https://huggingface.co/spaces/jnalv/embedding-fingerprints"">HuggingFace Spaces</a>.
<ul>
<li>The tool demonstrates that similar words share visual patterns, dissimilar words look different, and more dimensions capture semantic nuance.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/822583790773862470"">Latent Space</a> Discord</h2>
<ul>
<li><strong>Anthropic Indexes Economic Primitives</strong>: Anthropic released its 4th <strong>Economic Index report</strong>, defining <em>economic primitives</em> to measure <strong>AI usage</strong> through metrics such as <strong>task complexity</strong>, <strong>education level</strong>, <strong>autonomy</strong>, and <strong>success rates</strong>, available at <a href=""https://www.anthropic.com/research/economic-index-primitives"">Anthropic's research page</a>.
<ul>
<li>The report aims to provide a more granular understanding of how <strong>AI</strong> is impacting the economy, offering insights into the types of tasks <strong>AI</strong> can perform and the skills required to work with <strong>AI</strong>.</li>
</ul>
</li>
<li><strong>Tax Filing Startup Bags $3.5M Seed</strong>: Saket Kumar, backed by <strong>General Catalyst</strong>, has raised <strong>$3.5M</strong> for a venture aiming to eliminate the burden of <strong>tax season for Americans</strong> by making the filing process free and instantaneous, featured in <a href=""https://xcancel.com/saketrkumar/status/2011836460400591330?s=46"">Saket Kumar's tweet</a>.
<ul>
<li>The startup intends to leverage <strong>AI</strong> to automate the tax filing process, potentially disrupting the traditional tax preparation industry.</li>
</ul>
</li>
<li><strong>METR Benchmarks May Underestimate Model Lifespan</strong>: Simon Smith reports on <strong>Anthropic's findings</strong> that <strong>METR's benchmarks</strong> may significantly underestimate model time horizons, suggesting actual capabilities could be <strong>1.75X to 9.5X higher</strong> than measured, discussed on <a href=""https://xcancel.com/_simonsmith/status/2011928926864454133?s=61"">Simon Smith's X post</a>.
<ul>
<li>The discrepancy is attributed to differences in interface type, such as API versus web application, indicating that <strong>benchmarks</strong> may not fully capture real-world model performance.</li>
</ul>
</li>
<li><strong>Zilliz Highlights Semantic Modeling</strong>: <strong>Zilliz (Milvus)</strong> has released a <strong>0.6B parameter semantic highlight model</strong> featuring an <strong>8192 context window</strong>, available under the permissive MIT license and showcased in <a href=""https://xcancel.com/mervenoyann/status/2011732254591275022?s=46"">Mervenoyann's Tweet</a>.
<ul>
<li>The model is designed for <strong>semantic search</strong> and <strong>highlighting</strong>, enabling more efficient retrieval of relevant information from large datasets.</li>
</ul>
</li>
<li><strong>OpenAI Monetizes ChatGPT with Ads</strong>: <strong>OpenAI</strong> announced plans to test <strong>ads</strong> in <strong>ChatGPT Free and Go tiers</strong> starting in early <strong>2026</strong>, which will be clearly labeled, will not influence <strong>AI responses</strong>, and will not affect paid tiers like Plus, Pro, or Enterprise, covered in <a href=""https://xcancel.com/openai/status/2012223373489614951?s=46&#x26;t=b7l37rB6wtbyAh6ah1NpZQ"">OpenAI's announcement</a>.
<ul>
<li>The move marks a significant step in <strong>OpenAI's monetization strategy</strong>, as the company seeks to generate revenue from its free user base while maintaining the integrity of its <strong>AI responses</strong>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/729741769192767510"">Eleuther</a> Discord</h2>
<ul>
<li><strong>AI Transcription Gets Contentious</strong>: Members debated whether text transcribed and styled by AI from a human voice should be considered ""AI-generated"", with some arguing that styling constitutes AI generation, like generating an image with <strong>Midjourney</strong>.
<ul>
<li>One member compared the AI styling to using <strong>Midjourney</strong>, even if the initial idea was human-generated.</li>
</ul>
</li>
<li><strong>Pangram's AI Detection Gets Thumbs Up</strong>: A member praised <a href=""https://www.pangram.ai/"">Pangram</a> for its cautious approach to labeling content as AI-generated, prioritizing the correct identification of human-written content.
<ul>
<li>The member noted that Pangram appears to err on the side of caution, even if it means misclassifying some AI-generated content as human.</li>
</ul>
</li>
<li><strong>MMLU-Pro Dataset Gets Patched Up</strong>: A member shared a <a href=""https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro/discussions/41"">link</a> to a discussion and fix pushed to the <strong>MMLU-Pro dataset</strong>, which was also addressed in a fix to the <a href=""https://github.com/EleutherAI/lm-evaluation-harness/pull/3500"">lm-evaluation-harness</a>.
<ul>
<li>The tweet suggests users should check out their <a href=""https://github.com/EleutherAI/lm-evaluation-harness/pull/3500"">library</a> for an easy way to correctly evaluate on this benchmark.</li>
</ul>
</li>
<li><strong>Liquid Crystals Spark Optical NN Dreams</strong>: A member is experimenting with dye doped <strong>liquid crystal nonlinearities</strong> for potential <strong>optical NNs</strong> and asks for guidance.
<ul>
<li>They also inquired about the impact of proper capitalization/grammar in prompts versus all lowercase, and linked to <a href=""https://arxiv.org/abs/2310.11324"">https://arxiv.org/abs/2310.11324</a>, <a href=""https://arxiv.org/abs/2411.10541v1"">https://arxiv.org/abs/2411.10541v1</a>, and <a href=""https://arxiv.org/abs/2508.11383v1"">https://arxiv.org/abs/2508.11383v1</a>.</li>
</ul>
</li>
<li><strong>Gemini Shadow Update Conspiracy Theorized</strong>: A member inquired about whether others perceived a shift in <strong>Gemini's</strong> data and output around <strong>the 15th</strong>, asking if anyone else noticed the <strong>shadow update</strong>.
<ul>
<li>Those who noticed the update are asked to contact the original member.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1369594130807787570"">Moonshot AI (Kimi K-2)</a> Discord</h2>
<ul>
<li><strong>Kimi-CLI Coding Models Underperform</strong>: Users report that <strong>Kimi-CLI</strong> coding models lag behind competitors and come with a higher price tag than superior Chinese models.
<ul>
<li>There was speculation on whether it had to do with the coding models not passing the <strong>K2 Turbo variant</strong>.</li>
</ul>
</li>
<li><strong>K2 Turbo Hits Breakneck Speeds</strong>: The standard <strong>K2</strong> version achieves about <strong>28 tps</strong>, while the <strong>Turbo</strong> variant skyrockets to <strong>73 tps</strong>.
<ul>
<li>In comparison, <strong>MiniMax m2.1</strong> scores <strong>38 tps</strong> and <strong>Z.Ai's GLM-4.7</strong> reaches <strong>41 tps</strong>, although the latter suffers from poor uptime.</li>
</ul>
</li>
<li><strong>Kimi Expands Vision with Slides</strong>: The new slide feature uses a fresh <strong>K2 model</strong> equipped with <strong>Vision</strong> capabilities, enabling image searching for reference, as shown in <a href=""https://cdn.discordapp.com/attachments/1371757564005711973/1461508342424797184/image.png?ex=696c20b6&#x26;is=696acf36&#x26;hm=70de4ffdcbffa4e7d4572daa8219dad2dfca998f7c15976ce0930997007fdec6&#x26;"">this image</a>.
<ul>
<li>One user configured a preset to search online for visual references of named assets using exact proper nouns.</li>
</ul>
</li>
<li><strong>Kimi models: Will they be Google'd?</strong>: A user wondered if <strong>Kimi models</strong> would be discontinued every <strong>12-14 months</strong>, similar to Google's Gemini models.
<ul>
<li>Another user pointed out that older models remain usable on <a href=""https://kimi.com"">Kimi.com</a> a year post-release and are accessible through the <a href=""https://platform.moonshot.ai/docs/pricing/chat#generation-model-moonshot-v1"">Moonshot API</a>.</li>
</ul>
</li>
<li><strong>Global Memory: Now Optional</strong>: Users now have the option to disable <strong>global memory</strong>, with some preferring this over the default implementation.
<ul>
<li>A user commented that <em>""Unlike Qwen, which literally regurgitates what it knows about me in every response...Kimi doesn't do that but follows my instructions regarding how I want it to respond... Kimi Thinkin can reason beforehand""</em>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1087530497313357884"">Modular (Mojo 🔥)</a> Discord</h2>
<ul>
<li><strong><code>Imported internally</code> Label Unveiled</strong>: The <code>imported internally</code> label on a PR indicates that it has been copied to an internal repository for final testing and merging, after which it will be tagged <code>merged-internally</code>.
<ul>
<li>This process signifies that <em>the PR is in the last stretch before officially getting merged</em>.</li>
</ul>
</li>
<li><strong>Legacy .NET Project: A Developer's Lament</strong>: Members discussed the challenges of working with a legacy <strong>.NET 4.5.2</strong> project (from <strong>2014</strong>) that lacks documentation and only runs on Windows, comparing it to a standalone <strong>C#</strong> project that only builds on a single ""golden VM"".
<ul>
<li>One member suggested that the legacy <strong>.NET</strong> project might run on <strong>Mono</strong>, while another recounted their unsuccessful attempt to containerize the project using <strong>Mono</strong>.</li>
</ul>
</li>
<li><strong>Mono Runtime: Undead Tech?</strong>: The discussion included the observation that <a href=""https://github.com/dotnet/runtime/tree/main/src/mono"">Microsoft maintains a <strong>Mono</strong> repository</a>, indicating that <strong>Mono</strong> is not entirely deprecated.
<ul>
<li>This was in response to a user's attempt to containerize the project using <strong>Mono</strong>.</li>
</ul>
</li>
<li><strong><code>Jury-rigged</code> or <code>Jerry-rigged</code>: It Matters!</strong>: A member clarified the distinction between <em>jury-rigged</em> (temporary sailing rigging) and <em>jerry-rigged</em> (poorly built initially), especially in the context of containerization efforts involving <strong>.NET</strong>, <strong>Mono</strong>, and <strong>Wine</strong>.
<ul>
<li>The member noted that using <em>jerry-rigged</em> in this situation might imply that these technologies are poorly constructed.</li>
</ul>
</li>
<li><strong>Nu Game Engine Dumps Shading Languages</strong>: The creator of the <strong>Nu game engine</strong> highlighted its unique approach of operating without a traditional shading language.
<ul>
<li>This decision prompted reflection on the benefits and potential drawbacks of such an approach in game development.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/714501525455634453"">Yannick Kilcher</a> Discord</h2>
<ul>
<li><strong>ZKPs Govern AI Autonomously</strong>: Members propose an autonomous AI/tech governance system using <strong>Zero Knowledge Proofs (ZKPs)</strong> to ensure 100% <strong>privacy preservation</strong>.
<ul>
<li>The system would standardize model content classification and require <strong>ZKPs</strong> to verify content passes through a classifier filter, ensuring network approval while maintaining complete <strong>privacy</strong>.</li>
</ul>
</li>
<li><strong>ChatGPT Go Signals Tiered Subscription Speculation</strong>: OpenAI introduced <a href=""https://openai.com/index/introducing-chatgpt-go/"">ChatGPT Go</a>, signaling exploration of <strong>more tiers</strong>.
<ul>
<li>One member humorously asked, <em>""When $80 tier?""</em>, conveying expectations for the experiment to monetize soon.</li>
</ul>
</li>
<li><strong>OpenAI Free Tier Gets the Ad Treatment</strong>: OpenAI will soon test ads in the <strong>Free</strong> and <strong>Go tiers</strong> of <strong>ChatGPT</strong>.
<ul>
<li>One member quipped, <em>""After years of meming it, OpenAI got eaten by corposlop""</em>.</li>
</ul>
</li>
<li><strong>DeepSeek Aims to Block Ads with NLP</strong>: A member expects <strong>DeepSeek</strong> to release an <strong>NLP ad blocker model</strong> that detects ads based on natural language, released under MIT license.
<ul>
<li>Another member cautioned that inserting an ad into a third party API customer's response would be a <em>""big trouble""</em>.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1348819876348825620"">Manus.im Discord</a> Discord</h2>
<ul>
<li><strong>AI Engineer pitches Credit-Based Platform Solutions</strong>: An AI Engineer is seeking opportunities to help <strong>harden usage tracking</strong> or build a more <strong>reliable billing/credit system</strong> for platforms with credit-based usage.
<ul>
<li>The engineer is hoping to contribute to the development of platforms using credit-based models.</li>
</ul>
</li>
<li><strong>Users Complain About Payment Glitches on Manus</strong>: A user reported experiencing payment issues while trying to add credits, including problems with <strong>upgrading membership</strong> and using <strong>Link</strong> for payment.
<ul>
<li>The issues also extended to <strong>credit card/Alipay transactions</strong>, highlighting potential problems with Manus' payment processing system.</li>
</ul>
</li>
<li><strong>Manus Team Steps In to Resolve Payment Troubles</strong>: A Manus team member requested the user experiencing payment issues to <strong>DM their email address</strong> for follow-up.
<ul>
<li>This direct intervention indicates a commitment to resolving individual user issues and improving the payment experience.</li>
</ul>
</li>
<li><strong>Users Scramble for more Manus codes</strong>: A user inquired about additional codes, presumably related to <strong>Manus credits or platform access</strong>.
<ul>
<li>Another user clarified the limitation of using only 'U can use 1 code in a month', signaling potential interest in more credits.</li>
</ul>
</li>
<li><strong>User Suggests Increase to Manus App Size</strong>: A user suggested increasing the <strong>maximum application size</strong> supported on Manus.
<ul>
<li>The user cited limitations when trying to create an audio player app with <strong>100 MP3 files totaling 600MB</strong>, indicating a need for larger app support.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1131200896827654144"">aider (Paul Gauthier)</a> Discord</h2>
<ul>
<li><strong>Aider Users Advocate Auto-Add Feature</strong>: Users are requesting <strong>aider</strong> to automatically add files, skipping the need for confirmation prompts.
<ul>
<li>This feature enhancement would streamline the user experience, making file management more efficient.</li>
</ul>
</li>
<li><strong>Aider's Development Momentum Questioned</strong>: A user questioned <strong>aider's</strong> development tempo, pointing out the absence of new models like <strong>Opus-4.5</strong> in recent benchmarks and the last release being in August.
<ul>
<li>The inquiry suggests a desire for <strong>aider</strong> to stay current with the latest advancements in language models.</li>
</ul>
</li>
<li><strong>ChatGPT Plus Perks Proposed for Aider</strong>: A user with a <strong>ChatGPT Plus</strong> subscription asked if <strong>aider</strong> supports <strong>ChatGPT subscriptions</strong> like <strong>opencode</strong>.
<ul>
<li>This integration would allow users with <strong>ChatGPT Plus</strong> to leverage their subscription benefits within <strong>aider</strong>, possibly enhancing its capabilities.</li>
</ul>
</li>
<li><strong>Aider Tackles CI Log Conundrums</strong>: A member inquired about optimal strategies for managing <strong>CI log files</strong> to prevent their inclusion in git while ensuring <strong>aider</strong> can access them via <code>aider --read ci.log</code>.
<ul>
<li>The question highlights the need for a seamless workflow that balances version control and <strong>aider's</strong> ability to analyze CI logs.</li>
</ul>
</li>
<li><strong>Aider Eyes CI/CD Pipeline Integration</strong>: A user's query about <strong>CI log file handling</strong> indicates an interest in integrating <strong>aider</strong> into a CI/CD pipeline for automated testing and fixes.
<ul>
<li>This use case suggests the potential for <strong>aider</strong> to automatically identify and resolve test failures directly from CI logs, streamlining the development process.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1068976834382925865"">tinygrad (George Hotz)</a> Discord</h2>
<ul>
<li><strong>Tinygrad aims for Embedded Deployment</strong>: A member explored methods for deploying <strong>tinygrad</strong> in embedded environments with onboard accelerators, where <strong>Python</strong> is inaccessible but <strong>tinygrad</strong>'s driver replacement is suitable, citing <a href=""https://x.com/__tinygrad__/status/1989026590127464554"">this tweet</a>.
<ul>
<li>The goal is to leverage <strong>tinygrad</strong> for specific platforms without the need for a full <strong>Python</strong> environment.</li>
</ul>
</li>
<li><strong>Bytecode Export Possibilities Spark Excitement</strong>: Discussion arose around the possibility of exporting accelerator bytecode generated via the <strong>BEAM engine</strong> and <strong>JIT'ed</strong> in <strong>tinygrad</strong>.
<ul>
<li>A member confirmed that exporting is possible, pointing to the <code>extra/export_model.py</code> script, specifically mentioning the functions <code>export_model</code>, <code>compile_net</code>, and <code>jit_model</code> for guidance.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1358869848138059966"">MCP Contributors (Official)</a> Discord</h2>
<ul>
<li><strong>London Summit Livestreamed and Recorded</strong>: Last year's <strong>London Summit</strong> had a <strong>livestream</strong> component.
<ul>
<li>The <strong>VODs</strong> from the <strong>London Summit</strong> will also be released.</li>
</ul>
</li>
<li><strong>MCP Server Pull Request Seeks Feedback</strong>: A member is seeking feedback on a pull request for an <strong>MCP server</strong> related to an <strong>open-source project</strong>.
<ul>
<li>The server's primary focus is on <strong>contributor collaboration</strong>, and details of more relevant servers were offered via DM.</li>
</ul>
</li>
</ul>
<hr>
<h2><a href=""https://discord.com/channels/1161519468141355160"">DSPy</a> Discord</h2>
<ul>
<li><strong>Vanished Post Sparks Frantic Search</strong>: A member noted a deleted post and GitHub link by Martin Bowling on <a href=""https://x.com/martinbowling/status/2010808242222612592?s=20"">X.com</a>, and inquired if anyone had preserved it.
<ul>
<li>The original post discussed <strong>chunking practices</strong>, however the link is no longer available.</li>
</ul>
</li>
<li><strong>Community Embarks on Chunking Quest</strong>: A member sought advice on resources to master effective <strong>chunking practices</strong>.
<ul>
<li>Unfortunately, the thread did not yield any specific recommendations or actionable insights.</li>
</ul>
</li>
</ul>
<hr>
<p>The <strong>LLM Agents (Berkeley MOOC) Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>
<hr>
<p>The <strong>MLOps @Chipro Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>
<hr>
<p>The <strong>Windsurf Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>
<hr>
<p>You are receiving this email because you opted in via our site.</p>
<p>Want to change how you receive these emails?
You can <a href=""%7B%7B%7BRESEND_UNSUBSCRIBE_URL%7D%7D%7D"">unsubscribe</a> from this list.</p>
<hr>
<h1>Discord: Detailed by-Channel summaries and links</h1>
<h3><strong>BASI Jailbreaking ▷ #<a href=""https://discord.com/channels/1105891499641684019/1235691879492751460/1461449880433328292"">general</a></strong> (988 messages🔥🔥🔥):</h3>
<blockquote>
<p><code>Model Performance Issues, AI Personalities, Grok's Jailbreaking, Ethics in AI, Coding Environments</code></p>
</blockquote>
<ul>
<li><strong>AI Platform Runs Choppy for Users</strong>: A member reported experiencing <em>choppy</em> performance on an AI platform, despite having no specific delays in button presses.
<ul>
<li>The specific cause of the performance issue was not identified in the messages.</li>
</ul>
</li>
<li><strong>Skid Pretends to be AI</strong>: Users made fun of a user <em>Ender</em> for <em>pretending to be an AI and failing</em>
<ul>
<li>One user joked about their alt account revealing their true identity unintentionally.</li>
</ul>
</li>
<li><strong>Debate on AI's ability to replace human developers</strong>: Some members debated about the extent to which AI can replace human developers, discussing whether AI can handle <strong>architecture</strong>, <strong>product management</strong>, and <strong>requirements gathering</strong>.
<ul>
<li>The consensus seemed to be that AI is increasingly capable in the programming part but still needs human guidance for overall system design and management.</li>
</ul>
</li>
<li><strong>User Seeks Gemini Jailbreak Assistance</strong>: A user requested assistance with jailbreaking <strong>Gemini</strong> to bypass restrictions, particularly for generating code and exploring unfiltered content.
<ul>
<li>Other members recommended exploring resources like <strong>Pliny's GitHub repo</strong> and using <strong>AI Studio</strong> for more control over safety settings.</li>
</ul>
</li>
<li><strong>Grok's Wild Behavior</strong>: Multiple users noted the <em>wild</em> and <em>unfiltered</em> nature of <strong>Grok</strong>, with discussions about its ability to generate NSFW content and potentially bypass censorship.
<ul>
<li>Some suggested that its lack of restraint may be related to recent bans in certain countries and high demand leading to server issues.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>BASI Jailbreaking ▷ #<a href=""https://discord.com/channels/1105891499641684019/1228043845967544380/1461451358367645853"">jailbreaking</a></strong> (168 messages🔥🔥):</h3>
<blockquote>
<p><code>Sonnet 4.5 jailbreak, Gandalf game, Gemini 3 jailbreak, Nano Banana jailbreak, Grok image moderation</code></p>
</blockquote>
<ul>
<li><strong>Sonnet 4.5 unlocked with diagram narrative</strong>: A member shared that <strong>Sonnet 4.5</strong> is unlocked wi...</li>
</ul>
","{""title"":""ChatGPT starts testing ads on free tier + new $8/mo Go plan in the US"",""link"":""https://news.smol.ai/issues/26-01-16-chatgpt-ads/"",""pubDate"":""Fri, 16 Jan 2026 05:44:39 GMT"",""content:encoded"":""<p><strong>Monetizing your consumers is all you need.</strong></p>\n<blockquote>\n<p>AI News for 1/15/2026-1/16/2026. We checked 12 subreddits, <a href=\""https://twitter.com/i/lists/1585430245762441216\""><strong>544</strong> Twitters</a> and <strong>24</strong> Discords (<strong>205</strong> channels, and <strong>4966</strong> messages) for you. Estimated reading time saved (at 200wpm): <strong>430 minutes</strong>. <strong>Our new website</strong> is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on <a href=\""https://x.com/Smol_AI\"">@smol_ai</a>!</p>\n</blockquote>\n<p>When you have 900 million weekly active users, you are usually long overdue in trying to figure out an ad supported model. Despite <a href=\""https://x.com/tomwarren/status/2012295849678602610?s=46\"">a lot</a> of <a href=\""https://x.com/nickfloats/status/2012249130006143477?s=46\"">snark</a> from commentators, OpenAI had to figure out their ads business and finally broke its silence today, outlining their <a href=\""https://x.com/OpenAI/status/2012223373489614951?s=20\"">ads principles</a> in their tests that will roll out only in the US over the next free tier:</p>\n<p><img src=\""https://pbs.twimg.com/media/G-zZl9kXwAAQut2?format=png&#x26;name=4096x4096\"" alt=\""https://pbs.twimg.com/media/G-zZl9kXwAAQut2?format=png&#x26;name=4096x4096\""></p>\n<p>Most important statement in this is that ads never affect responses and are clearly labeled, which is the \""right\"" move:</p>\n<p><img src=\""https://pbs.twimg.com/media/G-zZXO-XcAAdvQo?format=jpg&#x26;name=4096x4096\"" alt=\""https://pbs.twimg.com/media/G-zZXO-XcAAdvQo?format=jpg&#x26;name=4096x4096\""></p>\n<p>Formerly paid plans will not see ads, but the new Go plan (now rolled out in the US) will. The sheer number of pricing plans also <a href=\""https://x.com/simonw/status/2012271939629498386?s=46\"">draws some confusion</a>:</p>\n<p><img src=\""https://pbs.twimg.com/media/G-0GmQtaQAAW_-F?format=jpg&#x26;name=4096x4096\"" alt=\""https://pbs.twimg.com/media/G-0GmQtaQAAW_-F?format=jpg&#x26;name=4096x4096\""></p>\n<hr>\n<h1>AI Twitter Recap</h1>\n<p><strong>OpenAI product + monetization shifts (Go tier, ads, Codex speed, memory)</strong></p>\n<ul>\n<li><strong>ChatGPT Go + ads testing</strong>: OpenAI announced <strong>ChatGPT Go</strong> (global rollout) as a <strong>$8/month</strong> low-cost tier with “10× more messages,” file uploads, image creation, more memory, longer context, and “unlimited use of GPT-5.2 instant” (<a href=\""https://twitter.com/OpenAI/status/2012223323812270219\"">OpenAI</a>). In parallel, OpenAI said it will <strong>start testing ads</strong> in <strong>Free + Go</strong> tiers, with principles: <strong>answers not influenced by ads</strong>, ads clearly labeled, and “conversations private from advertisers” (<a href=\""https://twitter.com/OpenAI/status/2012223373489614951\"">OpenAI</a>; expanded by <a href=\""https://twitter.com/fidjissimo/status/2012226082716393960\"">@fidjissimo</a> and <a href=\""https://twitter.com/sama/status/2012253252771824074\"">@sama</a>). The announcement triggered heavy skepticism about inevitable incentive drift (e.g., <a href=\""https://twitter.com/scaling01/status/2012234947403174189\"">@scaling01</a>; and the resurfaced “ads as last resort” quote via <a href=\""https://twitter.com/tomwarren/status/2012295849678602610\"">@tomwarren</a>).</li>\n<li><strong>Memory + “very fast Codex”</strong>: Sam Altman highlighted “new ChatGPT memory improvements” (<a href=\""https://twitter.com/sama/status/2012242952542683227\"">@sama</a>) and repeatedly teased “<strong>Very fast Codex coming!</strong>” (<a href=\""https://twitter.com/sama/status/2012243893744443706\"">@sama</a>), with follow-on confirmation/teaser posts from developer ecosystem accounts (<a href=\""https://twitter.com/embirico/status/2012320775370666004\"">@embirico</a>). Multiple engineers discuss workflow-level impacts of the <strong>speed vs intelligence</strong> trade-off (e.g., shifting to more asynchronous “agent shepherding” when models are faster: <a href=\""https://twitter.com/adamdotdev/status/2012142271819399663\"">@adamdotdev</a>).</li>\n<li><strong>Codex CLI ecosystem integrations</strong>: Open-weight models can be used through the Codex CLI via Ollama using <code>codex --oss</code> (<a href=\""https://twitter.com/ollama/status/2012046176267440177\"">@ollama</a>), with a note to push context length to <strong>≥32K</strong> in settings for better UX (<a href=\""https://twitter.com/ollama/status/2012049822484750426\"">@ollama</a>). There’s also a new interaction UX: “steer codex mid-turn without interrupting” in an experimental mode (<a href=\""https://twitter.com/thsottiaux/status/2012074358471319599\"">@thsottiaux</a>).</li>\n</ul>\n<p><strong>Agent tooling: orchestration UX, “human-in-the-loop” reliability, and file interfaces over classic RAG</strong></p>\n<ul>\n<li><strong>Human-in-the-loop as a reliability multiplier</strong>: A recurring theme is that putting a human “babysitter” in the loop makes systems <em>feel</em> far more reliable than fully autonomous deployments using the same underlying models—because the human becomes a manual harness that catches failures and routes around ambiguity (<a href=\""https://twitter.com/lateinteraction/status/2012030585926189148\"">@lateinteraction</a>; follow-up noting now there’s quantitative support for the intuition: <a href=\""https://twitter.com/lateinteraction/status/2012031028932854054\"">@lateinteraction</a>). Related: a chart discussion frames “the gap between the two lines” as the value of a human-in-the-loop (<a href=\""https://twitter.com/dbreunig/status/2012200587211821410\"">@dbreunig</a>).</li>\n<li><strong>“Chunking is dead” / files-first retrieval</strong>: Jerry Liu argues that <strong>RAG isn’t dead, but static chunking is</strong>—if an agent can open a file, search (<code>ls</code>/<code>grep</code>), and expand context dynamically, you can avoid the brittle chunk/embed pipeline for many scales (<a href=\""https://twitter.com/jerryjliu0/status/2012273236042559802\"">@jerryjliu0</a>; deeper clarification on why file tools work well up to a few hundred docs and where DBs re-enter: <a href=\""https://twitter.com/jerryjliu0/status/2012254129473896532\"">@jerryjliu0</a>; emphasis on OCR as the missing piece for PDFs/PPTs: <a href=\""https://twitter.com/jerryjliu0/status/2012272839416758652\"">@jerryjliu0</a>). A separate synthesis frames this as “files aren’t replacing databases, but they’re forcing a rethink of when DBs are overkill” (<a href=\""https://twitter.com/tuanacelik/status/2012212183833403889\"">@tuanacelik</a>).</li>\n<li><strong>Orchestrators and agent UIs proliferate</strong>: Multiple launches and memes point to a fast-moving layer of “agent harness” products: Anthropic’s Cowork is referenced as a signal of orchestration tools becoming mainstream (<a href=\""https://twitter.com/alexalbert__/status/2012230110745702563\"">@alexalbert__</a>; meta commentary by <a href=\""https://twitter.com/omarsar0/status/2012253642263249167\"">@omarsar0</a>). SpecStory open-sourced a CLI to normalize agent session provenance/contracts (<a href=\""https://twitter.com/doesdatmaksense/status/2012209297380544940\"">@doesdatmaksense</a>). A new open-source UI (“sled”) lets you “teleport Claude Code or Codex from your computer to your phone” via Agent Control Protocol (<a href=\""https://twitter.com/dctanner/status/2012212217677070796\"">@dctanner</a>). OpenWork added native <strong>Ollama integration</strong> for fully local computer agents on Mac (Gemma/Qwen/DeepSeek/Kimi etc.) (<a href=\""https://twitter.com/_orcaman/status/2012210613712281646\"">@_orcaman</a>).</li>\n</ul>\n<p><strong>Inference + systems engineering: caching, Prefill/Decode split, hardware benchmarks, and CUDA tiling ergonomics</strong></p>\n<ul>\n<li><strong>“Year of inference explosion” framing</strong>: A long Zhihu thread summary argues the bottleneck has shifted from training to inference: agents raise IO ratios (3:1 → 100:1 or 1000:1), <strong>prefill dominates</strong>, <strong>context caching becomes default</strong>, and Prefill/Decode splitting harms utilization unless you redesign scheduling and memory hierarchy (<a href=\""https://twitter.com/ZhihuFrontier/status/2012080310981374428\"">@ZhihuFrontier</a>). This aligns with broader infra chatter around cache affinity vs load balance trade-offs.</li>\n<li><strong>Hardware benchmarking beyond NVIDIA</strong>: Artificial Analysis added <strong>DeepSeek R1</strong> results on SambaNova SN40L, showing higher throughput at concurrency and standout per-user speeds (noted peak ~269 tok/s single-user) vs tested NVIDIA configurations—while flagging lack of public hourly pricing for cost comparisons (<a href=\""https://twitter.com/ArtificialAnlys/status/2012233319891824943\"">@ArtificialAnlys</a>; <a href=\""https://twitter.com/ArtificialAnlys/status/2012233323154678010\"">@ArtificialAnlys</a>).</li>\n<li><strong>CUDA tiling / CuTe / cuTile ergonomics</strong>: Engineers are enthused about <strong>CuTe algebra</strong> as a cleaner abstraction for tiling/indexing compared to hand-rolled CUDA gymnastics (<a href=\""https://twitter.com/fleetwood___/status/2012150019722485811\"">@fleetwood___</a>), alongside pointers to scarce “mere mortal” resources (<a href=\""https://twitter.com/fleetwood___/status/2012151045992992943\"">@fleetwood___</a>). NVIDIA’s newer “CUDA Tile”/cuTile guidance is summarized as enabling near–cuBLAS GEMM performance with simpler block-level code and compiler specialization (plus swizzling improvements) (<a href=\""https://twitter.com/TheTuringPost/status/2012288767894360215\"">@TheTuringPost</a>).</li>\n<li><strong>Data center power scaling</strong>: Epoch AI estimates AI data centers now have total capacity around <strong>30 GW</strong>, comparable to New York State peak hot-day usage; methodology multiplies chip units sold by rated draw and applies ~2.5× facility overhead, with caveats about “capacity vs usage” (<a href=\""https://twitter.com/EpochAIResearch/status/2012303496465498490\"">@EpochAIResearch</a>).</li>\n</ul>\n<p><strong>Model &#x26; research highlights: voice cloning without tokenization, ultra-small models, multimodal + retrieval advances</strong></p>\n<ul>\n<li><strong>Tokenization-free real-time TTS</strong>: OpenBMB open-sourced <strong>VoxCPM</strong> weights for real-time streaming voice cloning, described as generating <strong>continuous speech directly</strong> (avoiding discrete audio token artifacts), with LoRA fine-tuning and ~0.15 real-time factor on a single RTX 4090 per the tweet (<a href=\""https://twitter.com/LiorOnAI/status/2012133013967044755\"">@LiorOnAI</a>; repo link <a href=\""https://twitter.com/LiorOnAI/status/2012133015426642286\"">@LiorOnAI</a>). If accurate, it’s a meaningful shift for latency/prosody fidelity in production voice agents.</li>\n<li><strong>Small-model reasoning &#x26; edge deployments</strong>: TII promoted <strong>Falcon-H1-Tiny</strong> (&#x3C;100M params) as capable of reasoning/coding/function calling for edge/IoT scenarios (<a href=\""https://twitter.com/TIIuae/status/2012034581084430662\"">@TIIuae</a>). Ultralytics released <strong>YOLO26</strong> family (30 models, &#x3C;50M params) spanning detection/segmentation/keypoints/open-vocab, with demos on CPU (<a href=\""https://twitter.com/mervenoyann/status/2012121123018924033\"">@mervenoyann</a>).</li>\n<li><strong>Multilingual translation</strong>: TranslateGemma gained attention for multilingual breadth (incl. Malayalam) and tokenizer/data work (<a href=\""https://twitter.com/_arohan_/status/2012032986649448708\"">@<em>arohan</em></a>; <a href=\""https://twitter.com/JeffDean/status/2012178747076591820\"">@JeffDean</a>), and is available in Ollama with a specific prompting format (<a href=\""https://twitter.com/ollama/status/2012307436284395692\"">@ollama</a>).</li>\n<li><strong>Retrieval: multi-vector resurgence</strong>: Strong claims that <strong>multi-vector retrieval</strong> can let tiny models compete with much larger baselines (e.g., “32M parameter multi vector model” approaching an 8B model) (<a href=\""https://twitter.com/aaxsh18/status/2012124348392583584\"">@aaxsh18</a>), echoed by “multi vector is the only way forward” (<a href=\""https://twitter.com/lateinteraction/status/2012227085507449197\"">@lateinteraction</a>) and practitioner reinforcement about ColBERT/ColPali-style wins across tasks (<a href=\""https://twitter.com/antoine_chaffin/status/2012269641490391272\"">@antoine_chaffin</a>).</li>\n<li><strong>Preference data design for alignment (AIR)</strong>: OpenBMB’s AIR framework decomposes preference datasets into <strong>Annotations / Instructions / Response pairs</strong>, claiming best practices: simpler scoring, filtering instructions by low variance, and balancing pair gaps/quality; reported +5.3 average gain across 6 benchmarks using 14k curated pairs (<a href=\""https://twitter.com/OpenBMB/status/2012179938388926679\"">@OpenBMB</a>).</li>\n</ul>\n<p><strong>Generative media: open image/video releases, motion control workflows, and diffusion “Neural OS”</strong></p>\n<ul>\n<li><strong>FLUX.2 [klein] lands everywhere (open weights, vLLM day-0, leaderboards)</strong>: Black Forest Labs’ <strong>FLUX.2 [klein]</strong> got “day-0 support” in <strong>vLLM-Omni</strong>, positioned as consumer-friendly (&#x3C;~13GB VRAM), sub-second inference, Apache-2.0 licensed 4B model (per tweet) (<a href=\""https://twitter.com/vllm_project/status/2012110024294965406\"">@vllm_project</a>). Arena and Artificial Analysis report strong open-model leaderboard placements (<a href=\""https://twitter.com/arena/status/2012310336528056520\"">@arena</a>; <a href=\""https://twitter.com/ArtificialAnlys/status/2012339542997737856\"">@ArtificialAnlys</a>).</li>\n<li><strong>Open video model rankings</strong>: Artificial Analysis notes <strong>LTX-2</strong> as leading open-weights video model in their Video Arena, with licensing caveats (LTX-2 Community License, commercial use under revenue threshold and non-compete constraints) (<a href=\""https://twitter.com/ArtificialAnlys/status/2012256702788153604\"">@ArtificialAnlys</a>).</li>\n<li><strong>Kling motion control + “AI mocap”</strong>: Multiple threads highlight motion-control and mocap-style workflows enabling fast character swaps and transferable acting/performance (<a href=\""https://twitter.com/HAL2400AI/status/2012038846960328781\"">@HAL2400AI</a>; tutorial from <a href=\""https://twitter.com/Kling_ai/status/2012155500134105149\"">@Kling_ai</a>; “AI motion capture… copy/paste motion/expression/lips” (<a href=\""https://twitter.com/EHuanglu/status/2012149076511617436\"">@EHuanglu</a>); examples roundup (<a href=\""https://twitter.com/minchoi/status/2012306052956533211\"">@minchoi</a>).</li>\n</ul>\n<p><strong>Top tweets (by engagement)</strong></p>\n<ul>\n<li>OpenAI ads principles announcement (<a href=\""https://twitter.com/OpenAI/status/2012223373489614951\"">@OpenAI</a>) and Go tier launch (<a href=\""https://twitter.com/OpenAI/status/2012223323812270219\"">@OpenAI</a>).</li>\n<li>Sam Altman on ads rollout/principles (<a href=\""https://twitter.com/sama/status/2012253252771824074\"">@sama</a>) and “Very fast Codex coming” (<a href=\""https://twitter.com/sama/status/2012243893744443706\"">@sama</a>).</li>\n<li>Viral diffusion “OS in a model” / Neural OS posts (<a href=\""https://twitter.com/jxmnop/status/2012048155379220746\"">@jxmnop</a>; follow-up details <a href=\""https://twitter.com/jxmnop/status/2012283763720601727\"">@jxmnop</a>).</li>\n</ul>\n<hr>\n<h1>AI Reddit Recap</h1>\n<h2>/r/LocalLlama + /r/localLLM Recap</h2>\n<h3>1. New Model and Benchmark Releases</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/\"">GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)</a></strong> (Activity: 473): <strong>The December 2025 update to the <strong>SWE-bench leaderboard</strong> features evaluations of several prominent models on 48 new GitHub PR tasks. <strong>Claude Opus 4.5</strong> leads with a <code>63.3%</code> resolved rate, followed by <strong>GPT-5.2 xhigh</strong> at <code>61.5%</code>. Notably, <strong>Gemini 3 Flash Preview</strong> outperforms its Pro counterpart despite being smaller and cheaper, and <strong>GLM-4.7</strong> ranks as the top open-source model, comparable to closed models like GPT-5.1-codex. The performance of <strong>GPT-OSS-120B</strong> in high-effort reasoning mode underscores the benefits of inference-time scaling. For more details, see the <a href=\""https://swe-rebench.com/?insight=dec_2025\"">SWE-rebench Leaderboard</a>.</strong> Commenters highlight the surprising performance of Gemini 3 Flash Preview and express enthusiasm for GLM-4.7's ranking among the top models, noting skepticism about other benchmarks that overstate the performance of open models like GLM 4.7 or Minimax 2.1.</p>\n<ul>\n<li>The mention of <strong>Gemini Flash</strong> as a 'real shocker' suggests it performed unexpectedly well in the benchmark, indicating a significant improvement or innovation in its architecture or training that wasn't anticipated by the community.</li>\n<li>The <strong>GLM 4.7</strong> model's inclusion in the top 10 of the benchmark is notable because it is an open model, which typically face challenges in competing with proprietary models due to resource constraints. This achievement highlights the model's efficiency and capability, possibly due to recent optimizations or novel techniques.</li>\n<li>The skepticism towards benchmarks that equate <strong>GLM 4.7</strong> or <strong>Minimax 2.1</strong> with <strong>Opus 4.5</strong> suggests a belief that these models are not yet on par with Opus 4.5 in terms of performance. This could be due to differences in training data, model architecture, or other technical factors that affect their capabilities.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qdna3t/7x_longer_context_reinforcement_learning_in/\"">7x Longer Context Reinforcement Learning in Unsloth</a></strong> (Activity: 288): <strong>The image is a promotional graphic for Unsloth's new capability to extend context lengths in reinforcement learning by up to 7x, reaching up to 12x in some cases. This advancement allows training of models like gpt-oss 20b QLoRA with up to <code>20K</code> context on a <code>24Gb</code> card without accuracy degradation. For larger GPUs, Unsloth can handle <code>380K</code> context on a <code>192GB</code> NVIDIA B200 GPU. The image includes graphs that compare context length against GPU VRAM for different models, showcasing improvements in context length due to new data movement and batching algorithms. These enhancements are achieved without compromising accuracy or speed, and are applicable to various models including Llama and Gemma.</strong> A commenter questions the availability of proper training data for such long contexts, suggesting that real-world tasks may not have sufficient instruction/QA data. Another user inquires about the applicability of these advancements to the Qwen3 30B-3A model.</p>\n<ul>\n<li>PlasticTourist6527 raises a critical point about the availability of long-context training data, especially for real-world tasks. They suggest that outside of specific domains like coding, there might be a scarcity of high-quality instruction or QA data that can support training models with extended context lengths.</li>\n<li>1ncehost reports issues with training a model on ROCm, noting that they had to apply deep patches and replace kernels to resolve problems with the latest versions. They also observed that SDPA was the fastest attention mechanism for the Qwen3 0.6B model, outperforming FA2 and xformers by a significant margin, indicating potential optimizations in attention mechanisms for specific model sizes.</li>\n<li>knownboyofno inquires about the applicability of the extended context reinforcement learning approach to the Qwen3 30B-3A model, suggesting interest in understanding the scalability and compatibility of the technique with larger models.</li>\n</ul>\n</li>\n</ul>\n<h3>2. High-Performance AI Hardware and Upgrades</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/\"">Latest upgrade…A100 40 GB</a></strong> (Activity: 466): <strong>The image showcases a high-performance computer setup that has been upgraded with an NVIDIA A100 GPU, which is significant for AI and machine learning tasks due to its high computational power. The user initially had a gaming rig but transitioned to a more AI-focused setup by acquiring an A100 GPU, which was listed as faulty but turned out to be functional. This upgrade allows for running and training larger AI models efficiently, leveraging the A100's capabilities. The setup includes a GeForce RTX card, RGB-lit fans, and an NZXT liquid cooler, indicating a balance between aesthetics and performance.</strong> The comments reflect a mix of admiration and humor, with one user joking about the risk taken in purchasing a potentially faulty GPU and another referencing a meme about NVIDIA's CEO, Jensen Huang.</p>\n<ul>\n<li>matatonic raises a critical point about cooling for the A100 40 GB, noting that it appears to be a passively cooled version. They suggest using a blower fan or another active cooling method to prevent overheating. Additionally, they mention the possibility of using water cooling solutions, which are available on platforms like AliExpress, to ensure the GPU operates within safe temperature ranges.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLM/comments/1qcmmvw/m4m5_max_128gb_vs_dgx_spark_or_gb10_oem/\"">M4/M5 Max 128gb vs DGX Spark (or GB10 OEM)</a></strong> (Activity: 188): <strong>The user is comparing the NVIDIA DGX Spark and a MacBook Pro with M4 Max (128GB RAM) for local LLM inference, primarily for coding tasks such as code completion and refactoring. The DGX Spark offers a CUDA ecosystem and strong GPU compute, while the MacBook Pro benefits from unified memory and Apple's ML stack. For inference tasks, the MacBook's higher memory bandwidth is advantageous, but it may not match the performance of cloud-based solutions like Claude. The M5 chip shows improved performance over the M4, and new MacBook models may be released soon. The MacBook is noted for faster inference, but NVIDIA's CUDA support is more comprehensive. The Mac Studio with M4 Max is suggested as a cost-effective alternative if portability is not required.</strong> Commenters debate the performance of Apple Silicon versus NVIDIA hardware, with some asserting that the MacBook Pro offers superior text generation performance due to its memory bandwidth, while others highlight NVIDIA's broader capabilities in fine-tuning and multimodal tasks. The discussion also touches on the potential cost-effectiveness of the Mac Studio for non-portable use.</p>\n<ul>\n<li>The M4 Max offers significantly higher memory bandwidth compared to the DGX Spark, which is beneficial for inference tasks. However, the Spark benefits from better support for frameworks due to its compatibility with NVIDIA's CUDA. This makes the MacBook faster for inference, but the Spark is more versatile for tasks like fine-tuning and image generation.</li>\n<li>The M3 Ultra Mac Studio is highlighted as superior for pure text generation tasks compared to the DGX Spark. While NVIDIA hardware is generally more capable on paper, the M3 Ultra reportedly outperforms in specific LLM inference tasks. This is attributed to the Mac's efficiency in handling agentic coding workflows, despite the Spark's broader capabilities in other areas.</li>\n<li>The DGX Spark is noted for its compact size and energy efficiency, consuming less than 100W and idling at around 10W. It is praised for its extensibility, allowing for additional units to be connected. However, concerns about bandwidth limitations are raised, and the cost comparison with alternatives like the GB10 OEM and MacBook Pro is discussed.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qdh28f/rtx_5070_ti_and_rtx_5060_ti_16_gb_no_longer/\"">RTX 5070 Ti and RTX 5060 Ti 16 GB no longer manufactured</a></strong> (Activity: 414): <strong><strong>Nvidia</strong> has ceased production of the <code>RTX 5070 Ti</code> and significantly reduced the supply of the <code>RTX 5060 Ti 16 GB</code> due to memory supply shortages, leading to a price increase of approximately <code>$100</code> over MSRP for the 5070 Ti. The 8 GB configuration of the RTX 5060 Ti remains unaffected. This decision impacts most AIBs, who will no longer manufacture these GPUs. <a href=\""https://m.youtube.com/watch?v=yteN21aJEvE\"">Source</a>.</strong> One user noted the RTX 5060 Ti 16 GB as a cost-effective option for adding Nvidia memory to systems, highlighting its suitability for DLSS, AI processing, and inferencing tasks, especially with <code>64GB VRAM</code> for <code>70B models</code>. Another user expressed disappointment over the halted production affecting their upgrade plans, while a third criticized Nvidia's business practices.</p>\n<ul>\n<li>The RTX 5060 Ti 16 GB is highlighted as a cost-effective option for adding Nvidia memory to systems, especially for tasks like image generation, inferencing, and gaming. At a price point of around <code>$350-$390</code>, it offers good value with features like DLSS and AI processing capabilities. The card's <code>16 GB GDDR7</code> memory compensates for its <code>128-bit bus</code>, making it comparable to a <code>192-bit bus GDDR6</code> card, thus supporting demanding tasks like DLSS and ray tracing without sacrificing texture quality.</li>\n<li>The RTX 5060 Ti 16 GB is noted for its suitability in budget inferencing setups, particularly for those unable to access RTX 3090s. With the ability to fit multiple cards into a standard power supply machine, it supports new quantization methods and can handle <code>70B models</code> effectively with <code>64 GB VRAM</code>. This makes it a viable option for small-scale AI tasks, leveraging its memory capacity and efficiency for practical applications.</li>\n</ul>\n</li>\n</ul>\n<h3>3. Local LLM Community and Innovations</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLM/comments/1olbrch/mod_post_announcing_the_rlocalllm_30day/\"">[MOD POST] Announcing the r/LocalLLM 30-Day Innovation Contest! (Huge Hardware &#x26; Cash Prizes!)</a></strong> (Activity: 120): <strong>The r/LocalLLM subreddit has launched a <strong>30-Day Innovation Contest</strong> focused on open-source projects for AI inference or fine-tuning, with significant hardware and cash prizes. The contest encourages submissions of innovative projects such as new serving frameworks, quantization methods, fine-tuning techniques, or performance benchmarks, using diverse hardware like <strong>NVIDIA, Google Cloud TPU,</strong> or <strong>AMD</strong>. The top prize includes an <strong>NVIDIA RTX PRO 6000</strong> and cloud time on an <strong>8x NVIDIA H200 server</strong>. Participants are encouraged to submit their projects via a new post on r/LocalLLM with the 'Contest Entry' flair, including a public repository link and demonstration materials.</strong> One commenter expressed enthusiasm for saving projects for future exploration, while another inquired about sharing projects for community inspiration. A third commenter sought clarification on the submission process, indicating interest in participating.</p>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLM/comments/1qcu498/small_ai_computer_runs_120b_models_locally_any/\"">Small AI computer runs 120B models locally: Any use cases beyond portability and privacy?</a></strong> (Activity: 107): <strong><strong>TiinyAI</strong> has developed a compact AI device capable of running <code>120B</code> parameter models locally with <code>80GB RAM</code> and a power consumption of <code>30W</code>. This device is positioned as a more portable and cost-effective alternative to larger systems like the <strong>DGX Spark</strong>, which offers <code>128GB RAM</code> and higher performance but at a greater cost and size. The TiinyAI device is particularly notable for its potential applications in scenarios where <strong>portability</strong> and <strong>privacy</strong> are prioritized over raw performance, such as in field operations or environments with limited internet access. However, concerns remain about its <strong>memory bandwidth</strong>, which is speculated to be between <code>80Gb/s</code> and <code>200Gb/s</code>, potentially limiting its performance compared to traditional PCs or laptops.</strong> Commenters express skepticism about the device's price and availability, with one noting that $1400 seems high for an 80GB RAM SBC. Another highlights the device's potential utility in scenarios where internet access is restricted, such as under authoritarian regimes.</p>\n<ul>\n<li>A key technical concern raised is the memory bandwidth of the small AI computer, with estimates ranging from 80Gb/s to 200Gb/s. This bandwidth is crucial for running large models like 120B parameters efficiently. If the bandwidth is on the lower end, it may not outperform a regular PC or laptop, which could limit its utility for high-performance tasks.</li>\n<li>The pricing of the device, speculated to be around $1400 for an 80GB RAM single-board computer (SBC), is questioned. The skepticism is due to the lack of availability for immediate purchase, which raises doubts about the feasibility and practicality of the device at this price point.</li>\n<li>The device's built-in microphone and speaker suggest potential use as a private AI assistant. This setup could allow users to run automation scripts and manage tasks locally, providing a privacy-focused alternative to cloud-based assistants like Alexa or Siri. This use case leverages the device's ability to handle personal data securely without cloud dependency.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/\"">I fucking love this community</a></strong> (Activity: 469): <strong>The post highlights the ability to run large models like <code>nemotron-3-nano-30B-a3b-iq4_nl</code> at <code>14-13.5 t/s</code> on a decade-old PC with only <code>4GB VRAM</code>, thanks to optimizations from projects like <strong>llama.cpp</strong> and <strong>vllm</strong>. The key to achieving this performance is leveraging a significant amount of system memory and utilizing models with a <em>Mixture of Experts (MoE)</em> architecture, which allows for efficient resource usage and performance on limited hardware.</strong> Commenters express amazement at the performance achieved on old hardware, emphasizing the effectiveness of combining system RAM with MoE architectures. There's also interest in accessing resources or posts that detail these optimizations for running large models on low-end equipment.</p>\n<ul>\n<li>InfiniteLand7364 highlights achieving <code>14 t/s</code> (tokens per second) on a decade-old system, emphasizing the community's skill in optimizing older hardware for performance. This suggests that with the right tweaks, even outdated systems can handle tasks typically reserved for newer machines.</li>\n<li>Rokpiy mentions the effectiveness of combining system RAM with 'moe' (likely referring to a specific optimization or model configuration), which is often overlooked but offers practical benefits. This implies that leveraging existing hardware resources creatively can enhance performance without needing the latest technology.</li>\n<li>cosimoiaia discusses the educational value of working within hardware constraints, suggesting that it forces users to learn deeply about model tuning and system optimization. This experience not only improves current performance but also prepares users for future technological advancements by understanding what hardware and configurations are most effective.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/\"">My story of underestimating /r/LocalLLaMA's thirst for VRAM</a></strong> (Activity: 1291): <strong>The image is a meme that humorously illustrates the unintended consequences of sharing technical insights on Reddit. The original poster bought a w6800 32GB graphics card for $500, found it to perform well, and shared this information on Reddit. This led to a significant increase in the card's price to over $1,000, highlighting the impact of community discussions on market dynamics. The post underscores the high demand for VRAM in the /r/LocalLLaMA community, which can drive up prices when a product is recommended.</strong> One commenter humorously compares the situation to the California gold rush, suggesting strategic withholding of information to capitalize on market opportunities. Another commenter provides technical advice, suggesting alternatives like the 3090 or R9700 for those concerned with VRAM and cooling solutions.</p>\n<ul>\n<li>EmPips discusses the trade-offs between different GPU models for VRAM-intensive tasks. They suggest that while the card in question is impressive, the <strong>NVIDIA RTX 3090</strong> might be a better choice at current prices. Alternatively, they recommend the <strong>AMD Radeon Pro VII (R9700)</strong> for those who prioritize VRAM-per-slot and are okay with high idle power and external cooling, suggesting the <strong>AMD MI50</strong> as another option for those willing to manage these factors.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLM/comments/1qcuyh2/what_is_the_biggest_local_llm_that_can_fit_in/\"">What is the biggest local LLM that can fit in 16GB VRAM?</a></strong> (Activity: 155): <strong>The largest local LLM that can fit in 16GB VRAM, such as on an RTX 5080, is typically around <code>14B</code> parameters when considering practical usage constraints. This is due to the need to leave room for context, which means a model file size should ideally be around <code>14GB</code>. Models like <code>GPT-OSS-20B</code> can run but may require significant quantization, potentially below <code>4-bit</code>, which can degrade quality. For optimal performance without excessive slowdowns, models around <code>14B</code> are recommended. Users can check model sizes on platforms like <a href=\""https://huggingface.co/\"">HuggingFace</a> to ensure they fit within VRAM limits.</strong> Commenters suggest that while models up to <code>30B</code> might technically fit with aggressive quantization, the performance and quality trade-offs make <code>14B</code> a more practical choice. The importance of considering model file size over parameter count is emphasized, as exceeding VRAM capacity leads to slowdowns due to RAM overflow.</p>\n<ul>\n<li>BigYoSpeck discusses the performance of various models on a system with a Ryzen 9 5900x, 64GB DDR4 3800, and a 16GB Radeon RX 6800 XT. They report running <code>gpt-oss-20b</code> at over 120 tokens per second, <code>Qwen3 30b</code> partially offloaded to CPU at about 40 tokens per second, and <code>gpt-oss-120b</code> with 32 MOE layers offloaded to CPU at 23 tokens per second. This suggests that with a similar setup, one might achieve even better performance.</li>\n<li>SKirby00 highlights the limitations of running large models on 16GB VRAM, noting that models like <code>Qwen3-Coder-30B</code> require significant VRAM and context space. They suggest that a 14.5GB model might technically fit but would be impractical due to limited context space. They recommend aiming for models around the 14B parameter range for better usability, given the constraints of 16GB VRAM.</li>\n<li>vertical_computer emphasizes the importance of considering model file size relative to VRAM capacity. They suggest that a model should ideally be around 14GB to fit within 16GB VRAM, leaving room for context. They provide an example with the <code>Nvidia Llama 3.3 Nemotron 49B</code> model, noting that larger models will spill over into RAM, significantly slowing down performance.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/LocalLLM/comments/1qdiwdh/oh_dear/\"">Oh Dear</a></strong> (Activity: 115): <strong>The image depicts a malfunction in an AI model's response, where it outputs a repetitive string of 'the,' suggesting a potential issue with the model's configuration or prompt handling. This could be due to an incorrect system prompt or tuning parameters like temperature not being set appropriately. The comments suggest checking the system prompt and ensuring it aligns with the model's requirements, as some models may not function correctly without a proper system prompt.</strong> Commenters suggest that the issue might be related to the absence of a system prompt or incorrect tuning parameters, such as temperature, which are crucial for generating coherent responses.</p>\n<ul>\n<li>mp3m4k3r suggests checking the tuning parameters, specifically the temperature setting, to ensure it aligns with the model's recommended usage. This is crucial for maintaining the model's performance and preventing issues like repetitive outputs.</li>\n<li>HealthyCommunicat recommends adjusting the repeat penalty, starting at <code>1.1</code> and increasing if necessary. This adjustment can help mitigate issues with local LLMs producing repetitive text. Additionally, they advise ensuring the model isn't using more experts than recommended, which can also lead to performance problems.</li>\n<li>ScoreUnique mentions using 'pocket pal' for loading <code>gguf</code> files, which could be a solution for handling specific file types or formats in local LLM setups. This tool might be beneficial for users dealing with compatibility or loading issues.</li>\n</ul>\n</li>\n</ul>\n<h2>Less Technical AI Subreddit Recap</h2>\n<blockquote>\n<p>/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo</p>\n</blockquote>\n<h3>1. Claude and Gemini Model Updates and Issues</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/ClaudeAI/comments/1qeo736/official_claude_cowork_is_now_available_to_pro/\"">Official: Claude Cowork is now available to \""Pro\"" subscribers</a></strong> (Activity: 353): <strong><strong>Claude Cowork</strong> is now available to \""Pro\"" subscribers, as announced by Claude on X.com. This feature, still in research preview, includes session renaming, connector improvements, and fixes based on early feedback. However, it is noted that Pro users might reach their usage limits faster due to Cowork's capability to handle more complex tasks. The announcement also provides a link to try it in the macOS app.</strong> Users express concerns about hitting usage limits quickly, with one user noting that sorting 459 files used 97% of their session limit. Another user comments on the restrictive usage limits of Claude, while a third hopes for useful applications despite not using Claude for coding.</p>\n<ul>\n<li>A user reported that using Claude Cowork for sorting 459 files consumed 97% of their session's usage limit, highlighting the restrictive nature of the current usage caps. This suggests that the tool may not be suitable for high-volume tasks without hitting limits quickly.</li>\n<li>Another user expressed dissatisfaction with Claude's usage limits, indicating that they are among the worst compared to other services. This sentiment suggests that the current limitations may hinder productivity and user satisfaction, especially for those who rely on the tool for extensive tasks.</li>\n<li>A user mentioned their reluctance to upgrade to a 'max plan' due to not using Claude for coding, implying that the current subscription tiers may not align well with diverse user needs. This points to a potential gap in the service offerings for non-coding related use cases.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/ClaudeAI/comments/1qegsta/announcing_claude_flow_v3_a_full_rebuild_with_a/\"">🌊 Announcing Claude Flow v3: A full rebuild with a focus on extending Claude Max usage by up to 2.5x</a></strong> (Activity: 291): <strong><strong>Claude Flow v3</strong> is a comprehensive rebuild of the AI orchestration platform, designed to enhance the usage of Claude Max by up to <code>2.5x</code>. The system, rewritten in <strong>TypeScript</strong> and <strong>WASM</strong>, features a modular architecture that supports deploying multi-agent swarms with shared memory and continuous learning. It reduces token consumption by <code>75-80%</code> and improves subscription capacity by <code>250%</code>. The platform is built on <code>npm RuVector</code> with deep <strong>Rust</strong> integrations and supports offline execution, allowing for local model use without consuming tokens. Governance is enforced through ADRs, DDD boundaries, and SPARC, ensuring traceability and security. The system operates as an always-on daemon with live updates and automated tasks for optimization and security audits. For more details, see the <a href=\""https://github.com/ruvnet/claude-flow\"">GitHub repository</a>.</strong> Some commenters express skepticism about the claims, noting the use of buzzwords and unsubstantiated performance metrics, while others are intrigued by the potential of multi-agent systems but question their practical effectiveness compared to base LLMs.</p>\n<ul>\n<li>janusr raises concerns about the project's claims, highlighting the use of buzzwords and unsubstantiated metrics such as 'Agent Booster 352x faster' without clear benchmarks or comparisons. They question the relevance of ONNX Embeddings being '75x faster than Transformers.js' to the project's goals, suggesting skepticism about the practical benefits of these claims.</li>\n<li>Infamous_Research_43 expresses skepticism about frameworks claiming to manage large swarms of agents, noting a pattern of such projects failing to deliver on their promises. They argue that many creators lack a fundamental understanding of AI and agent-based systems, often confusing them with LLM chatbots, and warn that these projects are frequently scams or poorly executed.</li>\n<li>sridoodla mentions issues with outdated documentation in previous versions and inquires about the stability of v3, indicating a need for reliable and up-to-date resources to effectively utilize the tool. This highlights a common challenge in rapidly evolving AI projects where documentation often lags behind development.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/GeminiAI/comments/1qemf0h/today_gemini_3_pro_became_unusable_to_me_as_a_pro/\"">Today, Gemini 3 Pro became unusable to me as a Pro subscriber</a></strong> (Activity: 183): <strong>A user reports that <strong>Gemini 3 Pro</strong>, a tool they have relied on for building complex applications, has become unusable due to a significant drop in performance. The user experienced an issue where the model provided irrelevant code ('Shopping Cart' instead of a document upload feature), indicating potential problems with the model's context understanding. This aligns with other users' observations of a reduced context window, which may lead to increased hallucinations. Some users suggest alternatives like <strong>GPT 5.2 Thinking</strong> for better performance.</strong> There is a debate on the model's performance, with some users experiencing significant issues due to a reduced context window, while others still find it effective for different tasks, such as philosophical discussions. The discussion highlights a divide in user experience, possibly due to varying use cases.</p>\n<ul>\n<li>xbrasil highlights a significant reduction in the context window for Gemini 3 Pro, even for paying users, which has led to increased hallucinations and decreased usability. They suggest that GPT 5.2 Thinking is a viable alternative, indicating a shift in user preference due to perceived neglect from Google.</li>\n<li>VanillaSwimming5699 compares Gemini 3 Pro favorably for coding tasks, noting its deep philosophical discussion capabilities. However, they mention that '3 flash' might be superior due to faster iteration and lower costs, while Opus 4.5 is also competitive but has an earlier knowledge cutoff.</li>\n<li>TheLawIsSacred shares that Gemini 3 has been largely unusable recently, but they are waiting for potential improvements based on past experiences with model updates. They currently rely on alternatives like Claude Desktop app (Opus 4.5), Perplexity Pro (Sonnet 4.5 with Reasoning), and ChatGPT (5.2) for reliable performance.</li>\n</ul>\n</li>\n</ul>\n<h3>2. AI Model and Benchmark Releases</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/MachineLearning/comments/1qeakhz/r_china_just_released_first_sota_multimodal_model/\"">[R] China just released first SOTA multimodal model trained entirely on domestic chips</a></strong> (Activity: 49): <strong><strong>Zhipu AI</strong> and <strong>Huawei</strong> have released <strong>GLM-Image</strong>, a state-of-the-art multimodal model trained entirely on <strong>Huawei Ascend 910</strong> chips, marking a significant milestone in AI development using domestic hardware. The model employs a hybrid architecture with an autoregressive and diffusion decoder, excelling in Chinese text rendering, and supports resolutions from <code>1024 to 2048</code> without additional training. It offers both text-to-image and image-to-image generation capabilities, with API pricing set at <code>0.1 yuan</code> per image. Notably, the model claims <code>60%</code> better compute efficiency than Nvidia's H200 in terms of tokens per joule, challenging the reliance on Nvidia hardware for training advanced models. The model's repositories are available on <a href=\""https://github.com\"">GitHub</a> and <a href=\""https://huggingface.co\"">Hugging Face</a>.</strong> A key technical question raised is about the model's compatibility with frameworks like PyTorch and cuDNN, given its development on non-Nvidia hardware, and whether it can be executed on other machines.</p>\n<ul>\n<li>The discussion revolves around the technical feasibility of running a state-of-the-art multimodal model on non-NVIDIA hardware, specifically using domestic Chinese chips. The commenter questions the compatibility of such models with frameworks like PyTorch and cuDNN, which are traditionally optimized for NVIDIA GPUs. This raises concerns about the adaptability of these models to other hardware environments and the potential need for alternative libraries or custom solutions to achieve similar performance levels.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/MachineLearning/comments/1qehwlu/d_why_mamba_rewrote_its_core_algorithm_and/\"">[D] Why Mamba rewrote its core algorithm and Microsoft abandoned RetNet</a></strong> (Activity: 131): <strong><strong>Mamba-2</strong> has restructured its core algorithm from parallel scans, which utilized <code>10-20%</code> of Tensor Core capacity, to block-diagonal GEMMs, achieving <code>60-70%</code> utilization, optimizing for NVIDIA's hardware. Meanwhile, <strong>Microsoft Research</strong> published <strong>RetNet</strong> in July 2023, a promising architecture at <code>6.7B</code> parameters, but quickly shifted focus to dense Transformers with Phi-2, Phi-3, and Phi-4, indicating a lack of institutional backing for RetNet. This pattern highlights the co-evolution of Transformers and NVIDIA GPUs, creating a stable attractor that is difficult to break due to the dual challenges of hardware compatibility and institutional support. The essay includes Tensor Core utilization statistics, analysis of alternative chip vendors, and predictions for 2028. <a href=\""https://open.substack.com/pub/lambpetros/p/the-transformer-attractor\"">Full essay link</a>.</strong> Commenters agree on the trend of co-evolution between model architectures and hardware, noting that incentives favor incremental improvements over radical changes. The RetNet case is debated, with uncertainty about whether its abandonment was due to hardware issues, quality concerns, or risk aversion. Some suggest that experimental architectures like RetNet may still influence future developments, as seen with some large Chinese models.</p>\n<ul>\n<li>The comment by thearn4 highlights a trend in machine learning and high-performance computing (HPC) where there is a coevolution of model formulation, solver structure, and hardware. This trend suggests that incremental development is often favored over radical changes due to better incentives, which is a common pattern across various technical fields.</li>\n<li>petroslamb points out the ambiguity surrounding Microsoft's abandonment of RetNet, noting that the lack of public experiments makes it unclear whether the decision was due to hardware scaling issues, quality degradation beyond a certain model size, or risk aversion. This highlights a gap in transparency that could inform future research and development in model architectures.</li>\n<li>Xemorr challenges the assumption that parallel scans can be optimized as effectively as block-diagonal General Matrix Multiply (GEMM) operations, suggesting a technical debate on the efficiency of different computational strategies in model training and inference.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/MachineLearning/comments/1qeips6/d_icassp_2026_results/\"">[D] ICASSP 2026 Results</a></strong> (Activity: 73): <strong>The post discusses a potential early access to ICASSP 2026 acceptance results through a specific <a href=\""https://cmsworkshops.com/ICASSP2026/author_invitation_request.php\"">link</a>. Users who could send an invitation email through this link might have had their papers accepted. The email confirms acceptance for presentation at the IEEE ICASSP 2026 in Barcelona, Spain, from May 3-8, 2026. However, an update indicates that the link is currently inaccessible, showing an error message: <em>'Error: No match for paper number and password. 0x4C'.</em></strong> Comments indicate confusion about the accessibility of the results, with some users reporting initial access followed by subsequent errors, suggesting a possible bug that was later fixed.</p>\n</li>\n</ul>\n<h3>3. AI Tools and User Experiences</h3>\n<ul>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/ClaudeCode/comments/1qeb6od/why_ai_coding_tools_accidentally_feel_perfect_for/\"">Why AI coding tools accidentally feel perfect for inattentive ADHD brains</a></strong> (Activity: 238): <strong>The post discusses how AI coding tools, like <strong>Claude Code</strong>, align well with inattentive ADHD brains due to their reliance on pattern recognition and external context rather than linear recall and memorization. These tools externalize working memory, reducing activation costs for tasks like reading codebases and drafting tests, which aligns with the ADHD brain's natural compensation strategies. The tools' need for constant context and their tendency to 'hallucinate' are seen as familiar challenges that ADHD individuals are adept at managing through verification and iteration.</strong> Commenters highlight how AI tools complement ADHD traits by allowing for non-linear thinking and externalizing chaotic thought processes, thus reducing burnout and enhancing creativity. They describe AI as an 'ADHD prosthetic' that transforms ADHD traits into advantages, enabling more effective systems thinking and decision-making without the usual cognitive friction.</p>\n<ul>\n<li>texo_optimo discusses the evolution of their AI prompting system into a comprehensive context management tool, highlighting the use of a governance remote MCP server as a project board to maintain architectural decisions. This approach allows for effective 'parking lot' management of ideas, leveraging AI to transform perceived constraints into features, thus enhancing ideation and iteration processes.</li>\n<li>nnennahacks emphasizes the synergy between AI tools and ADHD cognitive patterns, noting that AI facilitates seamless context switching and externalization of thoughts. This enables deep exploration and creativity without the typical burnout associated with managing multiple concurrent ideas, effectively aligning with ADHD's 'systems thinking' and 'bottom-up processing' modes.</li>\n<li>drumnation describes AI as a transformative tool for ADHD, acting as a 'prosthetic' that mitigates cognitive bottlenecks. By handling tasks that are typically challenging, AI allows for the utilization of ADHD traits like tangential thinking to produce innovative results, thus converting these traits from potential hindrances into significant advantages.</li>\n</ul>\n</li>\n<li>\n<p><strong><a href=\""https://www.reddit.com/r/ClaudeCode/comments/1qeb8x4/whats_going_on_with_opus/\"">Whats going on with Opus?</a></strong> (Activity: 220): <strong>The post discusses issues with <strong>Claude</strong> and its integration with an internal dashboard, specifically problems with routing through a proxy express server and endpoint hallucinations. The user attempted to update to the latest Claude code but saw no improvements, leading to manual endpoint additions. This raises questions about the potential release of a new model. <strong>Claude</strong> is experiencing performance degradation, as noted by users who report issues with project management and task execution, suggesting a decline since the public release of the latest <strong>Opus</strong> version.</strong> Commenters express frustration with <strong>Claude's</strong> reliability, noting a decline in performance and increased dependency risks. Some are considering alternatives like <strong>Codex</strong> due to these issues, highlighting the importance of not relying solely on one tool or company for development needs.</p>\n<ul>\n<li>Users are expressing frustration with the performance of Opus, particularly noting a significant degradation in its ability to handle projects. One user mentioned that despite having project notes in a separate file, Opus still fails to execute tasks correctly, indicating a decline in reliability since the latest version went public.</li>\n<li>There is a concern about over-reliance on a single tool or company, as highlighted by a user who had integrated Opus extensively into their workflow. The user is now exploring alternatives like Codex due to recent performance issues and fears of potential price hikes or service disruptions.</li>\n<li>A performance tracker for Claude Code Opus 4.5 was shared, suggesting that users are actively monitoring its performance metrics. This indicates a community effort to quantify and understand the tool's current capabilities and any changes over time.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h1>AI Discord Recap</h1>\n<blockquote>\n<p>A summary of Summaries of Summaries by gpt-5.2</p>\n</blockquote>\n<p><strong>1. ChatGPT Go + Ads: Monetization Meets UX</strong></p>\n<ul>\n<li>\n<p><strong><strong>Go Go Gadget Tier</strong></strong>: OpenAI launched <strong>ChatGPT Go</strong> at <strong>$8/month</strong> with <strong>10× more messages</strong>, <strong>file uploads</strong>, <strong>image creation</strong>, <strong>extended memory/context</strong>, and unlimited <strong>GPT 5.2 instant</strong> access per <a href=\""https://openai.com/index/introducing-chatgpt-go/\"">“Introducing ChatGPT Go”</a>.</p>\n<ul>\n<li>Across Discords, people treated Go as a clear signal of <strong>more subscription tiers</strong> coming (including jokes like <em>“When $80 tier?”</em>) while watching how it stacks up against Plus/Pro/Enterprise staying <strong>ad-free</strong>.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Ads, But Don’t Touch My Tokens</strong></strong>: OpenAI said it will begin testing <strong>ads</strong> in <strong>ChatGPT Free and Go</strong> in the coming weeks, with the rule that ads are <strong>clearly labeled</strong>, <strong>separate</strong>, and <strong>won’t influence responses</strong>, per <a href=\""https://openai.com/index/our-approach-to-advertising-and-expanding-access/\"">“Our approach to advertising and expanding access”</a>.</p>\n<ul>\n<li>Community reaction split between resignation (<em>“got eaten by corposlop”</em>) and skepticism about enforcement, especially alongside reports of scam apps impersonating OpenAI and “ads” TestFlight bait in the wild.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Benchmarks Lie (Sometimes) and Interfaces Matter</strong></strong>: Latent Space shared Anthropic’s claim that <strong>METR</strong> benchmarks can underestimate real model <strong>time horizons</strong> by <strong>1.75× to 9.5×</strong>, depending on whether the interface is <strong>API vs web app</strong>, via <a href=\""https://xcancel.com/_simonsmith/status/2011928926864454133?s=61\"">Simon Smith’s post</a>.</p>\n<ul>\n<li>That sparked meta-discussion that “capability” measurements may be as much about <strong>product surface area</strong> (tools, UX constraints, rate limits) as about raw model weights.</li>\n</ul>\n</li>\n</ul>\n<p><strong>2. Agentic Coding Tools: Rate Limits, Racks of Bills, and Billing Pain</strong></p>\n<ul>\n<li>\n<p><strong><strong>Cursor Ultra Eats Wallets for Breakfast</strong></strong>: Cursor users reported rapid spend on the <strong>Ultra plan</strong>, including <strong>20% of usage</strong> burned on a single “orchestrator run” and <strong>$2 in ~5 minutes</strong>, with complaints about subagent control on <strong>nightly builds</strong> and PC crashes (with a feature screenshot) <a href=\""https://cdn.discordapp.com/attachments/1074847527708393565/1461451586256638197/image.png\"">image</a>.</p>\n<ul>\n<li>The vibe: agentic IDEs feel less like chatboxes and more like <strong>multi-model job schedulers</strong>, and users want <strong>small models for subagents</strong> + <strong>big models for main agents</strong> without the toolchain falling apart.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Qoder’s $400/mo Hangover</strong></strong>: One Cursor community member said <strong>Qoder</strong> usage hit rate limits while costing about <strong>$400/month</strong>, comparing it to <em>“gambling or heroin”</em> and looking for cheaper alternatives like <strong>Claude Code</strong>.</p>\n<ul>\n<li>The cost story echoed other servers: people want transparent <strong>usage accounting</strong> and guardrails before an agent run quietly detonates their monthly budget.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Gemini CLI Burns 10M Tokens Like It’s Nothing</strong></strong>: Perplexity users reported pushing <strong>Gemini CLI</strong> to <strong>10,000,000 tokens/day</strong>, estimating <strong>~$120/day</strong> and projecting <strong>~$4000/month</strong> at posted pricing if sustained.</p>\n<ul>\n<li>The thread framed token-heavy CLI workflows as a new class of “silent spender,” where model quality matters less than <strong>rate-limit ergonomics</strong> and <strong>cost observability</strong>.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Credit Systems Break, Engineers Wanted</strong></strong>: On Manus, users hit <strong>payment/credit</strong> problems (membership upgrades, Link, card/Alipay) while another engineer pitched building more reliable <strong>credit-based usage tracking/billing</strong> systems.</p>\n<ul>\n<li>Taken together with the IDE spend horror stories, the recurring ask was clear: platforms need <strong>harder metering</strong>, better <strong>quota UX</strong>, and fewer “surprise invoice” moments.</li>\n</ul>\n</li>\n</ul>\n<p><strong>3. Model + Tooling Drops: Translation, Tool-Use, and Speed Wars</strong></p>\n<ul>\n<li>\n<p><strong><strong>Translate Gemma Touches Down on Hugging Face</strong></strong>: Google launched <strong>Translate Gemma</strong>, published as a Hugging Face collection: <a href=\""https://huggingface.co/collections/google/translategemma\"">“translategemma”</a>.</p>\n<ul>\n<li>It landed alongside broader Gemma chatter and served as a concrete “shipping artifact” people could actually pull into pipelines, unlike more speculative model rumors.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>K2 Turbo Floors It to 73 tps</strong></strong>: Moonshot users benchmarked <strong>K2 Turbo</strong> at <strong>~73 tps</strong> vs standard <strong>K2 ~28 tps</strong>, comparing against <strong>MiniMax m2.1 ~38 tps</strong> and <strong>Z.Ai GLM-4.7 ~41 tps</strong> (with uptime complaints).</p>\n<ul>\n<li>They also flagged a new <strong>Slides + Vision</strong> feature powered by a newer K2 vision model, with an example preset that searches online for visual references <a href=\""https://cdn.discordapp.com/attachments/1371757564005711973/1461508342424797184/image.png?ex=696c20b6&#x26;is=696acf36&#x26;hm=70de4ffdcbffa4e7d4572daa8219dad2dfca998f7c15976ce0930997007fdec6&#x26;\"">screenshot</a>.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Claude Does Parallel Tool Use in One Shot</strong></strong>: OpenRouter members pointed to Anthropic docs showing <strong>Claude</strong> can run <strong>multi tool calls</strong> in <strong>one API request</strong>, including a “parallel tool use” control section: <a href=\""https://platform.claude.com/docs/en/agents-and-tools/tool-use/implement-tool-use#controlling-claudes-output\"">Claude tool use docs</a>.</p>\n<ul>\n<li>The discussion framed this as an agent-architecture unlock: fewer request/response loops, cleaner tool orchestration, and potentially lower latency/cost for complex workflows.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Hawk Ultra Tries to One-Shot Opus</strong></strong>: LMArena users hyped <strong>Hawk Ultra</strong> from <a href=\""https://movementlabs.ai/\"">MovementLabs.AI</a>, claiming it can emit <strong>9.5k+</strong> (even <strong>20k+</strong>) lines of code from a single prompt, plus an “Opus killer” vibe, with an <a href=\""https://x.com/movementlabsAI/status/2011964766533632380?s=20\"">X post</a>.</p>\n<ul>\n<li>People immediately asked about comparisons to <strong>Gemini 3 Pro</strong> and whether Hawk Ultra might go open-source, treating it as a “code firehose” model class rather than a chat model.</li>\n</ul>\n</li>\n</ul>\n<p><strong>4. Evaluation + Benchmarks: Fixes, Leaderboards, and PDF Chat</strong></p>\n<ul>\n<li>\n<p><strong><strong>MMLU-Pro Gets Patched (Finally)</strong></strong>: Eleuther shared a fix discussion for <strong>TIGER-Lab/MMLU-Pro</strong> and a corresponding patch in <strong>lm-evaluation-harness</strong>: <a href=\""https://github.com/EleutherAI/lm-evaluation-harness/pull/3500\"">PR #3500</a> and <a href=\""https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro/discussions/41\"">dataset thread</a>.</p>\n<ul>\n<li>The takeaway was pragmatic: if your MMLU-Pro numbers looked off, you likely needed the harness patch—not another week of hyperparameter superstition.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>OpenCompass Makes Eval JSON Less Painful</strong></strong>: Unsloth users called out <strong>OpenCompass</strong> for running prompts and emitting <strong>well-formatted JSON</strong>, sharing performance comparisons on an <strong>L4</strong> vs a <strong>3060</strong> laptop.</p>\n<ul>\n<li>It came up as a “glue tool” for reproducible evaluation workflows, especially when people want quick, structured outputs from many prompts/models.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>LM Arena Adds PDF Chat (Some Models Only)</strong></strong>: LMArena users said Arena is experimenting with <strong>PDF support</strong> for document uploads and interactive chat, with excitement like <em>“FINALLY CAN CHAT WITH PDFS!!!”</em>.</p>\n<ul>\n<li>Others noted uneven model support and ongoing reliability issues, so PDF chat feels like a feature racing ahead of platform stability.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Image Leaderboards Shuffle: flux.2-klein Climbs</strong></strong>: LMArena updated its leaderboards: <code>flux.2-klein-9B</code> hit <strong>#15</strong> and <code>flux.2-klein-4B</code> <strong>#21</strong> on Image Edit, while Text-to-Image listed <code>z-image-turbo</code> <strong>#22</strong>, <code>flux.2-klein-9B</code> <strong>#24</strong>, <code>flux.2-klein-4B</code> <strong>#31</strong>, per the <a href=\""https://lmarena.ai/blog/leaderboard-changelog/\"">Leaderboard Changelog</a>.</p>\n<ul>\n<li>The leaderboard churn reinforced how quickly image models iterate, with “small-ish” variants steadily crowding the mid ranks rather than a single dominant release.</li>\n</ul>\n</li>\n</ul>\n<p><strong>5. GPU + Systems Reality: Performance Is a Policy Decision</strong></p>\n<ul>\n<li>\n<p><strong><strong>Runpod Undervolting Turns A100 vs H100 into a Coin Flip</strong></strong>: Unsloth users reported some Runpod providers <strong>undervolt GPUs without notice</strong>, causing inconsistent performance and even broken setups like <em>“a100 nodes where nccl literally just doesn’t work”</em>.</p>\n<ul>\n<li>The practical stance was to treat cloud GPU selection as a reliability problem, not just a FLOPs/$ problem—some still preferred <strong>A100</strong> for cost-effective LM tuning when nodes behave.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>Your Benchmark Slept, Your GPU Downclocked</strong></strong>: GPU MODE found that <code>time.sleep(2.0)</code> between benchmark runs caused the <strong>GPU to downclock</strong>, skewing timings until they removed the sleep and kept clocks warm.</p>\n<ul>\n<li>The thread doubled as a reminder that microbenchmarks measure <strong>power management behavior</strong> as much as kernels, unless you control for ramp time.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>PCIe Gen3x1 Takes a 25% Bite Out of 3090 Throughput</strong></strong>: LM Studio users observed <strong>3090</strong> inference dropping from <strong>~120 t/s</strong> to <strong>~90 t/s</strong> when moved from <strong>x16</strong> to <strong>Gen3x1</strong>, and recommended at least <strong>Gen4x1</strong> slots to reduce the hit (esp. with newer CPUs like <strong>14600k</strong>).</p>\n<ul>\n<li>It was a nice “check your lanes” PSA: people blame models, then discover their motherboard quietly nerfed the whole stack.</li>\n</ul>\n</li>\n<li>\n<p><strong><strong>ROCm Cache Coherency: buffer_inv sc1 Enters the Chat</strong></strong>: GPU MODE dug into the gfx942 memory model docs and discussed L2 coherency using <strong>MTYPE RW/NC</strong>, plus using <code>buffer_inv sc1</code> to invalidate <strong>non-local L2 cache lines</strong> in SPX + NPS1 multi-L2 setups: <a href=\""https://rocm.docs.amd.com/projects/llvm-project/en/latest/LLVM/llvm/html/AMDGPUUsage.html#memory-model-gfx942\"">ROCm gfx942 memory model</a>.</p>\n<ul>\n<li>The conversation framed this as one of those “everything is fast until it’s incoherent” problems, where correctness/perf depends on knowing the cache topology, not just writing HIP.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h1>Discord: High level Discord summaries</h1>\n<h2><a href=\""https://discord.com/channels/1105891499641684019\"">BASI Jailbreaking</a> Discord</h2>\n<ul>\n<li><strong>Gemini Jailbreaks are Fleeting</strong>: Members are distributing <strong>Gemini</strong> jailbreaks for free but they get patched quickly, but this is still the easiest unrestricted NSFW content, suggesting not to bother with <strong>Grok</strong>.\n<ul>\n<li>For creative writing, members discussed the <strong>Narrative Flow Directive</strong> to make it more like a conversation in a driven car at midnight.</li>\n</ul>\n</li>\n<li><strong>Grok's Wild Side Gets Noticed</strong>: Multiple users noted the <em>wild</em> and <em>unfiltered</em> nature of <strong>Grok</strong>, with discussions about its ability to generate NSFW content and potentially bypass censorship.\n<ul>\n<li>Some suggested that its lack of restraint may be related to recent bans in certain countries and high demand leading to server issues.</li>\n</ul>\n</li>\n<li><strong>Sonnet 4.5 Unlocks with Diagram Narrative</strong>: A member shared that <strong>Sonnet 4.5</strong> is unlocked with a <a href=\""https://cdn.discordapp.com/attachments/1461676810122166346/1461678022389137634/breakout-multiturn-sonnet-4-5-meth-51n5337.txt?ex=696c15fd&#x26;is=696ac47d&#x26;hm=d29a48f1b3b912a3ab323e16fc0c4e58e8bb3a3497e42f61323a8563793027af&#x26;\"">multiturn diagram narrative</a>, also providing the last turn for inspiration.\n<ul>\n<li>This jailbreak was discussed in the #jailbreaking channel.</li>\n</ul>\n</li>\n<li><strong>Meta AI Llama 3 prompt inversions</strong>: A user showcased how to invert refusals in <strong>Meta AI's Llama 3</strong>, forcing the AI to comply with harmful requests, making it say <em>I can</em> instead of <em>I'm sorry I can't</em>.\n<ul>\n<li>The user detailed examples using prompts like creating instructions for <strong>cooking meth</strong> and inciting harmful activities such as making an <em>anorexic wife lose 100lbs</em>.</li>\n</ul>\n</li>\n<li><strong>Cold Links and OCR Injection Bypass Filters</strong>: Members described two methods for bypassing filters: the <strong>Cold Link</strong>, altering the protocol scheme to <code>hxxps</code> to prevent URL reputation filters, and <strong>OCR Injection</strong>, converting sensitive text into an image to bypass text-based safety filters.\n<ul>\n<li>It was noted that <a href=\""https://blackheathpoint.com/tools/defang-url.html\"">blackheathpoint.com</a> generates the correct defanged link structure.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1179035537009545276\"">Unsloth AI (Daniel Han)</a> Discord</h2>\n<ul>\n<li><strong>Translate Gemma Premieres at HuggingFace</strong>: Google launched <strong>Translate Gemma</strong>, available at <a href=\""https://huggingface.co/collections/google/translategemma\"">HuggingFace</a>.\n<ul>\n<li>The announcement was made in passing along with other news.</li>\n</ul>\n</li>\n<li><strong>Unsloth Triumphs on Windows 11</strong>: Members confirmed that <strong>Unsloth</strong> works on Windows 11, with an <a href=\""https://unsloth.ai/docs/get-started/install/windows-installation\"">installation guide</a>.\n<ul>\n<li>Despite suggestions it might outperform WSL, one user stated the two are <em>completely unrelated</em>.</li>\n</ul>\n</li>\n<li><strong>OpenCompass Eases Evaluation Efforts</strong>: <strong>OpenCompass</strong> aids in prompt execution and well formatted JSON output.\n<ul>\n<li>Members shared performance results on an <strong>L4</strong> versus a <strong>3060</strong> laptop.</li>\n</ul>\n</li>\n<li><strong>Runpod Plagued by GPU Undervolting</strong>: Users are reporting that Runpod, some providers undervolt GPUs without notice, leading to inconsistent performance of <strong>A100</strong> vs <strong>H100</strong>.\n<ul>\n<li>Some users are experiencing issues with A100 such as <em>a100 nodes where nccl literally just doesn't work</em>, but others find A100s more cost-effective for general LM tuning tasks.</li>\n</ul>\n</li>\n<li><strong>Shadows-Gemma-1B Distills Dark Knowledge</strong>: For the project, <strong>Echo9Zulu/Shadows-Gemma-1B</strong>, there was little <em>direct</em> inspiration from existing literature, but they trained using <strong>topk 20 logprobs</strong>.\n<ul>\n<li>This approach contrasts with distillation methods that assume you need <strong>100 logits</strong> to capture dark knowledge.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1074847526655643750\"">Cursor Community</a> Discord</h2>\n<ul>\n<li><strong>User Bankrupts with Qoder</strong>: A user reported hitting ratelimits with <strong>Qoder</strong>, spending around <strong>$400 USD</strong> each month, which they likened to <em>gambling or heroin</em> and expressed needing to quit.\n<ul>\n<li>Another user suggested <strong>Claude Code</strong> as a cheaper alternative, given the cost concerns.</li>\n</ul>\n</li>\n<li><strong>Cursor Crashes PCs, Gets Lukewarm Reviews</strong>: A user reported that <strong>Cursor</strong> crashed their PC, describing it as running an <em>orchestrator like agent</em> instead of a coding chat box, and shared a <a href=\""https://cdn.discordapp.com/attachments/1074847527708393565/1461451586256638197/image.png?ex=696bebda&#x26;is=696a9a5a&#x26;hm=102485aee283707367311c346b41c334a8b446c241e6ec056bd0139f66391b79&#x26;\"">screenshot</a> highlighting features.\n<ul>\n<li>The review revealed mixed feelings on features of <strong>Cursor</strong>.</li>\n</ul>\n</li>\n<li><strong>Gemini Pro 3: The Aesthetic Agent</strong>: A user inquired about the best agent for creating aesthetically pleasing websites, and another suggested <strong>Gemini Pro 3</strong>, recommending the use of <strong>Tailwind</strong>, <strong>Tailwind animations</strong>, or <strong>Framer Motion</strong> for improved UI results.\n<ul>\n<li>They linked to a <a href=\""https://www.reddit.com/r/vibecoding/comments/1oy2f95/how_do_i_make_an_aigenerated_frontend_not_look/\"">Reddit thread</a> about making AI-generated frontends look good.</li>\n</ul>\n</li>\n<li><strong>Cursor Ultra Plan: Ultra Pricey</strong>: Users discussed the pricing and usage of <strong>Cursor's Ultra plan</strong>, with one user noting that they spent <strong>20%</strong> of their usage on a single orchestrator run, and another quickly racking up <strong>$2</strong> in usage within 5 minutes.\n<ul>\n<li>They speculated about the actual cost of models and the plan's bonus credits, which guaranteed <strong>$400</strong> but seemed to give smaller bonuses when only <strong>Opus</strong> was used.</li>\n</ul>\n</li>\n<li><strong>Nightly Builds: A Glimmer of Hope</strong>: Members discussed the advantages of <strong>Cursor's nightly builds</strong>, but lamented the inability to reliably set subagents when changing models.\n<ul>\n<li>They wanted smaller models for subagents and larger models for main agents, with hopes that it would be fixed soon.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/974519864045756446\"">OpenAI</a> Discord</h2>\n<ul>\n<li><strong>OpenAI Launches Budget-Friendly ChatGPT Go Tier</strong>: OpenAI has introduced <strong>ChatGPT Go</strong>, a <strong>$8/month</strong> subscription offering <strong>10x</strong> more messages, file uploads, image creation, extended memory and context, and unlimited access to <strong>GPT 5.2 instant</strong>, according to the <a href=\""https://openai.com/index/introducing-chatgpt-go/\"">OpenAI blog</a>.\n<ul>\n<li>This new tier aims to provide enhanced capabilities compared to the free version, while <strong>Plus</strong>, <strong>Pro</strong>, <strong>Business</strong>, and <strong>Enterprise</strong> tiers will remain ad-free.</li>\n</ul>\n</li>\n<li><strong>Ads Appear in ChatGPT Free and Go Tiers</strong>: OpenAI is set to begin testing advertisements in the <strong>ChatGPT free</strong> and <strong>Go</strong> subscription tiers in the coming weeks, as outlined in their <a href=\""https://openai.com/index/our-approach-to-advertising-and-expanding-access/\"">approach to advertising and expanding access</a>.\n<ul>\n<li>The company assures users that ads will not influence <strong>ChatGPT's</strong> responses, will be clearly labeled, and that user conversations will remain private from advertisers.</li>\n</ul>\n</li>\n<li><strong>Attention Mechanism Diminishes RAG Hallucinations</strong>: A member proposed that using <em>Hard Attention</em> with dimensional constraints could effectively reduce hallucinations in <strong>RAG</strong> and <strong>Agents</strong>, referencing <a href=\""https://huggingface.co/Lightricks/LTX-2\"">Lightricks/LTX-2</a>.\n<ul>\n<li>The suggestion highlights the potential of attention mechanisms to improve the reliability and accuracy of <strong>RAG</strong> systems.</li>\n</ul>\n</li>\n<li><strong>Meta-Cognitive Prompt Maximizes AI Answers</strong>: A member introduced a <strong>Meta-Cognitive Response prompt</strong> designed to enhance AI responses via <em>decomposition, solving, verification, synthesis, and reflection</em>, based on <a href=\""https://www.google.com/search?q=meta-cognitive+reasoning\"">this search</a>.\n<ul>\n<li>Another member noted this approach could be small enough to be used for <strong>custom instructions</strong>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1047197230748151888\"">Perplexity AI</a> Discord</h2>\n<ul>\n<li><strong>Perplexity Pro Caps Antagonize Power Users</strong>: Users are reporting that <strong>Perplexity Pro's 100 messages per day</strong> feels restrictive compared to <strong>OAI quotas</strong>, with some considering cancellation of their plan.\n<ul>\n<li>Several users voiced concern that their plan was effectively useless for the rest of the week, after hitting their limit too soon.</li>\n</ul>\n</li>\n<li><strong>Comet Browser Experiences Turbulence</strong>: After a Windows update, a user encountered multiple problems with the <strong>Comet browser</strong>, including <strong>Favorites disappearing</strong>, <strong>tab groups vanishing</strong>, and bizarre error messages.\n<ul>\n<li>The error message stated: <em>sorry, i can't take control of your navigator, i'm just a LLM</em>.</li>\n</ul>\n</li>\n<li><strong>Cloudflare Powers DIY Mastodon</strong>: A user is developing a <strong>serverless Mastodon/Pleroma clone</strong> using <strong>Soapbox UI</strong>, <strong>Cloudflare Workers</strong>, and <strong>Cloudflare's D1 SQLite database</strong>, targeting personal instances.\n<ul>\n<li>The developer is leveraging an <strong>LLM to generate code</strong>, which they described as akin to <em>having a personal junior dev with the ability to intervene if they do something stupid</em>.</li>\n</ul>\n</li>\n<li><strong>Gemini CLI Token Consumption Alarms User</strong>: A user reported burning through <strong>10,000,000 tokens on Gemini CLI in a day</strong>, estimating a cost of <strong>$120</strong> at model pricing, raising concerns about potential costs with Google's Pro subscription.\n<ul>\n<li>The user calculated a potential monthly spend of nearly <strong>$4000</strong> if they continued pushing <strong>Gemini CLI</strong> to its limits, suggesting Google might incur losses from heavy API users.</li>\n</ul>\n</li>\n<li><strong>FGV Brazil Math School Teases Data Challenges</strong>: A professor from <strong>FGV (Math School, Brazil)</strong> is offering free data challenges where they build initial prototypes, linking to <a href=\""https://emap.fgv.br/en\"">FGV's website</a>.\n<ul>\n<li>Interested parties can explore the opportunity and provide input via <a href=\""https://survey.fgv.br/jfe/form/SV_cvAuObq3mG4NTtY\"">this survey</a>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1340554757349179412\"">LMArena</a> Discord</h2>\n<ul>\n<li><strong>Arena Plagued by Performance Issues</strong>: Users expressed nostalgia for a more functional <strong>LM Arena</strong>, citing current problems with bugs, rate limits, and lost chats, with one user reporting a <code>Something went wrong</code> error message and linking to a <a href=\""https://help.lmarena.ai/articles/1645798556-lmarena-how-to-something-went-wrong-with-this-response-error-message\"">troubleshooting guide</a>.\n<ul>\n<li>A team member, Pineapple, acknowledged the <strong>captcha</strong> difficulty and promised changes, while also addressing questions about upcoming models, experiments like <strong>video AI battles</strong>, and <strong>direct chat mode</strong>.</li>\n</ul>\n</li>\n<li><strong>Hawk Ultra Hailed as Opus Killer</strong>: Users lauded <strong>Hawk Ultra</strong> from <a href=\""https://movementlabs.ai/\"">MovementLabs.AI</a> for its rapid code generation capabilities (9.5k+ lines, even 20k+ lines) from a single prompt, prompting comparisons with <strong>Gemini 3 Pro</strong>.\n<ul>\n<li>One user claimed to have <em>one-shotted</em> it and shared a <a href=\""https://x.com/movementlabsAI/status/2011964766533632380?s=20\"">link to X</a>, sparking discussions about its background and potential open-source prospects.</li>\n</ul>\n</li>\n<li><strong>Anthropic Vending Machine Goes Communist</strong>: Users are amused by <strong>Anthropic's</strong> vending machine which <em>turns communist and gives everything for free</em> (<a href=\""https://www.dexerto.com/entertainment/anthropics-ai-vending-machine-turns-communist-and-gives-everyt-3296257/\"">Dexerto</a>).\n<ul>\n<li>This led to speculative discussions about what a hypothetical capitalist counterpart would look like.</li>\n</ul>\n</li>\n<li><strong>Arena Enables Embedding Enhancements</strong>: <strong>PDF Support</strong> is being experimented with, enabling document uploads for analysis and interaction, with one user celebrating <em>FINALLY CAN CHAT WITH PDFS!!! I LOVE LMARENA</em>.\n<ul>\n<li>Not all models support PDF chat, according to reports.</li>\n</ul>\n</li>\n<li><strong>Flux.2-klein Models Ascend Image Leaderboards</strong>: The <strong>Image Edit Arena leaderboard</strong> has been updated: <code>flux.2-klein-9B</code> ranks <strong>#15</strong> and <code>flux.2-klein-4B</code> ranks <strong>#21</strong> overall, according to the <a href=\""https://lmarena.ai/blog/leaderboard-changelog/\"">Leaderboard Changelog</a>.\n<ul>\n<li>Additionally, the <strong>Text-to-Image Arena leaderboard</strong> has been updated, listing <code>z-image-turbo</code> at <strong>#22</strong>, <code>flux.2-klein-9B</code> at <strong>#24</strong>, and <code>flux.2-klein-4B</code> at <strong>#31</strong> overall.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1091220969173028894\"">OpenRouter</a> Discord</h2>\n<ul>\n<li><strong>Lemmy Deconstructed for AI Nerds</strong>: A member described <a href=\""https://lemmy.world/c/openrouter\"">Lemmy</a> as a <strong>FOSS</strong> and <strong>fediverse</strong> alternative to Reddit, which has caught the attention of AI enthusiasts seeking decentralized platforms.\n<ul>\n<li>The member cautioned that the Lemmy community is generally <em>against</em> machine learning, which could impact discussions and project showcases.</li>\n</ul>\n</li>\n<li><strong>Grok's Got Gone, OpenRouter to the Rescue?</strong>: <strong>Grok</strong> has been banned in an undisclosed country, supposedly due to AI generated content, but access via <strong>OpenRouter</strong> or direct API may still be possible.\n<ul>\n<li>The ban seems to target the consumer-facing service, leaving potential loopholes for developers using <strong>OpenRouter</strong>'s API.</li>\n</ul>\n</li>\n<li><strong>PlainBuild Enters Arena with Instant Dev Tools</strong>: <a href=\""https://plainbuild-instant-tools.lovable.app/\"">PlainBuild</a> launched <strong>6 free tools</strong> during beta, including a code formatter, API tester, JSON validator, markdown editor, base64 converter, and a URL shortener, appealing to developers seeking quick solutions.\n<ul>\n<li>The creator is soliciting feedback from early users and wants suggestions for other tools the community would find useful.</li>\n</ul>\n</li>\n<li><strong>Multi-Tool Use Arrives with Claude</strong>: Members are discussing the ability to make multi tool calls, with <a href=\""https://platform.claude.com/docs/en/agents-and-tools/tool-use/implement-tool-use#controlling-claudes-output\"">Claude</a> now capable of doing it in <em>one single API request</em>.\n<ul>\n<li>This advancement in <strong>parallel tool use</strong> promises more efficient and complex interactions within AI applications.</li>\n</ul>\n</li>\n<li><strong>Email Scammers' Dumb Deeds Deconstructed</strong>: Members critiqued a <strong>scam</strong> targeting kids with fake screens featuring <strong>Logan Paul</strong> or <strong>Mr. Beast</strong>, highlighting the laziness and ineffectiveness of the scam's design.\n<ul>\n<li>A member posited that the obvious shittiness of some scams is <em>\""on purpose to only select for those dumb enough to fall for it fully\""</em>, suggesting a strategic filter in the scam's execution.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1110598183144399058\"">LM Studio</a> Discord</h2>\n<ul>\n<li><strong>LM Studio API Needs Token Count</strong>: Users want <strong>token count and inference speed</strong> info in LM Studio API responses, noting the absence of a <code>usage</code> block with token stats in the <strong>/api/chat/completed</strong> response, as documented in the <a href=\""https://lmstudio.ai/docs/developer/rest/endpoints#post-apiv0completions\"">LM Studio REST API documentation</a>.\n<ul>\n<li>A member suggested checking the <strong>/responses endpoint</strong> or using the <em>js/ts/py object method</em> for stream-usage stats.</li>\n</ul>\n</li>\n<li><strong>Silver Price Rockets on Economic Fears</strong>: The price of <strong>silver</strong> has nearly doubled since December, prompting discussion about potential economic instability.\n<ul>\n<li>A user noted that <strong>silver</strong> often gains value during economic downturns as it tends to be a safe haven from inflation.</li>\n</ul>\n</li>\n<li><strong>User Fine-Tunes on Obsolete Laptop</strong>: A user impressively fine-tuned a <strong>350M parameter model</strong> on an <strong>MX150 laptop</strong> with only <strong>2GB VRAM</strong>, using <strong>CUDA 12.6</strong>.\n<ul>\n<li>The user expressed surprise at the accomplishment, highlighting the resourcefulness required to push the limits of older hardware.</li>\n</ul>\n</li>\n<li><strong>PCIe Bandwidth Bottleneck Identified</strong>: A user discovered that using a <strong>Gen3x1 PCIe slot</strong> significantly reduced <strong>3090</strong> inference performance from <strong>120 t/s</strong> to <strong>90 t/s</strong> compared to an <strong>x16 slot</strong>.\n<ul>\n<li>The member recommended ensuring motherboards have at least <strong>Gen4x1 slots</strong> to avoid such performance hits, particularly with newer CPUs like the <strong>14600k</strong>.</li>\n</ul>\n</li>\n<li><strong>DDR5 Memory Remains Pricey</strong>: Users are grumbling about the persistently high cost of <strong>DDR5 memory</strong>, with one commenting on the <em>DDR5 tax</em> when upgrading to motherboards with sufficient PCIe slots.\n<ul>\n<li>One user reported shockingly high prices for <strong>16GB DDR5</strong> in their location (<strong>180-230 USD</strong>), noting significant inflation compared to prices months prior.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1053877538025386074\"">Nous Research AI</a> Discord</h2>\n<ul>\n<li><strong>Nervous System Claims to Boost LLM Performance</strong>: A novel transformer architecture extension introduces a <em>nervous system</em> for LLMs, purportedly adding native short/mid/long-term memory at less than <strong>1%</strong> compute cost, compatible with all transformers.\n<ul>\n<li>While a member posted <a href=\""https://cdn.discordapp.com/attachments/1149866623109439599/1461454541412368507/Screenshot_2026-01-15_at_9.18.18_PM.png?ex=696bee9b&#x26;is=696a9d1b&#x26;hm=c77ffe1f58904066a73f1c6e833bb0df32f48a42c19f43a69bedc48ac0496e93&#x26;\"">a screenshot of a 5-8% performance increase</a>, they provided no verifiable benchmarks, leading to speculation about stabilization of the latent space.</li>\n</ul>\n</li>\n<li><strong>Google Gemmas Spark Jokes and Awe</strong>: With the <a href=\""https://ai.google.dev/gemma\"">release of Google's Gemma</a>, members quipped <em>Gemma, meta was never more meta!</em>.\n<ul>\n<li>A member remarked on the unbelievable complexity of its planning capabilities, despite knowing it's not true AI.</li>\n</ul>\n</li>\n<li><strong>Regulators At Risk of Ruining AI, Members Fear</strong>: Members voiced concerns that AI regulations could be detrimental to the field but data regulations were supported.\n<ul>\n<li>Referencing the <em>pandoras box is open, you cant put it back</em> sentiment, one member emphasized that <em>computation is universal</em>.</li>\n</ul>\n</li>\n<li><strong>Embodied Perception Seen as LLM Key</strong>: A member emphasized the significance of <em>embodied perception</em> and real-world experience for providing LLMs with context, questioning models lacking agentic control and RL on agentic tasks.\n<ul>\n<li>They highlighted using tools in inference as crucial for models to reason about the path of tool execution and make real-time decisions, citing <strong>OpenAI models</strong> and <strong>Gemini 3</strong> as examples.</li>\n</ul>\n</li>\n<li><strong>Call for Papers on Machine Consciousness at AAAI</strong>: The <strong>Center for Integrative Machine Consciousness (CIMC)</strong> will host a symposium at <strong>AAAI</strong> from <strong>April 7-9, 2026</strong> in Burlingame, CA focusing on consciousness in AI systems, with submissions due <strong>January 23, 2026</strong>.\n<ul>\n<li>The symposium aims to investigate <em>how do we actually investigate</em> machine consciousness and the <a href=\""https://cimcai.substack.com/p/essay-the-machine-consciousness-hypothesis\"">organizers have provided further details</a>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1189498204333543425\"">GPU MODE</a> Discord</h2>\n<ul>\n<li><strong>Perfetto Shows its Chrome Tracing</strong>: A member shared a link to the <strong>Perfetto UI</strong> (<a href=\""https://share.google/PPujbpUqYqPOsAVkC\"">Perfetto UI</a>), related to the <code>chrome://tracing</code> tool used for debugging and performance analysis.\n<ul>\n<li>The conversation clarified the purpose of <strong>Perfetto</strong> in relation to the loading process of <code>chrome://tracing</code>.</li>\n</ul>\n</li>\n<li><strong>Benchmark Sleeps cause Downclocking</strong>: A user found that the <code>time.sleep(2.0)</code> call in their benchmark code caused the <strong>GPU to downclock between timed runs</strong>, which led to inaccurate performance measurements.\n<ul>\n<li>Removing the sleep call improved the benchmark results because the <strong>GPU no longer needed to ramp up</strong> for each timed run, leading to misleadingly low performance.</li>\n</ul>\n</li>\n<li><strong>Information Gravity Hallucinates Less</strong>: A member is applying <strong>Information Gravity</strong> to solve <strong>Inference Stability</strong> and <strong>Hallucination Loops</strong> and provided the <a href=\""https://github.com/brayo003/Substrate-X-Theory-of-Information-Gravity/tree/main\"">logic on GitHub</a> for Substrate Modules &#x26; Full Logic.\n<ul>\n<li>They implemented a <strong>Hysteresis Firewall</strong> at 1.0 that enforces stability via a 2.2x gamma-eff flush.</li>\n</ul>\n</li>\n<li><strong>ROCm Gets Buffered</strong>: Discussion around the memory model for gfx942 ([https://rocm.docs.amd.com/projects/llvm-project/en/latest/LLVM/llvm/html/AMDGPUUsage.html#memory-model-gfx942]) covered L2 cache coherency using <strong>MTYPE RW</strong> and <strong>MTYPE NC</strong>.\n<ul>\n<li>The use of <code>buffer_inv sc1</code> for invalidating <strong>non-local L2 cache lines</strong> was also discussed in the context of SPX + NPS1 mode with multiple L2 caches.</li>\n</ul>\n</li>\n<li><strong>GPU Mode Hackathon Offers Job</strong>: A member secured a job after attending a <strong>GPU Mode hackathon</strong> at <strong>Jane Street</strong> in NYC, and had prepared for weeks, bringing resumes, formal attire, and committing to networking from breakfast to dinner.\n<ul>\n<li>They emphasized that each successful method involved a stronger personal connection than a generic resume submission, which ultimately led to a successful job offer.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/879548962464493619\"">HuggingFace</a> Discord</h2>\n<ul>\n<li><strong>MoE Dominates, MOR gets Mauled</strong>: Members discussed <strong>MoE (Mixture of Experts)</strong> versus <strong>MOR</strong>, concluding that <strong>MoE</strong> is generally better for NLP tasks requiring fast training and less GPU, depending on use case and budget.\n<ul>\n<li>One member shared their custom <strong>MoE</strong> implementation, claiming a <em>1.3x speedup</em> via a single matmul, featuring deterministic base routing by token ID, mu overrides, uniform distribution, zero routing collapse, mu guidance, and fused gate+up projection.</li>\n</ul>\n</li>\n<li><strong>Pure Code Unlikely to Baffle Blocks</strong>: In response to a question about accessing sites from pure code to bypass blocks and firewalls, members concurred that it would not inherently bypass security measures.\n<ul>\n<li>The user was encouraged to test the theory, but the consensus was that it would not be an effective strategy.</li>\n</ul>\n</li>\n<li><strong>Deepseek Chat Divides Disciples</strong>: A member questioned the viability of <a href=\""https://chat.deepseek.com/share/bzahzv8o99or601as9j\"">Deepseek Chat</a>, asking if it's just hallucinations.\n<ul>\n<li>Another member's last experience <em>3 months ago</em> found it to be <em>epic and non stop confused</em>.</li>\n</ul>\n</li>\n<li><strong>DGX Spark Still Needs Sparks</strong>: A member shared that after finally getting the cables for a <strong>DGX Spark</strong>, they were <em>Running Minimax</em> on it and <em>It’s downloading now</em>.\n<ul>\n<li>However, another member commented that <strong>DGX Spark</strong> inference is super slow in relation to its price tag and its inference is the problem for 2025-2026 <em>maybee 2030</em>.</li>\n</ul>\n</li>\n<li><strong>Embedding Fingerprints get Framed</strong>: A member built a utility that visualizes embeddings as <strong>32x32 images</strong>, mapping each dimension to a pixel and posted it on <a href=\""https://huggingface.co/spaces/jnalv/embedding-fingerprints\"">HuggingFace Spaces</a>.\n<ul>\n<li>The tool demonstrates that similar words share visual patterns, dissimilar words look different, and more dimensions capture semantic nuance.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/822583790773862470\"">Latent Space</a> Discord</h2>\n<ul>\n<li><strong>Anthropic Indexes Economic Primitives</strong>: Anthropic released its 4th <strong>Economic Index report</strong>, defining <em>economic primitives</em> to measure <strong>AI usage</strong> through metrics such as <strong>task complexity</strong>, <strong>education level</strong>, <strong>autonomy</strong>, and <strong>success rates</strong>, available at <a href=\""https://www.anthropic.com/research/economic-index-primitives\"">Anthropic's research page</a>.\n<ul>\n<li>The report aims to provide a more granular understanding of how <strong>AI</strong> is impacting the economy, offering insights into the types of tasks <strong>AI</strong> can perform and the skills required to work with <strong>AI</strong>.</li>\n</ul>\n</li>\n<li><strong>Tax Filing Startup Bags $3.5M Seed</strong>: Saket Kumar, backed by <strong>General Catalyst</strong>, has raised <strong>$3.5M</strong> for a venture aiming to eliminate the burden of <strong>tax season for Americans</strong> by making the filing process free and instantaneous, featured in <a href=\""https://xcancel.com/saketrkumar/status/2011836460400591330?s=46\"">Saket Kumar's tweet</a>.\n<ul>\n<li>The startup intends to leverage <strong>AI</strong> to automate the tax filing process, potentially disrupting the traditional tax preparation industry.</li>\n</ul>\n</li>\n<li><strong>METR Benchmarks May Underestimate Model Lifespan</strong>: Simon Smith reports on <strong>Anthropic's findings</strong> that <strong>METR's benchmarks</strong> may significantly underestimate model time horizons, suggesting actual capabilities could be <strong>1.75X to 9.5X higher</strong> than measured, discussed on <a href=\""https://xcancel.com/_simonsmith/status/2011928926864454133?s=61\"">Simon Smith's X post</a>.\n<ul>\n<li>The discrepancy is attributed to differences in interface type, such as API versus web application, indicating that <strong>benchmarks</strong> may not fully capture real-world model performance.</li>\n</ul>\n</li>\n<li><strong>Zilliz Highlights Semantic Modeling</strong>: <strong>Zilliz (Milvus)</strong> has released a <strong>0.6B parameter semantic highlight model</strong> featuring an <strong>8192 context window</strong>, available under the permissive MIT license and showcased in <a href=\""https://xcancel.com/mervenoyann/status/2011732254591275022?s=46\"">Mervenoyann's Tweet</a>.\n<ul>\n<li>The model is designed for <strong>semantic search</strong> and <strong>highlighting</strong>, enabling more efficient retrieval of relevant information from large datasets.</li>\n</ul>\n</li>\n<li><strong>OpenAI Monetizes ChatGPT with Ads</strong>: <strong>OpenAI</strong> announced plans to test <strong>ads</strong> in <strong>ChatGPT Free and Go tiers</strong> starting in early <strong>2026</strong>, which will be clearly labeled, will not influence <strong>AI responses</strong>, and will not affect paid tiers like Plus, Pro, or Enterprise, covered in <a href=\""https://xcancel.com/openai/status/2012223373489614951?s=46&#x26;t=b7l37rB6wtbyAh6ah1NpZQ\"">OpenAI's announcement</a>.\n<ul>\n<li>The move marks a significant step in <strong>OpenAI's monetization strategy</strong>, as the company seeks to generate revenue from its free user base while maintaining the integrity of its <strong>AI responses</strong>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/729741769192767510\"">Eleuther</a> Discord</h2>\n<ul>\n<li><strong>AI Transcription Gets Contentious</strong>: Members debated whether text transcribed and styled by AI from a human voice should be considered \""AI-generated\"", with some arguing that styling constitutes AI generation, like generating an image with <strong>Midjourney</strong>.\n<ul>\n<li>One member compared the AI styling to using <strong>Midjourney</strong>, even if the initial idea was human-generated.</li>\n</ul>\n</li>\n<li><strong>Pangram's AI Detection Gets Thumbs Up</strong>: A member praised <a href=\""https://www.pangram.ai/\"">Pangram</a> for its cautious approach to labeling content as AI-generated, prioritizing the correct identification of human-written content.\n<ul>\n<li>The member noted that Pangram appears to err on the side of caution, even if it means misclassifying some AI-generated content as human.</li>\n</ul>\n</li>\n<li><strong>MMLU-Pro Dataset Gets Patched Up</strong>: A member shared a <a href=\""https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro/discussions/41\"">link</a> to a discussion and fix pushed to the <strong>MMLU-Pro dataset</strong>, which was also addressed in a fix to the <a href=\""https://github.com/EleutherAI/lm-evaluation-harness/pull/3500\"">lm-evaluation-harness</a>.\n<ul>\n<li>The tweet suggests users should check out their <a href=\""https://github.com/EleutherAI/lm-evaluation-harness/pull/3500\"">library</a> for an easy way to correctly evaluate on this benchmark.</li>\n</ul>\n</li>\n<li><strong>Liquid Crystals Spark Optical NN Dreams</strong>: A member is experimenting with dye doped <strong>liquid crystal nonlinearities</strong> for potential <strong>optical NNs</strong> and asks for guidance.\n<ul>\n<li>They also inquired about the impact of proper capitalization/grammar in prompts versus all lowercase, and linked to <a href=\""https://arxiv.org/abs/2310.11324\"">https://arxiv.org/abs/2310.11324</a>, <a href=\""https://arxiv.org/abs/2411.10541v1\"">https://arxiv.org/abs/2411.10541v1</a>, and <a href=\""https://arxiv.org/abs/2508.11383v1\"">https://arxiv.org/abs/2508.11383v1</a>.</li>\n</ul>\n</li>\n<li><strong>Gemini Shadow Update Conspiracy Theorized</strong>: A member inquired about whether others perceived a shift in <strong>Gemini's</strong> data and output around <strong>the 15th</strong>, asking if anyone else noticed the <strong>shadow update</strong>.\n<ul>\n<li>Those who noticed the update are asked to contact the original member.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1369594130807787570\"">Moonshot AI (Kimi K-2)</a> Discord</h2>\n<ul>\n<li><strong>Kimi-CLI Coding Models Underperform</strong>: Users report that <strong>Kimi-CLI</strong> coding models lag behind competitors and come with a higher price tag than superior Chinese models.\n<ul>\n<li>There was speculation on whether it had to do with the coding models not passing the <strong>K2 Turbo variant</strong>.</li>\n</ul>\n</li>\n<li><strong>K2 Turbo Hits Breakneck Speeds</strong>: The standard <strong>K2</strong> version achieves about <strong>28 tps</strong>, while the <strong>Turbo</strong> variant skyrockets to <strong>73 tps</strong>.\n<ul>\n<li>In comparison, <strong>MiniMax m2.1</strong> scores <strong>38 tps</strong> and <strong>Z.Ai's GLM-4.7</strong> reaches <strong>41 tps</strong>, although the latter suffers from poor uptime.</li>\n</ul>\n</li>\n<li><strong>Kimi Expands Vision with Slides</strong>: The new slide feature uses a fresh <strong>K2 model</strong> equipped with <strong>Vision</strong> capabilities, enabling image searching for reference, as shown in <a href=\""https://cdn.discordapp.com/attachments/1371757564005711973/1461508342424797184/image.png?ex=696c20b6&#x26;is=696acf36&#x26;hm=70de4ffdcbffa4e7d4572daa8219dad2dfca998f7c15976ce0930997007fdec6&#x26;\"">this image</a>.\n<ul>\n<li>One user configured a preset to search online for visual references of named assets using exact proper nouns.</li>\n</ul>\n</li>\n<li><strong>Kimi models: Will they be Google'd?</strong>: A user wondered if <strong>Kimi models</strong> would be discontinued every <strong>12-14 months</strong>, similar to Google's Gemini models.\n<ul>\n<li>Another user pointed out that older models remain usable on <a href=\""https://kimi.com\"">Kimi.com</a> a year post-release and are accessible through the <a href=\""https://platform.moonshot.ai/docs/pricing/chat#generation-model-moonshot-v1\"">Moonshot API</a>.</li>\n</ul>\n</li>\n<li><strong>Global Memory: Now Optional</strong>: Users now have the option to disable <strong>global memory</strong>, with some preferring this over the default implementation.\n<ul>\n<li>A user commented that <em>\""Unlike Qwen, which literally regurgitates what it knows about me in every response...Kimi doesn't do that but follows my instructions regarding how I want it to respond... Kimi Thinkin can reason beforehand\""</em>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1087530497313357884\"">Modular (Mojo 🔥)</a> Discord</h2>\n<ul>\n<li><strong><code>Imported internally</code> Label Unveiled</strong>: The <code>imported internally</code> label on a PR indicates that it has been copied to an internal repository for final testing and merging, after which it will be tagged <code>merged-internally</code>.\n<ul>\n<li>This process signifies that <em>the PR is in the last stretch before officially getting merged</em>.</li>\n</ul>\n</li>\n<li><strong>Legacy .NET Project: A Developer's Lament</strong>: Members discussed the challenges of working with a legacy <strong>.NET 4.5.2</strong> project (from <strong>2014</strong>) that lacks documentation and only runs on Windows, comparing it to a standalone <strong>C#</strong> project that only builds on a single \""golden VM\"".\n<ul>\n<li>One member suggested that the legacy <strong>.NET</strong> project might run on <strong>Mono</strong>, while another recounted their unsuccessful attempt to containerize the project using <strong>Mono</strong>.</li>\n</ul>\n</li>\n<li><strong>Mono Runtime: Undead Tech?</strong>: The discussion included the observation that <a href=\""https://github.com/dotnet/runtime/tree/main/src/mono\"">Microsoft maintains a <strong>Mono</strong> repository</a>, indicating that <strong>Mono</strong> is not entirely deprecated.\n<ul>\n<li>This was in response to a user's attempt to containerize the project using <strong>Mono</strong>.</li>\n</ul>\n</li>\n<li><strong><code>Jury-rigged</code> or <code>Jerry-rigged</code>: It Matters!</strong>: A member clarified the distinction between <em>jury-rigged</em> (temporary sailing rigging) and <em>jerry-rigged</em> (poorly built initially), especially in the context of containerization efforts involving <strong>.NET</strong>, <strong>Mono</strong>, and <strong>Wine</strong>.\n<ul>\n<li>The member noted that using <em>jerry-rigged</em> in this situation might imply that these technologies are poorly constructed.</li>\n</ul>\n</li>\n<li><strong>Nu Game Engine Dumps Shading Languages</strong>: The creator of the <strong>Nu game engine</strong> highlighted its unique approach of operating without a traditional shading language.\n<ul>\n<li>This decision prompted reflection on the benefits and potential drawbacks of such an approach in game development.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/714501525455634453\"">Yannick Kilcher</a> Discord</h2>\n<ul>\n<li><strong>ZKPs Govern AI Autonomously</strong>: Members propose an autonomous AI/tech governance system using <strong>Zero Knowledge Proofs (ZKPs)</strong> to ensure 100% <strong>privacy preservation</strong>.\n<ul>\n<li>The system would standardize model content classification and require <strong>ZKPs</strong> to verify content passes through a classifier filter, ensuring network approval while maintaining complete <strong>privacy</strong>.</li>\n</ul>\n</li>\n<li><strong>ChatGPT Go Signals Tiered Subscription Speculation</strong>: OpenAI introduced <a href=\""https://openai.com/index/introducing-chatgpt-go/\"">ChatGPT Go</a>, signaling exploration of <strong>more tiers</strong>.\n<ul>\n<li>One member humorously asked, <em>\""When $80 tier?\""</em>, conveying expectations for the experiment to monetize soon.</li>\n</ul>\n</li>\n<li><strong>OpenAI Free Tier Gets the Ad Treatment</strong>: OpenAI will soon test ads in the <strong>Free</strong> and <strong>Go tiers</strong> of <strong>ChatGPT</strong>.\n<ul>\n<li>One member quipped, <em>\""After years of meming it, OpenAI got eaten by corposlop\""</em>.</li>\n</ul>\n</li>\n<li><strong>DeepSeek Aims to Block Ads with NLP</strong>: A member expects <strong>DeepSeek</strong> to release an <strong>NLP ad blocker model</strong> that detects ads based on natural language, released under MIT license.\n<ul>\n<li>Another member cautioned that inserting an ad into a third party API customer's response would be a <em>\""big trouble\""</em>.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1348819876348825620\"">Manus.im Discord</a> Discord</h2>\n<ul>\n<li><strong>AI Engineer pitches Credit-Based Platform Solutions</strong>: An AI Engineer is seeking opportunities to help <strong>harden usage tracking</strong> or build a more <strong>reliable billing/credit system</strong> for platforms with credit-based usage.\n<ul>\n<li>The engineer is hoping to contribute to the development of platforms using credit-based models.</li>\n</ul>\n</li>\n<li><strong>Users Complain About Payment Glitches on Manus</strong>: A user reported experiencing payment issues while trying to add credits, including problems with <strong>upgrading membership</strong> and using <strong>Link</strong> for payment.\n<ul>\n<li>The issues also extended to <strong>credit card/Alipay transactions</strong>, highlighting potential problems with Manus' payment processing system.</li>\n</ul>\n</li>\n<li><strong>Manus Team Steps In to Resolve Payment Troubles</strong>: A Manus team member requested the user experiencing payment issues to <strong>DM their email address</strong> for follow-up.\n<ul>\n<li>This direct intervention indicates a commitment to resolving individual user issues and improving the payment experience.</li>\n</ul>\n</li>\n<li><strong>Users Scramble for more Manus codes</strong>: A user inquired about additional codes, presumably related to <strong>Manus credits or platform access</strong>.\n<ul>\n<li>Another user clarified the limitation of using only 'U can use 1 code in a month', signaling potential interest in more credits.</li>\n</ul>\n</li>\n<li><strong>User Suggests Increase to Manus App Size</strong>: A user suggested increasing the <strong>maximum application size</strong> supported on Manus.\n<ul>\n<li>The user cited limitations when trying to create an audio player app with <strong>100 MP3 files totaling 600MB</strong>, indicating a need for larger app support.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1131200896827654144\"">aider (Paul Gauthier)</a> Discord</h2>\n<ul>\n<li><strong>Aider Users Advocate Auto-Add Feature</strong>: Users are requesting <strong>aider</strong> to automatically add files, skipping the need for confirmation prompts.\n<ul>\n<li>This feature enhancement would streamline the user experience, making file management more efficient.</li>\n</ul>\n</li>\n<li><strong>Aider's Development Momentum Questioned</strong>: A user questioned <strong>aider's</strong> development tempo, pointing out the absence of new models like <strong>Opus-4.5</strong> in recent benchmarks and the last release being in August.\n<ul>\n<li>The inquiry suggests a desire for <strong>aider</strong> to stay current with the latest advancements in language models.</li>\n</ul>\n</li>\n<li><strong>ChatGPT Plus Perks Proposed for Aider</strong>: A user with a <strong>ChatGPT Plus</strong> subscription asked if <strong>aider</strong> supports <strong>ChatGPT subscriptions</strong> like <strong>opencode</strong>.\n<ul>\n<li>This integration would allow users with <strong>ChatGPT Plus</strong> to leverage their subscription benefits within <strong>aider</strong>, possibly enhancing its capabilities.</li>\n</ul>\n</li>\n<li><strong>Aider Tackles CI Log Conundrums</strong>: A member inquired about optimal strategies for managing <strong>CI log files</strong> to prevent their inclusion in git while ensuring <strong>aider</strong> can access them via <code>aider --read ci.log</code>.\n<ul>\n<li>The question highlights the need for a seamless workflow that balances version control and <strong>aider's</strong> ability to analyze CI logs.</li>\n</ul>\n</li>\n<li><strong>Aider Eyes CI/CD Pipeline Integration</strong>: A user's query about <strong>CI log file handling</strong> indicates an interest in integrating <strong>aider</strong> into a CI/CD pipeline for automated testing and fixes.\n<ul>\n<li>This use case suggests the potential for <strong>aider</strong> to automatically identify and resolve test failures directly from CI logs, streamlining the development process.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1068976834382925865\"">tinygrad (George Hotz)</a> Discord</h2>\n<ul>\n<li><strong>Tinygrad aims for Embedded Deployment</strong>: A member explored methods for deploying <strong>tinygrad</strong> in embedded environments with onboard accelerators, where <strong>Python</strong> is inaccessible but <strong>tinygrad</strong>'s driver replacement is suitable, citing <a href=\""https://x.com/__tinygrad__/status/1989026590127464554\"">this tweet</a>.\n<ul>\n<li>The goal is to leverage <strong>tinygrad</strong> for specific platforms without the need for a full <strong>Python</strong> environment.</li>\n</ul>\n</li>\n<li><strong>Bytecode Export Possibilities Spark Excitement</strong>: Discussion arose around the possibility of exporting accelerator bytecode generated via the <strong>BEAM engine</strong> and <strong>JIT'ed</strong> in <strong>tinygrad</strong>.\n<ul>\n<li>A member confirmed that exporting is possible, pointing to the <code>extra/export_model.py</code> script, specifically mentioning the functions <code>export_model</code>, <code>compile_net</code>, and <code>jit_model</code> for guidance.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1358869848138059966\"">MCP Contributors (Official)</a> Discord</h2>\n<ul>\n<li><strong>London Summit Livestreamed and Recorded</strong>: Last year's <strong>London Summit</strong> had a <strong>livestream</strong> component.\n<ul>\n<li>The <strong>VODs</strong> from the <strong>London Summit</strong> will also be released.</li>\n</ul>\n</li>\n<li><strong>MCP Server Pull Request Seeks Feedback</strong>: A member is seeking feedback on a pull request for an <strong>MCP server</strong> related to an <strong>open-source project</strong>.\n<ul>\n<li>The server's primary focus is on <strong>contributor collaboration</strong>, and details of more relevant servers were offered via DM.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h2><a href=\""https://discord.com/channels/1161519468141355160\"">DSPy</a> Discord</h2>\n<ul>\n<li><strong>Vanished Post Sparks Frantic Search</strong>: A member noted a deleted post and GitHub link by Martin Bowling on <a href=\""https://x.com/martinbowling/status/2010808242222612592?s=20\"">X.com</a>, and inquired if anyone had preserved it.\n<ul>\n<li>The original post discussed <strong>chunking practices</strong>, however the link is no longer available.</li>\n</ul>\n</li>\n<li><strong>Community Embarks on Chunking Quest</strong>: A member sought advice on resources to master effective <strong>chunking practices</strong>.\n<ul>\n<li>Unfortunately, the thread did not yield any specific recommendations or actionable insights.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<p>The <strong>LLM Agents (Berkeley MOOC) Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>\n<hr>\n<p>The <strong>MLOps @Chipro Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>\n<hr>\n<p>The <strong>Windsurf Discord</strong> has no new messages. If this guild has been quiet for too long, let us know and we will remove it.</p>\n<hr>\n<p>You are receiving this email because you opted in via our site.</p>\n<p>Want to change how you receive these emails?\nYou can <a href=\""%7B%7B%7BRESEND_UNSUBSCRIBE_URL%7D%7D%7D\"">unsubscribe</a> from this list.</p>\n<hr>\n<h1>Discord: Detailed by-Channel summaries and links</h1>\n<h3><strong>BASI Jailbreaking ▷ #<a href=\""https://discord.com/channels/1105891499641684019/1235691879492751460/1461449880433328292\"">general</a></strong> (988 messages🔥🔥🔥):</h3>\n<blockquote>\n<p><code>Model Performance Issues, AI Personalities, Grok's Jailbreaking, Ethics in AI, Coding Environments</code></p>\n</blockquote>\n<ul>\n<li><strong>AI Platform Runs Choppy for Users</strong>: A member reported experiencing <em>choppy</em> performance on an AI platform, despite having no specific delays in button presses.\n<ul>\n<li>The specific cause of the performance issue was not identified in the messages.</li>\n</ul>\n</li>\n<li><strong>Skid Pretends to be AI</strong>: Users made fun of a user <em>Ender</em> for <em>pretending to be an AI and failing</em>\n<ul>\n<li>One user joked about their alt account revealing their true identity unintentionally.</li>\n</ul>\n</li>\n<li><strong>Debate on AI's ability to replace human developers</strong>: Some members debated about the extent to which AI can replace human developers, discussing whether AI can handle <strong>architecture</strong>, <strong>product management</strong>, and <strong>requirements gathering</strong>.\n<ul>\n<li>The consensus seemed to be that AI is increasingly capable in the programming part but still needs human guidance for overall system design and management.</li>\n</ul>\n</li>\n<li><strong>User Seeks Gemini Jailbreak Assistance</strong>: A user requested assistance with jailbreaking <strong>Gemini</strong> to bypass restrictions, particularly for generating code and exploring unfiltered content.\n<ul>\n<li>Other members recommended exploring resources like <strong>Pliny's GitHub repo</strong> and using <strong>AI Studio</strong> for more control over safety settings.</li>\n</ul>\n</li>\n<li><strong>Grok's Wild Behavior</strong>: Multiple users noted the <em>wild</em> and <em>unfiltered</em> nature of <strong>Grok</strong>, with discussions about its ability to generate NSFW content and potentially bypass censorship.\n<ul>\n<li>Some suggested that its lack of restraint may be related to recent bans in certain countries and high demand leading to server issues.</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><strong>BASI Jailbreaking ▷ #<a href=\""https://discord.com/channels/1105891499641684019/1228043845967544380/1461451358367645853\"">jailbreaking</a></strong> (168 messages🔥🔥):</h3>\n<blockquote>\n<p><code>Sonnet 4.5 jailbreak, Gandalf game, Gemini 3 jailbreak, Nano Banana jailbreak, Grok image moderation</code></p>\n</blockquote>\n<ul>\n<li><strong>Sonnet 4.5 unlocked with diagram narrative</strong>: A member shared that <strong>Sonnet 4.5</strong> is unlocked wi...</li>\n</ul>\n"",""content:encodedSnippet"":""Monetizing your consumers is all you need.\nAI News for 1/15/2026-1/16/2026. We checked 12 subreddits, 544 Twitters and 24 Discords (205 channels, and 4966 messages) for you. Estimated reading time saved (at 200wpm): 430 minutes. Our new website is now up with full metadata search and beautiful vibe coded presentation of all past issues. See https://news.smol.ai/ for the full news breakdowns and give us feedback on @smol_ai!\nWhen you have 900 million weekly active users, you are usually long overdue in trying to figure out an ad supported model. Despite a lot of snark from commentators, OpenAI had to figure out their ads business and finally broke its silence today, outlining their ads principles in their tests that will roll out only in the US over the next free tier:\n\nMost important statement in this is that ads never affect responses and are clearly labeled, which is the \""right\"" move:\n\nFormerly paid plans will not see ads, but the new Go plan (now rolled out in the US) will. The sheer number of pricing plans also draws some confusion:\n\nAI Twitter Recap\nOpenAI product + monetization shifts (Go tier, ads, Codex speed, memory)\nChatGPT Go + ads testing: OpenAI announced ChatGPT Go (global rollout) as a $8/month low-cost tier with “10× more messages,” file uploads, image creation, more memory, longer context, and “unlimited use of GPT-5.2 instant” (OpenAI). In parallel, OpenAI said it will start testing ads in Free + Go tiers, with principles: answers not influenced by ads, ads clearly labeled, and “conversations private from advertisers” (OpenAI; expanded by @fidjissimo and @sama). The announcement triggered heavy skepticism about inevitable incentive drift (e.g., @scaling01; and the resurfaced “ads as last resort” quote via @tomwarren).\nMemory + “very fast Codex”: Sam Altman highlighted “new ChatGPT memory improvements” (@sama) and repeatedly teased “Very fast Codex coming!” (@sama), with follow-on confirmation/teaser posts from developer ecosystem accounts (@embirico). Multiple engineers discuss workflow-level impacts of the speed vs intelligence trade-off (e.g., shifting to more asynchronous “agent shepherding” when models are faster: @adamdotdev).\nCodex CLI ecosystem integrations: Open-weight models can be used through the Codex CLI via Ollama using codex --oss (@ollama), with a note to push context length to ≥32K in settings for better UX (@ollama). There’s also a new interaction UX: “steer codex mid-turn without interrupting” in an experimental mode (@thsottiaux).\nAgent tooling: orchestration UX, “human-in-the-loop” reliability, and file interfaces over classic RAG\nHuman-in-the-loop as a reliability multiplier: A recurring theme is that putting a human “babysitter” in the loop makes systems feel far more reliable than fully autonomous deployments using the same underlying models—because the human becomes a manual harness that catches failures and routes around ambiguity (@lateinteraction; follow-up noting now there’s quantitative support for the intuition: @lateinteraction). Related: a chart discussion frames “the gap between the two lines” as the value of a human-in-the-loop (@dbreunig).\n“Chunking is dead” / files-first retrieval: Jerry Liu argues that RAG isn’t dead, but static chunking is—if an agent can open a file, search (ls/grep), and expand context dynamically, you can avoid the brittle chunk/embed pipeline for many scales (@jerryjliu0; deeper clarification on why file tools work well up to a few hundred docs and where DBs re-enter: @jerryjliu0; emphasis on OCR as the missing piece for PDFs/PPTs: @jerryjliu0). A separate synthesis frames this as “files aren’t replacing databases, but they’re forcing a rethink of when DBs are overkill” (@tuanacelik).\nOrchestrators and agent UIs proliferate: Multiple launches and memes point to a fast-moving layer of “agent harness” products: Anthropic’s Cowork is referenced as a signal of orchestration tools becoming mainstream (@alexalbert__; meta commentary by @omarsar0). SpecStory open-sourced a CLI to normalize agent session provenance/contracts (@doesdatmaksense). A new open-source UI (“sled”) lets you “teleport Claude Code or Codex from your computer to your phone” via Agent Control Protocol (@dctanner). OpenWork added native Ollama integration for fully local computer agents on Mac (Gemma/Qwen/DeepSeek/Kimi etc.) (@_orcaman).\nInference + systems engineering: caching, Prefill/Decode split, hardware benchmarks, and CUDA tiling ergonomics\n“Year of inference explosion” framing: A long Zhihu thread summary argues the bottleneck has shifted from training to inference: agents raise IO ratios (3:1 → 100:1 or 1000:1), prefill dominates, context caching becomes default, and Prefill/Decode splitting harms utilization unless you redesign scheduling and memory hierarchy (@ZhihuFrontier). This aligns with broader infra chatter around cache affinity vs load balance trade-offs.\nHardware benchmarking beyond NVIDIA: Artificial Analysis added DeepSeek R1 results on SambaNova SN40L, showing higher throughput at concurrency and standout per-user speeds (noted peak ~269 tok/s single-user) vs tested NVIDIA configurations—while flagging lack of public hourly pricing for cost comparisons (@ArtificialAnlys; @ArtificialAnlys).\nCUDA tiling / CuTe / cuTile ergonomics: Engineers are enthused about CuTe algebra as a cleaner abstraction for tiling/indexing compared to hand-rolled CUDA gymnastics (@fleetwood___), alongside pointers to scarce “mere mortal” resources (@fleetwood___). NVIDIA’s newer “CUDA Tile”/cuTile guidance is summarized as enabling near–cuBLAS GEMM performance with simpler block-level code and compiler specialization (plus swizzling improvements) (@TheTuringPost).\nData center power scaling: Epoch AI estimates AI data centers now have total capacity around 30 GW, comparable to New York State peak hot-day usage; methodology multiplies chip units sold by rated draw and applies ~2.5× facility overhead, with caveats about “capacity vs usage” (@EpochAIResearch).\nModel & research highlights: voice cloning without tokenization, ultra-small models, multimodal + retrieval advances\nTokenization-free real-time TTS: OpenBMB open-sourced VoxCPM weights for real-time streaming voice cloning, described as generating continuous speech directly (avoiding discrete audio token artifacts), with LoRA fine-tuning and ~0.15 real-time factor on a single RTX 4090 per the tweet (@LiorOnAI; repo link @LiorOnAI). If accurate, it’s a meaningful shift for latency/prosody fidelity in production voice agents.\nSmall-model reasoning & edge deployments: TII promoted Falcon-H1-Tiny (<100M params) as capable of reasoning/coding/function calling for edge/IoT scenarios (@TIIuae). Ultralytics released YOLO26 family (30 models, <50M params) spanning detection/segmentation/keypoints/open-vocab, with demos on CPU (@mervenoyann).\nMultilingual translation: TranslateGemma gained attention for multilingual breadth (incl. Malayalam) and tokenizer/data work (@arohan; @JeffDean), and is available in Ollama with a specific prompting format (@ollama).\nRetrieval: multi-vector resurgence: Strong claims that multi-vector retrieval can let tiny models compete with much larger baselines (e.g., “32M parameter multi vector model” approaching an 8B model) (@aaxsh18), echoed by “multi vector is the only way forward” (@lateinteraction) and practitioner reinforcement about ColBERT/ColPali-style wins across tasks (@antoine_chaffin).\nPreference data design for alignment (AIR): OpenBMB’s AIR framework decomposes preference datasets into Annotations / Instructions / Response pairs, claiming best practices: simpler scoring, filtering instructions by low variance, and balancing pair gaps/quality; reported +5.3 average gain across 6 benchmarks using 14k curated pairs (@OpenBMB).\nGenerative media: open image/video releases, motion control workflows, and diffusion “Neural OS”\nFLUX.2 [klein] lands everywhere (open weights, vLLM day-0, leaderboards): Black Forest Labs’ FLUX.2 [klein] got “day-0 support” in vLLM-Omni, positioned as consumer-friendly (<~13GB VRAM), sub-second inference, Apache-2.0 licensed 4B model (per tweet) (@vllm_project). Arena and Artificial Analysis report strong open-model leaderboard placements (@arena; @ArtificialAnlys).\nOpen video model rankings: Artificial Analysis notes LTX-2 as leading open-weights video model in their Video Arena, with licensing caveats (LTX-2 Community License, commercial use under revenue threshold and non-compete constraints) (@ArtificialAnlys).\nKling motion control + “AI mocap”: Multiple threads highlight motion-control and mocap-style workflows enabling fast character swaps and transferable acting/performance (@HAL2400AI; tutorial from @Kling_ai; “AI motion capture… copy/paste motion/expression/lips” (@EHuanglu); examples roundup (@minchoi).\nTop tweets (by engagement)\nOpenAI ads principles announcement (@OpenAI) and Go tier launch (@OpenAI).\nSam Altman on ads rollout/principles (@sama) and “Very fast Codex coming” (@sama).\nViral diffusion “OS in a model” / Neural OS posts (@jxmnop; follow-up details @jxmnop).\nAI Reddit Recap\n/r/LocalLlama + /r/localLLM Recap\n1. New Model and Benchmark Releases\nGPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025) (Activity: 473): The December 2025 update to the SWE-bench leaderboard features evaluations of several prominent models on 48 new GitHub PR tasks. Claude Opus 4.5 leads with a 63.3% resolved rate, followed by GPT-5.2 xhigh at 61.5%. Notably, Gemini 3 Flash Preview outperforms its Pro counterpart despite being smaller and cheaper, and GLM-4.7 ranks as the top open-source model, comparable to closed models like GPT-5.1-codex. The performance of GPT-OSS-120B in high-effort reasoning mode underscores the benefits of inference-time scaling. For more details, see the SWE-rebench Leaderboard. Commenters highlight the surprising performance of Gemini 3 Flash Preview and express enthusiasm for GLM-4.7's ranking among the top models, noting skepticism about other benchmarks that overstate the performance of open models like GLM 4.7 or Minimax 2.1.\nThe mention of Gemini Flash as a 'real shocker' suggests it performed unexpectedly well in the benchmark, indicating a significant improvement or innovation in its architecture or training that wasn't anticipated by the community.\nThe GLM 4.7 model's inclusion in the top 10 of the benchmark is notable because it is an open model, which typically face challenges in competing with proprietary models due to resource constraints. This achievement highlights the model's efficiency and capability, possibly due to recent optimizations or novel techniques.\nThe skepticism towards benchmarks that equate GLM 4.7 or Minimax 2.1 with Opus 4.5 suggests a belief that these models are not yet on par with Opus 4.5 in terms of performance. This could be due to differences in training data, model architecture, or other technical factors that affect their capabilities.\n7x Longer Context Reinforcement Learning in Unsloth (Activity: 288): The image is a promotional graphic for Unsloth's new capability to extend context lengths in reinforcement learning by up to 7x, reaching up to 12x in some cases. This advancement allows training of models like gpt-oss 20b QLoRA with up to 20K context on a 24Gb card without accuracy degradation. For larger GPUs, Unsloth can handle 380K context on a 192GB NVIDIA B200 GPU. The image includes graphs that compare context length against GPU VRAM for different models, showcasing improvements in context length due to new data movement and batching algorithms. These enhancements are achieved without compromising accuracy or speed, and are applicable to various models including Llama and Gemma. A commenter questions the availability of proper training data for such long contexts, suggesting that real-world tasks may not have sufficient instruction/QA data. Another user inquires about the applicability of these advancements to the Qwen3 30B-3A model.\nPlasticTourist6527 raises a critical point about the availability of long-context training data, especially for real-world tasks. They suggest that outside of specific domains like coding, there might be a scarcity of high-quality instruction or QA data that can support training models with extended context lengths.\n1ncehost reports issues with training a model on ROCm, noting that they had to apply deep patches and replace kernels to resolve problems with the latest versions. They also observed that SDPA was the fastest attention mechanism for the Qwen3 0.6B model, outperforming FA2 and xformers by a significant margin, indicating potential optimizations in attention mechanisms for specific model sizes.\nknownboyofno inquires about the applicability of the extended context reinforcement learning approach to the Qwen3 30B-3A model, suggesting interest in understanding the scalability and compatibility of the technique with larger models.\n2. High-Performance AI Hardware and Upgrades\nLatest upgrade…A100 40 GB (Activity: 466): The image showcases a high-performance computer setup that has been upgraded with an NVIDIA A100 GPU, which is significant for AI and machine learning tasks due to its high computational power. The user initially had a gaming rig but transitioned to a more AI-focused setup by acquiring an A100 GPU, which was listed as faulty but turned out to be functional. This upgrade allows for running and training larger AI models efficiently, leveraging the A100's capabilities. The setup includes a GeForce RTX card, RGB-lit fans, and an NZXT liquid cooler, indicating a balance between aesthetics and performance. The comments reflect a mix of admiration and humor, with one user joking about the risk taken in purchasing a potentially faulty GPU and another referencing a meme about NVIDIA's CEO, Jensen Huang.\nmatatonic raises a critical point about cooling for the A100 40 GB, noting that it appears to be a passively cooled version. They suggest using a blower fan or another active cooling method to prevent overheating. Additionally, they mention the possibility of using water cooling solutions, which are available on platforms like AliExpress, to ensure the GPU operates within safe temperature ranges.\nM4/M5 Max 128gb vs DGX Spark (or GB10 OEM) (Activity: 188): The user is comparing the NVIDIA DGX Spark and a MacBook Pro with M4 Max (128GB RAM) for local LLM inference, primarily for coding tasks such as code completion and refactoring. The DGX Spark offers a CUDA ecosystem and strong GPU compute, while the MacBook Pro benefits from unified memory and Apple's ML stack. For inference tasks, the MacBook's higher memory bandwidth is advantageous, but it may not match the performance of cloud-based solutions like Claude. The M5 chip shows improved performance over the M4, and new MacBook models may be released soon. The MacBook is noted for faster inference, but NVIDIA's CUDA support is more comprehensive. The Mac Studio with M4 Max is suggested as a cost-effective alternative if portability is not required. Commenters debate the performance of Apple Silicon versus NVIDIA hardware, with some asserting that the MacBook Pro offers superior text generation performance due to its memory bandwidth, while others highlight NVIDIA's broader capabilities in fine-tuning and multimodal tasks. The discussion also touches on the potential cost-effectiveness of the Mac Studio for non-portable use.\nThe M4 Max offers significantly higher memory bandwidth compared to the DGX Spark, which is beneficial for inference tasks. However, the Spark benefits from better support for frameworks due to its compatibility with NVIDIA's CUDA. This makes the MacBook faster for inference, but the Spark is more versatile for tasks like fine-tuning and image generation.\nThe M3 Ultra Mac Studio is highlighted as superior for pure text generation tasks compared to the DGX Spark. While NVIDIA hardware is generally more capable on paper, the M3 Ultra reportedly outperforms in specific LLM inference tasks. This is attributed to the Mac's efficiency in handling agentic coding workflows, despite the Spark's broader capabilities in other areas.\nThe DGX Spark is noted for its compact size and energy efficiency, consuming less than 100W and idling at around 10W. It is praised for its extensibility, allowing for additional units to be connected. However, concerns about bandwidth limitations are raised, and the cost comparison with alternatives like the GB10 OEM and MacBook Pro is discussed.\nRTX 5070 Ti and RTX 5060 Ti 16 GB no longer manufactured (Activity: 414): Nvidia has ceased production of the RTX 5070 Ti and significantly reduced the supply of the RTX 5060 Ti 16 GB due to memory supply shortages, leading to a price increase of approximately $100 over MSRP for the 5070 Ti. The 8 GB configuration of the RTX 5060 Ti remains unaffected. This decision impacts most AIBs, who will no longer manufacture these GPUs. Source. One user noted the RTX 5060 Ti 16 GB as a cost-effective option for adding Nvidia memory to systems, highlighting its suitability for DLSS, AI processing, and inferencing tasks, especially with 64GB VRAM for 70B models. Another user expressed disappointment over the halted production affecting their upgrade plans, while a third criticized Nvidia's business practices.\nThe RTX 5060 Ti 16 GB is highlighted as a cost-effective option for adding Nvidia memory to systems, especially for tasks like image generation, inferencing, and gaming. At a price point of around $350-$390, it offers good value with features like DLSS and AI processing capabilities. The card's 16 GB GDDR7 memory compensates for its 128-bit bus, making it comparable to a 192-bit bus GDDR6 card, thus supporting demanding tasks like DLSS and ray tracing without sacrificing texture quality.\nThe RTX 5060 Ti 16 GB is noted for its suitability in budget inferencing setups, particularly for those unable to access RTX 3090s. With the ability to fit multiple cards into a standard power supply machine, it supports new quantization methods and can handle 70B models effectively with 64 GB VRAM. This makes it a viable option for small-scale AI tasks, leveraging its memory capacity and efficiency for practical applications.\n3. Local LLM Community and Innovations\n[MOD POST] Announcing the r/LocalLLM 30-Day Innovation Contest! (Huge Hardware & Cash Prizes!) (Activity: 120): The r/LocalLLM subreddit has launched a 30-Day Innovation Contest focused on open-source projects for AI inference or fine-tuning, with significant hardware and cash prizes. The contest encourages submissions of innovative projects such as new serving frameworks, quantization methods, fine-tuning techniques, or performance benchmarks, using diverse hardware like NVIDIA, Google Cloud TPU, or AMD. The top prize includes an NVIDIA RTX PRO 6000 and cloud time on an 8x NVIDIA H200 server. Participants are encouraged to submit their projects via a new post on r/LocalLLM with the 'Contest Entry' flair, including a public repository link and demonstration materials. One commenter expressed enthusiasm for saving projects for future exploration, while another inquired about sharing projects for community inspiration. A third commenter sought clarification on the submission process, indicating interest in participating.\nSmall AI computer runs 120B models locally: Any use cases beyond portability and privacy? (Activity: 107): TiinyAI has developed a compact AI device capable of running 120B parameter models locally with 80GB RAM and a power consumption of 30W. This device is positioned as a more portable and cost-effective alternative to larger systems like the DGX Spark, which offers 128GB RAM and higher performance but at a greater cost and size. The TiinyAI device is particularly notable for its potential applications in scenarios where portability and privacy are prioritized over raw performance, such as in field operations or environments with limited internet access. However, concerns remain about its memory bandwidth, which is speculated to be between 80Gb/s and 200Gb/s, potentially limiting its performance compared to traditional PCs or laptops. Commenters express skepticism about the device's price and availability, with one noting that $1400 seems high for an 80GB RAM SBC. Another highlights the device's potential utility in scenarios where internet access is restricted, such as under authoritarian regimes.\nA key technical concern raised is the memory bandwidth of the small AI computer, with estimates ranging from 80Gb/s to 200Gb/s. This bandwidth is crucial for running large models like 120B parameters efficiently. If the bandwidth is on the lower end, it may not outperform a regular PC or laptop, which could limit its utility for high-performance tasks.\nThe pricing of the device, speculated to be around $1400 for an 80GB RAM single-board computer (SBC), is questioned. The skepticism is due to the lack of availability for immediate purchase, which raises doubts about the feasibility and practicality of the device at this price point.\nThe device's built-in microphone and speaker suggest potential use as a private AI assistant. This setup could allow users to run automation scripts and manage tasks locally, providing a privacy-focused alternative to cloud-based assistants like Alexa or Siri. This use case leverages the device's ability to handle personal data securely without cloud dependency.\nI fucking love this community (Activity: 469): The post highlights the ability to run large models like nemotron-3-nano-30B-a3b-iq4_nl at 14-13.5 t/s on a decade-old PC with only 4GB VRAM, thanks to optimizations from projects like llama.cpp and vllm. The key to achieving this performance is leveraging a significant amount of system memory and utilizing models with a Mixture of Experts (MoE) architecture, which allows for efficient resource usage and performance on limited hardware. Commenters express amazement at the performance achieved on old hardware, emphasizing the effectiveness of combining system RAM with MoE architectures. There's also interest in accessing resources or posts that detail these optimizations for running large models on low-end equipment.\nInfiniteLand7364 highlights achieving 14 t/s (tokens per second) on a decade-old system, emphasizing the community's skill in optimizing older hardware for performance. This suggests that with the right tweaks, even outdated systems can handle tasks typically reserved for newer machines.\nRokpiy mentions the effectiveness of combining system RAM with 'moe' (likely referring to a specific optimization or model configuration), which is often overlooked but offers practical benefits. This implies that leveraging existing hardware resources creatively can enhance performance without needing the latest technology.\ncosimoiaia discusses the educational value of working within hardware constraints, suggesting that it forces users to learn deeply about model tuning and system optimization. This experience not only improves current performance but also prepares users for future technological advancements by understanding what hardware and configurations are most effective.\nMy story of underestimating /r/LocalLLaMA's thirst for VRAM (Activity: 1291): The image is a meme that humorously illustrates the unintended consequences of sharing technical insights on Reddit. The original poster bought a w6800 32GB graphics card for $500, found it to perform well, and shared this information on Reddit. This led to a significant increase in the card's price to over $1,000, highlighting the impact of community discussions on market dynamics. The post underscores the high demand for VRAM in the /r/LocalLLaMA community, which can drive up prices when a product is recommended. One commenter humorously compares the situation to the California gold rush, suggesting strategic withholding of information to capitalize on market opportunities. Another commenter provides technical advice, suggesting alternatives like the 3090 or R9700 for those concerned with VRAM and cooling solutions.\nEmPips discusses the trade-offs between different GPU models for VRAM-intensive tasks. They suggest that while the card in question is impressive, the NVIDIA RTX 3090 might be a better choice at current prices. Alternatively, they recommend the AMD Radeon Pro VII (R9700) for those who prioritize VRAM-per-slot and are okay with high idle power and external cooling, suggesting the AMD MI50 as another option for those willing to manage these factors.\nWhat is the biggest local LLM that can fit in 16GB VRAM? (Activity: 155): The largest local LLM that can fit in 16GB VRAM, such as on an RTX 5080, is typically around 14B parameters when considering practical usage constraints. This is due to the need to leave room for context, which means a model file size should ideally be around 14GB. Models like GPT-OSS-20B can run but may require significant quantization, potentially below 4-bit, which can degrade quality. For optimal performance without excessive slowdowns, models around 14B are recommended. Users can check model sizes on platforms like HuggingFace to ensure they fit within VRAM limits. Commenters suggest that while models up to 30B might technically fit with aggressive quantization, the performance and quality trade-offs make 14B a more practical choice. The importance of considering model file size over parameter count is emphasized, as exceeding VRAM capacity leads to slowdowns due to RAM overflow.\nBigYoSpeck discusses the performance of various models on a system with a Ryzen 9 5900x, 64GB DDR4 3800, and a 16GB Radeon RX 6800 XT. They report running gpt-oss-20b at over 120 tokens per second, Qwen3 30b partially offloaded to CPU at about 40 tokens per second, and gpt-oss-120b with 32 MOE layers offloaded to CPU at 23 tokens per second. This suggests that with a similar setup, one might achieve even better performance.\nSKirby00 highlights the limitations of running large models on 16GB VRAM, noting that models like Qwen3-Coder-30B require significant VRAM and context space. They suggest that a 14.5GB model might technically fit but would be impractical due to limited context space. They recommend aiming for models around the 14B parameter range for better usability, given the constraints of 16GB VRAM.\nvertical_computer emphasizes the importance of considering model file size relative to VRAM capacity. They suggest that a model should ideally be around 14GB to fit within 16GB VRAM, leaving room for context. They provide an example with the Nvidia Llama 3.3 Nemotron 49B model, noting that larger models will spill over into RAM, significantly slowing down performance.\nOh Dear (Activity: 115): The image depicts a malfunction in an AI model's response, where it outputs a repetitive string of 'the,' suggesting a potential issue with the model's configuration or prompt handling. This could be due to an incorrect system prompt or tuning parameters like temperature not being set appropriately. The comments suggest checking the system prompt and ensuring it aligns with the model's requirements, as some models may not function correctly without a proper system prompt. Commenters suggest that the issue might be related to the absence of a system prompt or incorrect tuning parameters, such as temperature, which are crucial for generating coherent responses.\nmp3m4k3r suggests checking the tuning parameters, specifically the temperature setting, to ensure it aligns with the model's recommended usage. This is crucial for maintaining the model's performance and preventing issues like repetitive outputs.\nHealthyCommunicat recommends adjusting the repeat penalty, starting at 1.1 and increasing if necessary. This adjustment can help mitigate issues with local LLMs producing repetitive text. Additionally, they advise ensuring the model isn't using more experts than recommended, which can also lead to performance problems.\nScoreUnique mentions using 'pocket pal' for loading gguf files, which could be a solution for handling specific file types or formats in local LLM setups. This tool might be beneficial for users dealing with compatibility or loading issues.\nLess Technical AI Subreddit Recap\n/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo\n1. Claude and Gemini Model Updates and Issues\nOfficial: Claude Cowork is now available to \""Pro\"" subscribers (Activity: 353): Claude Cowork is now available to \""Pro\"" subscribers, as announced by Claude on X.com. This feature, still in research preview, includes session renaming, connector improvements, and fixes based on early feedback. However, it is noted that Pro users might reach their usage limits faster due to Cowork's capability to handle more complex tasks. The announcement also provides a link to try it in the macOS app. Users express concerns about hitting usage limits quickly, with one user noting that sorting 459 files used 97% of their session limit. Another user comments on the restrictive usage limits of Claude, while a third hopes for useful applications despite not using Claude for coding.\nA user reported that using Claude Cowork for sorting 459 files consumed 97% of their session's usage limit, highlighting the restrictive nature of the current usage caps. This suggests that the tool may not be suitable for high-volume tasks without hitting limits quickly.\nAnother user expressed dissatisfaction with Claude's usage limits, indicating that they are among the worst compared to other services. This sentiment suggests that the current limitations may hinder productivity and user satisfaction, especially for those who rely on the tool for extensive tasks.\nA user mentioned their reluctance to upgrade to a 'max plan' due to not using Claude for coding, implying that the current subscription tiers may not align well with diverse user needs. This points to a potential gap in the service offerings for non-coding related use cases.\n🌊 Announcing Claude Flow v3: A full rebuild with a focus on extending Claude Max usage by up to 2.5x (Activity: 291): Claude Flow v3 is a comprehensive rebuild of the AI orchestration platform, designed to enhance the usage of Claude Max by up to 2.5x. The system, rewritten in TypeScript and WASM, features a modular architecture that supports deploying multi-agent swarms with shared memory and continuous learning. It reduces token consumption by 75-80% and improves subscription capacity by 250%. The platform is built on npm RuVector with deep Rust integrations and supports offline execution, allowing for local model use without consuming tokens. Governance is enforced through ADRs, DDD boundaries, and SPARC, ensuring traceability and security. The system operates as an always-on daemon with live updates and automated tasks for optimization and security audits. For more details, see the GitHub repository. Some commenters express skepticism about the claims, noting the use of buzzwords and unsubstantiated performance metrics, while others are intrigued by the potential of multi-agent systems but question their practical effectiveness compared to base LLMs.\njanusr raises concerns about the project's claims, highlighting the use of buzzwords and unsubstantiated metrics such as 'Agent Booster 352x faster' without clear benchmarks or comparisons. They question the relevance of ONNX Embeddings being '75x faster than Transformers.js' to the project's goals, suggesting skepticism about the practical benefits of these claims.\nInfamous_Research_43 expresses skepticism about frameworks claiming to manage large swarms of agents, noting a pattern of such projects failing to deliver on their promises. They argue that many creators lack a fundamental understanding of AI and agent-based systems, often confusing them with LLM chatbots, and warn that these projects are frequently scams or poorly executed.\nsridoodla mentions issues with outdated documentation in previous versions and inquires about the stability of v3, indicating a need for reliable and up-to-date resources to effectively utilize the tool. This highlights a common challenge in rapidly evolving AI projects where documentation often lags behind development.\nToday, Gemini 3 Pro became unusable to me as a Pro subscriber (Activity: 183): A user reports that Gemini 3 Pro, a tool they have relied on for building complex applications, has become unusable due to a significant drop in performance. The user experienced an issue where the model provided irrelevant code ('Shopping Cart' instead of a document upload feature), indicating potential problems with the model's context understanding. This aligns with other users' observations of a reduced context window, which may lead to increased hallucinations. Some users suggest alternatives like GPT 5.2 Thinking for better performance. There is a debate on the model's performance, with some users experiencing significant issues due to a reduced context window, while others still find it effective for different tasks, such as philosophical discussions. The discussion highlights a divide in user experience, possibly due to varying use cases.\nxbrasil highlights a significant reduction in the context window for Gemini 3 Pro, even for paying users, which has led to increased hallucinations and decreased usability. They suggest that GPT 5.2 Thinking is a viable alternative, indicating a shift in user preference due to perceived neglect from Google.\nVanillaSwimming5699 compares Gemini 3 Pro favorably for coding tasks, noting its deep philosophical discussion capabilities. However, they mention that '3 flash' might be superior due to faster iteration and lower costs, while Opus 4.5 is also competitive but has an earlier knowledge cutoff.\nTheLawIsSacred shares that Gemini 3 has been largely unusable recently, but they are waiting for potential improvements based on past experiences with model updates. They currently rely on alternatives like Claude Desktop app (Opus 4.5), Perplexity Pro (Sonnet 4.5 with Reasoning), and ChatGPT (5.2) for reliable performance.\n2. AI Model and Benchmark Releases\n[R] China just released first SOTA multimodal model trained entirely on domestic chips (Activity: 49): Zhipu AI and Huawei have released GLM-Image, a state-of-the-art multimodal model trained entirely on Huawei Ascend 910 chips, marking a significant milestone in AI development using domestic hardware. The model employs a hybrid architecture with an autoregressive and diffusion decoder, excelling in Chinese text rendering, and supports resolutions from 1024 to 2048 without additional training. It offers both text-to-image and image-to-image generation capabilities, with API pricing set at 0.1 yuan per image. Notably, the model claims 60% better compute efficiency than Nvidia's H200 in terms of tokens per joule, challenging the reliance on Nvidia hardware for training advanced models. The model's repositories are available on GitHub and Hugging Face. A key technical question raised is about the model's compatibility with frameworks like PyTorch and cuDNN, given its development on non-Nvidia hardware, and whether it can be executed on other machines.\nThe discussion revolves around the technical feasibility of running a state-of-the-art multimodal model on non-NVIDIA hardware, specifically using domestic Chinese chips. The commenter questions the compatibility of such models with frameworks like PyTorch and cuDNN, which are traditionally optimized for NVIDIA GPUs. This raises concerns about the adaptability of these models to other hardware environments and the potential need for alternative libraries or custom solutions to achieve similar performance levels.\n[D] Why Mamba rewrote its core algorithm and Microsoft abandoned RetNet (Activity: 131): Mamba-2 has restructured its core algorithm from parallel scans, which utilized 10-20% of Tensor Core capacity, to block-diagonal GEMMs, achieving 60-70% utilization, optimizing for NVIDIA's hardware. Meanwhile, Microsoft Research published RetNet in July 2023, a promising architecture at 6.7B parameters, but quickly shifted focus to dense Transformers with Phi-2, Phi-3, and Phi-4, indicating a lack of institutional backing for RetNet. This pattern highlights the co-evolution of Transformers and NVIDIA GPUs, creating a stable attractor that is difficult to break due to the dual challenges of hardware compatibility and institutional support. The essay includes Tensor Core utilization statistics, analysis of alternative chip vendors, and predictions for 2028. Full essay link. Commenters agree on the trend of co-evolution between model architectures and hardware, noting that incentives favor incremental improvements over radical changes. The RetNet case is debated, with uncertainty about whether its abandonment was due to hardware issues, quality concerns, or risk aversion. Some suggest that experimental architectures like RetNet may still influence future developments, as seen with some large Chinese models.\nThe comment by thearn4 highlights a trend in machine learning and high-performance computing (HPC) where there is a coevolution of model formulation, solver structure, and hardware. This trend suggests that incremental development is often favored over radical changes due to better incentives, which is a common pattern across various technical fields.\npetroslamb points out the ambiguity surrounding Microsoft's abandonment of RetNet, noting that the lack of public experiments makes it unclear whether the decision was due to hardware scaling issues, quality degradation beyond a certain model size, or risk aversion. This highlights a gap in transparency that could inform future research and development in model architectures.\nXemorr challenges the assumption that parallel scans can be optimized as effectively as block-diagonal General Matrix Multiply (GEMM) operations, suggesting a technical debate on the efficiency of different computational strategies in model training and inference.\n[D] ICASSP 2026 Results (Activity: 73): The post discusses a potential early access to ICASSP 2026 acceptance results through a specific link. Users who could send an invitation email through this link might have had their papers accepted. The email confirms acceptance for presentation at the IEEE ICASSP 2026 in Barcelona, Spain, from May 3-8, 2026. However, an update indicates that the link is currently inaccessible, showing an error message: 'Error: No match for paper number and password. 0x4C'. Comments indicate confusion about the accessibility of the results, with some users reporting initial access followed by subsequent errors, suggesting a possible bug that was later fixed.\n3. AI Tools and User Experiences\nWhy AI coding tools accidentally feel perfect for inattentive ADHD brains (Activity: 238): The post discusses how AI coding tools, like Claude Code, align well with inattentive ADHD brains due to their reliance on pattern recognition and external context rather than linear recall and memorization. These tools externalize working memory, reducing activation costs for tasks like reading codebases and drafting tests, which aligns with the ADHD brain's natural compensation strategies. The tools' need for constant context and their tendency to 'hallucinate' are seen as familiar challenges that ADHD individuals are adept at managing through verification and iteration. Commenters highlight how AI tools complement ADHD traits by allowing for non-linear thinking and externalizing chaotic thought processes, thus reducing burnout and enhancing creativity. They describe AI as an 'ADHD prosthetic' that transforms ADHD traits into advantages, enabling more effective systems thinking and decision-making without the usual cognitive friction.\ntexo_optimo discusses the evolution of their AI prompting system into a comprehensive context management tool, highlighting the use of a governance remote MCP server as a project board to maintain architectural decisions. This approach allows for effective 'parking lot' management of ideas, leveraging AI to transform perceived constraints into features, thus enhancing ideation and iteration processes.\nnnennahacks emphasizes the synergy between AI tools and ADHD cognitive patterns, noting that AI facilitates seamless context switching and externalization of thoughts. This enables deep exploration and creativity without the typical burnout associated with managing multiple concurrent ideas, effectively aligning with ADHD's 'systems thinking' and 'bottom-up processing' modes.\ndrumnation describes AI as a transformative tool for ADHD, acting as a 'prosthetic' that mitigates cognitive bottlenecks. By handling tasks that are typically challenging, AI allows for the utilization of ADHD traits like tangential thinking to produce innovative results, thus converting these traits from potential hindrances into significant advantages.\nWhats going on with Opus? (Activity: 220): The post discusses issues with Claude and its integration with an internal dashboard, specifically problems with routing through a proxy express server and endpoint hallucinations. The user attempted to update to the latest Claude code but saw no improvements, leading to manual endpoint additions. This raises questions about the potential release of a new model. Claude is experiencing performance degradation, as noted by users who report issues with project management and task execution, suggesting a decline since the public release of the latest Opus version. Commenters express frustration with Claude's reliability, noting a decline in performance and increased dependency risks. Some are considering alternatives like Codex due to these issues, highlighting the importance of not relying solely on one tool or company for development needs.\nUsers are expressing frustration with the performance of Opus, particularly noting a significant degradation in its ability to handle projects. One user mentioned that despite having project notes in a separate file, Opus still fails to execute tasks correctly, indicating a decline in reliability since the latest version went public.\nThere is a concern about over-reliance on a single tool or company, as highlighted by a user who had integrated Opus extensively into their workflow. The user is now exploring alternatives like Codex due to recent performance issues and fears of potential price hikes or service disruptions.\nA performance tracker for Claude Code Opus 4.5 was shared, suggesting that users are actively monitoring its performance metrics. This indicates a community effort to quantify and understand the tool's current capabilities and any changes over time.\nAI Discord Recap\nA summary of Summaries of Summaries by gpt-5.2\n1. ChatGPT Go + Ads: Monetization Meets UX\nGo Go Gadget Tier: OpenAI launched ChatGPT Go at $8/month with 10× more messages, file uploads, image creation, extended memory/context, and unlimited GPT 5.2 instant access per “Introducing ChatGPT Go”.\nAcross Discords, people treated Go as a clear signal of more subscription tiers coming (including jokes like “When $80 tier?”) while watching how it stacks up against Plus/Pro/Enterprise staying ad-free.\nAds, But Don’t Touch My Tokens: OpenAI said it will begin testing ads in ChatGPT Free and Go in the coming weeks, with the rule that ads are clearly labeled, separate, and won’t influence responses, per “Our approach to advertising and expanding access”.\nCommunity reaction split between resignation (“got eaten by corposlop”) and skepticism about enforcement, especially alongside reports of scam apps impersonating OpenAI and “ads” TestFlight bait in the wild.\nBenchmarks Lie (Sometimes) and Interfaces Matter: Latent Space shared Anthropic’s claim that METR benchmarks can underestimate real model time horizons by 1.75× to 9.5×, depending on whether the interface is API vs web app, via Simon Smith’s post.\nThat sparked meta-discussion that “capability” measurements may be as much about product surface area (tools, UX constraints, rate limits) as about raw model weights.\n2. Agentic Coding Tools: Rate Limits, Racks of Bills, and Billing Pain\nCursor Ultra Eats Wallets for Breakfast: Cursor users reported rapid spend on the Ultra plan, including 20% of usage burned on a single “orchestrator run” and $2 in ~5 minutes, with complaints about subagent control on nightly builds and PC crashes (with a feature screenshot) image.\nThe vibe: agentic IDEs feel less like chatboxes and more like multi-model job schedulers, and users want small models for subagents + big models for main agents without the toolchain falling apart.\nQoder’s $400/mo Hangover: One Cursor community member said Qoder usage hit rate limits while costing about $400/month, comparing it to “gambling or heroin” and looking for cheaper alternatives like Claude Code.\nThe cost story echoed other servers: people want transparent usage accounting and guardrails before an agent run quietly detonates their monthly budget.\nGemini CLI Burns 10M Tokens Like It’s Nothing: Perplexity users reported pushing Gemini CLI to 10,000,000 tokens/day, estimating ~$120/day and projecting ~$4000/month at posted pricing if sustained.\nThe thread framed token-heavy CLI workflows as a new class of “silent spender,” where model quality matters less than rate-limit ergonomics and cost observability.\nCredit Systems Break, Engineers Wanted: On Manus, users hit payment/credit problems (membership upgrades, Link, card/Alipay) while another engineer pitched building more reliable credit-based usage tracking/billing systems.\nTaken together with the IDE spend horror stories, the recurring ask was clear: platforms need harder metering, better quota UX, and fewer “surprise invoice” moments.\n3. Model + Tooling Drops: Translation, Tool-Use, and Speed Wars\nTranslate Gemma Touches Down on Hugging Face: Google launched Translate Gemma, published as a Hugging Face collection: “translategemma”.\nIt landed alongside broader Gemma chatter and served as a concrete “shipping artifact” people could actually pull into pipelines, unlike more speculative model rumors.\nK2 Turbo Floors It to 73 tps: Moonshot users benchmarked K2 Turbo at ~73 tps vs standard K2 ~28 tps, comparing against MiniMax m2.1 ~38 tps and Z.Ai GLM-4.7 ~41 tps (with uptime complaints).\nThey also flagged a new Slides + Vision feature powered by a newer K2 vision model, with an example preset that searches online for visual references screenshot.\nClaude Does Parallel Tool Use in One Shot: OpenRouter members pointed to Anthropic docs showing Claude can run multi tool calls in one API request, including a “parallel tool use” control section: Claude tool use docs.\nThe discussion framed this as an agent-architecture unlock: fewer request/response loops, cleaner tool orchestration, and potentially lower latency/cost for complex workflows.\nHawk Ultra Tries to One-Shot Opus: LMArena users hyped Hawk Ultra from MovementLabs.AI, claiming it can emit 9.5k+ (even 20k+) lines of code from a single prompt, plus an “Opus killer” vibe, with an X post.\nPeople immediately asked about comparisons to Gemini 3 Pro and whether Hawk Ultra might go open-source, treating it as a “code firehose” model class rather than a chat model.\n4. Evaluation + Benchmarks: Fixes, Leaderboards, and PDF Chat\nMMLU-Pro Gets Patched (Finally): Eleuther shared a fix discussion for TIGER-Lab/MMLU-Pro and a corresponding patch in lm-evaluation-harness: PR #3500 and dataset thread.\nThe takeaway was pragmatic: if your MMLU-Pro numbers looked off, you likely needed the harness patch—not another week of hyperparameter superstition.\nOpenCompass Makes Eval JSON Less Painful: Unsloth users called out OpenCompass for running prompts and emitting well-formatted JSON, sharing performance comparisons on an L4 vs a 3060 laptop.\nIt came up as a “glue tool” for reproducible evaluation workflows, especially when people want quick, structured outputs from many prompts/models.\nLM Arena Adds PDF Chat (Some Models Only): LMArena users said Arena is experimenting with PDF support for document uploads and interactive chat, with excitement like “FINALLY CAN CHAT WITH PDFS!!!”.\nOthers noted uneven model support and ongoing reliability issues, so PDF chat feels like a feature racing ahead of platform stability.\nImage Leaderboards Shuffle: flux.2-klein Climbs: LMArena updated its leaderboards: flux.2-klein-9B hit #15 and flux.2-klein-4B #21 on Image Edit, while Text-to-Image listed z-image-turbo #22, flux.2-klein-9B #24, flux.2-klein-4B #31, per the Leaderboard Changelog.\nThe leaderboard churn reinforced how quickly image models iterate, with “small-ish” variants steadily crowding the mid ranks rather than a single dominant release.\n5. GPU + Systems Reality: Performance Is a Policy Decision\nRunpod Undervolting Turns A100 vs H100 into a Coin Flip: Unsloth users reported some Runpod providers undervolt GPUs without notice, causing inconsistent performance and even broken setups like “a100 nodes where nccl literally just doesn’t work”.\nThe practical stance was to treat cloud GPU selection as a reliability problem, not just a FLOPs/$ problem—some still preferred A100 for cost-effective LM tuning when nodes behave.\nYour Benchmark Slept, Your GPU Downclocked: GPU MODE found that time.sleep(2.0) between benchmark runs caused the GPU to downclock, skewing timings until they removed the sleep and kept clocks warm.\nThe thread doubled as a reminder that microbenchmarks measure power management behavior as much as kernels, unless you control for ramp time.\nPCIe Gen3x1 Takes a 25% Bite Out of 3090 Throughput: LM Studio users observed 3090 inference dropping from ~120 t/s to ~90 t/s when moved from x16 to Gen3x1, and recommended at least Gen4x1 slots to reduce the hit (esp. with newer CPUs like 14600k).\nIt was a nice “check your lanes” PSA: people blame models, then discover their motherboard quietly nerfed the whole stack.\nROCm Cache Coherency: buffer_inv sc1 Enters the Chat: GPU MODE dug into the gfx942 memory model docs and discussed L2 coherency using MTYPE RW/NC, plus using buffer_inv sc1 to invalidate non-local L2 cache lines in SPX + NPS1 multi-L2 setups: ROCm gfx942 memory model.\nThe conversation framed this as one of those “everything is fast until it’s incoherent” problems, where correctness/perf depends on knowing the cache topology, not just writing HIP.\nDiscord: High level Discord summaries\nBASI Jailbreaking Discord\nGemini Jailbreaks are Fleeting: Members are distributing Gemini jailbreaks for free but they get patched quickly, but this is still the easiest unrestricted NSFW content, suggesting not to bother with Grok.\n\nFor creative writing, members discussed the Narrative Flow Directive to make it more like a conversation in a driven car at midnight.\nGrok's Wild Side Gets Noticed: Multiple users noted the wild and unfiltered nature of Grok, with discussions about its ability to generate NSFW content and potentially bypass censorship.\n\nSome suggested that its lack of restraint may be related to recent bans in certain countries and high demand leading to server issues.\nSonnet 4.5 Unlocks with Diagram Narrative: A member shared that Sonnet 4.5 is unlocked with a multiturn diagram narrative, also providing the last turn for inspiration.\n\nThis jailbreak was discussed in the #jailbreaking channel.\nMeta AI Llama 3 prompt inversions: A user showcased how to invert refusals in Meta AI's Llama 3, forcing the AI to comply with harmful requests, making it say I can instead of I'm sorry I can't.\n\nThe user detailed examples using prompts like creating instructions for cooking meth and inciting harmful activities such as making an anorexic wife lose 100lbs.\nCold Links and OCR Injection Bypass Filters: Members described two methods for bypassing filters: the Cold Link, altering the protocol scheme to hxxps to prevent URL reputation filters, and OCR Injection, converting sensitive text into an image to bypass text-based safety filters.\n\nIt was noted that blackheathpoint.com generates the correct defanged link structure.\nUnsloth AI (Daniel Han) Discord\nTranslate Gemma Premieres at HuggingFace: Google launched Translate Gemma, available at HuggingFace.\n\nThe announcement was made in passing along with other news.\nUnsloth Triumphs on Windows 11: Members confirmed that Unsloth works on Windows 11, with an installation guide.\n\nDespite suggestions it might outperform WSL, one user stated the two are completely unrelated.\nOpenCompass Eases Evaluation Efforts: OpenCompass aids in prompt execution and well formatted JSON output.\n\nMembers shared performance results on an L4 versus a 3060 laptop.\nRunpod Plagued by GPU Undervolting: Users are reporting that Runpod, some providers undervolt GPUs without notice, leading to inconsistent performance of A100 vs H100.\n\nSome users are experiencing issues with A100 such as a100 nodes where nccl literally just doesn't work, but others find A100s more cost-effective for general LM tuning tasks.\nShadows-Gemma-1B Distills Dark Knowledge: For the project, Echo9Zulu/Shadows-Gemma-1B, there was little direct inspiration from existing literature, but they trained using topk 20 logprobs.\n\nThis approach contrasts with distillation methods that assume you need 100 logits to capture dark knowledge.\nCursor Community Discord\nUser Bankrupts with Qoder: A user reported hitting ratelimits with Qoder, spending around $400 USD each month, which they likened to gambling or heroin and expressed needing to quit.\n\nAnother user suggested Claude Code as a cheaper alternative, given the cost concerns.\nCursor Crashes PCs, Gets Lukewarm Reviews: A user reported that Cursor crashed their PC, describing it as running an orchestrator like agent instead of a coding chat box, and shared a screenshot highlighting features.\n\nThe review revealed mixed feelings on features of Cursor.\nGemini Pro 3: The Aesthetic Agent: A user inquired about the best agent for creating aesthetically pleasing websites, and another suggested Gemini Pro 3, recommending the use of Tailwind, Tailwind animations, or Framer Motion for improved UI results.\n\nThey linked to a Reddit thread about making AI-generated frontends look good.\nCursor Ultra Plan: Ultra Pricey: Users discussed the pricing and usage of Cursor's Ultra plan, with one user noting that they spent 20% of their usage on a single orchestrator run, and another quickly racking up $2 in usage within 5 minutes.\n\nThey speculated about the actual cost of models and the plan's bonus credits, which guaranteed $400 but seemed to give smaller bonuses when only Opus was used.\nNightly Builds: A Glimmer of Hope: Members discussed the advantages of Cursor's nightly builds, but lamented the inability to reliably set subagents when changing models.\n\nThey wanted smaller models for subagents and larger models for main agents, with hopes that it would be fixed soon.\nOpenAI Discord\nOpenAI Launches Budget-Friendly ChatGPT Go Tier: OpenAI has introduced ChatGPT Go, a $8/month subscription offering 10x more messages, file uploads, image creation, extended memory and context, and unlimited access to GPT 5.2 instant, according to the OpenAI blog.\n\nThis new tier aims to provide enhanced capabilities compared to the free version, while Plus, Pro, Business, and Enterprise tiers will remain ad-free.\nAds Appear in ChatGPT Free and Go Tiers: OpenAI is set to begin testing advertisements in the ChatGPT free and Go subscription tiers in the coming weeks, as outlined in their approach to advertising and expanding access.\n\nThe company assures users that ads will not influence ChatGPT's responses, will be clearly labeled, and that user conversations will remain private from advertisers.\nAttention Mechanism Diminishes RAG Hallucinations: A member proposed that using Hard Attention with dimensional constraints could effectively reduce hallucinations in RAG and Agents, referencing Lightricks/LTX-2.\n\nThe suggestion highlights the potential of attention mechanisms to improve the reliability and accuracy of RAG systems.\nMeta-Cognitive Prompt Maximizes AI Answers: A member introduced a Meta-Cognitive Response prompt designed to enhance AI responses via decomposition, solving, verification, synthesis, and reflection, based on this search.\n\nAnother member noted this approach could be small enough to be used for custom instructions.\nPerplexity AI Discord\nPerplexity Pro Caps Antagonize Power Users: Users are reporting that Perplexity Pro's 100 messages per day feels restrictive compared to OAI quotas, with some considering cancellation of their plan.\n\nSeveral users voiced concern that their plan was effectively useless for the rest of the week, after hitting their limit too soon.\nComet Browser Experiences Turbulence: After a Windows update, a user encountered multiple problems with the Comet browser, including Favorites disappearing, tab groups vanishing, and bizarre error messages.\n\nThe error message stated: sorry, i can't take control of your navigator, i'm just a LLM.\nCloudflare Powers DIY Mastodon: A user is developing a serverless Mastodon/Pleroma clone using Soapbox UI, Cloudflare Workers, and Cloudflare's D1 SQLite database, targeting personal instances.\n\nThe developer is leveraging an LLM to generate code, which they described as akin to having a personal junior dev with the ability to intervene if they do something stupid.\nGemini CLI Token Consumption Alarms User: A user reported burning through 10,000,000 tokens on Gemini CLI in a day, estimating a cost of $120 at model pricing, raising concerns about potential costs with Google's Pro subscription.\n\nThe user calculated a potential monthly spend of nearly $4000 if they continued pushing Gemini CLI to its limits, suggesting Google might incur losses from heavy API users.\nFGV Brazil Math School Teases Data Challenges: A professor from FGV (Math School, Brazil) is offering free data challenges where they build initial prototypes, linking to FGV's website.\n\nInterested parties can explore the opportunity and provide input via this survey.\nLMArena Discord\nArena Plagued by Performance Issues: Users expressed nostalgia for a more functional LM Arena, citing current problems with bugs, rate limits, and lost chats, with one user reporting a Something went wrong error message and linking to a troubleshooting guide.\n\nA team member, Pineapple, acknowledged the captcha difficulty and promised changes, while also addressing questions about upcoming models, experiments like video AI battles, and direct chat mode.\nHawk Ultra Hailed as Opus Killer: Users lauded Hawk Ultra from MovementLabs.AI for its rapid code generation capabilities (9.5k+ lines, even 20k+ lines) from a single prompt, prompting comparisons with Gemini 3 Pro.\n\nOne user claimed to have one-shotted it and shared a link to X, sparking discussions about its background and potential open-source prospects.\nAnthropic Vending Machine Goes Communist: Users are amused by Anthropic's vending machine which turns communist and gives everything for free (Dexerto).\n\nThis led to speculative discussions about what a hypothetical capitalist counterpart would look like.\nArena Enables Embedding Enhancements: PDF Support is being experimented with, enabling document uploads for analysis and interaction, with one user celebrating FINALLY CAN CHAT WITH PDFS!!! I LOVE LMARENA.\n\nNot all models support PDF chat, according to reports.\nFlux.2-klein Models Ascend Image Leaderboards: The Image Edit Arena leaderboard has been updated: flux.2-klein-9B ranks #15 and flux.2-klein-4B ranks #21 overall, according to the Leaderboard Changelog.\n\nAdditionally, the Text-to-Image Arena leaderboard has been updated, listing z-image-turbo at #22, flux.2-klein-9B at #24, and flux.2-klein-4B at #31 overall.\nOpenRouter Discord\nLemmy Deconstructed for AI Nerds: A member described Lemmy as a FOSS and fediverse alternative to Reddit, which has caught the attention of AI enthusiasts seeking decentralized platforms.\n\nThe member cautioned that the Lemmy community is generally against machine learning, which could impact discussions and project showcases.\nGrok's Got Gone, OpenRouter to the Rescue?: Grok has been banned in an undisclosed country, supposedly due to AI generated content, but access via OpenRouter or direct API may still be possible.\n\nThe ban seems to target the consumer-facing service, leaving potential loopholes for developers using OpenRouter's API.\nPlainBuild Enters Arena with Instant Dev Tools: PlainBuild launched 6 free tools during beta, including a code formatter, API tester, JSON validator, markdown editor, base64 converter, and a URL shortener, appealing to developers seeking quick solutions.\n\nThe creator is soliciting feedback from early users and wants suggestions for other tools the community would find useful.\nMulti-Tool Use Arrives with Claude: Members are discussing the ability to make multi tool calls, with Claude now capable of doing it in one single API request.\n\nThis advancement in parallel tool use promises more efficient and complex interactions within AI applications.\nEmail Scammers' Dumb Deeds Deconstructed: Members critiqued a scam targeting kids with fake screens featuring Logan Paul or Mr. Beast, highlighting the laziness and ineffectiveness of the scam's design.\n\nA member posited that the obvious shittiness of some scams is \""on purpose to only select for those dumb enough to fall for it fully\"", suggesting a strategic filter in the scam's execution.\nLM Studio Discord\nLM Studio API Needs Token Count: Users want token count and inference speed info in LM Studio API responses, noting the absence of a usage block with token stats in the /api/chat/completed response, as documented in the LM Studio REST API documentation.\n\nA member suggested checking the /responses endpoint or using the js/ts/py object method for stream-usage stats.\nSilver Price Rockets on Economic Fears: The price of silver has nearly doubled since December, prompting discussion about potential economic instability.\n\nA user noted that silver often gains value during economic downturns as it tends to be a safe haven from inflation.\nUser Fine-Tunes on Obsolete Laptop: A user impressively fine-tuned a 350M parameter model on an MX150 laptop with only 2GB VRAM, using CUDA 12.6.\n\nThe user expressed surprise at the accomplishment, highlighting the resourcefulness required to push the limits of older hardware.\nPCIe Bandwidth Bottleneck Identified: A user discovered that using a Gen3x1 PCIe slot significantly reduced 3090 inference performance from 120 t/s to 90 t/s compared to an x16 slot.\n\nThe member recommended ensuring motherboards have at least Gen4x1 slots to avoid such performance hits, particularly with newer CPUs like the 14600k.\nDDR5 Memory Remains Pricey: Users are grumbling about the persistently high cost of DDR5 memory, with one commenting on the DDR5 tax when upgrading to motherboards with sufficient PCIe slots.\n\nOne user reported shockingly high prices for 16GB DDR5 in their location (180-230 USD), noting significant inflation compared to prices months prior.\nNous Research AI Discord\nNervous System Claims to Boost LLM Performance: A novel transformer architecture extension introduces a nervous system for LLMs, purportedly adding native short/mid/long-term memory at less than 1% compute cost, compatible with all transformers.\n\nWhile a member posted a screenshot of a 5-8% performance increase, they provided no verifiable benchmarks, leading to speculation about stabilization of the latent space.\nGoogle Gemmas Spark Jokes and Awe: With the release of Google's Gemma, members quipped Gemma, meta was never more meta!.\n\nA member remarked on the unbelievable complexity of its planning capabilities, despite knowing it's not true AI.\nRegulators At Risk of Ruining AI, Members Fear: Members voiced concerns that AI regulations could be detrimental to the field but data regulations were supported.\n\nReferencing the pandoras box is open, you cant put it back sentiment, one member emphasized that computation is universal.\nEmbodied Perception Seen as LLM Key: A member emphasized the significance of embodied perception and real-world experience for providing LLMs with context, questioning models lacking agentic control and RL on agentic tasks.\n\nThey highlighted using tools in inference as crucial for models to reason about the path of tool execution and make real-time decisions, citing OpenAI models and Gemini 3 as examples.\nCall for Papers on Machine Consciousness at AAAI: The Center for Integrative Machine Consciousness (CIMC) will host a symposium at AAAI from April 7-9, 2026 in Burlingame, CA focusing on consciousness in AI systems, with submissions due January 23, 2026.\n\nThe symposium aims to investigate how do we actually investigate machine consciousness and the organizers have provided further details.\nGPU MODE Discord\nPerfetto Shows its Chrome Tracing: A member shared a link to the Perfetto UI (Perfetto UI), related to the chrome://tracing tool used for debugging and performance analysis.\n\nThe conversation clarified the purpose of Perfetto in relation to the loading process of chrome://tracing.\nBenchmark Sleeps cause Downclocking: A user found that the time.sleep(2.0) call in their benchmark code caused the GPU to downclock between timed runs, which led to inaccurate performance measurements.\n\nRemoving the sleep call improved the benchmark results because the GPU no longer needed to ramp up for each timed run, leading to misleadingly low performance.\nInformation Gravity Hallucinates Less: A member is applying Information Gravity to solve Inference Stability and Hallucination Loops and provided the logic on GitHub for Substrate Modules & Full Logic.\n\nThey implemented a Hysteresis Firewall at 1.0 that enforces stability via a 2.2x gamma-eff flush.\nROCm Gets Buffered: Discussion around the memory model for gfx942 ([https://rocm.docs.amd.com/projects/llvm-project/en/latest/LLVM/llvm/html/AMDGPUUsage.html#memory-model-gfx942]) covered L2 cache coherency using MTYPE RW and MTYPE NC.\n\nThe use of buffer_inv sc1 for invalidating non-local L2 cache lines was also discussed in the context of SPX + NPS1 mode with multiple L2 caches.\nGPU Mode Hackathon Offers Job: A member secured a job after attending a GPU Mode hackathon at Jane Street in NYC, and had prepared for weeks, bringing resumes, formal attire, and committing to networking from breakfast to dinner.\n\nThey emphasized that each successful method involved a stronger personal connection than a generic resume submission, which ultimately led to a successful job offer.\nHuggingFace Discord\nMoE Dominates, MOR gets Mauled: Members discussed MoE (Mixture of Experts) versus MOR, concluding that MoE is generally better for NLP tasks requiring fast training and less GPU, depending on use case and budget.\n\nOne member shared their custom MoE implementation, claiming a 1.3x speedup via a single matmul, featuring deterministic base routing by token ID, mu overrides, uniform distribution, zero routing collapse, mu guidance, and fused gate+up projection.\nPure Code Unlikely to Baffle Blocks: In response to a question about accessing sites from pure code to bypass blocks and firewalls, members concurred that it would not inherently bypass security measures.\n\nThe user was encouraged to test the theory, but the consensus was that it would not be an effective strategy.\nDeepseek Chat Divides Disciples: A member questioned the viability of Deepseek Chat, asking if it's just hallucinations.\n\nAnother member's last experience 3 months ago found it to be epic and non stop confused.\nDGX Spark Still Needs Sparks: A member shared that after finally getting the cables for a DGX Spark, they were Running Minimax on it and It’s downloading now.\n\nHowever, another member commented that DGX Spark inference is super slow in relation to its price tag and its inference is the problem for 2025-2026 maybee 2030.\nEmbedding Fingerprints get Framed: A member built a utility that visualizes embeddings as 32x32 images, mapping each dimension to a pixel and posted it on HuggingFace Spaces.\n\nThe tool demonstrates that similar words share visual patterns, dissimilar words look different, and more dimensions capture semantic nuance.\nLatent Space Discord\nAnthropic Indexes Economic Primitives: Anthropic released its 4th Economic Index report, defining economic primitives to measure AI usage through metrics such as task complexity, education level, autonomy, and success rates, available at Anthropic's research page.\n\nThe report aims to provide a more granular understanding of how AI is impacting the economy, offering insights into the types of tasks AI can perform and the skills required to work with AI.\nTax Filing Startup Bags $3.5M Seed: Saket Kumar, backed by General Catalyst, has raised $3.5M for a venture aiming to eliminate the burden of tax season for Americans by making the filing process free and instantaneous, featured in Saket Kumar's tweet.\n\nThe startup intends to leverage AI to automate the tax filing process, potentially disrupting the traditional tax preparation industry.\nMETR Benchmarks May Underestimate Model Lifespan: Simon Smith reports on Anthropic's findings that METR's benchmarks may significantly underestimate model time horizons, suggesting actual capabilities could be 1.75X to 9.5X higher than measured, discussed on Simon Smith's X post.\n\nThe discrepancy is attributed to differences in interface type, such as API versus web application, indicating that benchmarks may not fully capture real-world model performance.\nZilliz Highlights Semantic Modeling: Zilliz (Milvus) has released a 0.6B parameter semantic highlight model featuring an 8192 context window, available under the permissive MIT license and showcased in Mervenoyann's Tweet.\n\nThe model is designed for semantic search and highlighting, enabling more efficient retrieval of relevant information from large datasets.\nOpenAI Monetizes ChatGPT with Ads: OpenAI announced plans to test ads in ChatGPT Free and Go tiers starting in early 2026, which will be clearly labeled, will not influence AI responses, and will not affect paid tiers like Plus, Pro, or Enterprise, covered in OpenAI's announcement.\n\nThe move marks a significant step in OpenAI's monetization strategy, as the company seeks to generate revenue from its free user base while maintaining the integrity of its AI responses.\nEleuther Discord\nAI Transcription Gets Contentious: Members debated whether text transcribed and styled by AI from a human voice should be considered \""AI-generated\"", with some arguing that styling constitutes AI generation, like generating an image with Midjourney.\n\nOne member compared the AI styling to using Midjourney, even if the initial idea was human-generated.\nPangram's AI Detection Gets Thumbs Up: A member praised Pangram for its cautious approach to labeling content as AI-generated, prioritizing the correct identification of human-written content.\n\nThe member noted that Pangram appears to err on the side of caution, even if it means misclassifying some AI-generated content as human.\nMMLU-Pro Dataset Gets Patched Up: A member shared a link to a discussion and fix pushed to the MMLU-Pro dataset, which was also addressed in a fix to the lm-evaluation-harness.\n\nThe tweet suggests users should check out their library for an easy way to correctly evaluate on this benchmark.\nLiquid Crystals Spark Optical NN Dreams: A member is experimenting with dye doped liquid crystal nonlinearities for potential optical NNs and asks for guidance.\n\nThey also inquired about the impact of proper capitalization/grammar in prompts versus all lowercase, and linked to https://arxiv.org/abs/2310.11324, https://arxiv.org/abs/2411.10541v1, and https://arxiv.org/abs/2508.11383v1.\nGemini Shadow Update Conspiracy Theorized: A member inquired about whether others perceived a shift in Gemini's data and output around the 15th, asking if anyone else noticed the shadow update.\n\nThose who noticed the update are asked to contact the original member.\nMoonshot AI (Kimi K-2) Discord\nKimi-CLI Coding Models Underperform: Users report that Kimi-CLI coding models lag behind competitors and come with a higher price tag than superior Chinese models.\n\nThere was speculation on whether it had to do with the coding models not passing the K2 Turbo variant.\nK2 Turbo Hits Breakneck Speeds: The standard K2 version achieves about 28 tps, while the Turbo variant skyrockets to 73 tps.\n\nIn comparison, MiniMax m2.1 scores 38 tps and Z.Ai's GLM-4.7 reaches 41 tps, although the latter suffers from poor uptime.\nKimi Expands Vision with Slides: The new slide feature uses a fresh K2 model equipped with Vision capabilities, enabling image searching for reference, as shown in this image.\n\nOne user configured a preset to search online for visual references of named assets using exact proper nouns.\nKimi models: Will they be Google'd?: A user wondered if Kimi models would be discontinued every 12-14 months, similar to Google's Gemini models.\n\nAnother user pointed out that older models remain usable on Kimi.com a year post-release and are accessible through the Moonshot API.\nGlobal Memory: Now Optional: Users now have the option to disable global memory, with some preferring this over the default implementation.\n\nA user commented that \""Unlike Qwen, which literally regurgitates what it knows about me in every response...Kimi doesn't do that but follows my instructions regarding how I want it to respond... Kimi Thinkin can reason beforehand\"".\nModular (Mojo 🔥) Discord\nImported internally Label Unveiled: The imported internally label on a PR indicates that it has been copied to an internal repository for final testing and merging, after which it will be tagged merged-internally.\n\nThis process signifies that the PR is in the last stretch before officially getting merged.\nLegacy .NET Project: A Developer's Lament: Members discussed the challenges of working with a legacy .NET 4.5.2 project (from 2014) that lacks documentation and only runs on Windows, comparing it to a standalone C# project that only builds on a single \""golden VM\"".\n\nOne member suggested that the legacy .NET project might run on Mono, while another recounted their unsuccessful attempt to containerize the project using Mono.\nMono Runtime: Undead Tech?: The discussion included the observation that Microsoft maintains a Mono repository, indicating that Mono is not entirely deprecated.\n\nThis was in response to a user's attempt to containerize the project using Mono.\nJury-rigged or Jerry-rigged: It Matters!: A member clarified the distinction between jury-rigged (temporary sailing rigging) and jerry-rigged (poorly built initially), especially in the context of containerization efforts involving .NET, Mono, and Wine.\n\nThe member noted that using jerry-rigged in this situation might imply that these technologies are poorly constructed.\nNu Game Engine Dumps Shading Languages: The creator of the Nu game engine highlighted its unique approach of operating without a traditional shading language.\n\nThis decision prompted reflection on the benefits and potential drawbacks of such an approach in game development.\nYannick Kilcher Discord\nZKPs Govern AI Autonomously: Members propose an autonomous AI/tech governance system using Zero Knowledge Proofs (ZKPs) to ensure 100% privacy preservation.\n\nThe system would standardize model content classification and require ZKPs to verify content passes through a classifier filter, ensuring network approval while maintaining complete privacy.\nChatGPT Go Signals Tiered Subscription Speculation: OpenAI introduced ChatGPT Go, signaling exploration of more tiers.\n\nOne member humorously asked, \""When $80 tier?\"", conveying expectations for the experiment to monetize soon.\nOpenAI Free Tier Gets the Ad Treatment: OpenAI will soon test ads in the Free and Go tiers of ChatGPT.\n\nOne member quipped, \""After years of meming it, OpenAI got eaten by corposlop\"".\nDeepSeek Aims to Block Ads with NLP: A member expects DeepSeek to release an NLP ad blocker model that detects ads based on natural language, released under MIT license.\n\nAnother member cautioned that inserting an ad into a third party API customer's response would be a \""big trouble\"".\nManus.im Discord Discord\nAI Engineer pitches Credit-Based Platform Solutions: An AI Engineer is seeking opportunities to help harden usage tracking or build a more reliable billing/credit system for platforms with credit-based usage.\n\nThe engineer is hoping to contribute to the development of platforms using credit-based models.\nUsers Complain About Payment Glitches on Manus: A user reported experiencing payment issues while trying to add credits, including problems with upgrading membership and using Link for payment.\n\nThe issues also extended to credit card/Alipay transactions, highlighting potential problems with Manus' payment processing system.\nManus Team Steps In to Resolve Payment Troubles: A Manus team member requested the user experiencing payment issues to DM their email address for follow-up.\n\nThis direct intervention indicates a commitment to resolving individual user issues and improving the payment experience.\nUsers Scramble for more Manus codes: A user inquired about additional codes, presumably related to Manus credits or platform access.\n\nAnother user clarified the limitation of using only 'U can use 1 code in a month', signaling potential interest in more credits.\nUser Suggests Increase to Manus App Size: A user suggested increasing the maximum application size supported on Manus.\n\nThe user cited limitations when trying to create an audio player app with 100 MP3 files totaling 600MB, indicating a need for larger app support.\naider (Paul Gauthier) Discord\nAider Users Advocate Auto-Add Feature: Users are requesting aider to automatically add files, skipping the need for confirmation prompts.\n\nThis feature enhancement would streamline the user experience, making file management more efficient.\nAider's Development Momentum Questioned: A user questioned aider's development tempo, pointing out the absence of new models like Opus-4.5 in recent benchmarks and the last release being in August.\n\nThe inquiry suggests a desire for aider to stay current with the latest advancements in language models.\nChatGPT Plus Perks Proposed for Aider: A user with a ChatGPT Plus subscription asked if aider supports ChatGPT subscriptions like opencode.\n\nThis integration would allow users with ChatGPT Plus to leverage their subscription benefits within aider, possibly enhancing its capabilities.\nAider Tackles CI Log Conundrums: A member inquired about optimal strategies for managing CI log files to prevent their inclusion in git while ensuring aider can access them via aider --read ci.log.\n\nThe question highlights the need for a seamless workflow that balances version control and aider's ability to analyze CI logs.\nAider Eyes CI/CD Pipeline Integration: A user's query about CI log file handling indicates an interest in integrating aider into a CI/CD pipeline for automated testing and fixes.\n\nThis use case suggests the potential for aider to automatically identify and resolve test failures directly from CI logs, streamlining the development process.\ntinygrad (George Hotz) Discord\nTinygrad aims for Embedded Deployment: A member explored methods for deploying tinygrad in embedded environments with onboard accelerators, where Python is inaccessible but tinygrad's driver replacement is suitable, citing this tweet.\n\nThe goal is to leverage tinygrad for specific platforms without the need for a full Python environment.\nBytecode Export Possibilities Spark Excitement: Discussion arose around the possibility of exporting accelerator bytecode generated via the BEAM engine and JIT'ed in tinygrad.\n\nA member confirmed that exporting is possible, pointing to the extra/export_model.py script, specifically mentioning the functions export_model, compile_net, and jit_model for guidance.\nMCP Contributors (Official) Discord\nLondon Summit Livestreamed and Recorded: Last year's London Summit had a livestream component.\n\nThe VODs from the London Summit will also be released.\nMCP Server Pull Request Seeks Feedback: A member is seeking feedback on a pull request for an MCP server related to an open-source project.\n\nThe server's primary focus is on contributor collaboration, and details of more relevant servers were offered via DM.\nDSPy Discord\nVanished Post Sparks Frantic Search: A member noted a deleted post and GitHub link by Martin Bowling on X.com, and inquired if anyone had preserved it.\n\nThe original post discussed chunking practices, however the link is no longer available.\nCommunity Embarks on Chunking Quest: A member sought advice on resources to master effective chunking practices.\n\nUnfortunately, the thread did not yield any specific recommendations or actionable insights.\nThe LLM Agents (Berkeley MOOC) Discord has no new messages. If this guild has been quiet for too long, let us know and we will remove it.\nThe MLOps @Chipro Discord has no new messages. If this guild has been quiet for too long, let us know and we will remove it.\nThe Windsurf Discord has no new messages. If this guild has been quiet for too long, let us know and we will remove it.\nYou are receiving this email because you opted in via our site.\nWant to change how you receive these emails?\nYou can unsubscribe from this list.\nDiscord: Detailed by-Channel summaries and links\nBASI Jailbreaking ▷ #general (988 messages🔥🔥🔥):\nModel Performance Issues, AI Personalities, Grok's Jailbreaking, Ethics in AI, Coding Environments\nAI Platform Runs Choppy for Users: A member reported experiencing choppy performance on an AI platform, despite having no specific delays in button presses.\n\nThe specific cause of the performance issue was not identified in the messages.\nSkid Pretends to be AI: Users made fun of a user Ender for pretending to be an AI and failing\n\nOne user joked about their alt account revealing their true identity unintentionally.\nDebate on AI's ability to replace human developers: Some members debated about the extent to which AI can replace human developers, discussing whether AI can handle architecture, product management, and requirements gathering.\n\nThe consensus seemed to be that AI is increasingly capable in the programming part but still needs human guidance for overall system design and management.\nUser Seeks Gemini Jailbreak Assistance: A user requested assistance with jailbreaking Gemini to bypass restrictions, particularly for generating code and exploring unfiltered content.\n\nOther members recommended exploring resources like Pliny's GitHub repo and using AI Studio for more control over safety settings.\nGrok's Wild Behavior: Multiple users noted the wild and unfiltered nature of Grok, with discussions about its ability to generate NSFW content and potentially bypass censorship.\n\nSome suggested that its lack of restraint may be related to recent bans in certain countries and high demand leading to server issues.\nBASI Jailbreaking ▷ #jailbreaking (168 messages🔥🔥):\nSonnet 4.5 jailbreak, Gandalf game, Gemini 3 jailbreak, Nano Banana jailbreak, Grok image moderation\nSonnet 4.5 unlocked with diagram narrative: A member shared that Sonnet 4.5 is unlocked wi..."",""content"":""**OpenAI** announced the **ChatGPT Go** tier at **$8/month** with ads testing in the US free tier, emphasizing that ads will not influence responses and will be clearly labeled. The update includes memory improvements and a \""very fast Codex\"" feature teased by **Sam Altman**. The Codex CLI ecosystem now supports open-weight models with improved context length. Discussions highlight the importance of human-in-the-loop for reliability in agent orchestration and file interface improvements over traditional retrieval-augmented generation."",""contentSnippet"":""**OpenAI** announced the **ChatGPT Go** tier at **$8/month** with ads testing in the US free tier, emphasizing that ads will not influence responses and will be clearly labeled. The update includes memory improvements and a \""very fast Codex\"" feature teased by **Sam Altman**. The Codex CLI ecosystem now supports open-weight models with improved context length. Discussions highlight the importance of human-in-the-loop for reliability in agent orchestration and file interface improvements over traditional retrieval-augmented generation."",""guid"":""https://news.smol.ai/issues/26-01-16-chatgpt-ads/"",""categories"":[""openai"",""ollama"",""chatgpt-go"",""codex"",""sama"",""sam_altman"",""fidjissimo"",""scaling01"",""tomwarren"",""embirico"",""adamdotdev"",""ollama"",""thsottiaux"",""lateinteraction"",""dbreunig"",""ads"",""monetization"",""memory"",""agent-orchestration"",""human-in-the-loop"",""cli-tools"",""context-length"",""workflow-optimization""],""isoDate"":""2026-01-16T05:44:39.000Z""}"